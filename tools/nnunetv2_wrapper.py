"""
Regenerated Google-style docstrings for module 'nnunetv2'.
README source: others/readme/nnunetv2/readme.md
Generated at: 2025-12-02T00:31:07.926255Z

Total functions: 20
"""


import numpy
import torch
from typing import List

################################################################################
# Source: nnunetv2.dataset_conversion.Dataset042_BraTS18.convert_folder_with_preds_back_to_BraTS_labeling_convention
# File: nnunetv2/dataset_conversion/Dataset042_BraTS18.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for convert_folder_with_preds_back_to_BraTS_labeling_convention because the docstring has no description for the argument 'input_folder'
################################################################################

def nnunetv2_dataset_conversion_Dataset042_BraTS18_convert_folder_with_preds_back_to_BraTS_labeling_convention(
    input_folder: str,
    output_folder: str,
    num_processes: int = 12
):
    """Converts all nifti prediction files in a folder to the BraTS (BraTS18) labeling convention and writes the converted nifti files to an output folder.
    
    This function is used in the nnU-Net dataset conversion utilities to take model prediction outputs (nifti files, typically generated by nnU-Net inference) and transform their label encoding back to the labeling convention required by the Brain Tumor Segmentation (BraTS) 2018 challenge. It locates all files in input_folder with the suffix ".nii.gz", ensures output_folder exists, and then runs the conversion in parallel using a multiprocessing pool. The actual per-file conversion is delegated to load_convert_labels_back_to_BraTS; filenames (basenames) discovered by subfiles (join=False) are passed together with input_folder and output_folder to that worker function. This function enables preparing predicted segmentations for evaluation or submission to BraTS-style benchmarks and for downstream analyses that expect BraTS label semantics.
    
    Args:
        input_folder (str): Path to the directory that contains predicted segmentation files in NIfTI format with the suffix ".nii.gz". The function uses subfiles(input_folder, suffix='.nii.gz', join=False) to enumerate basenames of prediction files. This folder must be readable; if it does not contain any ".nii.gz" files, no conversions will be performed and no output files will be written.
        output_folder (str): Path to the directory where converted NIfTI files (with BraTS labeling convention) will be written. The function calls maybe_mkdir_p(output_folder) so the directory will be created if it does not already exist. Converted files are written as side effects; filenames are preserved at the basename level (i.e., the same base filename as found in input_folder is used by the worker function when writing into output_folder).
        num_processes (int): Number of worker processes to spawn for parallel conversion (default: 12). This value is passed to multiprocessing.get_context("spawn").Pool(num_processes) to create a pool using the "spawn" start method. A larger value can speed up batch conversions on multi-core machines but increases concurrent memory and I/O pressure; setting this too high may lead to high CPU/IO contention or out-of-memory errors depending on system resources.
    
    Returns:
        None: This function does not return a value. Its primary effect is the side effect of creating output_folder (if needed) and writing converted NIfTI files into output_folder. Errors during per-file conversion (for example due to unreadable/corrupted NIfTI files, missing permissions, or exceptions raised inside load_convert_labels_back_to_BraTS) will propagate and may terminate the multiprocessing pool, possibly leaving a partial set of converted files in output_folder. The order of processing is not guaranteed due to parallel execution.
    """
    from nnunetv2.dataset_conversion.Dataset042_BraTS18 import convert_folder_with_preds_back_to_BraTS_labeling_convention
    return convert_folder_with_preds_back_to_BraTS_labeling_convention(
        input_folder,
        output_folder,
        num_processes
    )


################################################################################
# Source: nnunetv2.dataset_conversion.Dataset043_BraTS19.convert_folder_with_preds_back_to_BraTS_labeling_convention
# File: nnunetv2/dataset_conversion/Dataset043_BraTS19.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for convert_folder_with_preds_back_to_BraTS_labeling_convention because the docstring has no description for the argument 'input_folder'
################################################################################

def nnunetv2_dataset_conversion_Dataset043_BraTS19_convert_folder_with_preds_back_to_BraTS_labeling_convention(
    input_folder: str,
    output_folder: str,
    num_processes: int = 12
):
    """nnunetv2.dataset_conversion.Dataset043_BraTS19.convert_folder_with_preds_back_to_BraTS_labeling_convention converts all prediction NIfTI files in a folder to the BraTS labeling convention and writes the converted files to a target folder.
    
    This function is used in the nnU-Net dataset conversion utilities to postprocess model prediction outputs for the BraTS (Brain Tumor Segmentation) domain so that labels follow the BraTS challenge / dataset labeling convention expected by downstream evaluation and analysis. It reads every file with the suffix ".nii.gz" found in input_folder, invokes the module-level helper load_convert_labels_back_to_BraTS on each file to perform the label mapping back to the BraTS convention, and saves the converted NIfTI files into output_folder. The function creates output_folder if it does not exist, and it processes files in parallel using Python's multiprocessing (spawn start method) to improve throughput for large prediction sets.
    
    Args:
        input_folder (str): Path to the directory containing prediction files in NIfTI format (.nii.gz). The function searches this directory for files whose names end with ".nii.gz" (using subfiles with join=False) and passes each basename together with input_folder to the conversion helper. This folder must be readable by the process invoking this function.
        output_folder (str): Path to the directory where converted NIfTI files will be written. The function will create this directory if it does not already exist (via maybe_mkdir_p). Converted files are written using the same filename basenames that were found in input_folder.
        num_processes (int): Number of worker processes to spawn for parallel conversion. This is passed to multiprocessing.get_context("spawn").Pool and defaults to 12. Must be a positive integer; using more processes may increase throughput but also increases CPU and memory usage and the number of simultaneous file handles.
    
    Returns:
        None: This function does not return a value. Its effect is purely side-effecting: it creates output_folder if necessary and writes converted NIfTI files into that folder. If no ".nii.gz" files are found in input_folder, no files are written and the function returns after creating the output folder (if it did not exist).
    
    Failure modes and notes:
        The function will raise exceptions originating from filesystem operations (e.g., permission errors when reading input_folder or writing to output_folder) and from the conversion helper load_convert_labels_back_to_BraTS (e.g., if a NIfTI file is unreadable or does not contain expected label values). Multiprocessing-related errors (worker crashes, serialization errors) will propagate as exceptions from the Pool. The caller should ensure input_folder exists and contains valid ".nii.gz" prediction files and that output_folder is writable. The function uses the "spawn" start method for multiprocessing to improve cross-platform reliability (important for some environments); this increases process isolation compared to fork-based methods.
    """
    from nnunetv2.dataset_conversion.Dataset043_BraTS19 import convert_folder_with_preds_back_to_BraTS_labeling_convention
    return convert_folder_with_preds_back_to_BraTS_labeling_convention(
        input_folder,
        output_folder,
        num_processes
    )


################################################################################
# Source: nnunetv2.dataset_conversion.Dataset137_BraTS21.convert_folder_with_preds_back_to_BraTS_labeling_convention
# File: nnunetv2/dataset_conversion/Dataset137_BraTS21.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for convert_folder_with_preds_back_to_BraTS_labeling_convention because the docstring has no description for the argument 'input_folder'
################################################################################

def nnunetv2_dataset_conversion_Dataset137_BraTS21_convert_folder_with_preds_back_to_BraTS_labeling_convention(
    input_folder: str,
    output_folder: str,
    num_processes: int = 12
):
    """Convert all NIfTI prediction files in input_folder to the BraTS21 labeling convention and save the converted files to output_folder.
    
    This function is part of the dataset conversion utilities for the nnU-Net V2 project and is used when working with the BraTS (Brain Tumor Segmentation) dataset variant BraTS21. In practical use, nnU-Net prediction outputs (segmentation label volumes in NIfTI format, .nii.gz) are sometimes produced with internal label ids that differ from the BraTS challenge labeling convention. This function automates reading those prediction files, converting their label ids back to the convention expected by BraTS21 (so that downstream evaluation or submission to BraTS21-compatible tools is correct), and writing the converted NIfTI files to a specified output directory. The conversion work for each file is delegated to load_convert_labels_back_to_BraTS in parallel worker processes.
    
    Args:
        input_folder (str): Path to the folder containing nnU-Net prediction files in NIfTI format with suffix ".nii.gz". This is the source directory from which file names are discovered (using subfiles with join=False). The function expects the files to be readable NIfTI prediction volumes that can be consumed by load_convert_labels_back_to_BraTS. If the folder contains no ".nii.gz" files, the function completes without creating outputs.
        output_folder (str): Path to the destination folder where converted NIfTI files will be written. The function ensures this directory exists by calling maybe_mkdir_p(output_folder) before processing. Converted files are written by the worker function load_convert_labels_back_to_BraTS; typically the original filenames are used for the outputs, so existing files in output_folder with the same names may be overwritten if present.
        num_processes (int): Number of worker processes to spawn for parallel conversion (default: 12). A multiprocessing Pool with spawn start method is created via multiprocessing.get_context("spawn").Pool(num_processes). Choose this value according to available CPU cores and I/O bandwidth; too many processes may increase contention and memory usage, too few will reduce parallel throughput. The default 12 is a practical choice for multi-core systems but not mandatory.
    
    Returns:
        None: This function does not return a value. Its side effects are: creating output_folder if it does not exist, reading each ".nii.gz" prediction file found in input_folder, converting label ids to the BraTS21 convention by invoking load_convert_labels_back_to_BraTS for each file, and writing the converted NIfTI files to output_folder.
    
    Behavior, side effects, defaults, and failure modes:
        - The function discovers files via subfiles(input_folder, suffix='.nii.gz', join=False) and processes the returned file list in parallel.
        - maybe_mkdir_p(output_folder) is called at the start to ensure the output directory exists.
        - Parallel conversion uses a multiprocessing Pool with the "spawn" start method to avoid fork-related issues common in some Python environments and heavy C/C++ backed libraries.
        - If input_folder contains no ".nii.gz" files, the function exits normally without writing any files.
        - If a worker (load_convert_labels_back_to_BraTS) raises an exception for a particular file (for example due to unreadable/corrupted NIfTI, incompatible volume shape, missing headers, or mapping errors), that exception may propagate and cause the Pool to terminate; calling code should be prepared to catch exceptions raised by this function.
        - File system permission errors when reading from input_folder or writing to output_folder will raise exceptions.
        - The function may overwrite existing files in output_folder if filenames collide; ensure backups if necessary.
        - num_processes must be a non-negative integer; passing 0 or a negative number will result in an error from multiprocessing.Pool. Selecting a num_processes larger than available CPU cores or memory may degrade performance or cause resource exhaustion.
        - This function assumes load_convert_labels_back_to_BraTS implements the precise label-id mapping required for BraTS21; incorrect or outdated mapping in that helper will lead to incorrect outputs.
        - Designed for use in the nnU-Net dataset conversion workflow when preparing predictions for BraTS21-compatible evaluation or submission.
    """
    from nnunetv2.dataset_conversion.Dataset137_BraTS21 import convert_folder_with_preds_back_to_BraTS_labeling_convention
    return convert_folder_with_preds_back_to_BraTS_labeling_convention(
        input_folder,
        output_folder,
        num_processes
    )


################################################################################
# Source: nnunetv2.dataset_conversion.Dataset218_Amos2022_task1.convert_amos_task1
# File: nnunetv2/dataset_conversion/Dataset218_Amos2022_task1.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for convert_amos_task1 because the docstring has no description for the argument 'amos_base_dir'
################################################################################

def nnunetv2_dataset_conversion_Dataset218_Amos2022_task1_convert_amos_task1(
    amos_base_dir: str,
    nnunet_dataset_id: int = 218
):
    """Convert AMOS2022 Task 1 (post-challenge) into the nnU-Net v2 raw dataset folder structure and metadata, with the AMOS validation set merged into the training set. This function is used within the dataset conversion stage of nnU-Net to prepare the AMOS2022 dataset for training and cross-validation in the nnU-Net pipeline (see documentation/dataset_format.md). The conversion enforces selection of CT cases only (based on the numeric suffix of case identifiers), creates the required nnU-Net raw subfolders (imagesTr, imagesTs, labelsTr), copies and renames image and label files into the nnU-Net expected naming scheme, and generates a dataset.json describing modalities and label mappings so that nnU-Net can automatically configure segmentation pipelines and run 5-fold cross-validation.
    
    Args:
        amos_base_dir (str): Path to the root directory of the unmodified AMOS2022 dataset as downloaded/extracted. The function expects this directory to contain an AMOS dataset.json file at amos_base_dir/dataset.json and the following subfolders with NIfTI files following the AMOS naming convention: imagesTr, labelsTr, imagesVa, labelsVa, imagesTs. File names are expected to end with the suffix '.nii.gz'. The function will read dataset.json to obtain lists of training, validation and test image identifiers and will copy files from the above subfolders into the nnU-Net raw folder structure. Missing dataset.json, missing expected subfolders or unexpected filename formats will raise standard I/O or KeyError exceptions (e.g., FileNotFoundError, KeyError) and the function will fail unless the caller ensures the expected AMOS layout.
        nnunet_dataset_id (int): Integer dataset identifier to embed into the target nnU-Net folder name. Default is 218. The function constructs an output folder name of the form "Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task1" and places converted files under the global nnUNet_raw path joined with that folder name. This id determines the final dataset folder used by nnU-Net and must be chosen to avoid collisions with existing nnU-Net dataset IDs.
    
    Returns:
        None: This function does not return a Python value. Instead it performs side effects required for nnU-Net dataset conversion:
            - Creates the nnU-Net raw dataset directory structure at join(nnUNet_raw, "Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task1") including subfolders "imagesTr", "imagesTs", and "labelsTr" (using maybe_mkdir_p).
            - Copies CT image files from the AMOS folders into imagesTr and imagesTs and copies label files into labelsTr. Image files are renamed to the nnU-Net convention by appending "_0000.nii.gz" to the case identifier (e.g., caseID_0000.nii.gz). Label files are copied with their case identifier and '.nii.gz' ending (e.g., caseID.nii.gz).
            - Incorporates validation cases into the training set (AMOS validation ground truth is included in the post-challenge release) rather than creating a separate validation split; this is intentional because AMOS does not specify how the validation set should be used and nnU-Net prefers 5-fold cross-validation over a single train:val split.
            - Calls generate_dataset_json to write a dataset.json in the output folder with modalities set to {0: "CT"}, label mapping taken from the source AMOS dataset.json (converted to integer keys), num_training_cases set to the number of copied training+validation CT cases, file_ending '.nii.gz', dataset_name "AMOS2022_postChallenge_task1", and metadata fields reference and release. The generated dataset.json uses the image reader/writer override 'NibabelIOWithReorient' to ensure consistent orientation handling.
    
    Behavioral details and important implementation notes:
        - Identification of case identifiers: the function reads entries from the AMOS dataset.json lists (training, validation, test). For each entry it extracts the case identifier by splitting the 'image' path on '/' and taking the last path component, then removing the final seven characters (expected to strip the '.nii.gz' suffix). This identifier is used to locate source files in the AMOS subfolders and to construct destination filenames.
        - CT-only filtering: the code determines which AMOS cases correspond to CT images by parsing the numeric suffix after the final underscore in the case identifier (int(case_id.split("_")[-1])). Only cases with numeric suffixes <= 410 are treated as training CT images, <= 409 as validation CT images (these are merged into training), and <= 500 as test CT images. These numeric thresholds are derived from how the AMOS release enumerates modalities and are applied exactly as in the source conversion logic.
        - File operations: files are copied using shutil.copy. If a source file is missing, a FileNotFoundError will be raised. If copying would overwrite existing files in the destination, the existing files will be silently overwritten by shutil.copy; the generate_dataset_json call may also overwrite or update dataset metadata depending on implementation of that helper.
        - Global dependency: the function assumes a module-level variable nnUNet_raw is defined and points to the base path where nnU-Net raw datasets are stored. If nnUNet_raw is not defined in the module scope, a NameError will be raised.
        - Metadata and labels: the labels mapping is constructed by inverting the source dataset_json_source['labels'] mapping into a mapping label_value -> integer index as performed in the source code ({v: int(k) for k,v in dataset_json_source['labels'].items()}). If the source labels mapping does not follow the expected structure, a KeyError or ValueError may occur.
        - Overwrite behavior and id collisions: because the function constructs a deterministic output folder name from nnunet_dataset_id and a fixed task name, calling this function multiple times with the same nnunet_dataset_id may overwrite previously converted datasets. Ensure unique ids or move/remove existing dataset folders to avoid unintended overwrites.
    
    Exceptions and failure modes:
        - FileNotFoundError: raised when expected source files (dataset.json or NIfTI files under imagesTr/imagesVa/imagesTs/labelsTr/labelsVa) are missing.
        - KeyError: raised when the expected keys ('training', 'validation', 'test', 'labels') are absent from the AMOS dataset.json.
        - ValueError: raised when case identifier parsing (e.g., stripping suffix or converting numeric suffix to int) fails due to unexpected filename formats.
        - NameError: raised if the module-level nnUNet_raw variable is undefined.
        - PermissionError or OSError: can be raised on filesystem operations if the process lacks the required permissions or disk space.
    
    Practical significance:
        - This conversion enables domain scientists and nnU-Net users to take the AMOS2022 post-challenge release (which includes validation ground truth) and prepare it for immediate use with nnU-Net's automated pipeline generation, training, and 5-fold cross-validation. The function automates the dataset layout and metadata creation required by nnU-Net so no manual renaming or JSON editing is necessary when the AMOS dataset conforms to the expected layout.
    """
    from nnunetv2.dataset_conversion.Dataset218_Amos2022_task1 import convert_amos_task1
    return convert_amos_task1(amos_base_dir, nnunet_dataset_id)


################################################################################
# Source: nnunetv2.dataset_conversion.Dataset219_Amos2022_task2.convert_amos_task2
# File: nnunetv2/dataset_conversion/Dataset219_Amos2022_task2.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for convert_amos_task2 because the docstring has no description for the argument 'amos_base_dir'
################################################################################

def nnunetv2_dataset_conversion_Dataset219_Amos2022_task2_convert_amos_task2(
    amos_base_dir: str,
    nnunet_dataset_id: int = 219
):
    """Converts the AMOS2022 task2 dataset (post-challenge release) into the nnU-Net raw dataset layout so it can be used by nnU-Net training and 5-fold cross-validation pipelines. This function is used in the nnU-Net dataset conversion stage (see README -> "Dataset conversion") to transform the AMOS2022 directory and dataset.json into the folder structure, filenames and dataset.json format expected by nnU-Net. It also incorporates the AMOS validation cases into the nnU-Net training set (AMOS does not specify how validation should be used), which enables nnU-Net's recommended 5-fold cross-validation instead of a single train/val split.
    
    Args:
        amos_base_dir (str): Path to the root directory of the downloaded AMOS2022 dataset (post-challenge release). The function expects this directory to contain:
            - a dataset.json file describing the AMOS release (with keys 'training', 'test', 'validation', and 'labels'),
            - subfolders imagesTr, labelsTr, imagesTs, imagesVa, labelsVa containing .nii.gz files.
            Practical significance: this is the source biomedical imaging data (CT/MR abdominal scans and their labels) that will be reorganized into nnU-Net's raw dataset layout so nnU-Net can analyze dataset properties, configure networks, and train segmentation models.
        nnunet_dataset_id (int): Numeric identifier used to build the output dataset folder name inside nnU-Net's raw data root (default: 219). The folder created is "Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task2" (for example, Dataset219_AMOS2022_postChallenge_task2). Practical significance: this id uniquely identifies the converted dataset within the local nnU-Net installation and is used by downstream nnU-Net tooling to reference the dataset.
    
    Behavior and side effects:
        - Reads amos_base_dir/dataset.json to discover the lists of training, test and validation cases and the label name-to-index mapping. The dataset.json's 'training', 'test' and 'validation' entries are expected to contain dictionaries with an 'image' entry whose value is a path; the code extracts per-case identifiers from these entries to map to file basenames.
        - Creates the output directory under nnUNet_raw named Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task2 and the standard nnU-Net subdirectories imagesTr, imagesTs and labelsTr. Practical significance: these directories and naming conventions are required by nnU-Net's preprocessing and training scripts.
        - Copies AMOS training images and labels into imagesTr and labelsTr respectively. Image filenames in imagesTr are renamed to include the modality suffix "_0000.nii.gz" (nnU-Net convention for single-modality cases) while labels retain the .nii.gz filename without modality suffix.
        - Copies AMOS test images into imagesTs and renames them to include the modality suffix "_0000.nii.gz".
        - Copies AMOS validation images and labels into imagesTr and labelsTr (i.e., validation cases are appended to the training set). Practical significance: this yields a single training set comprised of the original training + validation released by AMOS so that nnU-Net can perform 5-fold cross-validation across all available labeled cases rather than relying on an undefined train/val split.
        - Calls generate_dataset_json(...) to produce an nnU-Net style dataset.json in the output folder with:
            - modalities set to {0: "either_CT_or_MR"},
            - labels derived from the AMOS dataset.json by converting AMOS label keys/values into the integer-indexed mapping expected by nnU-Net,
            - num_training_cases equal to the number of AMOS training plus validation cases,
            - file_ending='.nii.gz',
            - dataset_name set to "AMOS2022_postChallenge_task2",
            - reference and release URLs set to the AMOS challenge pages listed in the source code,
            - overwrite_image_reader_writer set to 'NibabelIOWithReorient',
            - and a descriptive text noting that the validation GT is included and that validation images were added to training for 5-fold CV.
          Practical significance: the generated dataset.json documents the converted dataset for nnU-Net's preprocessing, training and evaluation steps and records provenance information.
        - No value is returned; the function's effect is the creation/population of files and folders under nnUNet_raw/Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task2.
    
    Failure modes and exceptions:
        - File and directory access errors: if amos_base_dir does not exist, required files (dataset.json) or expected subfolders (imagesTr, labelsTr, imagesTs, imagesVa, labelsVa) are missing, or the process lacks filesystem permissions, functions such as load_json and shutil.copy will raise FileNotFoundError, PermissionError or related OSError exceptions.
        - Malformed dataset.json: if dataset.json is not valid JSON or does not contain the expected keys ('training', 'test', 'validation', 'labels') or entries with an 'image' field, the function will raise a KeyError, IndexError or JSONDecodeError depending on the failure mode.
        - Label mapping inconsistencies: if dataset_json_source['labels'] does not contain the expected mapping of string keys to label names convertible to integers via the code's comprehension, generate_dataset_json may produce an incorrect labels section or raise an error.
        - Disk space and I/O errors during copying will raise appropriate exceptions from shutil or the OS.
        - The function does not perform input validation beyond relying on the existence and structure of the AMOS files and dataset.json; callers should ensure the AMOS download is complete and intact before calling.
    
    Returns:
        None: This function does not return a Python value. Instead, it creates and populates the nnU-Net raw dataset directory named Dataset{nnunet_dataset_id:03d}_AMOS2022_postChallenge_task2 under nnUNet_raw, copies and renames image and label files into imagesTr, imagesTs and labelsTr, and writes an nnU-Net style dataset.json describing the converted dataset. These filesystem side effects are the intended output for use by subsequent nnU-Net preprocessing, configuration and training steps.
    """
    from nnunetv2.dataset_conversion.Dataset219_Amos2022_task2 import convert_amos_task2
    return convert_amos_task2(amos_base_dir, nnunet_dataset_id)


################################################################################
# Source: nnunetv2.dataset_conversion.convert_raw_dataset_from_old_nnunet_format.convert
# File: nnunetv2/dataset_conversion/convert_raw_dataset_from_old_nnunet_format.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_dataset_conversion_convert_raw_dataset_from_old_nnunet_format_convert(
    source_folder: str,
    target_dataset_name: str
):
    """nnunetv2.dataset_conversion.convert_raw_dataset_from_old_nnunet_format.convert converts a dataset organized in the old nnU-Net (v1) task layout (commonly named TaskXXX_YYY with directories like imagesTr and labelsTr and a dataset.json that uses keys such as "modality" and "labels") into the nnunetv2 raw dataset layout under the global nnUNet_raw directory (datasets in v2 are typically called DatasetXXX_YYY). This function is used when migrating datasets created for the old nnU-Net to the nnunetv2 expected raw format so they can be processed by nnU-Net V2 preprocessing and training pipelines.
    
    Args:
        source_folder (str): Filesystem path to the source dataset in the old nnU-Net format. This folder is expected to contain at least imagesTr, labelsTr and a dataset.json file using the old schema (for example including the keys 'modality' and 'labels'). The value is used as the source root from which imagesTr/labelsTr (and optional imagesTs/labelsTs/imagesVal/labelsVal) and the dataset.json are copied.
        target_dataset_name (str): Name of the target dataset directory to create under the nnUNet_raw global directory (for example DatasetXXX_YYY). This exact string becomes the name of the new directory created under nnUNet_raw (join(nnUNet_raw, target_dataset_name)). The function will abort if this target directory already exists to avoid accidental overwrites.
    
    Behavior, side effects, defaults, and failure modes:
        The function first checks if a directory join(nnUNet_raw, target_dataset_name) already exists; if it does, a RuntimeError is raised and no files are modified. This guard prevents accidental overwriting of existing nnunetv2 raw datasets.
        If the target directory does not exist, the function creates it (using maybe_mkdir_p) and then copies the following items from source_folder into the newly created target folder under nnUNet_raw:
        - imagesTr and labelsTr are copied via shutil.copytree and are required for a successful conversion; missing these will typically raise an OSError/FileNotFoundError from the copy operation.
        - imagesTs, labelsTs, imagesVal, labelsVal are optional: if the corresponding directories exist in source_folder they are copied; if they do not exist they are skipped without error.
        - dataset.json is copied and then loaded, modified, and re-saved in the target directory. The modifications implement the migration from the old dataset.json schema to the new one expected by nnunetv2:
        - The keys 'tensorImageSize', 'numTest', 'training', and 'test' are removed because the v2 raw dataset schema does not use them in the same way.
        - The old 'modality' field (which mapped channel indices to modality names) is deep-copied to a new 'channel_names' field, and then the original 'modality' field is deleted. This creates a channel_names entry accessible to v2 preprocessing code.
        - The 'labels' mapping is converted from the old mapping (where keys were stringified integers and values label names) into a mapping keyed by label names with integer label IDs as values. Concretely, for each (i, j) in the original dataset_json['labels'].items(), the new dataset_json['labels'] will have an entry j: int(i). This inverts the mapping and ensures label IDs are integers as required by v2 tools.
        - The dataset JSON is augmented with 'file_ending' set to ".nii.gz" to reflect the expected file extension of image and label files in the nnunetv2 raw dataset.
        - The modified dataset.json is saved back to join(nnUNet_raw, target_dataset_name, 'dataset.json') using save_json(..., sort_keys=False) so key order is preserved where relevant.
        The function performs filesystem-level copies; as a result it duplicates the dataset content under the nnUNet_raw tree rather than moving it. Because of these side effects the caller should ensure sufficient disk space is available.
        Expected failure modes include: RuntimeError if the target dataset already exists; FileNotFoundError/OSError if required source directories or the dataset.json are missing or not readable; JSON parsing errors if the dataset.json is malformed; KeyError if expected keys such as 'modality' or 'labels' are absent from dataset.json. These lower-level exceptions originate from shutil, the filesystem, or the JSON loader and are not caught inside the function.
    
    Returns:
        None: The function does not return a value. Instead it has the side effect of creating a new directory join(nnUNet_raw, target_dataset_name) containing copied image/label directories (imagesTr, labelsTr and optionally imagesTs/labelsTs/imagesVal/labelsVal) and a migrated dataset.json prepared for nnunetv2 preprocessing and training.
    """
    from nnunetv2.dataset_conversion.convert_raw_dataset_from_old_nnunet_format import convert
    return convert(source_folder, target_dataset_name)


################################################################################
# Source: nnunetv2.evaluation.evaluate_predictions.save_summary_json
# File: nnunetv2/evaluation/evaluate_predictions.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for save_summary_json because the docstring has no description for the argument 'results'
################################################################################

def nnunetv2_evaluation_evaluate_predictions_save_summary_json(
    results: dict,
    output_file: str
):
    """Save a JSON summary of evaluation metrics for nnU-Net prediction runs, converting any non-JSON-friendly keys (notably tuple keys used to represent labels or regions) into string keys before writing.
    
    This function is used in the nnU-Net evaluation pipeline to persist aggregated and per-case metrics produced when evaluating segmentation predictions. In the nnU-Net domain (biomedical semantic segmentation), metric dictionaries often use tuples to represent composite label/region identifiers; JSON does not allow non-string mapping keys, so this function performs a deterministic conversion of those keys and writes a deep-copied summary to disk. The function expects the results dictionary to contain a 'mean' mapping of metrics aggregated across cases and a 'metric_per_case' list where each entry contains a 'metrics' mapping for that case. It converts keys in results['mean'] and in each results['metric_per_case'][i]['metrics'] by applying the repository's label_or_region_to_key conversion helper, then writes the converted structure using save_json with sort_keys=True (so that keys such as "foreground_mean" appear first and the output ordering is stable).
    
    Args:
        results (dict): Evaluation results dictionary produced by the evaluation pipeline. It must include at least the following structure: a 'mean' mapping of metric names/labels to values (these keys may be tuples representing labels or regions) and a 'metric_per_case' sequence (typically a list) whose elements are mappings that include a 'metrics' mapping for that specific case. This function will deepcopy this object, convert mapping keys that are not JSON-friendly (e.g., tuple keys) to string keys via label_or_region_to_key, and leave metric values unchanged. If the structure deviates (for example missing 'mean', missing 'metric_per_case', non-iterable metric_per_case, or missing 'metrics' entries), the function will raise the corresponding KeyError/TypeError produced by the underlying operations.
        output_file (str): Filesystem path where the JSON summary should be written. The function uses save_json to write the converted dictionary to this path with sort_keys=True. If the path is not writable, save_json (or the underlying file I/O) will raise an IOError/OSError. If a file already exists at this path it will be overwritten by the write operation performed by save_json.
    
    Returns:
        None: The function does not return a value. Its observable side effect is that a JSON file is written to output_file containing a deep-copied, JSON-key-safe version of the input results with deterministic key ordering (sort_keys=True). Possible failure modes include missing expected keys in results (KeyError), invalid structure types (TypeError), failures in the label_or_region_to_key conversion, failures during deepcopy, and file I/O errors raised by save_json when attempting to write output_file.
    """
    from nnunetv2.evaluation.evaluate_predictions import save_summary_json
    return save_summary_json(results, output_file)


################################################################################
# Source: nnunetv2.evaluation.find_best_configuration.dumb_trainer_config_plans_to_trained_models_dict
# File: nnunetv2/evaluation/find_best_configuration.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for dumb_trainer_config_plans_to_trained_models_dict because the docstring has no description for the argument 'trainers'
################################################################################

def nnunetv2_evaluation_find_best_configuration_dumb_trainer_config_plans_to_trained_models_dict(
    trainers: List[str],
    configs: List[str],
    plans: List[str]
):
    """nnunetv2.evaluation.find_best_configuration.dumb_trainer_config_plans_to_trained_models_dict: Create a deterministic enumeration of trained-model descriptors by taking the Cartesian product of the provided trainer identifiers, configuration names, and plan identifiers. This helper is used in the evaluation pipeline to produce a simple list of candidate trained-model specifications that downstream code (for example: model lookup, evaluation, or selection logic in find_best_configuration) can iterate over when empirically determining the best U-Net configuration for a dataset. The name "dumb" reflects that this function performs no validation, filtering, or existence checks — it only composes the inputs into dictionaries.
    
    Args:
        trainers (List[str]): A list of trainer identifiers (strings). In the nnU-Net domain these typically correspond to trainer classes or trainer name strings that implement the training loop and augmentation/optimization choices (for example variants of nnUNetTrainer). Each entry represents one distinct training strategy that should be considered when evaluating model performance.
        configs (List[str]): A list of configuration names (strings). In nnU-Net these are the U-Net configuration labels such as '2d', '3d_fullres', and '3d_lowres' (see README description of created configurations). Each configuration encodes a different network topology / dimensionality choice that will be evaluated for the given dataset.
        plans (List[str]): A list of plan identifiers or plan file names (strings). Plans encode dataset-specific preprocessing and topology decisions (patch sizes, spacing handling, pooling behavior) that nnU-Net derives during dataset analysis. Each plan string identifies one such set of choices to be paired with trainers and configs.
    
    Behavior and practical details: The function performs three nested loops in the order trainers -> configs -> plans and for each combination appends a dictionary with exactly the keys 'plans', 'configuration', and 'trainer' mapping to the corresponding string values from the inputs. The returned sequence preserves this iteration order, which provides a deterministic enumeration for downstream evaluation and selection code. There are no side effects: the function does not read or write files, check model existence on disk, or validate that any trainer/config/plan combination is meaningful or supported. Common practical consequences: if any input list is empty, the result is an empty tuple; if input lists contain duplicate strings, the output will contain duplicate descriptor dictionaries in corresponding order.
    
    Failure modes and expectations: The function expects each list element to be a string as declared in the signature. Passing non-string elements violates the documented types and may lead to unexpected contents in the result; the function itself performs no type checks and will not raise type-specific errors, but downstream consumers typically expect string identifiers. The function does not attempt to verify that the referenced trainers, configurations, or plans correspond to actually trained models or known nnU-Net presets — such validation must be performed by calling code.
    
    Returns:
        tuple: An immutable tuple of dictionaries. Each dictionary has the exact keys 'plans', 'configuration', and 'trainer' (all string values) and represents one candidate trained-model specification formed from the inputs. The tuple order follows the nested iteration order (for trainer in trainers, for config in configs, for plan in plans).
    """
    from nnunetv2.evaluation.find_best_configuration import dumb_trainer_config_plans_to_trained_models_dict
    return dumb_trainer_config_plans_to_trained_models_dict(trainers, configs, plans)


################################################################################
# Source: nnunetv2.experiment_planning.experiment_planners.network_topology.get_pool_and_conv_props
# File: nnunetv2/experiment_planning/experiment_planners/network_topology.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_experiment_planning_experiment_planners_network_topology_get_pool_and_conv_props(
    spacing: list,
    patch_size: list,
    min_feature_map_size: int,
    max_numpool: int
):
    """get_pool_and_conv_props computes per-axis pooling and convolution kernel configurations used by nnU-Net's automatic network topology planner. This function analyzes image spacing and patch size to decide how many downsampling (pooling) operations to perform along each axis, which axes to pool together at each stage, and which convolution kernel sizes to use at each network stage. It mirrors the behavior of get_pool_and_conv_props_v2 from the old nnU-Net implementation and is used during experiment planning to generate a U-Net topology that respects voxel anisotropy, a minimum feature map size in the bottleneck, and a maximum number of pooling operations per axis.
    
    Args:
        spacing (list): Physical voxel spacing for each image axis (length determines spatial dimensionality dim). Typical values are positive numbers (floats) describing real-world voxel spacing in millimeters or comparable units. This is used to detect anisotropy and to decide which axes may be pooled together (axes with spacing within a factor of two are pooled jointly).
        patch_size (list): Patch size expressed in voxel counts for each axis (list of positive integers). The function internally pads this patch size so that it is divisible by the cumulative downsampling factors (see must_be_divisible_by). The input list is deep-copied and therefore not modified in-place; the returned patch_size is the potentially padded shape that must be used for subsequent preprocessing and network input.
        min_feature_map_size (int): Minimum allowed edge length (in voxels) of feature maps in the bottleneck (smallest feature map after all pooling). This integer prevents excessive pooling along small axes. If an axis would produce feature maps smaller than this value, pooling along that axis is disallowed. Practical significance: ensures that feature maps at the network bottleneck remain large enough for meaningful convolutional feature learning.
        max_numpool (int): Maximum allowed number of pooling operations per axis (non-negative integer). This caps network depth per axis to control memory usage and prevent over-downsampling even if spacings/patch sizes would allow further pooling.
    
    Behavior and details:
        The function determines the spatial dimensionality dim from len(spacing) and expects spacing and patch_size to have compatible dimensionality; if they differ, an IndexError or unexpected behavior may occur because the code indexes both by dim. Internally, spacing and patch_size are deep-copied to avoid mutating the caller's lists.
        Pooling decisions are made iteratively. At each iteration, axes that would yield a feature map edge length below min_feature_map_size after pooling are excluded. Among the remaining axes, those whose current spacing is within a factor of two of the minimum current spacing are considered for pooling together. The per-axis pooling count is capped by max_numpool.
        Convolution kernel sizes are tracked per stage. Kernel sizes start at 1 on every axis and are set to 3 for an axis once that axis becomes within a factor of two of the smallest spacing; once an axis's kernel size is set to 3 it remains 3 for subsequent stages. A final bottleneck convolution kernel size of 3 on every axis is appended prior to return.
        pool_op_kernel_sizes begins with an initial entry corresponding to "no pooling" ([1]*dim) and accumulates one entry per pooling stage describing the pooling factor (1 or 2) for each axis at that stage.
        num_pool_per_axis counts how many times each axis was pooled. must_be_divisible_by is computed from num_pool_per_axis (via get_shape_must_be_divisible_by) and used to pad the input patch_size so that it is divisible by the cumulative downsampling factors; the padding is applied via pad_shape and the padded shape is returned.
        Numeric operations use numpy.ceil when halving sizes to simulate integer rounding of feature map sizes after pooling.
    
    Side effects, defaults, and failure modes:
        The function has no global side effects. It deep-copies inputs, so the caller's spacing and patch_size lists are not modified. It does, however, return a (possibly padded) patch_size that must be used downstream; failing to use the returned patch_size (and must_be_divisible_by) can cause mismatches between network topology and actual input shapes.
        If spacing and patch_size have mismatched lengths the function may raise IndexError or produce incorrect results; callers must provide lists of equal length. The function assumes positive numeric spacing values and positive integer patch sizes; non-positive or non-numeric values may produce incorrect behavior or runtime errors. If no axis can be pooled due to the min_feature_map_size and max_numpool constraints, the function will break out without adding pooling stages and will return initial kernel settings and counts reflecting zero pooling.
    
    Returns:
        tuple: A 5-tuple containing the computed network topology properties in this order:
            num_pool_per_axis (list): List of length dim with integers indicating how many pooling operations were applied along each axis. Practical significance: used to determine network depth per axis and cumulative downsampling factors for preprocessing and architecture construction.
            pool_op_kernel_sizes (tuple): Nested tuple-of-tuples describing pooling kernels per stage. The first entry corresponds to the initial (no-pooling) stage ([1]*dim). Each subsequent entry is a tuple of length dim with values 1 or 2 indicating the pooling factor applied on each axis at that stage. Practical significance: defines the sequence of downsampling operations that the network will perform.
            conv_kernel_sizes (tuple): Nested tuple-of-tuples describing convolution kernel sizes per stage. There is one conv kernel-size tuple per stage where convolutions are applied; the final entry corresponds to the bottleneck and is [3]*dim. Each tuple contains integers (1 or 3) per axis. Practical significance: informs the network builder which convolution kernel size to use along each axis at each level, reflecting anisotropy-aware decisions.
            patch_size (tuple): The (possibly padded) patch size as a tuple of integers after calling pad_shape with must_be_divisible_by. This is the patch shape that is guaranteed to be divisible by the cumulative downsampling factors implied by num_pool_per_axis and must be used for preprocessing and training to ensure tensor size compatibility with the planned topology.
            must_be_divisible_by (tuple): Tuple of integers (one per axis) representing the per-axis divisibility requirement computed from num_pool_per_axis (via get_shape_must_be_divisible_by). Practical significance: tells downstream code how to pad/crop input images so that their shapes are compatible with the planned pooling configuration.
    """
    from nnunetv2.experiment_planning.experiment_planners.network_topology import get_pool_and_conv_props
    return get_pool_and_conv_props(
        spacing,
        patch_size,
        min_feature_map_size,
        max_numpool
    )


################################################################################
# Source: nnunetv2.experiment_planning.experiment_planners.network_topology.pad_shape
# File: nnunetv2/experiment_planning/experiment_planners/network_topology.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_experiment_planning_experiment_planners_network_topology_pad_shape(
    shape: tuple,
    must_be_divisible_by: list
):
    """Compute a padded spatial shape such that each dimension is divisible by the corresponding divisibility factor. This function is used during nnU-Net experiment planning (experiment_planning.experiment_planners.network_topology) to ensure patch sizes and image shapes are compatible with U-Net downsampling/pooling schedules and other topology rules: the returned shape is the minimal shape greater than or equal to the input shape for which each axis length is a multiple of the requested divisor.
    
    Args:
        shape (tuple): Original spatial shape given as a tuple of integers (e.g., a patch size or image shape in voxels/pixels). Each element represents the length of one spatial axis. This value is used as the baseline size that may be increased so that network pooling/downsampling operations divide the size evenly. The function expects len(shape) to be the number of spatial dimensions considered by the network topology.
        must_be_divisible_by (list or tuple or numpy.ndarray or int): Per-axis divisibility targets. In typical nnU-Net usage these are integers derived from the planned pooling factors or cumulative downsampling strides so that each spatial axis of the network input is divisible by these values. If an iterable is provided, its length must equal len(shape) (otherwise an AssertionError is raised). If a single scalar integer is provided instead of an iterable, it is broadcast to all axes (the code will create a list by repeating the scalar for each axis). The elements should be positive integers; providing zero will raise an exception (division/modulo by zero) and negative values produce undefined or unintended behavior.
    
    Returns:
        numpy.ndarray: 1-D integer numpy array of length len(shape). Each element is the minimal integer >= shape[i] such that result[i] % must_be_divisible_by[i] == 0. Practically, this is the padded shape that nnU-Net uses to configure patch sizes and pooling so that all downsampling stages divide evenly. The function performs no in-place mutation of the input arguments; it returns a newly allocated numpy array with dtype int.
    
    Behavior and failure modes:
        The function computes new_shp[i] by rounding shape[i] up to the nearest multiple of must_be_divisible_by[i]. If shape[i] is already exactly divisible by must_be_divisible_by[i], the returned value for that axis equals shape[i] (no padding is added). If must_be_divisible_by is an iterable of different length than shape, an AssertionError is raised. If must_be_divisible_by contains zero, a runtime error (division/modulo by zero) will occur. Inputs should be integer-valued; non-integer inputs will be cast/truncated when converted to integers in the returned numpy array and may lead to unexpected results. There are no side effects (inputs are not mutated). The function always returns an integer numpy array suitable for further topology computations in nnU-Net.
    """
    from nnunetv2.experiment_planning.experiment_planners.network_topology import pad_shape
    return pad_shape(shape, must_be_divisible_by)


################################################################################
# Source: nnunetv2.experiment_planning.plan_and_preprocess_api.extract_fingerprints
# File: nnunetv2/experiment_planning/plan_and_preprocess_api.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for extract_fingerprints because the docstring has no description for the argument 'dataset_ids'
################################################################################

def nnunetv2_experiment_planning_plan_and_preprocess_api_extract_fingerprints(
    dataset_ids: List[int],
    fingerprint_extractor_class_name: str = "DatasetFingerprintExtractor",
    num_processes: int = 8,
    check_dataset_integrity: bool = False,
    clean: bool = True,
    verbose: bool = True
):
    """nnunetv2.experiment_planning.plan_and_preprocess_api.extract_fingerprints
    Extract dataset fingerprints for one or more nnU-Net datasets and persist them for use by the experiment planning pipeline.
    
    This function automates the dataset analysis step described in the nnU-Net README: it computes a "dataset fingerprint" for each dataset id provided (an analysis of image sizes, spacings, modalities, class balance and other dataset properties). Those fingerprints are used by nnU-Net's rule-based experiment planning to configure architecture topology, patch sizes, batch sizes and other pipeline parameters automatically. The function locates a fingerprint extractor class implementation by name inside the nnunetv2.experiment_planning package, instantiates/uses it (via recursive class lookup) and then invokes the per-dataset extractor routine extract_fingerprint_dataset for every id in dataset_ids. This operation performs file IO and dataset analysis and therefore can be CPU- and disk-intensive; it may run work in parallel using multiple processes.
    
    Args:
        dataset_ids (List[int]): A list of integer dataset identifiers for which fingerprints should be extracted. Each integer corresponds to a dataset prepared in nnU-Net's dataset storage (the dataset conversion step in the README creates these datasets). The function will iterate over this list and attempt to extract a fingerprint for each id in order.
        fingerprint_extractor_class_name (str): The exact class name (as a string) to locate inside the nnunetv2.experiment_planning package and use for fingerprint extraction. The default 'DatasetFingerprintExtractor' selects the standard extractor implementation provided by nnU-Net that analyzes training cases and derives the dataset fingerprint used for automated pipeline configuration. The name must match a class available in that package; if the class cannot be found an exception will be raised.
        num_processes (int): Number of worker processes to use for parallel parts of fingerprint extraction (for example reading image headers, computing statistics, or other per-file analysis). Default is 8. Increasing this can speed up IO-bound or CPU-bound parts on machines with many cores, but using too many processes may overload the host (CPU, memory, or I/O).
        check_dataset_integrity (bool): If True, run dataset integrity and format checks while extracting fingerprints. Integrity checks validate that the dataset follows nnU-Net's expected conversion/format and may abort or raise errors on inconsistencies (for example missing files, mismatching channel counts, or other format violations). Default is False to skip expensive integrity validation.
        clean (bool): When True (default), perform the actual fingerprint extraction and persist results. When False, act as a no-op switch used by higher-level orchestration (for example nnUNetv2_plan_and_preprocess) to skip re-running fingerprint extraction when it is not desired. Note: the implementation forwards this flag to the per-dataset extractor; behavior when False is to avoid re-running or writing fingerprint results.
        verbose (bool): If True (default), emit progress and diagnostic messages describing which dataset id is being processed and any extractor-level logging. If False, run quietly; however warnings and exceptions may still be raised for errors.
    
    Returns:
        None: This function does not return a value. Its primary effects are side effects: it computes dataset fingerprints and persists them into nnU-Net's experiment planning storage so that subsequent planning and preprocessing steps can read them. Side effects include file I/O, possible creation or update of planning files, and console/log output when verbose is True. Errors during class lookup, dataset reading, integrity checks, or the per-dataset extraction routine will raise exceptions (for example if the requested fingerprint extractor class cannot be found, if a dataset id does not correspond to a prepared dataset, or if integrity checks fail when enabled).
    """
    from nnunetv2.experiment_planning.plan_and_preprocess_api import extract_fingerprints
    return extract_fingerprints(
        dataset_ids,
        fingerprint_extractor_class_name,
        num_processes,
        check_dataset_integrity,
        clean,
        verbose
    )


################################################################################
# Source: nnunetv2.experiment_planning.verify_dataset_integrity.verify_dataset_integrity
# File: nnunetv2/experiment_planning/verify_dataset_integrity.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for verify_dataset_integrity because the docstring has no description for the argument 'num_processes'
################################################################################

def nnunetv2_experiment_planning_verify_dataset_integrity_verify_dataset_integrity(
    folder: str,
    num_processes: int = 8
):
    """nnunetv2.experiment_planning.verify_dataset_integrity.verify_dataset_integrity: Verify that a dataset folder is correctly structured and internally consistent for use with nnU-Net's automatic experiment planning and preprocessing. This function performs file-system and content-level checks that prevent common dataset preparation mistakes that would cause downstream failures during nnU-Net pipeline generation, training, or inference.
    
    Performs the following concrete checks and actions (practical significance: ensures the dataset adheres to nnU-Net expectations so automatic configuration, preprocessing and training can proceed without subtle errors):
    - Requires a dataset.json file in folder and validates presence of required metadata keys. This is critical because nnU-Net's planner and readers rely on dataset.json to know modalities, label mappings, file endings and the expected number of training cases.
    - If dataset.json contains a top-level "dataset" key, the function expects dataset.json to list explicit image and label file paths and checks that each referenced image and label file exists. This mode is used when dataset.json fully enumerates file locations.
    - If the "dataset" key is absent (legacy layout), the function requires imagesTr and labelsTr subfolders and checks that a label file exists for every training case reported in dataset.json. This preserves backwards compatibility with the common nnU-Net folder layout.
    - Validates that dataset.json contains the required keys ['labels', 'channel_names', 'numTraining', 'file_ending'] and reports which keys are missing or unused. This prevents mis-specified metadata that would break label interpretation and I/O.
    - Computes expected_num_training from dataset_json['numTraining'] and compares it to the number of discovered training cases. Mismatch indicates mis-specified dataset.json or missing files.
    - Determines num_modalities from dataset_json['channel_names'] (or legacy 'modality') to validate that image files contain the expected number of channels/modalities; this prevents errors where readers load images with unexpected channel counts.
    - Uses get_filenames_of_train_images_and_targets to construct the mapping from case identifiers to image file paths and label file paths.
    - Instantiates LabelManager(dataset_json['labels'], regions_class_order=dataset_json.get('regions_class_order')) to derive the expected label set. If LabelManager reports an ignore label, that label is included in the allowed label set. Enforces that label values are strictly consecutive (0, 1, 2, ...) because nnU-Net expects consecutive label numbering for consistent region handling.
    - Determines the appropriate reader/writer class by calling determine_reader_writer_from_dataset_json(dataset_json, sample_image_path) to ensure file I/O uses the correct backend (practical significance: supports different image formats and metadata readers supported by nnU-Net).
    - In a multiprocessing spawn Pool (num_processes workers), calls verify_labels on every label file with the determined reader/writer class and the expected label set to detect unexpected label values in segmentation images. Unexpected labels cause a RuntimeError and list the offending files.
    - In the same multiprocessing pool, calls check_cases for each case to validate that image and label shapes, voxel spacings and other spatial metadata are compatible (pixel/voxel grid alignment). Mismatches cause a RuntimeError and identify problematic cases.
    - Prints a short confirmation banner on successful completion to indicate that no integrity errors were detected.
    
    Failure modes and exceptions (how the function signals problems and what they mean):
    - AssertionError is raised early for missing dataset.json, missing mandatory keys in dataset.json, missing imagesTr/labelsTr folders in legacy mode, or non-consecutive labels. These indicate structural or metadata problems that must be corrected before planning.
    - FileNotFoundError is raised when dataset.json enumerates file paths (via the "dataset" key) but referenced image or label files are missing. The exception message includes lists of missing image and label paths for debugging.
    - RuntimeError is raised when verify_labels or check_cases detect content-level problems: unexpected label values in segmentation images or mismatched shapes/spacings between images and labels. The RuntimeError messages prompt inspection of printed output to find offending files.
    - Other I/O or reader-specific exceptions may be raised by the underlying reader/writer class when reading files; these propagate and indicate problematic files or unsupported formats.
    
    Performance and resource notes:
    - The default num_processes is 8. The function uses multiprocessing.get_context("spawn").Pool(num_processes) to parallelize label and image checks; "spawn" is explicitly chosen to avoid forking issues with certain readers and libraries. Reduce num_processes on machines with limited CPU or memory to avoid excessive resource contention.
    - The function performs read-only checks except for printing to stdout; it does not modify dataset files or metadata.
    
    Args:
        folder (str): Path to the dataset root folder expected by nnU-Net. This folder must contain a dataset.json file. In the legacy layout the folder must also contain imagesTr and labelsTr subfolders. The dataset.json provides metadata used by nnU-Net (labels mapping, channel names or modality, numTraining, file_ending) and may either enumerate files via a top-level "dataset" key or rely on the imagesTr/labelsTr layout. Supplying the correct folder is required for automatic experiment planning and preprocessing to function.
        num_processes (int): Number of worker processes to spawn for parallel validation of label files and image/label consistency. Defaults to 8. A larger value speeds up checking on machines with many CPUs but increases concurrent I/O and memory usage; set lower on resource-constrained systems.
    
    Returns:
        None: This function does not return a value. On success it prints a confirmation banner stating that verify_dataset_integrity completed and that the dataset is most likely OK. On failure it raises an exception (AssertionError, FileNotFoundError, or RuntimeError) describing the detected integrity problem so the caller can correct the dataset before using nnU-Net.
    """
    from nnunetv2.experiment_planning.verify_dataset_integrity import verify_dataset_integrity
    return verify_dataset_integrity(folder, num_processes)


################################################################################
# Source: nnunetv2.preprocessing.cropping.cropping.create_nonzero_mask
# File: nnunetv2/preprocessing/cropping/cropping.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_preprocessing_cropping_cropping_create_nonzero_mask(data: numpy.ndarray):
    """nnunetv2.preprocessing.cropping.cropping.create_nonzero_mask: Create a boolean foreground mask for cropping by marking spatial voxels/pixels where any input channel is nonzero and filling interior holes.
    
    Args:
        data (numpy.ndarray): A channel-first image array used in nnU-Net preprocessing. Expected shapes are either (C, X, Y, Z) for 3D cases or (C, X, Y) for 2D cases, where C is the number of input modalities/channels and (X, Y, Z) or (X, Y) are the spatial dimensions. Each channel corresponds to an input modality (e.g., different MR sequences or CT) as described in the nnU-Net README; nonzero values indicate signal/foreground in that modality. The function treats any nonzero value as foreground. The function does not modify this input array in-place.
    
    Returns:
        numpy.ndarray: A boolean mask (dtype bool) with the spatial shape data.shape[1:] (i.e., (X, Y, Z) or (X, Y)). The mask is True at spatial locations where at least one channel in data is nonzero, and interior holes in that foreground region have been filled using binary_fill_holes. This mask is intended for use in nnU-Net preprocessing cropping to determine the region of interest for subsequent normalization, patch extraction, and training, thereby reducing memory and computation by excluding pure-background regions.
    
    Raises:
        AssertionError: If data.ndim is not 3 or 4. The function enforces that data has shape (C, X, Y) or (C, X, Y, Z) and will raise an assertion with the message "data must have shape (C, X, Y, Z) or shape (C, X, Y)" when this requirement is not met.
    
    Behavior and side effects:
        The function computes a per-voxel/pixel logical OR across channels to identify any nonzero signal and then applies a hole-filling operation (binary_fill_holes) to produce a contiguous foreground mask. There are no other side effects; the input is not modified and the returned mask is a new numpy.ndarray. This behavior is useful in biomedical image segmentation preprocessing to focus subsequent processing on relevant anatomy and to avoid excluding interior regions that are surrounded by foreground but contain zero-valued voxels due to acquisition or preprocessing artifacts.
    """
    from nnunetv2.preprocessing.cropping.cropping import create_nonzero_mask
    return create_nonzero_mask(data)


################################################################################
# Source: nnunetv2.preprocessing.cropping.cropping.crop_to_nonzero
# File: nnunetv2/preprocessing/cropping/cropping.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_preprocessing_cropping_cropping_crop_to_nonzero(
    data: numpy.ndarray,
    seg: numpy.ndarray = None,
    nonzero_label: int = -1
):
    """nnunetv2.preprocessing.cropping.cropping.crop_to_nonzero crops an image volume (and an optional segmentation) to the minimal axis-aligned bounding box that contains all nonzero voxels in the input image data. This function is used in nnU-Net preprocessing to reduce memory use and accelerate downstream processing by removing empty background regions while preserving the channel (first) axis ordering expected by nnU-Net pipelines. The function relies on the helper functions create_nonzero_mask, get_bbox_from_mask and bounding_box_to_slice to compute the mask, bounding box and corresponding slicer.
    
    Args:
        data (numpy.ndarray): Input image array to be cropped. In nnU-Net this is typically a multi-channel image with the channel axis first (for example shape (C, H, W) or (C, D, H, W)). The function computes a nonzero mask over all spatial axes (not including the preserved first/channel axis) using create_nonzero_mask and returns a view or copy of data restricted to the bounding box that contains all nonzero voxels. The channel axis is preserved (the function prepends slice(None) to the spatial slicer).
        seg (numpy.ndarray): Optional segmentation array aligned with data. If provided, seg is sliced with the same bounding box as data. After cropping, all voxels in seg that equal 0 but lie outside the data nonzero mask are set to nonzero_label (this marks background regions that were empty in the image but are outside the preserved nonzero region). Note that when seg is provided the assignment seg[...] = ... operates on the sliced array and may modify the underlying memory of the original seg array (i.e., a view into the original seg may be modified). If seg is None, a new segmentation array is created with dtype np.int8 containing 0 inside the nonzero region and nonzero_label outside.
        nonzero_label (int): Integer label value to write into the segmentation map for voxels outside the nonzero image region. Default is -1. This value is written into seg for locations where the original seg equals 0 and the image nonzero mask is False; when seg is None this value is used for the outside-mask entries in the newly created segmentation array.
    
    Returns:
        tuple: A 3-tuple (data, seg, bbox) where:
            data (numpy.ndarray): The cropped image array corresponding to the minimal bounding box containing all nonzero voxels of the input data. The first/channel axis is preserved. The returned array may be a view into the original input data (depending on NumPy slicing semantics).
            seg (numpy.ndarray): The cropped segmentation array aligned to the returned data. If an input seg was provided, this is the sliced seg array with entries outside the image nonzero mask set to nonzero_label (this may have modified the original seg buffer). If seg was None, this is a newly created np.int8 array with 0 inside the nonzero region and nonzero_label outside.
            bbox: The bounding box object/structure as returned by get_bbox_from_mask and accepted by bounding_box_to_slice. This bbox identifies the spatial region in the original coordinate frame that was used for cropping and can be passed to bounding_box_to_slice to reconstruct the slicer used.
    
    Behavior and side effects:
        - The function computes a nonzero spatial mask from data via create_nonzero_mask and derives a bounding box with get_bbox_from_mask. The first axis (channels) is always preserved.
        - If seg is provided the function slices seg with the same bounding box and then writes nonzero_label into voxels where (seg == 0) and the nonzero mask is False. Because this write happens on the sliced array, it can mutate the original seg buffer if NumPy returns a view for the slice.
        - If seg is None the function returns a newly allocated segmentation array of dtype np.int8 constructed with np.where(nonzero_mask, np.int8(0), np.int8(nonzero_label)).
        - The function does not perform intensity normalization, resampling, or any interpolation; it only crops arrays and modifies segmentation values as described.
        - Default nonzero_label is -1, which is commonly used in nnU-Net preprocessing to mark ignored/background voxels outside the object-containing region.
    
    Failure modes and notes:
        - The function expects data to be a numpy.ndarray. Behavior is defined by the helper functions for cases such as when no nonzero voxels exist in data; in such cases bbox and the resulting cropped arrays are determined by get_bbox_from_mask and bounding_box_to_slice and should be checked by the caller before further use.
        - The function assumes spatial axes follow the channel axis; callers must ensure that the first axis of data and seg is the channel axis consistent with nnU-Net conventions.
    """
    from nnunetv2.preprocessing.cropping.cropping import crop_to_nonzero
    return crop_to_nonzero(data, seg, nonzero_label)


################################################################################
# Source: nnunetv2.preprocessing.normalization.map_channel_name_to_normalization.get_normalization_scheme
# File: nnunetv2/preprocessing/normalization/map_channel_name_to_normalization.py
# Category: fix_docstring
# Reason: Schema parsing failed: ("Couldn't parse this type hint, likely due to a custom class or object: ", typing.Type[nnunetv2.preprocessing.normalization.default_normalization_schemes.ImageNormalization])
################################################################################

def nnunetv2_preprocessing_normalization_map_channel_name_to_normalization_get_normalization_scheme(
    channel_name: str
):
    """nnunetv2.preprocessing.normalization.map_channel_name_to_normalization.get_normalization_scheme returns the ImageNormalization class that should be used to normalize intensities for a given input channel name in the nnU-Net preprocessing pipeline.
    
    This function maps a human-readable channel identifier (for example modality identifiers encountered in biomedical imaging such as CT or MR, or other channel names produced when converting a dataset to the nnU-Net format) to a concrete normalization scheme class defined in nnunetv2.preprocessing.normalization.default_normalization_schemes. The chosen class is intended to be used by the preprocessing stage of nnU-Net to standardize image intensities before training or inference, which is critical for robust segmentation across diverse datasets as described in the nnU-Net README.
    
    Args:
        channel_name (str): The channel name or modality identifier to look up. This must be a Python string; the lookup is performed case-insensitively by calling channel_name.casefold() internally. Typical values derive from dataset conversion metadata (e.g., modality labels such as "CT" or "MR") but can be any string key that appears in the module-level mapping channel_name_to_normalization_mapping.
    
    Returns:
        Type[nnunetv2.preprocessing.normalization.default_normalization_schemes.ImageNormalization]: The normalization scheme class corresponding to the provided channel_name. The function returns the class object (not an instantiated object). The returned class will usually be ZScoreNormalization or another ImageNormalization implementation registered in channel_name_to_normalization_mapping; callers are expected to instantiate or otherwise invoke the returned class according to the preprocessing workflow.
    
    Behavior and defaults:
    This function performs a dictionary lookup in the global channel_name_to_normalization_mapping using channel_name.casefold() to ensure case-insensitive matching. If no mapping entry is found for the provided channel name, the function defaults to returning ZScoreNormalization (the conventional fallback used by nnU-Net for intensity normalization). There are no side effects: the mapping is read-only with respect to this function and no global state is modified.
    
    Failure modes and errors:
    If channel_name is not a str (for example None or another type), calling channel_name.casefold() will raise an AttributeError; the function does not validate types beyond relying on the signature and Python string method. Unknown or unexpected channel name strings do not raise errors; they simply cause the function to return the default ZScoreNormalization class.
    """
    from nnunetv2.preprocessing.normalization.map_channel_name_to_normalization import get_normalization_scheme
    return get_normalization_scheme(channel_name)


################################################################################
# Source: nnunetv2.training.dataloading.utils.unpack_dataset
# File: nnunetv2/training/dataloading/utils.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for unpack_dataset because the docstring has no description for the argument 'folder'
################################################################################

def nnunetv2_training_dataloading_utils_unpack_dataset(
    folder: str,
    unpack_segmentation: bool = True,
    overwrite_existing: bool = False,
    num_processes: int = 8,
    verify: bool = False
):
    """Unpack all .npz files in a dataset folder into .npy arrays using a multiprocessing pool.
    
    This function is part of the nnU-Net v2 data preparation utilities and is used to convert dataset archives produced during dataset conversion or preprocessing into the flat numpy (.npy) files that the nnU-Net dataloading pipeline expects at training time. It locates all files with the .npz extension under the provided folder, and invokes the internal converter (_convert_to_npy) in parallel across multiple worker processes (multiprocessing.get_context("spawn").Pool). Typical use is to run this after dataset conversion so that the nnU-Net training dataloader can read per-sample numpy arrays instead of compressed archives, improving I/O patterns and integration with the rest of the preprocessing/training pipeline.
    
    Args:
        folder (str): Path to the dataset folder to scan for .npz files. All .npz files found under this folder are treated as belonging to the same dataset and will be passed to the unpacking routine. This folder should be the root or a directory that contains the dataset archives produced by nnU-Net dataset conversion; if the folder does not exist or is inaccessible, the function will raise an I/O error from the underlying filesystem operations.
        unpack_segmentation (bool): If True, instructs the converter to also unpack segmentation-related arrays contained in each .npz archive. In the nnU-Net context, segmentation arrays are the ground-truth label maps that the training pipeline needs for supervised learning. If False, only image/input arrays may be extracted depending on the archive contents. The value is forwarded to the internal _convert_to_npy function and controls which components are written out to disk.
        overwrite_existing (bool): If True, existing output files produced by prior unpacking attempts will be overwritten. If False, existing target .npy files will be left unchanged and the converter will typically skip re-writing them. Use True when you want to replace previously unpacked files (for example after changing preprocessing) and False to avoid unnecessary disk writes and to preserve earlier results.
        num_processes (int): Number of worker processes to spawn for parallel unpacking. This controls the size of the multiprocessing pool used to process multiple .npz files in parallel and therefore affects throughput, CPU utilization, and process startup overhead. A larger number increases parallelism but also increases memory and process-management overhead; set this according to available CPU cores and system memory.
        verify (bool): If True, request additional verification checks during unpacking. The flag is forwarded to the internal converter and typically triggers integrity checks such as validating that saved .npy files match the source contents or that critical fields exist in the .npz archive. Verification increases runtime and I/O but helps detect corrupted archives or incomplete writes.
    
    Returns:
        None: This function does not return a value. Its primary effect is to write unpacked .npy files (one or more per source .npz) to disk for each .npz file discovered in folder. It performs these writes via the internal _convert_to_npy routine executed in worker processes. On success, the dataset folder will contain the unpacked .npy files required by nnU-Net's dataloading and training pipelines.
    
    Behavior, side effects, and failure modes:
        The function searches for files with the ".npz" suffix using the project's subfiles utility and then dispatches conversions in parallel using multiprocessing.get_context("spawn").Pool. Because it uses spawned worker processes, there is process startup overhead and increased memory use proportional to num_processes. If no .npz files are found the function completes without writing files. If reading an .npz, writing an .npy, or a worker process fails (for example due to filesystem permission errors, corrupted archives, or out-of-memory conditions), the exception raised by the worker will propagate and may terminate the Pool; callers should handle such exceptions when integrating this function into preprocessing scripts. If overwrite_existing is False and target files already exist, those files will be preserved and conversion for those archives is typically skipped, reducing disk I/O. Verification (verify=True) slows execution but increases confidence in data integrity and is recommended when preparing datasets for long-running training runs in biomedical segmentation tasks where a corrupted training case would otherwise waste compute resources.
    """
    from nnunetv2.training.dataloading.utils import unpack_dataset
    return unpack_dataset(
        folder,
        unpack_segmentation,
        overwrite_existing,
        num_processes,
        verify
    )


################################################################################
# Source: nnunetv2.training.loss.dice.get_tp_fp_fn_tn
# File: nnunetv2/training/loss/dice.py
# Category: fix_args
# Reason: Missing type hints for some parameters
################################################################################

def nnunetv2_training_loss_dice_get_tp_fp_fn_tn(
    net_output: torch.Tensor,
    gt: torch.Tensor,
    axes: tuple = None,
    mask: torch.Tensor = None,
    square: bool = False
):
    """get_tp_fp_fn_tn(net_output, gt, axes=None, mask=None, square=False)
    Summary:
        Compute per-class true positives (TP), false positives (FP), false negatives (FN)
        and true negatives (TN) from network outputs and ground-truth labels for
        semantic segmentation tasks used in nnU-Net. This function is intended for
        probabilistic network outputs (soft predictions) and one-hot / label-map ground
        truth as produced or consumed by nnU-Net training and evaluation pipelines.
        It is useful for building Dice-like losses, metric computation, and any other
        logic that requires soft-counts of TP/FP/FN/TN aggregated over specified axes
        (for example spatial dimensions).
    
    Detailed behavior:
        net_output is treated as the model prediction for each class/channel and is
        multiplied with a derived one-hot ground-truth mask to obtain soft TP/FP/FN/TN
        contributions:
        TP = net_output * y_onehot
        FP = net_output * (~y_onehot)
        FN = (1 - net_output) * y_onehot
        TN = (1 - net_output) * (~y_onehot)
        This formulation treats net_output as the probability (or soft score) assigned
        to each class/channel; therefore values outside the interval [0, 1] will lead
        to unintuitive or invalid interpretations (e.g., negative FN/TN) and are not
        recommended. The one-hot encoding of ground truth is created without gradient
        tracking to save memory; subsequent multiplications with net_output will
        produce gradients only with respect to net_output (so gradients flow as
        expected into the network outputs).
    
        If mask is provided, it is broadcasted/tiled over the channel dimension and
        spatial dimensions and applied multiplicatively to TP/FP/FN/TN. This masks out
        invalid voxels/pixels (mask value 1 = valid, 0 = invalid) which is important
        in biomedical imaging pipelines (for example when ignoring padded regions or
        regions outside the field-of-view).
    
        If square is True, TP/FP/FN/TN are squared elementwise prior to any summation.
        This option can be used to implement squared-error style contributions or to
        change the emphasis of large errors before aggregation.
    
        The axes argument controls which dimensions are summed away. By default (axes
        is None) the function will sum across all spatial dimensions (i.e., all dims
        except batch and channel) which yields per-batch-per-class aggregated counts
        (shape typically (b, c)). If axes is an empty tuple () no summation is
        performed and the returned tensors keep the full shape of net_output (after
        any one-hot conversion). The device and final dtype follow the arithmetic:
        typically the returned tensors live on the same device as net_output and
        have a floating dtype derived from net_output.
    
    Args:
        net_output (torch.Tensor): Network outputs with shape (b, c, x, y(, z))). Each
            channel corresponds to a class and the values are treated as per-class
            soft scores or probabilities. In nnU-Net this is typically the output
            of a final softmax/sigmoid layer (floating-point tensor). Practical
            significance: these values encode the model's confidence about each
            class at every spatial location and are used to compute soft TP/FP/FN/TN.
        gt (torch.Tensor): Ground-truth labels. Either a label map with shape
            (b, 1, x, y(, z)) or (b, x, y(, z)) holding integer class indices per voxel,
            or a one-hot encoded tensor with shape (b, c, x, y(, z)). If gt has fewer
            dimensions than net_output the function will reshape gt to (b, 1, ...).
            Practical significance: gt provides the reference segmentation used to
            determine which voxels/pixels belong to each class when computing TP/FP/FN/TN.
        axes (tuple): Tuple of dimensions to sum over when aggregating TP/FP/FN/TN.
            If None (default) the function sets axes = tuple(range(2, net_output.ndim))
            which sums over all spatial dimensions and returns tensors aggregated over
            those spatial axes (typical result shape: (b, c)). If axes is an empty
            tuple () no summation is performed and the full spatial maps are returned.
            Practical significance: choose axes to control whether you want per-voxel
            maps or aggregated counts (for loss/metric computation).
        mask (torch.Tensor): Optional mask with shape (b, 1, x, y(, z))). The mask
            must use 1 for valid voxels/pixels and 0 for invalid ones. When provided,
            the mask is broadcasted to the channel dimension and applied multiplicatively
            to TP/FP/FN/TN. Practical significance: used in nnU-Net to ignore padding,
            slices with missing annotations, or other invalid regions during loss/metric
            computation.
        square (bool): If True, TP, FP, FN and TN are squared elementwise before
            summation. This changes the aggregation from a linear sum to a sum of
            squares (useful for designing alternative loss shapes or emphasis). Default
            is False.
    
    Failure modes, errors and side effects:
        - If gt contains integer class labels that are >= number of channels c in
          net_output, the scatter operation used to create a one-hot encoding will
          raise an error. Ensure gt values are in the range [0, c-1] when gt is a
          label map.
        - If net_output does not represent probabilities (values not in [0,1]) the
          interpretation of FN and TN using (1 - net_output) becomes invalid and can
          yield negative values; avoid passing arbitrary unbounded tensors.
        - If axes contains invalid dimension indices (outside [-net_output.ndim, net_output.ndim-1])
          or incompatible axes for the tensor rank, an exception will be raised by
          torch.sum.
        - The function does not modify the caller's net_output, gt or mask tensors in-place.
          It constructs internal tensors (one-hot encoding, tiled mask) without
          gradient tracking to reduce memory usage. Gradients flow back through
          net_output during downstream loss computations, but the internal one-hot and
          tiled mask creation is performed under torch.no_grad to avoid unnecessary
          autograd overhead.
        - Memory consideration: tiling the mask to match channels increases memory use;
          the implementation uses a tiled mask approach that is chosen for a small
          memory/speed trade-off in typical nnU-Net training scenarios.
    
    Returns:
        tp (torch.Tensor): True positive contributions. Per-element value equals
            net_output * y_onehot before any summation or squaring. After summation
            (when axes is not empty) this is the aggregated soft-count of predicted
            probability mass assigned to the true class. Typical shape with default
            axes: (b, c). Role: used directly for numerator terms in soft Dice losses
            or for computing sensitivity metrics.
        fp (torch.Tensor): False positive contributions. Per-element value equals
            net_output * (~y_onehot) and aggregates the probability mass the network
            assigned to a class where the ground truth is a different class. Typical
            shape with default axes: (b, c). Role: used for denominator terms and
            precision-related computations.
        fn (torch.Tensor): False negative contributions. Per-element value equals
            (1 - net_output) * y_onehot and aggregates the missing probability mass
            for the true class. Typical shape with default axes: (b, c). Role: used
            for computing recall-related terms and Dice denominators.
        tn (torch.Tensor): True negative contributions. Per-element value equals
            (1 - net_output) * (~y_onehot) and aggregates probability mass correctly
            assigned to non-true classes. Typical shape with default axes: (b, c).
            Role: can be useful for specificity calculations or loss diagnostics.
    
        All returned tensors are on the same device as net_output; their dtype is
        determined by the arithmetic with net_output (typically a floating dtype).
    """
    from nnunetv2.training.loss.dice import get_tp_fp_fn_tn
    return get_tp_fp_fn_tn(net_output, gt, axes, mask, square)


################################################################################
# Source: nnunetv2.utilities.collate_outputs.collate_outputs
# File: nnunetv2/utilities/collate_outputs.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for collate_outputs because the docstring has no description for the argument 'outputs'
################################################################################

def nnunetv2_utilities_collate_outputs_collate_outputs(outputs: List[dict]):
    """nnunetv2.utilities.collate_outputs.collate_outputs: Collate a list of per-step output dictionaries produced by nnU-Net training/validation steps into a single dictionary that aggregates values across the list. This function is used by the default train_step and validation_step implementations in nnU-Net to combine outputs (for example per-batch losses, numpy prediction arrays, or lists of identifiers) from multiple executions into a single structure suitable for epoch-level logging, metric computation, or further postprocessing in the semantic segmentation training pipeline.
    
    This collator expects a homogeneous list of dictionaries where each dictionary has the same keys and the same kind of value for each key. It is intentionally minimal: it handles three specific value types in a reproducible way and raises an error for other types. It performs no in-place modification of the input list; it returns a newly constructed dictionary. Because nnU-Net is applied to biomedical image segmentation tasks, typical use-cases include collating per-batch scalar losses into a list of loss values, stacking numpy prediction arrays into a batch axis for metric computation, or concatenating lists of case identifiers or file names produced during validation.
    
    Args:
        outputs (List[dict]): A non-empty list of dictionaries. Each dictionary corresponds to the outputs produced by a single train_step or validation_step call (for example from one batch, one device, or one sample). All dictionaries must have the same set of keys and each key must hold a value of one of the supported types described below. Typical dictionary entries in nnU-Net include scalars such as loss or metric values, numpy.ndarray predictions or probabilities, and lists such as lists of case identifiers or file names.
    
    Behavior details:
        - If the value associated with a key in outputs[0] is a scalar (detected via numpy.isscalar), the returned dictionary maps that key to a Python list containing the scalar value from each dictionary in the same order as in the inputs. This is useful for collecting per-batch scalar losses or numeric metrics across an epoch.
        - If the value associated with a key in outputs[0] is a numpy.ndarray, the function prepends a new first axis and vertically stacks these arrays using numpy.vstack on o[k][None] for each element o in outputs. The resulting numpy.ndarray has shape (N, ...) where N is len(outputs) and "..." are the original array dimensions. This is intended for aggregating per-step numpy prediction arrays (e.g., model outputs or probability maps) into a single array with an explicit leading index over steps.
        - If the value associated with a key in outputs[0] is a list, the function concatenates all lists from all dictionaries into a single Python list (flattening one level). This is useful for collecting lists of file names, case IDs, or other sequence-like metadata produced per step.
        - The function determines the handling behavior solely from the type of the value in the first dictionary (outputs[0]). It applies the chosen operation to the values for the same key across all dictionaries.
    
    Side effects and defaults:
        - No side effects on the input list or its dictionaries: the function constructs and returns a new dictionary (collated) and does not modify the entries of outputs.
        - The function relies on numpy being available (it uses numpy.isscalar and numpy.ndarray handling) and preserves numpy array dtypes and shapes when stacking.
        - There is no automatic type coercion: types are inspected using the first element and applied uniformly.
    
    Failure modes and exceptions:
        - IndexError: If outputs is empty, the function attempts to access outputs[0] and will raise IndexError. Callers must ensure outputs contains at least one dictionary.
        - KeyError: If dictionaries in outputs do not share the same set of keys, accessing o[k] for some o may raise KeyError. All dictionaries must contain the same keys.
        - ValueError: If the value type for a given key in outputs[0] is not one of the supported types (scalar as determined by numpy.isscalar, numpy.ndarray, or list), the function raises ValueError with a message indicating the unsupported type. Extend this function to add additional supported types if needed.
        - ValueError or other numpy errors: If numpy.ndarray values are present but their shapes are incompatible for stacking (for example differing shapes beyond the prepended axis), numpy.vstack will raise a ValueError; this indicates that per-step arrays for that key must have consistent shapes before collating.
        - Type inconsistencies between output dictionaries (for example the first dictionary has a numpy.ndarray for key 'pred' but another dictionary has a list for 'pred') will likely result in runtime errors or incorrect aggregation because the collator uses the type of outputs[0] to choose the aggregation strategy.
    
    Returns:
        dict: A newly constructed dictionary with the same keys as outputs[0]. For each key:
            - If the original per-step value was a scalar (numpy.isscalar), the value is a Python list of scalars with length len(outputs), preserving the order of inputs. This is typically used to collect per-batch numeric metrics (e.g., loss values) for epoch-level analysis.
            - If the original per-step value was a numpy.ndarray, the value is a numpy.ndarray created by stacking each per-step array along a new leading axis; its first dimension equals len(outputs). This is typically used to aggregate per-step model outputs or probability maps into a single array for metric computation or ensemble operations.
            - If the original per-step value was a list, the value is a flattened Python list containing all elements from each per-step list in input order. This is typically used to aggregate lists of case identifiers, file paths, or other per-step metadata.
    
    Notes for extension:
        - This function is intentionally small and is the default collator for nnU-Net. If your train_step or validation_step returns custom types (for example torch.Tensor, custom dataclasses, or other container types), extend this function to add explicit handling for those types, taking care to maintain consistent behavior across all dictionaries in the inputs.
    """
    from nnunetv2.utilities.collate_outputs import collate_outputs
    return collate_outputs(outputs)


################################################################################
# Source: nnunetv2.utilities.overlay_plots.select_slice_to_plot
# File: nnunetv2/utilities/overlay_plots.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for select_slice_to_plot because the docstring has no description for the argument 'image'
################################################################################

def nnunetv2_utilities_overlay_plots_select_slice_to_plot(
    image: numpy.ndarray,
    segmentation: numpy.ndarray
):
    """nnunetv2.utilities.overlay_plots.select_slice_to_plot selects a representative axial slice index from a 3D segmentation volume for visualization (overlay) by finding the slice that contains the largest amount of foreground voxels. This function is used in the nnU-Net visualization utilities to pick a single 2D slice from a 3D case (for example, a CT or MRI volume) so that overlays of model predictions and image intensities show the most content-bearing slice. The image parameter is accepted to preserve a stable API and allow future replacement logic that may use image intensities; in the current implementation image is not used.
    
    Args:
        image (numpy.ndarray): The 3D image volume corresponding to the segmentation. Expected to be a 3D array with shape (num_slices, height, width) where axis 0 indexes axial slices. In the nnU-Net context this is typically a medical image volume (e.g., CT or MRI) provided to overlay plotting code. This argument is currently unused by the function but is required in the signature so callers that provide both image and segmentation (as the overlay pipeline does) remain compatible with possible future selection strategies that depend on image intensity or modality.
        segmentation (numpy.ndarray): A 3D segmentation volume with the same slice ordering as image. Foreground is defined as voxels where segmentation != 0; all non-zero labels are treated equally (the function ignores the actual label values). The function sums foreground voxels per slice along axes 1 and 2 (height and width) to compute the amount of foreground in each axial slice.
    
    Behavior and side effects: The function computes a boolean foreground mask (segmentation != 0), sums that mask per slice to obtain foreground counts, and returns the index of the slice with the maximum count using numpy.argmax. There are no side effects: neither image nor segmentation are modified. The function is deterministic given the same inputs.
    
    Defaults and tie-breaking: If multiple slices have the same maximal foreground count, numpy.argmax returns the first such slice (lowest index), and that index is returned. If the segmentation contains no foreground voxels (all zeros), the foreground counts per slice are all zero and the function returns 0 (the first slice).
    
    Failure modes and errors: The function expects both inputs to be numpy.ndarray objects representing 3D volumes. If segmentation does not have at least three dimensions (ndim < 3) or its axes 1 and 2 cannot be summed over, numpy will raise an exception (for example numpy.AxisError); callers should ensure segmentation is a 3D array with shape (num_slices, height, width). The function does not validate that image and segmentation have identical shapes; if later code assumes such equality, mismatched shapes may cause errors elsewhere.
    
    Returns:
        int: Zero-based index of the selected axial slice (index along axis 0). This index identifies the slice with the largest number of foreground voxels (segmentation != 0) and is intended for use in overlay plotting within the nnU-Net visualization utilities.
    """
    from nnunetv2.utilities.overlay_plots import select_slice_to_plot
    return select_slice_to_plot(image, segmentation)


################################################################################
# Source: nnunetv2.utilities.overlay_plots.select_slice_to_plot2
# File: nnunetv2/utilities/overlay_plots.py
# Category: fix_docstring
# Reason: Schema parsing failed: Cannot generate JSON schema for select_slice_to_plot2 because the docstring has no description for the argument 'image'
################################################################################

def nnunetv2_utilities_overlay_plots_select_slice_to_plot2(
    image: numpy.ndarray,
    segmentation: numpy.ndarray
):
    """nnunetv2.utilities.overlay_plots.select_slice_to_plot2 selects a single slice index (along the first axis) from a 3D image/segmentation pair for visualization. It is intended for nnU-Net overlay plotting utilities to choose a representative 2D slice that contains the largest amount of foreground across all non-background classes (useful when creating overlay plots of predicted or ground-truth segmentations for medical image segmentation tasks).
    
    Args:
        image (numpy.ndarray): The image volume used to determine the number of slices and coordinate convention. Expected to be 3D with shape (S, H, W) where S is the number of axial slices (the function also accepts a shape like (1, H, W) for a single-slice volume). The image array itself is not inspected for intensity content by this implementation; only image.shape[0] (the slice count) is used. It is the caller's responsibility to ensure image and segmentation correspond spatially (same first dimension and slice ordering).
        segmentation (numpy.ndarray): The label volume with the same slice axis as image, expected to be 3D with shape (S, H, W). Segmentation values <= 0 are treated as background and ignored; positive distinct label values are treated as separate foreground classes. The function computes, for each positive class, the per-slice voxel counts and then converts these to relative proportions (percent of that class that is present in each slice). These per-class per-slice proportions are averaged across classes to obtain a single score per slice.
    
    Behavior, defaults, side effects, and failure modes:
        - The function computes the set of foreground classes using pandas' unique on segmentation.ravel(), sorts them, and filters out non-positive values. Only those positive classes contribute to the selection criterion.
        - For each foreground class c, the function computes fg_mask = (segmentation == c) and counts voxels per slice by summing over axes (1, 2). Those per-slice counts are normalized by the total count of foreground voxels across all slices and classes (fg_per_slice.sum()) to produce a per-class distribution across slices. The per-class distributions are then averaged across classes to produce a final per-slice score.
        - The selected slice index is the integer index (0-based) of the slice with the maximum averaged per-class proportion (np.argmax). The returned index lies in [0, image.shape[0] - 1] under normal conditions.
        - If segmentation contains no positive labels (no foreground classes), the intermediate normalization will divide by zero and produce NaN values; the mean across classes yields NaNs for all slices and np.argmax will return 0 in current NumPy behavior but a RuntimeWarning will be emitted. Callers should check for an all-background segmentation before calling if they require deterministic handling of that case.
        - If image.shape[0] != segmentation.shape[0], the per-slice operations assume matching slice counts and will produce incorrect results or raise indexing/shape errors; the caller must ensure both volumes share the same first dimension and slice ordering.
        - If image.shape[0] == 0 (zero slices), the function will attempt operations on empty arrays and np.argmax will raise ValueError ("attempt to get argmax of an empty sequence"). The caller must avoid passing empty volumes.
        - There are no other side effects: the function does not modify image or segmentation in place.
    
    Returns:
        int: The 0-based index of the slice (along the first axis) selected for plotting. This index corresponds to the slice with the highest average per-class foreground proportion and is intended to be used by overlay plotting routines in nnU-Net to display a representative 2D slice.
    """
    from nnunetv2.utilities.overlay_plots import select_slice_to_plot2
    return select_slice_to_plot2(image, segmentation)


from typing import Dict, Any


def get_tools() -> Dict[str, Dict[str, Any]]:
    """Extract JSON schemas for all functions in this module."""
    import sys
    import os
    
    # Add project root to path to import our json_schema module
    # Try multiple possible paths
    possible_paths = [
        os.path.join(os.path.dirname(__file__), '..', '..', 'utils'),
        '/app/utils',
        '/app/project/utils',
    ]
    
    json_schema_path = None
    for path in possible_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(os.path.join(abs_path, 'json_schema.py')):
            if abs_path not in sys.path:
                sys.path.insert(0, abs_path)
            json_schema_path = abs_path
            break
    
    if json_schema_path:
        from json_schema import get_json_schema
    else:
        # Fallback to transformers if our module not found
        from transformers.utils import get_json_schema
    
    tools = {}
    failed_count = 0
    
    for name, func in get_lib().items():
        try:
            tools[name] = get_json_schema(func)
        except Exception as e:
            failed_count += 1
            # Only print first few errors to avoid spam
            if failed_count <= 3:
                print(f"Failed to get schema for {name}: {type(e).__name__}: {e}", file=sys.stderr)
            continue
    
    if failed_count > 0:
        print(f"Warning: Failed to extract schemas for {failed_count} out of {len(get_lib())} functions", file=sys.stderr)
    
    return tools


def get_lib():
    """Get all functions defined in this module."""
    import inspect
    global_vars = inspect.currentframe().f_globals
    
    functions = {
        name: obj for name, obj in global_vars.items()
        if inspect.isfunction(obj) and obj.__module__ == __name__
    }
    functions.pop("get_lib", None)
    functions.pop("get_tools", None)
    return functions
