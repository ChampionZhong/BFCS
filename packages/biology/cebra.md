# cebra

[üîô Back to Main Repo](../../../README.md) | [üîó Original Repo](https://github.com/AdaptiveMotorControlLab/CEBRA)

![Tool Count](https://img.shields.io/badge/Agent_Tools-14-blue?style=flat-square)
![Category](https://img.shields.io/badge/Category-Biology-green?style=flat-square)
![Status](https://img.shields.io/badge/Import_Test-Passed-success?style=flat-square)

## üìñ Overview

**cebra** is a PyTorch-based self-supervised learning library that learns consistent low-dimensional embeddings from high-dimensional time-series recordings (e.g., neural data) using auxiliary variables such as behavior for label-informed analysis, clustering, and decoding.

> **Note**: This documentation lists the **agent-ready wrapper functions** generated for this package. These functions have been strictly typed, docstring-enhanced, and tested for import stability within a standardized Apptainer environment.

## üõ†Ô∏è Available Agent Tools

Below is the list of **14** functions optimized for LLM tool-use.

| **Tool Name (Wrapper)**   | **Source**          | **File Path**     | **Arguments (Type)**        | **Description**                |
| ------------------------- | ------------------- | ----------------- | --------------------------- | ------------------------------ |
| `cebra_data_assets_calculate_checksum` | `cebra.data.assets.calculate_checksum` | `cebra/data/assets.py` | `file_path: str` | `cebra.data.assets.calculate_checksum calculates the MD5 checksum (hexadecimal string) of a file's raw bytes for use in CEBRA's data and asset management workflows, enabling integrity checks of dataset files, model weight files, and other binary assets used in neuroscience and behavioral-data experiments. This function reads the specified file in binary mode using a fixed 4096-byte chunk size and incrementally updates an MD5 digest. The incremental, chunked read is memory-efficient and suitable for large files commonly encountered in CEBRA pipelines (for example, large recordings, preprocessed datasets, or serialized model checkpoints). The returned value is a deterministic lowercase hexadecimal string representation of the MD5 digest for the exact file contents.` |
| `cebra_data_assets_download_file_with_progress_bar` | `cebra.data.assets.download_file_with_progress_bar` | `cebra/data/assets.py` | `url: str, expected_checksum: str, location: str, file_name: str, retry_count: int = 0` | `Download a file from a given URL with a tqdm progress bar and MD5 checksum verification. This function is used by the CEBRA library to fetch remote dataset assets (for example, neuroscience or behavioral data files referenced in the README) and to ensure file integrity for reproducible analyses. It first checks for an existing local file at the provided location and file_name and verifies its MD5 checksum using calculate_checksum. If the local file is absent or has a mismatched checksum, the function issues a streamed HTTP GET request to url, expects the server to provide a Content-Disposition header containing the filename, writes the response to disk in chunks (1024 bytes) while updating a tqdm progress bar (units in bytes), computes an MD5 checksum on the fly, compares it against expected_checksum, and either returns the downloaded file path on success or deletes the corrupt file and retries the download up to a configured global _MAX_RETRY_COUNT. The function has important side effects: it creates the destination directory (including parents), writes the downloaded file, may delete files that fail checksum verification, and emits warnings and printed messages for progress and failures. The checksum algorithm used is MD5 (hashlib.md5), so expected_checksum must be the hex MD5 digest corresponding to the file content.` |
| `cebra_data_load_read_hdf` | `cebra.data.load.read_hdf` | `cebra/data/load.py` | `filename: str, key: str = None` | `Read an HDF5 file and return its contents as a pandas.DataFrame for use by the CEBRA library. This function is intended to load tabular datasets commonly used in CEBRA workflows (for example, neural recordings, behavioral annotations, or other time-series and metadata stored in HDF5 tables). It attempts to read the requested key using pandas.read_hdf and, if that fails, falls back to reading with h5py and converting the stored dataset into a pandas.DataFrame. The loaded DataFrame is what downstream CEBRA components (embedding training, decoding, preprocessing) expect as input.` |
| `cebra_datasets_get_datapath` | `cebra.datasets.get_datapath` | `cebra/datasets/__init__.py` | `path: str = None` | `Convert a relative dataset path into the system-dependent absolute data path used by CEBRA. This function is used throughout the CEBRA library to compute filesystem locations of datasets and other data files that the library and its dataset-loading utilities expect to find under a common data root. The root directory is determined by the helper get_data_root(), which in turn can be configured by the environment variable CEBRA_DATADIR; therefore, this function enforces the convention that dataset paths are resolved relative to that root. When given a non-None argument, the function converts the provided path to a string and joins it with the data root using os.path.join, producing the path that CEBRA will use to open or list dataset files. This function does not read or validate filesystem contents; it only computes the pathname string that other CEBRA functions will use.` |
| `cebra_datasets_set_datapath` | `cebra.datasets.set_datapath` | `cebra/datasets/__init__.py` | `path: str = None, update_environ: bool = True` | `cebra.datasets.set_datapath sets the global root data directory used by the cebra.datasets module and (by implementation) updates the process environment variable CEBRA_DATADIR so other parts of the library and external tools that read this environment variable will use the new directory when locating dataset files. This function is intended to change where CEBRA looks for and stores dataset files (common in neuroscience and biology workflows described in the project README). It performs validation on the supplied path and has observable side effects at the module and process level: it assigns the module-level __DATADIR variable and writes to os.environ["CEBRA_DATADIR"].` |
| `cebra_datasets_make_neuropixel_read_neuropixel` | `cebra.datasets.make_neuropixel.read_neuropixel` | `cebra/datasets/make_neuropixel.py` | `path: str = "/shared/neuropixel/*/*.nwb", cortex: str = "VISp", sampling_rate: float = 120.0` | `Load Neuropixels recordings for the "movie1" stimulus, filter units by recording area and quality, and convert spike times into binned spike-count matrices per session. This function is used in the CEBRA library to prepare Neuropixels neural data (originally recorded at high temporal resolution) into time-binned spike-count representations aligned to movie frames for downstream embedding, decoding, and joint behavioral/neural analysis. It searches for .nwb files matching a glob path, reads required datasets using h5py, applies area and quality filters via internal helper functions (_spikes_by_units, _filter_units, _get_area, _get_movie1, _spike_counts), and constructs a dictionary of per-session spike-count matrices together with per-timepoint movie-frame indices.` |
| `cebra_datasets_poisson_sample_parallel` | `cebra.datasets.poisson.sample_parallel` | `cebra/datasets/poisson.py` | `spike_rates: torch.Tensor, refractory_period: float = 0.0, n_jobs: int = 10` | `cebra.datasets.poisson.sample_parallel: Generate spike counts from input spike rates using parallelized sampling across neurons.` |
| `cebra_datasets_save_dataset_save_allen_dataset` | `cebra.datasets.save_dataset.save_allen_dataset` | `cebra/datasets/save_dataset.py` | `savepath: str = "data/allen_preload/"` | `cebra.datasets.save_dataset.save_allen_dataset loads and saves a precomputed subset of the Allen Institute dataset used for calcium (Ca) imaging experiments in the CEBRA framework to reduce data-loading time for downstream decoding and embedding tasks. This function iterates over a fixed set of neuron sample sizes and random seeds to initialize dataset objects via cebra.datasets.init and persist their neural recordings to disk. It is intended as a data-preloading utility for experiments that use the shared Allen decoding data in CEBRA (neural + behavioral analysis and self-supervised embedding workflows). The saved files contain only the neural data under the "neural" key and use a filename pattern derived from the dataset name, allowing experiments to quickly load pretrained subsets rather than querying the full remote/raw source repeatedly.` |
| `cebra_datasets_save_dataset_save_allen_decoding_dataset` | `cebra.datasets.save_dataset.save_allen_decoding_dataset` | `cebra/datasets/save_dataset.py` | `savepath: str = "data/allen_preload/"` | `Save and persist preprocessed Allen Institute "decoding" calcium-imaging datasets for use with CEBRA experiments. This function iterates over a predefined set of neuron subsample sizes, random seeds for neuron sampling, and train/test splits and uses cebra.datasets.init to load the corresponding Allen decoding dataset for calcium imaging ("ca"). For each combination it serializes and writes the loaded dataset's neural recording array to disk using joblib (jl.dump) as a dictionary with key "neural". The files are named using the pattern "allen-movie1-ca-decoding-{n}-{split_flag}-{seed}.jl" and are written under the provided savepath directory. This pre-saving of dataset variants is intended to reduce data loading time when running downstream decoding or embedding experiments with the CEBRA library (a toolkit for learning consistent latent embeddings of neural and behavioral recordings, as described in the project README).` |
| `cebra_datasets_save_dataset_save_monkey_dataset` | `cebra.datasets.save_dataset.save_monkey_dataset` | `cebra/datasets/save_dataset.py` | `savepath: str = "data/monkey_reaching_preload_smth_40/"` | `cebra.datasets.save_dataset.save_monkey_dataset Load and save the monkey reaching dataset to disk for use in CEBRA experiments that jointly analyze neural and behavioral data. This function iterates over all supported session types ("active", "passive", "all") and data splits ("all", "train", "valid", "test"), calls monkey_reaching._load_data(session=session, split=split) to load the neural and behavioral data for each combination, and serializes each dataset to a file named "{session}_{split}.jl" under the provided save directory. Its primary practical purpose in the CEBRA project is to reduce data-loading time for downstream self-supervised embedding experiments by materializing shared preprocessed data to disk so multiple experiments can reuse the saved files instead of repeatedly reloading and preprocessing raw recordings.` |
| `cebra_helper_download_file_from_url` | `cebra.helper.download_file_from_url` | `cebra/helper.py` | `url: str` | `Download a file from a remote URL and save it to a local temporary path with an ".h5" suffix, returning the path to the saved file. This helper is used in the CEBRA library to fetch remote dataset files and model/data artifacts (for example, HDF5 recordings of neural or behavioral data) so they can be opened by downstream data loaders or analysis routines.` |
| `cebra_helper_download_file_from_zip_url` | `cebra.helper.download_file_from_zip_url` | `cebra/helper.py` | `url: str, file: str` | `Directly download a ZIP archive from a remote URL, extract its members into a temporary directory, and return a pathlib.Path pointing to the requested file inside the archive's top-level "data" folder. This helper is used by CEBRA to fetch remote dataset archives (for example, neural or behavioral recording archives used in demos and data loaders) so higher-level code can access a particular file inside the archive without manually saving the ZIP to disk. Note: the current implementation creates a TemporaryDirectory, reads the entire archive into memory, extracts members with zipfile.ZipFile, and returns a path constructed as <temporary-directory>/data/<file>.` |
| `cebra_integrations_sklearn_utils_check_device` | `cebra.integrations.sklearn.utils.check_device` | `cebra/integrations/sklearn/utils.py` | `device: str` | `Select and normalize a PyTorch compute device string based on the requested device and the execution environment. This utility is used by CEBRA's scikit-learn integration to choose a compute device for PyTorch models, tensors, and training/inference workflows involved in self-supervised embedding estimation. It interprets the caller-provided device hint (a string) and returns a canonical device string that callers can pass to torch APIs or CEBRA components. The function checks CUDA availability, per-GPU indices, Apple Metal Performance Shaders (MPS) availability via cebra.helper._is_mps_availabe(torch) and torch.backends.mps, and otherwise falls back to CPU. The returned string should be treated as authoritative for device placement decisions in downstream CEBRA training, evaluation, and embedding computation.` |
| `cebra_io_reduce` | `cebra.io.reduce` | `cebra/io.py` | `data: numpy.ndarray, ratio: float = None, num_components: int = None` | `cebra.io.reduce maps high-dimensional sample recordings (for example neural or behavioral trial windows used in the CEBRA pipeline) to a lower-dimensional representation using principal component analysis (PCA). It fits sklearn.decomposition.PCA to the provided samples and returns the PCA-transformed data; the output dimensionality is determined either by an explained-variance threshold (ratio) or by an explicit number of principal components (num_components). In the CEBRA workflow this function is typically used as a preprocessing dimensionality reduction step to obtain a compact input representation for downstream embedding, decoding, or visualization of neural and behavioral recordings. The function reshapes the input data to (N, F) using data.reshape(len(data), -1) so that each of the len(data) entries is treated as one sample and all remaining axes are flattened into features. PCA is constructed by passing num_components to sklearn.decomposition.PCA; if num_components is None, sklearn's PCA default behavior for n_components applies. The function validates inputs and can raise ValueError for missing or out-of-range parameters; when ratio is provided the function selects components based on the cumulative explained variance reported by PCA and returns the corresponding slice of the transformed data (see failure modes below for edge cases).` |

## ‚öñÔ∏è License

Original Code License: Apache-2.0

Wrapper Code & Documentation: Apache-2.0

*This file was automatically generated on February 26, 2026.*
