# tape

[üîô Back to Main Repo](../../../README.md) | [üîó Original Repo](https://github.com/songlab-cal/tape)

![Tool Count](https://img.shields.io/badge/Agent_Tools-7-blue?style=flat-square)
![Category](https://img.shields.io/badge/Category-Biology-green?style=flat-square)
![Status](https://img.shields.io/badge/Import_Test-Passed-success?style=flat-square)

## üìñ Overview

`tape` (Tasks Assessing Protein Embeddings) is a PyTorch-based benchmark suite that provides datasets, pretrained protein language models, and evaluation code for training and assessing protein sequence embeddings across multiple downstream prediction tasks.

> **Note**: This documentation lists the **agent-ready wrapper functions** generated for this package. These functions have been strictly typed, docstring-enhanced, and tested for import stability within a standardized Apptainer environment.

## üõ†Ô∏è Available Agent Tools

Below is the list of **7** functions optimized for LLM tool-use.

| **Tool Name (Wrapper)**   | **Source**          | **File Path**     | **Arguments (Type)**        | **Description**                |
| ------------------------- | ------------------- | ----------------- | --------------------------- | ------------------------------ |
| `tape_models_file_utils_cached_path` | `tape.models.file_utils.cached_path` | `tape/models/file_utils.py` | `url_or_filename: str, force_download: bool = False, cache_dir: str = None` | `tape.models.file_utils.cached_path: Determine whether an input is a URL or a local file path; if it is a URL, download and cache the remote file (using the repository's model/data cache) and return the local cached file path; if it is a local path, validate existence and return it. This function is used throughout TAPE to ensure pretrained model weights, tokenizer files, and dataset artifacts referenced by URL (for example when calling model.from_pretrained, tape-embed, or other helpers documented in the README) are available locally. For HTTP(S) and S3 URLs the file is downloaded and stored in a cache directory so subsequent calls reuse the cached copy. For local paths the function verifies the file exists and returns the same path. pathlib.Path objects are accepted on Python 3 and are converted to strings before processing.` |
| `tape_models_file_utils_filename_to_url` | `tape.models.file_utils.filename_to_url` | `tape/models/file_utils.py` | `filename: str, cache_dir: str = None` | `Return the remote URL and stored ETag associated with a cached pretrained-model file name. This function is used by TAPE's model-loading and caching logic to map a local cached filename (under the module's pretrained models cache) to the original download URL and the HTTP ETag value recorded when the file was cached. The URL is used to re-download or verify the origin of a pretrained protein model (for example, transformer or UniRep weights listed in the TAPE README) and the ETag is used to determine whether a cached copy is current; the ETag may be None if no ETag was recorded for that file.` |
| `tape_models_file_utils_get_from_cache` | `tape.models.file_utils.get_from_cache` | `tape/models/file_utils.py` | `url: str, cache_dir: str = None, force_download: bool = False, resume_download: bool = False` | `tape.models.file_utils.get_from_cache retrieves a file referenced by a URL and ensures a stable local cached copy for use by the TAPE benchmark code (for example pretrained model weights, dataset archives, or other artifacts used when embedding, training, or evaluating protein models). This function locates a cached copy of the resource identified by url in cache_dir (or in the repository-wide PROTEIN_MODELS_CACHE when cache_dir is None). If a suitable cached copy is not present or force_download is True, it downloads the resource to a temporary file and atomically moves it into the cache. The cached filename incorporates the HTTP/S3 ETag when available so that different versions of the same URL are stored separately; when no ETag is available the function attempts to find previously downloaded candidates by filename pattern. The function also writes a small metadata JSON file next to the cached file containing the original URL and the ETag. A file-based lock prevents simultaneous parallel downloads of the same resource across processes. The function is used throughout TAPE to ensure pretrained models and data are automatically downloaded and reused (see the README section on loading pretrained models and embedding proteins).` |
| `tape_models_file_utils_s3_etag` | `tape.models.file_utils.s3_etag` | `tape/models/file_utils.py` | `url: str` | `tape.models.file_utils.s3_etag ‚Äî Return the ETag of an S3 object identified by a URL. This function obtains the ETag metadata for an object stored on Amazon S3. In the TAPE codebase this is used when interacting with datasets and pretrained model artifacts hosted on AWS (see README references to data hosted on AWS and automatic downloading of pretrained models). The ETag is commonly used for simple cache validation or to detect whether a remote S3 object has changed, so callers typically use this function to decide whether to re-download or to validate a cached copy. The function performs a network request via boto3 to read object metadata; it does not download the object contents.` |
| `tape_models_file_utils_split_s3_path` | `tape.models.file_utils.split_s3_path` | `tape/models/file_utils.py` | `url: str` | `Split a full S3 URL into the bucket identifier and the object key path. This function is used throughout TAPE when parsing S3 locations for downloading data, pretrained model weights, or saving artifacts to AWS S3: it extracts the bucket component that identifies the S3 bucket and the relative object key that identifies the object within that bucket (the value typically passed to S3 API calls such as get_object or put_object). The function performs only string parsing (no network access) and normalizes the returned path by removing a leading '/' if present so that the result can be used directly as an S3 object key.` |
| `tape_models_file_utils_url_to_filename` | `tape.models.file_utils.url_to_filename` | `tape/models/file_utils.py` | `url: str, etag: str = None` | `tape.models.file_utils.url_to_filename converts a URL (and optional ETag) into a deterministic hashed filename used by TAPE to cache downloaded resources such as pretrained model weights. In the TAPE workflow (see README), pretrained models and other remote assets are automatically downloaded and stored in a local cache; this function maps the original remote identifier (the URL) and an optional HTTP ETag to a repeatable filename suitable for use as a cache key.` |
| `tape_models_modeling_utils_gelu` | `tape.models.modeling_utils.gelu` | `tape/models/modeling_utils.py` | `x: torch.Tensor` | `Implementation of the Gaussian Error Linear Unit (GELU) activation used in TAPE model implementations. This function computes the GELU nonlinearity as used in many deep learning models provided in the TAPE repository (for example Transformer, LSTM, ResNet, UniRep and trRosetta implementations described in the README). GELU is a smooth, non-linear activation that weights each element of the input tensor by the Gaussian cumulative distribution function; it is commonly used in modern transformer-style architectures and language models to provide improved optimization and representational properties compared to hard rectifiers. The implementation follows the exact mathematical form: x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0))) which is equivalent to x * Phi(x) where Phi is the standard normal CDF expressed via the error function. See Hendrycks & Gimpel (2016) for the original GELU description: https://arxiv.org/abs/1606.08415 Note: an alternative approximation used in some implementations (for example OpenAI GPT) is 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3))) which is numerically different and can produce slightly different model outputs; the TAPE codebase uses the erf-based exact form above.` |

## ‚öñÔ∏è License

Original Code License: BSD-3-Clause

Wrapper Code & Documentation: Apache-2.0

*This file was automatically generated on February 26, 2026.*
