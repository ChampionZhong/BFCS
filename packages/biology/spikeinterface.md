# spikeinterface

[üîô Back to Main Repo](../../../README.md) | [üîó Original Repo](https://github.com/SpikeInterface/spikeinterface)

![Tool Count](https://img.shields.io/badge/Agent_Tools-159-blue?style=flat-square)
![Category](https://img.shields.io/badge/Category-Biology-green?style=flat-square)
![Status](https://img.shields.io/badge/Import_Test-Passed-success?style=flat-square)

## üìñ Overview

SpikeInterface is a unified Python framework for extracellular electrophysiology spike sorting that reads/writes many data formats, preprocesses recordings, runs and benchmarks multiple spike sorters, and provides postprocessing, quality metrics, visualization, and export tools.

> **Note**: This documentation lists the **agent-ready wrapper functions** generated for this package. These functions have been strictly typed, docstring-enhanced, and tested for import stability within a standardized Apptainer environment.

## üõ†Ô∏è Available Agent Tools

Below is the list of **159** functions optimized for LLM tool-use.

| **Tool Name (Wrapper)**   | **Source**          | **File Path**     | **Arguments (Type)**        | **Description**                |
| ------------------------- | ------------------- | ----------------- | --------------------------- | ------------------------------ |
| `spikeinterface_comparison_comparisontools_compare_spike_trains` | `spikeinterface.comparison.comparisontools.compare_spike_trains` | `spikeinterface/comparison/comparisontools.py` | `spiketrain1: numpy.ndarray, spiketrain2: numpy.ndarray, delta_frames: int = 10` | `Compares two spike trains and labels each spike as a true positive (TP), false negative (FN), or false positive (FP) for benchmarking spike sorting outputs. This function is intended for use in the SpikeInterface comparison/benchmarking workflow where spiketrain1 is treated as the ground-truth spike times and spiketrain2 is treated as the spike times produced by a sorter or another detection method. The function finds one-to-one matches between spikes in spiketrain1 and spikes in spiketrain2 within a symmetric time window defined by delta_frames (the effective half-width is delta_frames // 2 frames). If multiple spikes in spiketrain2 fall within the matching window of a single ground-truth spike, only the first unmatched spiketrain2 spike is paired (the others remain labeled as false positives). The comparison uses integer-frame arithmetic for spiketrain2 by casting it to int via numpy.astype(int), so inputs should generally represent spike times in frame indices.` |
| `spikeinterface_comparison_comparisontools_compute_agreement_score` | `spikeinterface.comparison.comparisontools.compute_agreement_score` | `spikeinterface/comparison/comparisontools.py` | `num_matches: int, num1: int, num2: int` | `spikeinterface.comparison.comparisontools.compute_agreement_score computes an agreement score between two spike trains or spike-sorting units. In the SpikeInterface domain this function is used when comparing and benchmarking spike sorting outputs (for example, when matching units from two sortings or comparing a sorter output to ground truth) to quantify how many events (spikes) are shared relative to the total distinct events across both spike trains.` |
| `spikeinterface_comparison_comparisontools_count_match_spikes` | `spikeinterface.comparison.comparisontools.count_match_spikes` | `spikeinterface/comparison/comparisontools.py` | `times1: numpy.ndarray, all_times2: list, delta_frames: int` | `Computes, for one reference spike train, the number of matching spikes found in each spike train of a second collection of spike trains. This function is used in the spike sorting comparison and benchmarking workflows of SpikeInterface to quantify how many spikes from a single unit (times1) are matched by units from another sorting (all_times2). For each spike train in all_times2 the function calls count_matching_events(times1, times2, delta=delta_frames) to count spikes in times2 that occur within a tolerance of delta_frames (measured in sample frames) of spikes in times1. The result is an array of integer counts with one entry per element of all_times2 in the same order. The function has no side effects on its inputs and is intended for use in comparing sorted outputs, computing per-unit overlap, and building confusion/matching matrices for benchmarking.` |
| `spikeinterface_comparison_comparisontools_do_confusion_matrix` | `spikeinterface.comparison.comparisontools.do_confusion_matrix` | `spikeinterface/comparison/comparisontools.py` | `event_counts1: dict, event_counts2: dict, match_12: dict, match_event_count: numpy.ndarray` | `Compute the confusion matrix between one ground-truth sorting and another sorting. This function is used in SpikeInterface's comparison and benchmarking workflow to aggregate per-unit true positive, false negative, and false positive event counts when comparing two spike-sortings (for example, a ground-truth sorting versus an automated sorter). It takes per-unit event counts for each sorting, a mapping from units in sorting1 to units in sorting2 (the match), and a matrix of matched-event counts between units, and returns a pandas DataFrame that encodes for each pair of matched units the number of matched events and, for each unit, the remaining false negatives (FN) and false positives (FP). The resulting confusion matrix is suitable for downstream quality metrics, summaries, and reports in the SpikeInterface benchmarking pipeline.` |
| `spikeinterface_comparison_comparisontools_do_count_score` | `spikeinterface.comparison.comparisontools.do_count_score` | `spikeinterface/comparison/comparisontools.py` | `event_counts1: dict, event_counts2: dict, match_12: dict, match_event_count: dict` | `Do a per-ground-truth-unit count of true positives (tp), false negatives (fn), false positives (fp) and related bookkeeping used when comparing two spike sortings (for example a ground-truth sorting vs a tested sorting) within the SpikeInterface comparison workflow. This function iterates over each unit in event_counts1 (treated as the ground-truth units) and uses the provided mapping from sorting1 to sorting2 and the precomputed match event counts to compute, for each ground-truth unit, how many events were correctly matched (tp), missed (fn), and extra in the tested unit (fp). The result is a pandas DataFrame with one row per ground-truth unit and columns that summarize counts necessary for downstream accuracy/score computations and benchmarking of spike sorting outputs.` |
| `spikeinterface_comparison_comparisontools_make_agreement_scores_from_count` | `spikeinterface.comparison.comparisontools.make_agreement_scores_from_count` | `spikeinterface/comparison/comparisontools.py` | `match_event_count: numpy.ndarray, event_counts1: numpy.ndarray, event_counts2: numpy.ndarray` | `Compute pairwise agreement (Jaccard-like) scores between units from two spike-sorting outputs using a precomputed matrix of matched event counts. This function is used in the comparison/benchmarking stage of SpikeInterface to quantify how well units from two sortings correspond to each other: each score expresses the fraction of shared events relative to the union of events for the two units being compared. It implements the same agreement definition used by make_agreement_scores but accepts a precomputed match-event-count matrix to avoid recomputing matches.` |
| `spikeinterface_comparison_comparisontools_make_matching_events` | `spikeinterface.comparison.comparisontools.make_matching_events` | `spikeinterface/comparison/comparisontools.py` | `times1: numpy.ndarray, times2: numpy.ndarray, delta: int` | `Compute matching (colliding) spike events between two spike trains and return the indices of the matching spikes together with their frame difference. This helper is used by spikeinterface.comparison tools for collision detection when comparing spike-sorted outputs: it identifies pairs of spikes (one from each train) that occur within a given frame window (delta) of each other and returns the corresponding indices into the original spike trains. The implementation concatenates the two input time arrays, tags membership (train 1 or train 2), sorts by time, and then selects adjacent cross-train pairs whose time difference is <= delta. Only adjacent cross-train neighbors in the sorted time sequence are considered (not all pairwise combinations within the window), and the final array is sorted by index1.` |
| `spikeinterface_core_channelsaggregationrecording_aggregate_channels` | `spikeinterface.core.channelsaggregationrecording.aggregate_channels` | `spikeinterface/core/channelsaggregationrecording.py` | `recording_list_or_dict: list = None, renamed_channel_ids: numpy.ndarray = None, recording_list: list = None` | `Aggregates channels from multiple recordings into a single ChannelsAggregationRecording wrapper used by SpikeInterface for preprocessing, sorting, waveform extraction, visualization, and other multi-recording operations. This function constructs and returns a ChannelsAggregationRecording that logically concatenates or combines channels provided by multiple BaseRecording objects so they can be treated as one recording by downstream SpikeInterface components. It accepts either a list or a dict of recordings, an optional array-like of renamed channel ids to override channel identifiers in the aggregate, and an optional alternate list parameter. The function itself is a thin factory that forwards these arguments to ChannelsAggregationRecording and performs no additional I/O; actual data access semantics (lazy, memory-mapped, or in-memory) depend on the underlying BaseRecording implementations.` |
| `spikeinterface_core_core_tools_check_paths_relative` | `spikeinterface.core.core_tools.check_paths_relative` | `spikeinterface/core/core_tools.py` | `input_dict: dict, relative_folder: str` | `spikeinterface.core.core_tools.check_paths_relative checks whether all filesystem paths contained in a dictionary that describes a BaseExtractor can be converted to relative paths with respect to a given folder. This function is used in the SpikeInterface framework (a unified framework for spike sorting) to decide if a dataset description (for example the dict returned by BaseExtractor.to_dict()) can be made portable by rewriting absolute file paths as paths relative to a chosen folder. It examines each path discovered in input_dict (these typically refer to recording files, probe files, or other data artifacts referenced by an extractor) and returns True only if every path can be expressed relative to relative_folder according to the same rules used by the library (URL and remote paths are excluded, Windows drive mismatches prevent relativity, and the Path.relative_to operation must succeed). The function performs checks such as string-based URL detection ("http" substring), remote-path detection via is_path_remote, drive equality for Windows paths (including UNC semantics where the host/share is treated as the drive), and a final attempt to compute a relative path using the library's _relative_to helper. The function does not modify files or the input_dict.` |
| `spikeinterface_core_core_tools_convert_bytes_to_str` | `spikeinterface.core.core_tools.convert_bytes_to_str` | `spikeinterface/core/core_tools.py` | `byte_value: int` | `Convert a number of bytes to a human-readable string using IEC binary prefixes. This utility converts an integer count of bytes into a formatted string that is easier for humans to read and compare in the context of SpikeInterface operations (for example, when reporting file sizes, memory footprints of recordings or processed datasets, export sizes for Phy/reports, or GUI displays). The function selects an appropriate IEC binary unit from bytes (B), kibibytes (KiB = 1024 B), mebibytes (MiB = 1024 KiB), gibibytes (GiB), up to tebibytes (TiB) and formats the numeric value with two decimal places followed by a single space and the unit (for example, "1.00 KiB"). This behavior matches how SpikeInterface presents storage and memory quantities when reading/writing many extracellular file formats, exporting reports, or displaying dataset sizes in the user interface.` |
| `spikeinterface_core_core_tools_convert_seconds_to_str` | `spikeinterface.core.core_tools.convert_seconds_to_str` | `spikeinterface/core/core_tools.py` | `seconds: float, long_notation: bool = True` | `spikeinterface.core.core_tools.convert_seconds_to_str converts a duration given in seconds into a human-readable string used throughout SpikeInterface to display recording durations, preprocessing and sorting runtimes, metric computation times, and other time-related values in logs and reports.` |
| `spikeinterface_core_core_tools_convert_string_to_bytes` | `spikeinterface.core.core_tools.convert_string_to_bytes` | `spikeinterface/core/core_tools.py` | `memory_string: str` | `Convert a memory size string to the corresponding number of bytes. This function is spikeinterface.core.core_tools.convert_string_to_bytes and is used within the SpikeInterface framework to translate human-readable memory specifications (commonly provided when configuring caching sizes, chunking windows, memory limits for sorting backends, exporters and other memory-sensitive components) into an integer byte count that the codebase can use for allocation, comparisons, and configuration.` |
| `spikeinterface_core_core_tools_extractor_dict_iterator` | `spikeinterface.core.core_tools.extractor_dict_iterator` | `spikeinterface/core/core_tools.py` | `extractor_dict: dict` | `Iterator for recursive traversal of a dictionary produced by extractors in the SpikeInterface framework. This function explores the nested mapping/list structure returned by BaseExtractor.to_dict() and yields a sequence of extractor_dict_element named tuples that describe every leaf value found in the structure. Each yielded element includes the actual leaf value, a name (the last dict key or the propagated list name), and an access_path tuple that records the sequence of dict keys and list indices required to reach that value from the top-level extractor_dict. In the SpikeInterface domain this is used to inspect, serialize, or process all scalar or non-dict/list entries stored by an extractor (for example metadata, numeric parameters, or paths) without copying the underlying data.` |
| `spikeinterface_core_core_tools_is_dict_extractor` | `spikeinterface.core.core_tools.is_dict_extractor` | `spikeinterface/core/core_tools.py` | `d: dict` | `is_dict_extractor(d) Determines whether a given Python dict describes an extractor object in the SpikeInterface framework. This function performs a lightweight, structural check used throughout SpikeInterface when handling serialized extractor descriptions (for example, when saving/loading RecordingExtractor or SortingExtractor metadata, exporting extractor descriptions to disk, or passing extractor definitions between processes). Concretely, it verifies that the input is a dict and that it contains the four keys expected by SpikeInterface extractor descriptions: "module", "class", "version", and "annotations". This check is intentionally shallow: it tests type and key presence only and does not validate the types or contents of the individual values associated with those keys (for example it does not parse or check the format of "version" or the structure of "annotations").` |
| `spikeinterface_core_core_tools_make_paths_absolute` | `spikeinterface.core.core_tools.make_paths_absolute` | `spikeinterface/core/core_tools.py` | `input_dict: dict, base_folder: str` | `spikeinterface.core.core_tools.make_paths_absolute: Recursively convert path-like entries in a BaseExtractor description dict into absolute POSIX paths relative to a given base folder. This function is used in the SpikeInterface framework (a unified framework for spike sorting) to post-process dictionaries produced by BaseExtractor.to_dict() so that any entries representing filesystem paths become absolute and machine-independent POSIX strings. It walks the input extractor dictionary, identifies entries that represent paths using extractor_dict_iterator(...) together with element_is_path(...), resolves each candidate path relative to base_folder, and writes the resolved POSIX path back into a deep copy of the input dictionary using set_value_in_extractor_dict(...). Only entries whose resolved absolute path exists on the filesystem are replaced; non-existing resolved paths are left unchanged. The original input_dict is not modified because the function operates on a deepcopy and returns a modified copy.` |
| `spikeinterface_core_core_tools_measure_memory_allocation` | `spikeinterface.core.core_tools.measure_memory_allocation` | `spikeinterface/core/core_tools.py` | `measure_in_process: bool = True` | `Measure memory allocation at a single point in time for use in spike sorting workflows. This utility is used within SpikeInterface, a framework for extracellular recording processing and spike sorting, to quantify memory consumption either for the current Python process or for the whole system. Measuring memory allocation is practically significant when loading large recordings, running preprocessors or spike sorters, or benchmarking memory usage of sorting pipelines as described in the SpikeInterface README. The function performs a one-time query using the psutil package and does not modify program state.` |
| `spikeinterface_core_core_tools_read_python` | `spikeinterface.core.core_tools.read_python` | `spikeinterface/core/core_tools.py` | `path: str` | `Parses a Python script file and returns a metadata dictionary built from the top-level names defined in that file. This function is used in SpikeInterface to load Python-based configuration or metadata files (for example, small modules that define recording parameters, channel maps, preprocessing parameters, or other dataset-specific Python objects) into a programmatic dictionary that downstream spike-sorting components can consume. The function reads the file at path, applies a small text transformation that converts simple numeric calls to range(...) into list(range(...)) to preserve expected sequence semantics, then executes the resulting code using six.exec_ in an isolated execution environment to collect the top-level names and values defined in the file. After execution, all top-level names are converted to lowercase and returned in a dictionary. Because the function executes arbitrary Python code, it can raise the same runtime or syntax errors as executing that code directly and may have side effects (file I/O, imports, network access, etc.). Use this function only on trusted files.` |
| `spikeinterface_core_core_tools_write_python` | `spikeinterface.core.core_tools.write_python` | `spikeinterface/core/core_tools.py` | `path: str, dict: dict` | `spikeinterface.core.core_tools.write_python saves a Python-style assignment file representing a simple dictionary of configuration or metadata values used in the SpikeInterface spike-sorting framework. This function writes each key/value pair from the provided dictionary as a Python assignment statement to a plain text file at the given path. It is commonly used in SpikeInterface to persist lightweight configuration dictionaries, sorter parameter sets, or small pieces of recording metadata in a human-readable Python format that can be inspected or executed (with caution) by downstream tools. The function performs minimal formatting: string values are wrapped in single quotes, Windows-style file paths are written as raw string literals when the key name suggests a path, and all other values are written using their str() representation. The file at the target path is opened for writing and will be created or overwritten.` |
| `spikeinterface_core_generate_clean_refractory_period` | `spikeinterface.core.generate.clean_refractory_period` | `spikeinterface/core/generate.py` | `times: numpy.ndarray, refractory_period: float` | `Remove spike times that violate a refractory period in a spike train. This function is used in spike sorting and spike-train post-processing to enforce a minimum time interval (the refractory period) between consecutive spike events. Given a 1-D array of spike event times (for example, sample indices or times in seconds or milliseconds), the function returns a new numpy.ndarray containing only those spike times that remain after removing events that occur too soon after a preceding event. The algorithm is greedy and deterministic: it first sorts the input times in ascending order, then iteratively removes the later event in any pair whose inter-event interval is less than or equal to the specified refractory period, repeating until no violations remain. In practice this is used to discard duplicate or temporally implausible detections that would violate a neuron's physiological or algorithmic refractory constraint.` |
| `spikeinterface_core_generate_create_sorting_npz` | `spikeinterface.core.generate.create_sorting_npz` | `spikeinterface/core/generate.py` | `num_seg: int, file_path: str` | `Create a NPZ-format sorting file used for tests and examples within the SpikeInterface framework. This function builds a minimal, synthetic sorting dataset and writes it to disk using numpy.savez at the given file_path. The produced NPZ file mirrors the small, fixed-format convention used by SpikeInterface test utilities: it contains a unit identifier array, a stored number-of-segments array, a sampling frequency array, and for each segment two arrays named spike_indexes_seg{seg_index} and spike_labels_seg{seg_index} that represent spike times (sample indices) and corresponding unit labels. This helper is intended to generate a predictable, toy sorting file for downstream consumers in the spike sorting pipeline (for example, loader tests, examples, or toy benchmarking), not to produce realistic experimental data.` |
| `spikeinterface_core_generate_generate_single_fake_waveform` | `spikeinterface.core.generate.generate_single_fake_waveform` | `spikeinterface/core/generate.py` | `sampling_frequency: float = None, ms_before: float = 1.0, ms_after: float = 3.0, negative_amplitude: float = -1, positive_amplitude: float = 0.15, depolarization_ms: float = 0.1, repolarization_ms: float = 0.6, recovery_ms: float = 1.1, smooth_ms: float = 0.05, dtype: str = "float32"` | `generate a single-channel synthetic spike waveform composed of three exponential phases (depolarization, repolarization, recovery), then apply Gaussian smoothing and align the negative peak to the sample index corresponding to ms_before. This helper is intended for use in SpikeInterface (a unified framework for spike sorting) to create simple, repeatable test waveforms for waveform extraction, sorter testing, tutorials, and unit tests where a realistic but parameterizable extracellular spike shape is required.` |
| `spikeinterface_core_generate_generate_snippets` | `spikeinterface.core.generate.generate_snippets` | `spikeinterface/core/generate.py` | `nbefore: int = 20, nafter: int = 44, num_channels: int = 2, wf_folder: str = None, sampling_frequency: float = 30000.0, durations: list = [10.325, 3.5], set_probe: bool = True, ndim: int = 2, num_units: int = 5, empty_units: list = None, **job_kwargs` | `Generates a synthetic Snippets object together with a corresponding synthetic Sorting object for use in spike-sorting workflows, using helper generators in the SpikeInterface test utilities. This function is used to create small, controlled datasets (recording + sorting) and then extract waveform snippets around detected spike times. It is useful for unit tests, examples, tutorials, and any situation where an in-memory or on-disk set of waveform snippets and a matching sorting are required in the SpikeInterface framework.` |
| `spikeinterface_core_generate_generate_templates` | `spikeinterface.core.generate.generate_templates` | `spikeinterface/core/generate.py` | `channel_locations: numpy.ndarray, units_locations: numpy.ndarray, sampling_frequency: float, ms_before: float, ms_after: float, seed: int = None, dtype: numpy.ndarray = "float32", upsample_factor: int = None, unit_params: dict = None, mode: str = "ellipsoid"` | `Generate templates (simulated spike waveforms) for multiple units given spatial channel positions and neuron locations. This function is used in the SpikeInterface framework to create synthetic templates that mimic extracellular action potentials across a probe's channels. It constructs a single-channel waveform per unit with generate_single_fake_waveform(), applies a spatial decay law to distribute that waveform across channels based on distances between neuron and channel locations, and optionally applies propagation delays and temporal upsampling to produce realistic, multi-channel templates suitable for simulation, benchmarking, or algorithm development in spike sorting workflows.` |
| `spikeinterface_core_generate_generate_unit_locations` | `spikeinterface.core.generate.generate_unit_locations` | `spikeinterface/core/generate.py` | `num_units: int, channel_locations: numpy.ndarray, margin_um: float = 20.0, minimum_z: float = 5.0, maximum_z: float = 40.0, minimum_distance: float = 20.0, max_iteration: int = 100, distance_strict: bool = False, distribution: str = "uniform", num_modes: int = 2, seed: int = None` | `spikeinterface.core.generate.generate_unit_locations: Generate random 3D unit locations for simulated neurons or test datasets, constrained by recording channel geometry and inter-unit distance rules. This function is part of the SpikeInterface framework used to create spatial layouts of neuronal units relative to extracellular recording channels for simulations, benchmarking, waveform extraction, quality-metric evaluation, and other spike-sorting workflows. The function samples num_units 3D coordinates (x, y, z) so that x and y lie within the bounding box of the provided channel_locations expanded by margin_um, z lies between minimum_z and maximum_z, and pairwise Euclidean distances between generated units meet a minimum_distance constraint when requested. Reproducible sampling is supported via an explicit random seed (seed). The function uses NumPy's Generator (numpy.random.default_rng) to draw uniform random values and an internal helper (_generate_multimodal) when distribution="multimodal" to mimic layered (multimodal) y-axis distributions.` |
| `spikeinterface_core_generate_get_ellipse` | `spikeinterface.core.generate.get_ellipse` | `spikeinterface/core/generate.py` | `positions: numpy.ndarray, center: numpy.ndarray, x_factor: float = 1, y_factor: float = 1, x_angle: float = 0, y_angle: float = 0, z_angle: float = 0` | `Compute per-channel radii (distances) to a specified 3D ellipsoidal volume used to model spatial inhomogeneities when generating templates in spike sorting pipelines. This function is used in SpikeInterface to transform 2D channel coordinates into the ellipsoid's centered, rotated reference frame, rescale with axis factors, and compute the scalar radius R for each channel using the ellipsoid equation R = (X/x_factor)**2 + (Y/y_factor)**2 + (Z/1)**2. The computed distances can be used to derive putative amplitudes or attenuation factors for template generation and spatial weighting in downstream spike sorting and post-processing.` |
| `spikeinterface_core_generate_synthesize_poisson_spike_vector` | `spikeinterface.core.generate.synthesize_poisson_spike_vector` | `spikeinterface/core/generate.py` | `num_units: int = 20, sampling_frequency: float = 30000.0, duration: float = 60.0, refractory_period_ms: float = 4.0, firing_rates: float = 3.0, seed: int = 0` | `synthesize_poisson_spike_vector generates discrete spike times (sample frames) for multiple neuronal units by simulating independent Poisson spike trains per unit and enforcing a refractory period. This function is used in SpikeInterface to create synthetic ground-truth spike data for testing, benchmarking, and development of spike-sorting workflows; it models each unit's spiking as a discrete-time Poisson process (geometric inter-spike counts), compensates for the refractory period by increasing the effective firing rate, discretizes times to sample frames using the provided sampling frequency, and returns sorted global spike frames with corresponding unit indices.` |
| `spikeinterface_core_generate_synthesize_random_firings` | `spikeinterface.core.generate.synthesize_random_firings` | `spikeinterface/core/generate.py` | `num_units: int = 20, sampling_frequency: float = 30000.0, duration: float = 60, refractory_period_ms: float = 4.0, firing_rates: list = 3.0, add_shift_shuffle: bool = False, seed: int = None` | `synthesize_random_firings generates a single-segment synthetic dataset of spike times and unit labels for use in spike-sorting development, testing, and benchmarking within the SpikeInterface framework. The function simulates num_units independent units firing with Poisson-like average rates over a recording of given duration and sampling frequency, enforces a refractory period in samples, optionally perturbs half of each unit's spikes to produce less-flat autocorrelograms, and returns concatenated, time-sorted spike times and their corresponding unit labels.` |
| `spikeinterface_core_generate_synthetize_spike_train_bad_isi` | `spikeinterface.core.generate.synthetize_spike_train_bad_isi` | `spikeinterface/core/generate.py` | `duration: float, baseline_rate: float, num_violations: int, violation_delta: float = 1e-05` | `Create a synthetic spike train with mostly uniform inter-spike intervals (ISIs) and a specified number of close temporal contaminating spikes that produce ISI violations. This function is intended for use in spike-sorting and quality-metric workflows (as in SpikeInterface) to simulate recordings with controlled firing rate and a controllable number of contaminating events that violate the expected refractory period, so downstream algorithms and metrics can be validated against known violations.` |
| `spikeinterface_core_globals_set_global_dataset_folder` | `spikeinterface.core.globals.set_global_dataset_folder` | `spikeinterface/core/globals.py` | `folder: str` | `Set the global dataset folder used by SpikeInterface. This function records a filesystem location (provided as a string) as the global dataset folder for the running Python process. The supplied folder string is converted internally to a pathlib.Path and stored in the module-level global variable dataset_folder. The module-level boolean dataset_folder_set is also set to True. Many SpikeInterface utilities that manage example datasets, dataset downloaders, and I/O helpers consult this global dataset_folder as the default location to read from or write to; calling this function therefore changes the default storage/lookup location for those components across the entire SpikeInterface runtime.` |
| `spikeinterface_core_globals_set_global_tmp_folder` | `spikeinterface.core.globals.set_global_tmp_folder` | `spikeinterface/core/globals.py` | `folder: str` | `Set the global temporary folder path used by SpikeInterface. This function defines the module-level temporary folder that many SpikeInterface components use to store intermediate files, caches, logs, or files passed to external sorters and exporters (for example during preprocessing, running sorters, waveform extraction, and export). Calling this function updates internal global state so that subsequent operations in the same Python process will read and write temporary artifacts under the provided path.` |
| `spikeinterface_core_job_tools_split_job_kwargs` | `spikeinterface.core.job_tools.split_job_kwargs` | `spikeinterface/core/job_tools.py` | `mixed_kwargs: dict` | `spikeinterface.core.job_tools.split_job_kwargs. Splits a mixed dictionary of keyword arguments into two separate dictionaries: one containing job-level control options used by SpikeInterface job management utilities and one containing function-specific parameters used by spike-sorting, preprocessing, or postprocessing routines. This function is used in the SpikeInterface framework (a unified framework for spike sorting) to allow functions with a generic signature that mix execution/control parameters (for example, parallelization, scheduling, or job-dispatch options) and algorithm- or dataset-specific parameters to separate those concerns. The implementation iterates over the provided mixed_kwargs and classifies each key as a job-level key if it is present in the module-level job_keys collection; all other keys are classified as specific (domain) kwargs. After collecting job-level keys, the job_kwargs dict is passed to the module-level helper fix_job_kwargs to normalize, validate, and possibly augment job-level settings (for example to set defaults or normalize formats used by downstream job runners). The original mixed_kwargs mapping is not mutated; two new dict objects are returned.` |
| `spikeinterface_core_motion_ensure_time_bins` | `spikeinterface.core.motion.ensure_time_bins` | `spikeinterface/core/motion.py` | `time_bin_centers_s: numpy.ndarray = None, time_bin_edges_s: numpy.ndarray = None` | `Ensure that both time bin centers and time bin edges are available for use in spike-sorting workflows (e.g., motion estimation, per-bin metrics, or temporal alignment). This function accepts either bin centers or bin edges (for a single recording segment or for multiple segments as a list) and reconstructs the missing representation. When converting edges to centers the midpoint of each adjacent edge pair is used. When converting centers to edges the function pads the left and right sides with the outer center values and computes midpoints for interior edges. This behavior ensures consistent 1D time-bin definitions in seconds across single- and multi-segment data used throughout SpikeInterface for preprocessing, quality metrics, and visualization.` |
| `spikeinterface_core_node_pipeline_check_graph` | `spikeinterface.core.node_pipeline.check_graph` | `spikeinterface/core/node_pipeline.py` | `nodes: list` | `Validate the ordering and composition of a SpikeInterface node pipeline. This function enforces the structural rules required by the peak-processing pipeline used in SpikeInterface (the unified spike sorting framework). It inspects the provided list of pipeline nodes and verifies: (1) the first element is a PeakSource (for example a PeakDetector, PeakRetriever, or SpikeRetriever), which is required because the pipeline must start from a source of detected peaks; (2) every element is an instance of PipelineNode, the base type used for nodes in the node pipeline; and (3) for each node, every declared parent (node.parents) is included in the provided list and appears earlier in the list so that parents precede children in execution order. The function treats a falsy node.parents value (None or empty) as no parents. This validation is intended to be called before running or composing pipeline stages to avoid runtime errors during spike detection, preprocessing, or downstream sorting/post-processing.` |
| `spikeinterface_core_node_pipeline_find_parent_of_type` | `spikeinterface.core.node_pipeline.find_parent_of_type` | `spikeinterface/core/node_pipeline.py` | `list_of_parents: list, parent_type: tuple` | `Find a single parent node of a specified type from a list of pipeline parents. This function is used within SpikeInterface's node_pipeline utilities to locate an upstream pipeline node (PipelineNode) of a particular class or classes when traversing or inspecting processing/sorting pipelines. In the spike sorting domain this is useful to find a specific kind of parent node (for example a preprocessing, extractor, or analysis node) among a node's parents so downstream logic can access configuration or results produced earlier in the pipeline.` |
| `spikeinterface_core_node_pipeline_find_parents_of_type` | `spikeinterface.core.node_pipeline.find_parents_of_type` | `spikeinterface/core/node_pipeline.py` | `list_of_parents: list, parent_type: tuple` | `Find all parents of a given type or types from a provided list of pipeline parent nodes used in SpikeInterface node pipelines.` |
| `spikeinterface_core_recording_tools_check_probe_do_not_overlap` | `spikeinterface.core.recording_tools.check_probe_do_not_overlap` | `spikeinterface/core/recording_tools.py` | `probes: list` | `Check that multiple probe objects do not overlap in 2D space so their channel/contact positions can be safely concatenated. This function is used in the SpikeInterface framework (a unified framework for spike sorting) to verify that several Probe-like objects, each representing a recording probe with spatial contact positions, occupy disjoint axis-aligned rectangular areas in the recorded coordinate frame. It computes the axis-aligned bounding box for each probe from probe.contact_positions (the array of contact coordinates), and then checks pairwise that no contact position from one probe lies within the bounding box of another. This check is commonly performed before concatenating channel positions from multiple probes into a single Recording object or channel map to avoid ambiguous or conflicting channel spatial assignments.` |
| `spikeinterface_core_sorting_tools_generate_unit_ids_for_merge_group` | `spikeinterface.core.sorting_tools.generate_unit_ids_for_merge_group` | `spikeinterface/core/sorting_tools.py` | `old_unit_ids: numpy.ndarray, merge_unit_groups: list, new_unit_ids: list = None, new_id_strategy: str = "append"` | `spikeinterface.core.sorting_tools.generate_unit_ids_for_merge_group generates identifiers for newly created merged units during a unit-merging procedure in SpikeInterface sorting workflows. This function is used when curating spike sorting results to combine multiple existing units into single units; it either validates user-provided new ids or produces new ids according to a chosen strategy that is compatible with the dtype of the existing unit ids.` |
| `spikeinterface_core_sorting_tools_generate_unit_ids_for_split` | `spikeinterface.core.sorting_tools.generate_unit_ids_for_split` | `spikeinterface/core/sorting_tools.py` | `old_unit_ids: numpy.ndarray, unit_splits: dict, new_unit_ids: list = None, new_id_strategy: str = "append"` | `Function to generate new unit ids used when splitting existing units during spike-sorting curation or automated split operations in SpikeInterface. This function is used by higher-level curation utilities to produce or validate identifiers for the new sub-units created when an existing unit is split (for example, during manual curation or algorithmic post-processing of a SortingExtractor/Sorting object). It either validates user-provided new ids against the requested splits or generates new ids according to a chosen strategy while avoiding id collisions with the existing unit id set.` |
| `spikeinterface_core_sorting_tools_spike_vector_to_indices` | `spikeinterface.core.sorting_tools.spike_vector_to_indices` | `spikeinterface/core/sorting_tools.py` | `spike_vector: list[numpy.array], unit_ids: numpy.array, absolute_index: bool = False` | `spikeinterface.core.sorting_tools.spike_vector_to_indices: Convert a spike-associated vector (for example spike amplitudes or spike locations) into a nested dictionary of spike indices organized by recording segment and by sorted unit. This function is used in the SpikeInterface spike-sorting workflow to take a global or per-segment vector that aligns one value per spike and split it back into per-unit spike-index arrays so downstream post-processing (quality metrics, waveform extraction, visualization, exporting) can access values for each unit and segment.` |
| `spikeinterface_core_sorting_tools_spike_vector_to_spike_trains` | `spikeinterface.core.sorting_tools.spike_vector_to_spike_trains` | `spikeinterface/core/sorting_tools.py` | `spike_vector: list[numpy.array], unit_ids: numpy.array` | `spikeinterface.core.sorting_tools.spike_vector_to_spike_trains computes per-segment spike trains for every unit given a spike vector produced by SpikeInterface sorting.to_spike_vector(concatenated=False). It converts a list of per-segment spike records (each containing sample and unit indices) into a dictionary that maps each segment index to an inner dictionary mapping unit identifiers to numpy arrays of spike sample indices. This transformation is commonly used in post-processing, metric computation, exporting results (for example to phy), and visualization workflows within the SpikeInterface framework.` |
| `spikeinterface_core_sorting_tools_vector_to_list_of_spiketrain_numpy` | `spikeinterface.core.sorting_tools.vector_to_list_of_spiketrain_numpy` | `spikeinterface/core/sorting_tools.py` | `sample_indices: numpy.ndarray, unit_indices: numpy.ndarray, num_units: int` | `Slower implementation of vector_to_dict for a single recording segment using a NumPy boolean mask. In the SpikeInterface framework this function is used during post-processing of spike sorting outputs to convert a vectorized representation of spikes (sample time indices and corresponding unit labels for one segment) into a Python list of per-unit spike trains (numpy arrays). This is useful when preparing per-unit spike trains for downstream tasks such as waveform extraction, quality metrics computation, visualization, or export.` |
| `spikeinterface_core_sortinganalyzer_get_default_analyzer_extension_params` | `spikeinterface.core.sortinganalyzer.get_default_analyzer_extension_params` | `spikeinterface/core/sortinganalyzer.py` | `extension_name: str` | `spikeinterface.core.sortinganalyzer.get_default_analyzer_extension_params: Retrieve the default parameter values declared by a SortingAnalyzer extension's _set_params method.` |
| `spikeinterface_core_sortinganalyzer_get_extension_class` | `spikeinterface.core.sortinganalyzer.get_extension_class` | `spikeinterface/core/sortinganalyzer.py` | `extension_name: str, auto_import: bool = True` | `spikeinterface.core.sortinganalyzer.get_extension_class: Retrieve an extension class by name and verify it is registered with the internal extension registry used by the SortingAnalyzer API in SpikeInterface. This function looks up a registered extension class by its declared extension_name in the global registry (_possible_extensions). It is used throughout the SortingAnalyzer and extension machinery of SpikeInterface to locate the concrete class that implements an extension (for example, post-processing, metrics, or other SortingAnalyzer extensions) so that the analyzer can instantiate or query that extension. If the requested extension is not yet registered but is a known builtin extension (listed in _builtin_extensions), the function can optionally import the module that provides and registers the extension, updating the global registry as a side effect. The function therefore both performs a registry lookup and may perform dynamic import to ensure builtin extensions become available to the SortingAnalyzer workflow.` |
| `spikeinterface_core_sortinganalyzer_load_sorting_analyzer` | `spikeinterface.core.sortinganalyzer.load_sorting_analyzer` | `spikeinterface/core/sortinganalyzer.py` | `folder: str, load_extensions: bool = True, format: str = "auto", backend_options: dict = None` | `Load a SortingAnalyzer object from a folder on disk or in cloud storage so it can be used by SpikeInterface code to inspect, post-process, and compute quality metrics on spike sorting outputs.` |
| `spikeinterface_core_waveform_tools_split_waveforms_by_units` | `spikeinterface.core.waveform_tools.split_waveforms_by_units` | `spikeinterface/core/waveform_tools.py` | `unit_ids: list, spikes: numpy.ndarray, all_waveforms: numpy.ndarray, sparsity_mask: numpy.ndarray = None, folder: str = None` | `split_waveforms_by_units splits a single contiguous buffer of extracted waveforms into per-unit waveform collections. In SpikeInterface workflows (spike sorting, waveform extraction and postprocessing), this function is used to separate a global waveform buffer that contains waveforms for all detected spikes into a dictionary keyed by unit identifier or, alternatively, to save each unit's waveforms to separate .npy files on disk and return memory-mapped views. The splitting uses the integer unit index stored in the provided spikes vector to group waveforms for each unit id in the same order as unit_ids.` |
| `spikeinterface_core_zarrextractors_get_default_zarr_compressor` | `spikeinterface.core.zarrextractors.get_default_zarr_compressor` | `spikeinterface/core/zarrextractors.py` | `clevel: int = 5` | `Return a configured Zarr/Blosc compressor optimized for storing int16 electrophysiology recordings used in SpikeInterface. This factory function constructs and returns a numcodecs Blosc compressor configured for good performance when saving extracellular electrophysiology data (commonly int16 samples) to Zarr stores. The configuration mirrors the defaults used in SpikeInterface for Zarr-backed recording storage: codec name "zstd" (zstandard) for efficient compression ratio, and BITSHUFFLE to improve compressibility of small integer samples. The clevel parameter controls the trade-off between compression ratio and CPU cost when writing data: higher values generally produce smaller files but require more CPU time.` |
| `spikeinterface_curation_auto_merge_binom_sf` | `spikeinterface.curation.auto_merge.binom_sf` | `spikeinterface/curation/auto_merge.py` | `x: int, n: float, p: float` | `spikeinterface.curation.auto_merge.binom_sf computes the binomial survival function (sf = 1 - cdf) for a given observed count of successes, using integer-valued binomial survival probabilities computed at nearby integer trial counts and a quadratic interpolation to support non-integer trial inputs. In the SpikeInterface curation/auto_merge context, this function is used to quantify the tail probability that a binomial random variable with probability p would exceed x successes across approximately n trials; this probability can be used as a statistical criterion when automatically deciding whether to merge putative units (for example, when evaluating the significance of coincident spike counts between units).` |
| `spikeinterface_curation_auto_merge_estimate_contamination` | `spikeinterface.curation.auto_merge.estimate_contamination` | `spikeinterface/curation/auto_merge.py` | `spike_train: numpy.ndarray, sf: float, T: int, refractory_period: tuple[float, float]` | `Estimate the contamination of a spike train by counting refractory period violations and converting that count into a contamination fraction used in spike sorting curation.` |
| `spikeinterface_curation_auto_merge_get_unit_adaptive_window` | `spikeinterface.curation.auto_merge.get_unit_adaptive_window` | `spikeinterface/curation/auto_merge.py` | `auto_corr: numpy.ndarray, threshold: float` | `Computes an adaptive window size from a correlogram for unit curation/auto-merge workflows in SpikeInterface. This function identifies the first relevant peak in the correlogram (interpreted as the first peak nearest to the center) by locating peaks in the negative second derivative of the correlogram, filtering those peaks by a minimum amplitude threshold, and selecting the last qualifying peak before the correlogram center. The resulting window size is computed as the distance (in array indices / samples) from that peak to the correlogram center and is intended to be used by downstream curation logic (for example, to define the temporal window for deciding whether two units should be merged based on their cross-correlogram).` |
| `spikeinterface_curation_auto_merge_normalize_correlogram` | `spikeinterface.curation.auto_merge.normalize_correlogram` | `spikeinterface/curation/auto_merge.py` | `correlogram: numpy.ndarray` | `spikeinterface.curation.auto_merge.normalize_correlogram normalizes a correlogram so its mean over time bins is 1. Normalizes an input correlogram array used in spike-sorting curation (for example in automated unit merging) by dividing all values by the arithmetic mean computed across the correlogram's elements (the mean across time bins / lag values). This ensures different correlograms are placed on a common scale so that their mean activity across time is 1, which is useful when comparing shapes of cross-correlograms or autocorrelograms between units during the auto-merge curation workflow in SpikeInterface. If the correlogram is all zeros, the function leaves it unchanged to avoid division by zero.` |
| `spikeinterface_curation_auto_merge_resolve_pairs` | `spikeinterface.curation.auto_merge.resolve_pairs` | `spikeinterface/curation/auto_merge.py` | `existing_merges: dict, new_merges: dict` | `Resolve nested unit-merge mappings produced when merging units recursively during curation. This function is used primarily by auto_merge_units within the SpikeInterface curation workflow to produce a condensed, non-nested representation of all unit merges that have been applied so far. The returned mapping is suitable for downstream visualization (for example, the plot_potential_merge widget) and for producing a final list of merged unit identifiers for post-processing and quality-metric calculations. In the SpikeInterface domain, "unit ids" refer to the identifiers used by a SortingExtractor (integers or strings identifying sorted units), and the mappings record which unit ids were merged into which surviving unit id.` |
| `spikeinterface_curation_auto_merge_smooth_correlogram` | `spikeinterface.curation.auto_merge.smooth_correlogram` | `spikeinterface/curation/auto_merge.py` | `correlograms: numpy.ndarray, bins: numpy.ndarray, sigma_smooth_ms: float = 0.6` | `spikeinterface.curation.auto_merge.smooth_correlogram smooths cross-correlograms by convolving them with a Gaussian kernel. This function is used in the automatic curation/auto-merge workflow of SpikeInterface to reduce high-frequency noise in computed cross-correlograms (time-lag histograms between spike trains) so that downstream heuristics or metrics that decide whether two units should be merged are more robust. The smoothing is implemented by constructing a normalized Gaussian kernel from the provided bins and sigma (in milliseconds) and applying a fast Fourier-based convolution (scipy.signal.fftconvolve) along the correlogram time-lag axis.` |
| `spikeinterface_curation_model_based_curation_load_model` | `spikeinterface.curation.model_based_curation.load_model` | `spikeinterface/curation/model_based_curation.py` | `model_folder: str = None, repo_id: str = None, model_name: str = None, trust_model: bool = False, trusted: list = None` | `Load a serialized model and its accompanying metadata (model_info) either from a local folder or from a HuggingFace Hub repository. This function is used within the SpikeInterface curation workflow to obtain a trained model and metadata that can be applied to automated, model-based curation of spike-sorting outputs (for example, to score or classify units based on learned features). The returned model is the deserialized object ready for inference (commonly a scikit-learn estimator, pipeline, or other Python object serialized with skops), and model_info contains metadata about the model (for example, training provenance, feature definitions, or versioning) that downstream curation code can inspect to ensure compatibility.` |
| `spikeinterface_curation_train_manual_curation_check_metric_names_are_the_same` | `spikeinterface.curation.train_manual_curation.check_metric_names_are_the_same` | `spikeinterface/curation/train_manual_curation.py` | `metrics_for_each_analyzer: list` | `Check that the set of metric names computed by each analyzer is identical.` |
| `spikeinterface_curation_train_manual_curation_train_model` | `spikeinterface.curation.train_manual_curation.train_model` | `spikeinterface/curation/train_manual_curation.py` | `mode: str = "analyzers", labels: list = None, analyzers: list = None, metrics_paths: list = None, folder: str = None, metric_names: list = None, imputation_strategies: list = None, scaling_techniques: list = None, classifiers: list = None, test_size: float = 0.2, overwrite: bool = False, seed: int = None, search_kwargs: dict = None, verbose: bool = True, enforce_metric_params: bool = False, **job_kwargs` | `Trains and evaluates machine learning models to assist manual curation of spike sorting outputs within the SpikeInterface framework. The function orchestrates creation of a CurationModelTrainer, validates inputs, loads metrics either from SortingAnalyzer objects or CSV files, preprocesses metrics (selection of metric names, imputation, scaling), searches classifier hyperparameters, evaluates model configurations on a train/test split, and saves evaluation artifacts and the best model to the specified output folder. This function is used in spike sorting curation workflows to build models that predict curated labels (for example, unit quality or accept/reject decisions) from computed quality metrics produced by SpikeInterface analyzers or from CSV exports of those metrics.` |
| `spikeinterface_extractors_bids_read_bids` | `spikeinterface.extractors.bids.read_bids` | `spikeinterface/extractors/bids.py` | `folder_path: str` | `spikeinterface.extractors.bids.read_bids loads a BIDS-formatted folder of electrophysiology data into SpikeInterface extractor objects for downstream preprocessing and spike sorting.` |
| `spikeinterface_extractors_cbin_ibl_extract_stream_info` | `spikeinterface.extractors.cbin_ibl.extract_stream_info` | `spikeinterface/extractors/cbin_ibl.py` | `meta_file: str, meta: dict` | `spikeinterface.extractors.cbin_ibl.extract_stream_info extracts channel and stream-level metadata from a SpikeGLX-style metadata dictionary and the metadata filename, returning a normalized info dictionary used by SpikeInterface to interpret binary stream files for spike sorting and downstream processing. This function is used within the SpikeInterface framework to translate SpikeGLX .meta file contents (and the metadata filename) into a canonical set of values needed when reading and preprocessing extracellular recordings: number of saved channels, sampling rate, stream kind and name (e.g., "ap" or "lf"), per-channel gains converted to microvolts, channel names, sample count, and whether a sync trace is present. The returned info dictionary is intended to be consumed by extractors and readers that map raw binary samples to physical units and channel labels for spike sorting, visualization, and quality metrics computation.` |
| `spikeinterface_extractors_mcsh5extractors_openMCSH5File` | `spikeinterface.extractors.mcsh5extractors.openMCSH5File` | `spikeinterface/extractors/mcsh5extractors.py` | `filename: str, stream_id: int` | `spikeinterface.extractors.mcsh5extractors.openMCSH5File opens a Multi Channel Systems (MCS) HDF5 recording file and extracts metadata required by SpikeInterface for downstream spike-sorting workflows. The function reads the specified AnalogStream inside the file, verifies timestamps and channel information, converts stored amplitude calibration to microvolts, computes the sampling frequency, and returns a dictionary of recording-level information used by SpikeInterface extractors to construct a RecordingExtractor.` |
| `spikeinterface_extractors_neoextractors_mearec_read_mearec` | `spikeinterface.extractors.neoextractors.mearec.read_mearec` | `spikeinterface/extractors/neoextractors/mearec.py` | `file_path: str` | `spikeinterface.extractors.neoextractors.mearec.read_mearec reads a MEArec HDF5 file and returns a pair of SpikeInterface extractor objects: a MEArecRecordingExtractor that provides programmatic access to the extracellular recording (continuous traces and channel metadata) and a MEArecSortingExtractor that provides the corresponding ground-truth spike times/units for benchmarking and analysis.` |
| `spikeinterface_extractors_neoextractors_neo_utils_get_neo_num_blocks` | `spikeinterface.extractors.neoextractors.neo_utils.get_neo_num_blocks` | `spikeinterface/extractors/neoextractors/neo_utils.py` | `extractor_name: str, *args, **kwargs` | `Returns the number of NEO blocks for a given NEO-based extractor. This utility is part of the SpikeInterface framework (a unified framework for spike sorting) and is used by extractor callers and higher-level I/O utilities to determine how many NEO "Block" records a dataset exposes before attempting to read data. Knowing the number of blocks is important in multi-block extracellular recordings (for example, concatenated recording sessions, segmented acquisitions, or datasets where temporal segments are stored as separate NEO Block objects) so downstream code can select a block via the block_index argument when calling the corresponding read_**extractor_name**() reader.` |
| `spikeinterface_extractors_neoextractors_neo_utils_get_neo_streams` | `spikeinterface.extractors.neoextractors.neo_utils.get_neo_streams` | `spikeinterface/extractors/neoextractors/neo_utils.py` | `extractor_name: str, *args, **kwargs` | `spikeinterface.extractors.neoextractors.neo_utils.get_neo_streams: Return the NEO stream names and stream ids associated with a dataset so callers (for example read_<extractor_name> functions in SpikeInterface) can enumerate available substreams and select which stream to read. This function is part of the SpikeInterface framework for reading many extracellular file formats via NEO-based extractors. It looks up the registered NEO extractor implementation by name, forwards the provided positional and keyword arguments to that extractor, and returns the extractor-reported list of stream names and the corresponding list of stream ids. For multi-stream datasets, these values let downstream code or users select the desired substream (for example by passing stream_id or stream_name to read_<extractor_name>).` |
| `spikeinterface_extractors_neoextractors_neuroscope_read_neuroscope` | `spikeinterface.extractors.neoextractors.neuroscope.read_neuroscope` | `spikeinterface/extractors/neoextractors/neuroscope.py` | `file_path: str, stream_id: str = None, keep_mua_units: bool = False, exclude_shanks: list = None, load_recording: bool = True, load_sorting: bool = False` | `Read neuroscope recording and sorting and return SpikeInterface extractor objects for downstream spike-sorting workflows. This function is a convenience loader used by the SpikeInterface framework to import NeuroScope-format data into the unified spike-sorting pipeline described in the project README. It expects an XML recording descriptor (file_path) and, optionally, associated clustering files (.res and .clu) in the same folder as that XML file. The function creates and returns NeuroScopeRecordingExtractor and/or NeuroScopeSortingExtractor instances (from the local neoextractors integration) depending on the load_recording and load_sorting flags. These extractor objects are the standard SpikeInterface abstractions for a recorded extracellular signal (recording) and a set of spike times with unit assignments (sorting) and can be used immediately for preprocessing, running sorters, computing quality metrics, visualization, and export as described in the README.` |
| `spikeinterface_extractors_neoextractors_openephys_read_openephys` | `spikeinterface.extractors.neoextractors.openephys.read_openephys` | `spikeinterface/extractors/neoextractors/openephys.py` | `folder_path: str, **kwargs` | `spikeinterface.extractors.neoextractors.openephys.read_openephys: Read an Open Ephys folder and return a SpikeInterface RecordingExtractor that can be used downstream in the SpikeInterface spike-sorting pipeline.` |
| `spikeinterface_extractors_neoextractors_openephys_read_openephys_event` | `spikeinterface.extractors.neoextractors.openephys.read_openephys_event` | `spikeinterface/extractors/neoextractors/openephys.py` | `folder_path: str, experiment_name: str = None, block_index: int = None` | `spikeinterface.extractors.neoextractors.openephys.read_openephys_event reads Open Ephys events from an Open Ephys "binary" folder and returns an OpenEphysBinaryEventExtractor instance that exposes event timestamps and metadata for use in SpikeInterface workflows (for example, aligning events with recordings, epoching, or triggering downstream spike-sorting steps). This function inspects the given folder to determine whether the session uses the Open Ephys binary format and then constructs an OpenEphysBinaryEventExtractor for the requested experiment/block. It is intended for use in SpikeInterface pipelines that need programmatic access to Open Ephys event data produced by the Open Ephys GUI in binary format.` |
| `spikeinterface_extractors_neoextractors_spikeglx_read_spikeglx_event` | `spikeinterface.extractors.neoextractors.spikeglx.read_spikeglx_event` | `spikeinterface/extractors/neoextractors/spikeglx.py` | `folder_path: str, block_index: int = None` | `Read SpikeGLX events and return a SpikeGLXEventExtractor suitable for use in the SpikeInterface pipeline.` |
| `spikeinterface_extractors_neuropixels_utils_get_neuropixels_channel_groups` | `spikeinterface.extractors.neuropixels_utils.get_neuropixels_channel_groups` | `spikeinterface/extractors/neuropixels_utils.py` | `num_channels: int = 384, num_channels_per_adc: int = 12` | `spikeinterface.extractors.neuropixels_utils.get_neuropixels_channel_groups returns groups of simultaneously sampled channels on a Neuropixels probe and documents the sampling grouping used to support operations that must act on synchronously digitized channels (for example, preprocessing.common_reference).` |
| `spikeinterface_extractors_nwbextractors_read_nwb` | `spikeinterface.extractors.nwbextractors.read_nwb` | `spikeinterface/extractors/nwbextractors.py` | `file_path: str, load_recording: bool = True, load_sorting: bool = False, electrical_series_path: str = None` | `spikeinterface.extractors.nwbextractors.read_nwb reads an NWB (Neurodata Without Borders) file and constructs SpikeInterface extractor objects for extracellular recordings and/or spike sorting outputs. This function is used within the SpikeInterface framework (a unified framework for spike sorting) to import data stored in NWB format so downstream preprocessing, sorting, post-processing, quality metrics, and visualization tools in SpikeInterface can operate on the data.` |
| `spikeinterface_extractors_sinapsrecordingextractors_parse_sinapse_h5` | `spikeinterface.extractors.sinapsrecordingextractors.parse_sinapse_h5` | `spikeinterface/extractors/sinapsrecordingextractors.py` | `filename: str` | `Open and parse a SiNAPS HDF5 recording file and return a dictionary of recording metadata needed by SpikeInterface for downstream processing (e.g., preprocessing, waveform extraction, and spike sorting). This function is used in the SpikeInterface extractors submodule to read SiNAPS-format HDF5 files produced by SiNAPS/real-time acquisition systems. It opens the file in read-only mode, locates expected groups and datasets (RealTimeProcessedData/FilteredData and Parameters, plus Advanced Recording Parameters), and extracts essential recording parameters and a live file handle for later data access. The returned information is intended to let SpikeInterface and downstream tools interpret the stored samples (shape, dtype, gain/offset, sampling frequency, probe description, and ADC resolution) and to provide a ready filehandle for reading raw/filtered data without re-opening the file.` |
| `spikeinterface_generation_drift_tools_interpolate_templates` | `spikeinterface.generation.drift_tools.interpolate_templates` | `spikeinterface/generation/drift_tools.py` | `templates_array: numpy.ndarray, source_locations: numpy.ndarray, dest_locations: numpy.ndarray, interpolation_method: str = "cubic"` | `Interpolate templates_array to new channel positions for use in spike sorting tasks such as simulating probe motion or remapping templates from one probe geometry to another.` |
| `spikeinterface_generation_drift_tools_make_linear_displacement` | `spikeinterface.generation.drift_tools.make_linear_displacement` | `spikeinterface/generation/drift_tools.py` | `start: numpy.ndarray, stop: numpy.ndarray, num_step: int = 10` | `make_linear_displacement generates a sequence of 2D positions that interpolate linearly between a start and stop position. In the SpikeInterface generation/drift_tools context, this function is used to create a temporal sequence of probe or source displacements (x,y coordinates) for simulating linear drift of an extracellular recording setup; the returned sequence can be fed into synthetic recording generation or drift simulation routines to model gradual movement of the probe relative to neural tissue.` |
| `spikeinterface_generation_drifting_generator_generate_drifting_recording` | `spikeinterface.generation.drifting_generator.generate_drifting_recording` | `spikeinterface/generation/drifting_generator.py` | `num_units: int = 250, duration: float = 600.0, sampling_frequency: float = 30000.0, probe_name: str = "Neuropixels1-128", generate_probe_kwargs: dict = None, generate_unit_locations_kwargs: dict = {'margin_um': 20.0, 'minimum_z': 5.0, 'maximum_z': 45.0, 'minimum_distance': 18.0, 'max_iteration': 100, 'distance_strict': False, 'distribution': 'uniform'}, generate_displacement_vector_kwargs: dict = {'displacement_sampling_frequency': 5.0, 'drift_start_um': [0, 20], 'drift_stop_um': [0, -20], 'drift_step_um': 1, 'motion_list': [{'drift_mode': 'zigzag', 'non_rigid_gradient': None, 't_start_drift': 60.0, 't_end_drift': None, 'period_s': 200}]}, generate_templates_kwargs: dict = {'ms_before': 1.5, 'ms_after': 3.0, 'mode': 'ellipsoid', 'unit_params': {'alpha': (100.0, 500.0), 'spatial_decay': (10, 45), 'ellipse_shrink': (0.4, 1), 'ellipse_angle': (0, 6.283185307179586)}}, generate_sorting_kwargs: dict = {'firing_rates': (2.0, 8.0), 'refractory_period_ms': 4.0}, generate_noise_kwargs: dict = {'noise_levels': (6.0, 8.0), 'spatial_decay': 25.0}, extra_outputs: bool = False, seed: int = None` | `spikeinterface.generation.drifting_generator.generate_drifting_recording Generates a pair of synthetic extracellular recordings that share identical spike times and unit identities but differ in motion: one "static" recording with no probe drift and one "drifting" recording with unit positions moved over time. This function is intended for benchmarking spike sorters and motion-correction algorithms in extracellular electrophysiology (see SpikeInterface README). It composes probe generation, unit placement, time-varying displacement vector generation, template generation (including multiple spatial positions for drift steps), and noise generation, and then injects the resulting drifting templates into the noise recording to produce Recording objects usable by the SpikeInterface API.` |
| `spikeinterface_generation_drifting_generator_make_one_displacement_vector` | `spikeinterface.generation.drifting_generator.make_one_displacement_vector` | `spikeinterface/generation/drifting_generator.py` | `drift_mode: str = "zigzag", duration: float = 600.0, amplitude_factor: float = 1.0, displacement_sampling_frequency: float = 5.0, t_start_drift: float = None, t_end_drift: float = None, period_s: float = 200, bump_interval_s: tuple = (30, 90.0), seed: int = None` | `spikeinterface.generation.drifting_generator.make_one_displacement_vector generates a single, time-indexed displacement vector used to simulate probe drift for spike-sorting workflows in SpikeInterface. The function constructs a normalized displacement shape (base range [-0.5, 0.5]) sampled at a specified rate over a recording duration, then scales it by amplitude_factor so it can be interpreted as a spatial displacement trace (commonly interpreted in micrometers when amplitude_factor is given in micrometers). This displacement vector can be applied to simulated extracellular recordings or waveform templates to model slow drift (zigzag), discrete alternating bumps, or a constrained random walk during a specified drift interval.` |
| `spikeinterface_postprocessing_amplitude_scalings_find_collisions` | `spikeinterface.postprocessing.amplitude_scalings.find_collisions` | `spikeinterface/postprocessing/amplitude_scalings.py` | `spikes: numpy.ndarray, spikes_within_margin: numpy.ndarray, delta_collision_samples: int, sparsity_mask: numpy.ndarray` | `spikeinterface.postprocessing.amplitude_scalings.find_collisions identifies temporal and spatial collisions between spikes recorded and sorted across units, for use during spike sorting post-processing (for example, in amplitude scaling steps where overlapping spikes can bias amplitude estimates). Finds, for each spike in the provided spikes array, other spikes that (1) occur within a temporal window defined by delta_collision_samples around the spike peak sample index, and (2) are produced by units that have overlapping spatial footprint on any recording channel as indicated by sparsity_mask. Temporal overlap is determined by comparing the spike["sample_index"] values and using numpy.searchsorted on spikes_within_margin["sample_index"] to efficiently locate candidate spikes within the time window. Spatial overlap is determined by comparing unit indices via the provided sparsity_mask (a boolean unit-by-channel mask) using the internal _are_units_spatially_overlapping check. The function returns a dictionary keyed by the index of the spike in the input spikes array; each value is a numpy.ndarray of spike records that overlap with that spike, where the spike itself appears at position 0 of the array.` |
| `spikeinterface_postprocessing_amplitude_scalings_fit_collision` | `spikeinterface.postprocessing.amplitude_scalings.fit_collision` | `spikeinterface/postprocessing/amplitude_scalings.py` | `collision: numpy.ndarray, traces_with_margin: numpy.ndarray, nbefore: int, all_templates: numpy.ndarray, sparsity_mask: numpy.ndarray, cut_out_before: int, cut_out_after: int` | `Compute the best-fit non-negative scaling factors for a collision between a target spike and its temporally and spatially overlapping spikes in the SpikeInterface postprocessing workflow. This function is used in SpikeInterface to estimate how much each colliding spike's unit template contributes to an observed multi-channel waveform that contains an overlapping set of spikes. It constructs a multivariate linear regression problem where the observed local waveform (y) is modeled as a non-negative linear combination of temporally shifted unit templates (columns of X). Each returned coefficient is the estimated scaling factor for the corresponding spike in the input collision array and can be used to decompose observed amplitudes into contributions from the spike of interest and its colliders, which is useful for amplitude-scaling corrections, deconvolution, or collision-aware waveform analyses.` |
| `spikeinterface_postprocessing_correlograms_correlogram_for_one_segment` | `spikeinterface.postprocessing.correlograms.correlogram_for_one_segment` | `spikeinterface/postprocessing/correlograms.py` | `spike_times: numpy.ndarray, spike_unit_indices: numpy.ndarray, window_size: int, bin_size: int` | `Compute cross-correlograms for one recording segment using an optimized algorithm adapted from the Phy package (originally by Cyrille Rossant). This function is used in SpikeInterface post-processing to quantify temporal relationships between spike trains produced by spike sorting: it bins time differences between spikes from all pairs of units within a specified window (measured in samples) and returns integer counts per lag-time bin for every unit pair. The implementation is optimized for large spike collections by iteratively shifting the spike train, masking out spikes outside the window, binning delays by floor division, and using ravel_multi_index plus bincount to increment counts efficiently.` |
| `spikeinterface_postprocessing_localization_tools_enforce_decrease_shells_data` | `spikeinterface.postprocessing.localization_tools.enforce_decrease_shells_data` | `spikeinterface/postprocessing/localization_tools.py` | `wf_data: numpy.ndarray, maxchan: int, radial_parents: dict, in_place: bool = False` | `spikeinterface.postprocessing.localization_tools.enforce_decrease_shells_data enforces a radial decrease constraint on a one-dimensional per-channel data vector used in localization postprocessing within the SpikeInterface framework. In the spike-sorting/localization domain (see README), this function is used to regularize per-channel scalar metrics (for example per-channel waveform amplitudes, energies, or localization scores computed for a single waveform or template) so that values on channels in outer radial shells do not exceed the maximum value of their parent (closer-to-center) channels; any channel value that exceeds its parents' maximum is scaled down to that maximum.` |
| `spikeinterface_postprocessing_localization_tools_get_grid_convolution_templates_and_weights` | `spikeinterface.postprocessing.localization_tools.get_grid_convolution_templates_and_weights` | `spikeinterface/postprocessing/localization_tools.py` | `contact_locations: numpy.ndarray, radius_um: float = 40, upsampling_um: float = 5, margin_um: float = 50, weight_method: dict = {'mode': 'exponential_3d'}` | `Get an upsampled 2D grid of artificial templates from a probe contact layout for use in localization and post-processing of spike-sorted extracellular recordings. This function is used in SpikeInterface post-processing to construct a dense, regularly spaced set of template positions (a grid) over the spatial extent of a probe's recorded contacts. The grid is created in physical units of micrometers (um) and is intended for downstream localization and template-weighting operations (for example, to estimate spike source locations or to compute convolutional template weights for channels). The function computes pairwise distances between each probe contact and each grid template, creates a sparsity mask of nearby templates per contact based on a radius, and delegates to get_convolution_weights() (via the weight_method argument) to compute per-channel template weights and any z-axis scaling factors required by the chosen weight model.` |
| `spikeinterface_postprocessing_localization_tools_make_radial_order_parents` | `spikeinterface.postprocessing.localization_tools.make_radial_order_parents` | `spikeinterface/postprocessing/localization_tools.py` | `geom: numpy.ndarray, neighbours_mask: numpy.ndarray, n_jumps_per_growth: int = 1, n_jumps_parent: int = 3` | `Pre-computes a radial parent lookup structure used by enforce_decrease_shells in the localization post-processing tools of SpikeInterface. This function analyzes the spatial probe geometry and a per-channel neighbor mask to determine, for each channel, an ordered list of "child" neighbor channels and for each child the set of already-seen "parent" neighbor indices that lie closer to the source channel. The resulting structure is intended to be used by enforce_decrease_shells to enforce a radial decrease constraint (for example, decreasing spike amplitude or score with radial distance) when post-processing localization or quality metrics.` |
| `spikeinterface_postprocessing_localization_tools_make_shell` | `spikeinterface.postprocessing.localization_tools.make_shell` | `spikeinterface/postprocessing/localization_tools.py` | `channel: int, geom: numpy.ndarray, n_jumps: int = 1` | `make_shell computes the set of channel indices forming a single "shell" of neighbors around a reference channel in electrode geometry. It is a helper used by spikeinterface.postprocessing.localization_tools.make_shells and by other localization/post-processing utilities in SpikeInterface to select neighbouring channels for waveform localization, template extraction, and downstream quality metrics. This function measures Euclidean distances between the reference channel and all channels using scipy.spatial.distance.cdist, selects the nth unique nonzero distance (controlled by n_jumps) as the shell radius, and returns the indices of all channels whose distance is less than or equal to that radius (within a floating-point tolerance). The returned indices exclude the reference channel itself.` |
| `spikeinterface_postprocessing_localization_tools_make_shells` | `spikeinterface.postprocessing.localization_tools.make_shells` | `spikeinterface/postprocessing/localization_tools.py` | `geom: numpy.ndarray, n_jumps: int = 1` | `Get neighbor-channel "shells" for each electrode channel based on inter-channel distances. This function is used in spike sorting post-processing (spikeinterface.postprocessing.localization_tools) to determine, for every recording channel, which other channels lie within successive nearest-neighbor distance shells. In practical spike-sorting workflows this is useful to restrict computations (for example waveform extraction, localization, template updates, or quality metric calculations) to channels that are spatially close to a given channel. The radius that defines each shell is computed from the empirical distances between channels: first the distance to the closest other channel, then the next distinct larger distance, and so on, up to n_jumps distinct distance levels.` |
| `spikeinterface_postprocessing_template_metrics_fit_velocity` | `spikeinterface.postprocessing.template_metrics.fit_velocity` | `spikeinterface/postprocessing/template_metrics.py` | `peak_times: numpy.ndarray, channel_dist: numpy.ndarray` | `Fit velocity from peak times and channel distances using a robust Theil‚ÄìSen linear estimator. This function is used in SpikeInterface post-processing (template_metrics) to estimate the propagation velocity of a spike waveform across recording channels. Given a set of peak times (for example, times of maximum amplitude of a template on each channel) and the corresponding channel distances from a chosen reference channel, the function fits a linear model of the form channel_dist = slope * peak_times + intercept using sklearn.linear_model.TheilSenRegressor. The fitted slope is interpreted as the propagation velocity (distance per unit time) when peak_times and channel_dist are provided in consistent physical units (for example, seconds and micrometers). The method is robust to outliers compared to ordinary least squares and returns a goodness-of-fit score (coefficient of determination, R^2) from the estimator.` |
| `spikeinterface_postprocessing_template_metrics_get_exp_decay` | `spikeinterface.postprocessing.template_metrics.get_exp_decay` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, sampling_frequency: float = None, **kwargs` | `Compute the exponential spatial decay constant of a template waveform's channel peak amplitudes used in SpikeInterface post-processing template metrics (units um/s). This function is intended for use in the postprocessing stage of SpikeInterface to quantify how a unit's template amplitude falls off with distance across recording channels. The returned decay constant is useful as a compact quality metric for assessing the spatial spread/localization of a putative neuron (e.g., for curation or comparison of sorters).` |
| `spikeinterface_postprocessing_template_metrics_get_half_width` | `spikeinterface.postprocessing.template_metrics.get_half_width` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, trough_idx: int = None, peak_idx: int = None, **kwargs` | `Return the half width of a single-unit template waveform in seconds. This function is part of SpikeInterface's postprocessing template metrics used in spike sorting quality assessment and curation. Given a 1D template waveform for a single unit (for example, the average or median spike waveform on a channel), get_half_width computes the temporal width at half the trough amplitude (half-width) measured in seconds. The half-width is a commonly used spike metric that captures spike duration and helps distinguish cell types and assess sorting quality. If trough_idx or peak_idx are not provided, they are inferred by calling get_trough_and_peak_idx(template_single). The function assumes a baseline of 0 when computing the half-amplitude threshold (threshold = 0.5 * trough_value). No in-place modifications are made to template_single.` |
| `spikeinterface_postprocessing_template_metrics_get_num_negative_peaks` | `spikeinterface.postprocessing.template_metrics.get_num_negative_peaks` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, **kwargs` | `Count the number of negative peaks in a single template waveform used for spike-sorting postprocessing. This function is part of the spikeinterface.postprocessing.template_metrics module and is used to quantify how many distinct negative deflections (negative-going peaks) appear in a neuron's average spike waveform (template). This metric is useful in spike sorting and template quality assessment to characterize waveform shape (for example, to detect multi-phasic waveforms or split templates).` |
| `spikeinterface_postprocessing_template_metrics_get_num_positive_peaks` | `spikeinterface.postprocessing.template_metrics.get_num_positive_peaks` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, **kwargs` | `Count the number of positive peaks in a single-unit template waveform. This function is part of SpikeInterface post-processing template metrics and is used to quantify a template's morphology for spike sorting validation and curation. It inspects a 1D template waveform for a single unit (template_single) and returns how many positive (local maximum) peaks exceed a threshold relative to the waveform's maximum absolute amplitude. The function uses scipy.signal.find_peaks with a height threshold computed as peak_relative_threshold * max(abs(template_single)) and a width constraint derived from peak_width_ms converted to samples using sampling_frequency. This metric can help characterize multi-peaked templates which may influence downstream quality metrics and manual curation decisions.` |
| `spikeinterface_postprocessing_template_metrics_get_peak_to_valley` | `spikeinterface.postprocessing.template_metrics.get_peak_to_valley` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, trough_idx: int = None, peak_idx: int = None, **kwargs` | `spikeinterface.postprocessing.template_metrics.get_peak_to_valley returns the peak-to-valley duration of a single template waveform in seconds, a common spike-sorting quality metric used in SpikeInterface post-processing to characterize waveform shape and temporal relationships between trough and peak. This function computes the time difference between a waveform peak and trough expressed in seconds. In the SpikeInterface context, this measurement is used in post-processing and quality metrics to help assess unit isolation and waveform morphology. If trough and peak sample indices are not provided, the function will call get_trough_and_peak_idx(template_single) to determine them automatically from the 1D template waveform. The computed value is (peak_idx - trough_idx) / sampling_frequency, so the sign indicates whether the peak occurs after (positive) or before (negative) the trough.` |
| `spikeinterface_postprocessing_template_metrics_get_peak_trough_ratio` | `spikeinterface.postprocessing.template_metrics.get_peak_trough_ratio` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float = None, trough_idx: int = None, peak_idx: int = None, **kwargs` | `Compute the peak-to-trough ratio of a single template waveform used in SpikeInterface post-processing metrics. This function is used in the SpikeInterface postprocessing.template_metrics module to quantify waveform asymmetry for a single 1D template waveform (for example, an average extracellular spike waveform computed by the framework). The ratio is computed as the sample value at peak_idx divided by the sample value at trough_idx and is returned as a unitless float that can be used as one feature in spike sorting quality assessment and curation workflows.` |
| `spikeinterface_postprocessing_template_metrics_get_recovery_slope` | `spikeinterface.postprocessing.template_metrics.get_recovery_slope` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, peak_idx: int = None, **kwargs` | `Return the recovery slope of a single template waveform after its peak, measured as change in amplitude per unit time (unit of template / second). In the SpikeInterface framework for spike sorting and post-processing, this function quantifies the post-spike repolarization/hyperpolarization trend (the slope of the waveform after the action potential peak) within a user-defined time window. This metric is useful for quality metrics and waveform characterization in spike sorting analyses. By default, templates may be scaled to microvolts (uV) when produced by SortingAnalyzer (controlled by sorting_analyzer.return_in_uV); in that case the returned slope will be in uV/s. The function computes the slope by performing a linear regression (scipy.stats.linregress) of the template amplitude versus time over the samples from the peak index up to a time corresponding to recovery_window_ms after the peak. Time is derived from sample indices divided by sampling_frequency (seconds). If peak_idx is not provided, the function calls get_trough_and_peak_idx(template_single) to determine the peak index. The window endpoint is clipped to the template length. The function asserts that the required kwarg recovery_window_ms is present and will raise an AssertionError if it is not. If the peak is at index 0 the function returns numpy.nan. Note that sampling_frequency is used as a divisor to compute time; providing sampling_frequency equal to zero will lead to a division error.` |
| `spikeinterface_postprocessing_template_metrics_get_repolarization_slope` | `spikeinterface.postprocessing.template_metrics.get_repolarization_slope` | `spikeinterface/postprocessing/template_metrics.py` | `template_single: numpy.ndarray, sampling_frequency: float, trough_idx: int = None, **kwargs` | `Return slope of the repolarization period of a single average spike template between its trough (maximum hyperpolarization) and the baseline. This function is used in SpikeInterface postprocessing (template_metrics) to quantify how quickly a neuron's membrane potential recovers after the trough of an action potential. The repolarization slope is computed as the linear regression slope (dV/dT) of the template waveform between the trough index and the first sample after the trough where the waveform returns to baseline (value >= 0). The returned value has units of (unit of template) per second; when templates are scaled to microvolts (uV) by SortingAnalyzer.return_in_uV, the result is in uV/s. The implementation derives time points from the sample indices using the provided sampling_frequency and uses scipy.stats.linregress to compute the slope.` |
| `spikeinterface_postprocessing_template_metrics_get_spread` | `spikeinterface.postprocessing.template_metrics.get_spread` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, sampling_frequency: float, **kwargs` | `Compute the spatial spread (in micrometers) of a unit template's amplitude along a specified depth axis. This function is used in the SpikeInterface postprocessing pipeline to quantify how far, across probe depth, a template's amplitude is distributed. It is intended for use after spike sorting and template extraction to provide a simple, interpretable metric (spread in um) that helps validate and compare units (for example, to identify spatially compact versus distributed templates). The computation first optionally restricts channels to a lateral column_range, sorts channels by depth, computes per-channel peak-to-peak amplitude of the template, optionally smooths that amplitude profile in depth using a Gaussian kernel (spread_smooth_um), normalizes the profile to its maximum, selects channels whose normalized amplitude exceeds spread_threshold, and returns the peak-to-peak extent (ptp) of those channel depths.` |
| `spikeinterface_postprocessing_template_metrics_get_trough_and_peak_idx` | `spikeinterface.postprocessing.template_metrics.get_trough_and_peak_idx` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray` | `spikeinterface.postprocessing.template_metrics.get_trough_and_peak_idx returns the indices into a 1D numpy template waveform corresponding to the detected trough (the minimum value) and the subsequent peak (the maximum value at or after the trough). This function is used in SpikeInterface postprocessing and template metrics to locate trough-to-peak features of averaged spike waveforms for tasks such as waveform alignment, amplitude measurement, and quality metric computation. The function expects a single-channel (1D) template waveform and assumes a negative trough followed by a positive peak. It does not modify the input array; it only reads values and computes two integer indices. The trough index is computed with numpy.argmin over the entire template. The peak index is computed relative to the trough as trough_idx + numpy.argmax(template[trough_idx:]), so the peak is searched for at or after the trough index. Because it uses numpy.argmin/argmax, tie-breaking follows numpy semantics (the first occurrence is returned). If the input is not 1-dimensional an AssertionError is raised by the internal assertion. If the template is empty, numpy.argmin/argmax will raise a ValueError. Presence of NaNs or other non-finite values will influence results according to numpy.argmin/argmax behavior.` |
| `spikeinterface_postprocessing_template_metrics_get_velocity_above` | `spikeinterface.postprocessing.template_metrics.get_velocity_above` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, sampling_frequency: float, **kwargs` | `get_velocity_above computes the propagation velocity of the template waveform above its maximum-amplitude channel, returning the velocity in micrometers per second (um/s) as used in spike sorting post-processing to quantify vertical spike propagation across channels. This function is used in the SpikeInterface post-processing pipeline to estimate how quickly the peak of a template travels away from the channel with the largest (most negative) deflection. The input template is a waveform extracted from extracellular recordings (num_samples, num_channels), and channel_locations gives the spatial coordinates of each channel (num_channels, 2). The function converts sample indices to times using sampling_frequency (Hz), selects channels located "above" the max channel according to depth_direction, fits a linear model relating peak time to distance, and returns the fitted slope as the velocity. The function will return numpy.nan when the velocity cannot be robustly estimated (insufficient channels or low fit quality).` |
| `spikeinterface_postprocessing_template_metrics_get_velocity_below` | `spikeinterface.postprocessing.template_metrics.get_velocity_below` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, sampling_frequency: float, **kwargs` | `Compute the propagation velocity of the spike waveform below the maximum-amplitude channel of a template. The function is used in spike sorting post-processing to estimate how quickly a spike peak travels across electrodes positioned in space (channel_locations). In the SpikeInterface workflow this metric helps characterize unit depth/propagation and can be used for quality metrics or anatomical interpretation. The original documentation states the velocity is reported in units um/s; note that the implementation computes peak times in milliseconds and distances in micrometers and fits a linear model, so the fitted slope returned by fit_velocity is derived from distances (um) versus times (ms) (see "Returns" for unit clarification and a note about a common units mismatch).` |
| `spikeinterface_postprocessing_template_metrics_sort_template_and_locations` | `spikeinterface.postprocessing.template_metrics.sort_template_and_locations` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, depth_direction: str = "y"` | `spikeinterface.postprocessing.template_metrics.sort_template_and_locations sorts a template waveform array and the corresponding channel location coordinates by electrode depth. This function is used in spike sorting post-processing to reorder channels and their spatial coordinates so that downstream metrics, visualizations, or curation workflows see channels in increasing depth order (e.g., from superficial to deep electrodes). The function implements a deterministic ascending sort of channels based on a single spatial axis (depth), where the axis used is selected by the depth_direction parameter.` |
| `spikeinterface_postprocessing_template_metrics_transform_column_range` | `spikeinterface.postprocessing.template_metrics.transform_column_range` | `spikeinterface/postprocessing/template_metrics.py` | `template: numpy.ndarray, channel_locations: numpy.ndarray, column_range: float, depth_direction: str = "y"` | `Transform template and channel locations to include only channels within a given column range around the template's peak channel. This function is used in the SpikeInterface postprocessing/template_metrics workflow to restrict a spike template and its associated channel coordinates to a spatial subset (a "column") centered on the channel that has the largest peak-to-peak amplitude in the provided template. The result is commonly used when computing template-based quality metrics or visualizing waveforms on a subset of electrode columns in extracellular recording arrays.` |
| `spikeinterface_postprocessing_template_similarity_check_equal_template_with_distribution_overlap` | `spikeinterface.postprocessing.template_similarity.check_equal_template_with_distribution_overlap` | `spikeinterface/postprocessing/template_similarity.py` | `waveforms0: numpy.ndarray, waveforms1: numpy.ndarray, template0: numpy.ndarray = None, template1: numpy.ndarray = None, num_shift: int = 2, quantile_limit: float = 0.8, return_shift: bool = False` | `Check whether two sets of spike waveforms come from the same underlying waveform distribution by measuring overlap of their projected scalar-value distributions along the vector between their templates. This function is used in the SpikeInterface postprocessing context (for example internally by tridesclous for the automatic merge step) and can also serve as a simple distance metric between two clusters of spikes when building or evaluating sorting pipelines. The function projects each waveform onto the normalized vector from template0 to template1, computes per-cluster scalar projections, and compares specified quantiles of these scalar distributions across a range of discrete sample shifts. If the chosen quantile of waveforms0 is greater than or equal to the complementary quantile of waveforms1 for any tested shift, the clusters are considered equal (i.e., too overlapping to be reliably distinct). The tested shifts allow the comparison to be robust to small temporal misalignments between clusters.` |
| `spikeinterface_preprocessing_detect_bad_channels_detect_bad_channels_ibl` | `spikeinterface.preprocessing.detect_bad_channels.detect_bad_channels_ibl` | `spikeinterface/preprocessing/detect_bad_channels.py` | `raw: numpy.ndarray, fs: float, psd_hf_threshold: float, dead_channel_thr: float = -0.5, noisy_channel_thr: float = 1.0, outside_channel_thr: float = -0.75, n_neighbors: int = 11, nyquist_threshold: float = 0.8, welch_window_ms: float = 0.3, outside_channels_location: str = "top"` | `Bad channels detection for Neuropixel probes developed by the International Brain Laboratory (IBL). This function is intended for pre-processing extracellular recordings in the SpikeInterface pipeline: it analyzes per-channel coherence and high-frequency power to flag channels that are likely dead (very low coherence/amplitude), noisy (high high-frequency power and/or high coherence), or located outside of the brain (contiguous channels at probe extremes with low trend coherence). The function implements the IBL heuristic using a Welch power spectral density (PSD) estimate and a local-detrending coherence measure computed across probe channels; it is typically used before spike sorting to exclude or mark channels that would degrade sorting quality.` |
| `spikeinterface_preprocessing_detect_bad_channels_detrend` | `spikeinterface.preprocessing.detect_bad_channels.detrend` | `spikeinterface/preprocessing/detect_bad_channels.py` | `x: numpy.ndarray, nmed: int` | `spikeinterface.preprocessing.detect_bad_channels.detrend subtracts a median-filtered trend from a 1-D signal (vector) using endpoint tapering. This function is used in spike preprocessing (for example, in detect_bad_channels) to remove slowly varying baseline or trend from an extracellular recording channel prior to computing metrics for bad-channel detection or other downstream analyses.` |
| `spikeinterface_preprocessing_highpass_spatial_filter_agc` | `spikeinterface.preprocessing.highpass_spatial_filter.agc` | `spikeinterface/preprocessing/highpass_spatial_filter.py` | `traces: numpy.ndarray, window: numpy.ndarray, epsilon: float = 1e-08` | `Automatic gain control (AGC) applied to multichannel time series used in SpikeInterface preprocessing. This function computes a local amplitude envelope per time sample and channel by convolving the absolute traces with a 1-D window kernel, uses a small epsilon regularizer to avoid division by zero, and divides the input traces by the computed gain to produce AGC-normalized traces. In the spike-sorting domain (SpikeInterface preprocessing of extracellular recordings) this reduces slow amplitude variations across time and channels, aiding consistent spike detection and downstream sorting.` |
| `spikeinterface_preprocessing_highpass_spatial_filter_fcn_cosine` | `spikeinterface.preprocessing.highpass_spatial_filter.fcn_cosine` | `spikeinterface/preprocessing/highpass_spatial_filter.py` | `bounds: tuple` | `Return a soft-thresholding callable that applies a cosine taper between two numeric bounds used by preprocessing highpass_spatial_filter in SpikeInterface. This function constructs and returns a lambda that implements a smooth, cosine-shaped transition (taper) between two threshold bounds. In the SpikeInterface preprocessing context, this is used to produce a soft thresholding function for spatial high-pass filtering or related preprocessing steps where abrupt clipping of values is undesirable; the cosine taper yields a gradual interpolation between passing values unchanged below the lower bound and clamping values to the upper bound above the upper bound.` |
| `spikeinterface_preprocessing_motion_get_motion_parameters_preset` | `spikeinterface.preprocessing.motion.get_motion_parameters_preset` | `spikeinterface/preprocessing/motion.py` | `preset: str` | `Get the parameters tree for a named preset used by the motion-correction steps in the preprocessing pipeline. This function is part of SpikeInterface's preprocessing.motion utilities and returns a fully-resolved parameters dictionary (a "parameters tree") for a given preset name. In the spike sorting domain, motion correction is a preprocessing stage that compensates for probe and tissue movement by running a sequence of algorithmic steps (for example: selecting peaks, estimating drift, splitting/merging segments). Each step can have multiple methods and method-specific parameters. Presets are high-level named configurations that select methods and override a subset of parameters for those steps. This function takes the named preset, deep-copies the preset definition from the internal registry, merges it with the framework's default motion parameters for each step and method, and returns the resulting parameter tree that the motion-correction pipeline consumes to configure and run each step.` |
| `spikeinterface_preprocessing_motion_load_motion_info` | `spikeinterface.preprocessing.motion.load_motion_info` | `spikeinterface/preprocessing/motion.py` | `folder: str` | `Loads motion-related metadata and Motion object from a folder produced by SpikeInterface preprocessing. This function reads JSON metadata, optional numpy arrays, and either a saved Motion object (current implementation) or a legacy set of numpy files and returns a dictionary summarizing all loaded motion information. In the SpikeInterface domain this is used to recover motion/drift estimation outputs produced during preprocessing so they can be inspected, visualized, or used by downstream postprocessing and quality-metric computations.` |
| `spikeinterface_preprocessing_motion_save_motion_info` | `spikeinterface.preprocessing.motion.save_motion_info` | `spikeinterface/preprocessing/motion.py` | `motion_info: dict, folder: str, overwrite: bool = False` | `spikeinterface.preprocessing.motion.save_motion_info saves motion-analysis results produced by compute_motion to disk in a structured folder so they can be reloaded for downstream preprocessing, drift correction, benchmarking, and reproducibility in spike sorting workflows provided by SpikeInterface. This function expects the motion_info dictionary returned by compute_motion and writes several files into the target folder: a JSON-serialized copy of the analysis parameters, a JSON file of run times, NumPy .npy files for detected peaks and their locations, and, if present, a serialized motion object saved via its own save method. The function creates the folder (including parent directories) when needed, and enforces an explicit overwrite policy to avoid accidental data loss.` |
| `spikeinterface_preprocessing_phase_shift_apply_frequency_shift` | `spikeinterface.preprocessing.phase_shift.apply_frequency_shift` | `spikeinterface/preprocessing/phase_shift.py` | `signal: numpy.ndarray, shift_samples: numpy.ndarray, axis: int = 0` | `Apply a sub-sample-accurate frequency (phase) shift to a multi-channel signal buffer. This function is intended for preprocessing extracellular recordings in SpikeInterface prior to spike sorting or waveform extraction. It shifts each channel by a (possibly fractional) number of samples using the Fourier shift theorem: the signal is transformed to the frequency domain with a real FFT (rFFT), a complex phase rotation that corresponds to the requested time shift is applied per-frequency-bin and per-channel, and the result is transformed back to the time domain with an inverse real FFT (irFFT). This produces time shifts that are accurate below the sampling-period resolution and so are useful for aligning channels, correcting propagation delays between channels, or fine temporal registration of multichannel recordings before downstream spike-sorting, quality-metric computation, or waveform extraction.` |
| `spikeinterface_preprocessing_phase_shift_apply_fshift_ibl` | `spikeinterface.preprocessing.phase_shift.apply_fshift_ibl` | `spikeinterface/preprocessing/phase_shift.py` | `w: numpy.ndarray, s: numpy.ndarray, axis: int = 0, ns: int = None` | `apply_fshift_ibl shifts a 1D or 2D signal in the frequency domain to implement accurate non-integer sample shifts used in spike preprocessing and alignment. This function is adapted from IBLIB (https://github.com/int-brain-lab/ibllib/blob/master/ibllib/dsp/fourier.py) and is used in SpikeInterface preprocessing.phase_shift to perform sub-sample shifts on extracellular recordings or their frequency-domain representations. It multiplies the Fourier transform of the input by a phase factor corresponding to the requested shift(s), allowing precise alignment of waveforms across channels or time without relying on interpolation in the time domain.` |
| `spikeinterface_preprocessing_pipeline_get_preprocessing_dict_from_analyzer` | `spikeinterface.preprocessing.pipeline.get_preprocessing_dict_from_analyzer` | `spikeinterface/preprocessing/pipeline.py` | `analyzer_folder: str, format: str = "auto", backend_options: dict = None` | `spikeinterface.preprocessing.pipeline.get_preprocessing_dict_from_analyzer generates a preprocessing dictionary from a saved analyzer folder so the dictionary can be passed to PreprocessingPipeline to recreate the same preprocessing steps used when the analyzer was created. This function is used within the SpikeInterface framework to extract preprocessing configuration (e.g., filters, referencing, channel selection) that was stored by an Analyzer or the newer SortingAnalyzer during preprocessing and recording preparation, enabling reproducible downstream processing and consistent post-processing or quality-metric computation.` |
| `spikeinterface_preprocessing_pipeline_get_preprocessing_dict_from_file` | `spikeinterface.preprocessing.pipeline.get_preprocessing_dict_from_file` | `spikeinterface/preprocessing/pipeline.py` | `recording_dictionary_path: str` | `spikeinterface.preprocessing.pipeline.get_preprocessing_dict_from_file generates a preprocessing dictionary from a saved recording descriptor file so it can be passed to apply_preprocessing_pipeline or used to construct a PreprocessPipeline. This function is part of the SpikeInterface preprocessing utilities for spike-sorting workflows; it reads a recording dictionary previously saved to disk (by SpikeInterface recording save routines) and extracts only the preprocessing steps that can be applied globally to any recording (it deliberately excludes per-channel or per-frame operations such as ChannelSlice and FrameSlice). The returned dictionary maps preprocessing step names to their keyword-argument dictionaries and is suitable for automated reproducibility of preprocessing pipelines or for re-applying the same global preprocessing to other recordings.` |
| `spikeinterface_preprocessing_preprocessing_tools_get_kriging_channel_weights` | `spikeinterface.preprocessing.preprocessing_tools.get_kriging_channel_weights` | `spikeinterface/preprocessing/preprocessing_tools.py` | `contact_positions1: numpy.ndarray, contact_positions2: numpy.ndarray, sigma_um: float, p: float, weight_threshold: float = 0.005` | `Calculate kriging interpolation weights between two sets of recording contacts. This function computes a kernel-based weight matrix that maps values on a source set of contacts (contact_positions1) to a target set of contacts (contact_positions2) using a spatial kernel controlled by sigma_um and p. Very small weights (below weight_threshold) are zeroed to improve numerical stability and sparsity. The implementation delegates kernel computation to get_kriging_kernel_distance and then normalizes weights so that, for each target contact (each column), the weights sum to 1 when possible. This function is used in extracellular preprocessing and bad-channel interpolation workflows in spike sorting pipelines (for example, the International Brain Laboratory pipeline referenced in the source) to estimate channel contributions from nearby contacts when reconstructing or interpolating signals.` |
| `spikeinterface_preprocessing_preprocessing_tools_get_kriging_kernel_distance` | `spikeinterface.preprocessing.preprocessing_tools.get_kriging_kernel_distance` | `spikeinterface/preprocessing/preprocessing_tools.py` | `locations_1: numpy.ndarray, locations_2: numpy.ndarray, sigma_um: list, p: float, distance_metric: str = "euclidean"` | `Get the kriging kernel between two sets of spatial channel locations for use in spike-sorting preprocessing.` |
| `spikeinterface_preprocessing_preprocessing_tools_get_spatial_interpolation_kernel` | `spikeinterface.preprocessing.preprocessing_tools.get_spatial_interpolation_kernel` | `spikeinterface/preprocessing/preprocessing_tools.py` | `source_location: numpy.ndarray, target_location: numpy.ndarray, method: str = "kriging", sigma_um: float = 20.0, p: int = 1, num_closest: int = 4, sparse_thresh: float = None, dtype: str = "float32", force_extrapolate: bool = False` | `Compute the spatial interpolation kernel used for linear spatial interpolation of extracellular recordings. This function is used in SpikeInterface preprocessing to interpolate signals from a set of source electrode/contact locations to a set of target locations. Typical use cases in the SpikeInterface framework include replacing bad channels by spatial interpolation and correcting drift by interpolating between contacts. The function implements three methods: "kriging" (the default, matching the approach used in kilosort and an adaptation of pykilosort), "idw" (inverse distance weighting), and "nearest" (assign the nearest source). The returned kernel is a weight matrix that maps source-channel signals to target locations: output_signal = interpolation_kernel.T @ source_signals (or equivalently each column of the kernel contains weights for a target location). The function is pure (no side effects on inputs) and performs numerical regularization for kriging; it may produce warnings or NaNs if extreme inputs (for example all-zero weights for a target) occur.` |
| `spikeinterface_preprocessing_whiten_compute_sklearn_covariance_matrix` | `spikeinterface.preprocessing.whiten.compute_sklearn_covariance_matrix` | `spikeinterface/preprocessing/whiten.py` | `data: numpy.ndarray, regularize_kwargs: dict` | `Compute the covariance matrix using a scikit-learn covariance estimator for preprocessing (whitening) of extracellular recordings in the SpikeInterface spike-sorting workflow. This function is used during preprocessing/whitening of extracellular electrophysiology data (as performed in SpikeInterface) to estimate the channels/features covariance matrix required to decorrelate or whiten the data prior to spike detection and sorting. It delegates estimation to a class from sklearn.covariance specified by the "method" entry in regularize_kwargs, forces centered-data behavior, and fits the estimator on the provided data after converting it to float64 (required by scikit-learn). Note that scikit-learn covariance implementations that are provided as standalone functions (rather than classes in sklearn.covariance) are not supported by this wrapper.` |
| `spikeinterface_preprocessing_whiten_compute_whitening_from_covariance` | `spikeinterface.preprocessing.whiten.compute_whitening_from_covariance` | `spikeinterface/preprocessing/whiten.py` | `cov: numpy.ndarray, eps: float` | `spikeinterface.preprocessing.whiten.compute_whitening_from_covariance computes the ZCA whitening matrix from a provided covariance matrix for use in preprocessing extracellular recordings (for example, to decorrelate channels prior to spike detection and sorting). The function performs a singular value decomposition (SVD) of the covariance matrix and builds a whitening transform that regularizes small or zero eigenvalues using the supplied eps parameter to ensure numerical stability.` |
| `spikeinterface_qualitymetrics_misc_metrics_amplitude_cutoff` | `spikeinterface.qualitymetrics.misc_metrics.amplitude_cutoff` | `spikeinterface/qualitymetrics/misc_metrics.py` | `amplitudes: numpy.ndarray, num_histogram_bins: int = 500, histogram_smoothing_value: int = 3, amplitudes_bins_min_ratio: int = 5` | `spikeinterface.qualitymetrics.misc_metrics.amplitude_cutoff: Compute an approximate fraction of spikes that are missing for a single unit by analyzing the empirical distribution of spike amplitudes. This function is used in spike sorting quality assessment to estimate how many low-amplitude spikes may have been lost because they fall below the detection threshold; it is intended to be used on the amplitudes (in microvolts) of all detected spikes for one unit and is referenced by higher-level helpers such as compute_amplitude_cutoffs.` |
| `spikeinterface_qualitymetrics_misc_metrics_isi_violations` | `spikeinterface.qualitymetrics.misc_metrics.isi_violations` | `spikeinterface/qualitymetrics/misc_metrics.py` | `spike_trains: list, total_duration_s: float, isi_threshold_s: float = 0.0015, min_isi_s: float = 0` | `Calculate Inter-Spike Interval (ISI) violations for a single sorted unit across one or more recording segments. This function is used in SpikeInterface quality metrics to quantify refractory-period violations that indicate contamination or mis-sorting of a unit. It counts adjacent spike pairs whose inter-spike interval (ISI) is below a biophysical threshold (isi_threshold_s), aggregates counts across provided recording segments, and computes two summary metrics: (1) isi_violations_ratio, the ratio of the violation rate to the overall firing rate (dimensionless fraction); and (2) isi_violations_rate, the absolute rate of violating spikes in spikes per second. The algorithm follows the implementation documented in compute_isi_violations: for each segment the function computes ISIs via successive differences, sums ISIs < isi_threshold_s to get num_violations, and computes violation_time = 2 * num_spikes * (isi_threshold_s - min_isi_s) to normalize the violation rate.` |
| `spikeinterface_qualitymetrics_misc_metrics_presence_ratio` | `spikeinterface.qualitymetrics.misc_metrics.presence_ratio` | `spikeinterface/qualitymetrics/misc_metrics.py` | `spike_train: numpy.ndarray, total_length: int, bin_edges: numpy.ndarray = None, num_bin_edges: int = None, bin_n_spikes_thres: int = 0` | `Calculate the presence ratio for a single unit across a recording by dividing the number of temporal bins in which the unit is "active" by the total number of temporal bins. This metric is used in spike sorting quality assessment (see SpikeInterface quality metrics) to quantify how consistently a sorted unit is present throughout the recording: values close to 1 indicate the unit fires across most of the recording, values close to 0 indicate the unit is only present in a small fraction of the recording.` |
| `spikeinterface_qualitymetrics_misc_metrics_slidingRP_violations` | `spikeinterface.qualitymetrics.misc_metrics.slidingRP_violations` | `spikeinterface/qualitymetrics/misc_metrics.py` | `spike_samples: numpy.ndarray, sample_rate: float, duration: float, bin_size_ms: float = 0.25, window_size_s: float = 1, exclude_ref_period_below_ms: float = 0.5, max_ref_period_ms: float = 10, contamination_values: numpy.ndarray = None, return_conf_matrix: bool = False` | `A metric developed in the IBL / SteinmetzLab tradition that estimates the minimum contamination (fraction of non-physiological or mis-assigned spikes) consistent with observed refractory-period violations using a sliding-refractory-period method. This function is part of SpikeInterface quality-metrics utilities for post-processing spike-sorted data: it computes autocorrelograms from spike times, tests a range of candidate refractory-period durations and candidate contamination fractions, and returns the smallest contamination fraction that yields >90% confidence of explaining the observed violations. The method is useful when validating/curating spike sorting results to detect units with excessive refractory-period violations indicative of contamination or merging.` |
| `spikeinterface_qualitymetrics_pca_metrics_lda_metrics` | `spikeinterface.qualitymetrics.pca_metrics.lda_metrics` | `spikeinterface/qualitymetrics/pca_metrics.py` | `all_pcs: numpy.ndarray, all_labels: numpy.ndarray, this_unit_id: int` | `spikeinterface.qualitymetrics.pca_metrics.lda_metrics computes a d-prime separability measure for a single sorted unit using Linear Discriminant Analysis (LDA). This function is used within the SpikeInterface quality-metrics workflow to quantify how well the principal-component (PC) feature distribution of spikes assigned to a target unit (this_unit_id) is separated from the distribution of all other spikes; a larger d-prime indicates better separability and therefore higher presumed unit quality during spike sorting validation and curation.` |
| `spikeinterface_qualitymetrics_pca_metrics_mahalanobis_metrics` | `spikeinterface.qualitymetrics.pca_metrics.mahalanobis_metrics` | `spikeinterface/qualitymetrics/pca_metrics.py` | `all_pcs: numpy.ndarray, all_labels: numpy.ndarray, this_unit_id: int` | `spikeinterface.qualitymetrics.pca_metrics.mahalanobis_metrics calculates two commonly used spike-sorting unit quality metrics, isolation distance and L-ratio, from Mahalanobis distances computed in PCA feature space. These metrics quantify how well spikes assigned to a given unit (this_unit_id) are separated from spikes assigned to other units, using the empirical covariance of the unit's principal components. This function is used in SpikeInterface quality metrics pipelines to evaluate unit isolation and aid automated or manual curation of spike-sorting results.` |
| `spikeinterface_qualitymetrics_pca_metrics_nearest_neighbors_metrics` | `spikeinterface.qualitymetrics.pca_metrics.nearest_neighbors_metrics` | `spikeinterface/qualitymetrics/pca_metrics.py` | `all_pcs: numpy.ndarray, all_labels: numpy.ndarray, this_unit_id: int, max_spikes: int, n_neighbors: int` | `spikeinterface.qualitymetrics.pca_metrics.nearest_neighbors_metrics: Calculate nearest-neighbor based contamination metrics for a single sorted unit using PCA features. This function is used in the SpikeInterface quality-metrics pipeline to quantify cluster contamination and isolation by examining nearest neighbors in PCA feature space computed from spike waveforms. It constructs a feature matrix that places spikes from the target unit first and then all other spikes, optionally subsamples spikes when max_spikes is smaller than the total number of spikes, fits a scikit-learn NearestNeighbors model (ball_tree), excludes the self-nearest neighbor, and computes two scalar metrics: the hit rate (how often neighbors of target-unit spikes are from the target unit) and the miss rate (how often neighbors of other-unit spikes are from the target unit). These metrics are useful to assess contamination and separation of a putative unit after spike sorting.` |
| `spikeinterface_qualitymetrics_pca_metrics_silhouette_score` | `spikeinterface.qualitymetrics.pca_metrics.silhouette_score` | `spikeinterface/qualitymetrics/pca_metrics.py` | `all_pcs: numpy.ndarray, all_labels: numpy.ndarray, this_unit_id: int` | `spikeinterface.qualitymetrics.pca_metrics.silhouette_score calculates the silhouette score for a single sorted unit using principal components (PCs) of spike waveforms. This score is a standard cluster-quality metric that ranges from -1 (poor clustering, spikes likely misassigned) to +1 (well-separated cluster). In the SpikeInterface domain, this function is used as a quality metric to validate and curate spike-sorting outputs by quantifying how well spikes assigned to a given unit separate from spikes of other units in PC feature space. The function computes pairwise Euclidean distances (via scipy.spatial.distance.cdist) between spikes in the PC space, estimates the average distance from spikes of the target unit to each other cluster, selects the nearest other cluster (the one with the smallest mean inter-cluster distance), and then computes per-spike silhouette distances using the standard formula (b - a) / max(a, b) where a is the intra-unit pairwise distance and b is the distance to the nearest other cluster. The returned unit silhouette score is the mean of those per-spike silhouette distances.` |
| `spikeinterface_qualitymetrics_pca_metrics_simplified_silhouette_score` | `spikeinterface.qualitymetrics.pca_metrics.simplified_silhouette_score` | `spikeinterface/qualitymetrics/pca_metrics.py` | `all_pcs: numpy.ndarray, all_labels: numpy.ndarray, this_unit_id: int` | `spikeinterface.qualitymetrics.pca_metrics.simplified_silhouette_score: Compute the simplified silhouette score for a single cluster (unit) using principal components (PCs) of spike waveforms. This metric is a centroid-based approximation of the classic silhouette score used in clustering and spike sorting quality assessment; it is intended to provide a fast, interpretable measure of how well a unit's spikes are separated from spikes of other units in PC space and is used in SpikeInterface workflows for validating and curating spike sorting outputs.` |
| `spikeinterface_sorters_container_tools_find_recording_folders` | `spikeinterface.sorters.container_tools.find_recording_folders` | `spikeinterface/sorters/container_tools.py` | `d: dict` | `spikeinterface.sorters.container_tools.find_recording_folders finds the minimal set of filesystem folders that contain recording file paths described in a SpikeInterface-style dictionary and prepares them for use as container mount points. This function is used in the container tooling of SpikeInterface (a framework to run spike sorters in Docker/Singularity containers) to determine which host folders need to be mounted into a container so the sorter can access the recording files. It extracts file paths from the provided dictionary using the internal helper _get_paths_list, resolves each path, reduces them to their parent directories, and attempts to compute a single common parent folder when possible to minimize the number of mounts. The function performs only path computations and does not perform any I/O, create mounts, or modify the input dictionary.` |
| `spikeinterface_sorters_container_tools_path_to_unix` | `spikeinterface.sorters.container_tools.path_to_unix` | `spikeinterface/sorters/container_tools.py` | `path: str` | `spikeinterface.sorters.container_tools.path_to_unix converts a filesystem path string to a POSIX-style (Unix) path string. This helper is intended for use inside the SpikeInterface container tooling (spikeinterface.sorters.container_tools) when preparing host filesystem paths for use with Unix-based container runtimes (for example Docker or Singularity) that do not use Windows drive letters. It makes Windows-style paths compatible with Unix-style containers by removing the drive letter and colon and returning a forward-slash-separated path.` |
| `spikeinterface_sorters_external_yass_merge_params_dict` | `spikeinterface.sorters.external.yass.merge_params_dict` | `spikeinterface/sorters/external/yass.py` | `yass_params: dict, params: dict` | `spikeinterface.sorters.external.yass.merge_params_dict merges a default YASS parameter dictionary with caller-provided SpikeInterface-level parameters to produce a single configuration dictionary that the YASS external sorter wrapper can use. This function is used inside the SpikeInterface YASS sorter integration to translate generic sorter parameters (provided by SpikeInterface or the user) into specific entries expected by YASS, and to expose these combined parameters for running or configuring the YASS pipeline.` |
| `spikeinterface_sorters_launcher_run_sorter_jobs` | `spikeinterface.sorters.launcher.run_sorter_jobs` | `spikeinterface/sorters/launcher.py` | `job_list: list, engine: str = "loop", engine_kwargs: dict = None, return_output: bool = False` | `Run several run_sorter() calls either sequentially or using a parallel/asynchronous engine. This function is part of the SpikeInterface suite for running spike sorter jobs described in job dictionaries (see SpikeInterface README and run_sorter usage). Each entry in job_list is treated as the keyword arguments to a single call to run_sorter(...). The function selects an execution engine (loop, joblib, processpoolexecutor, dask, or slurm) and dispatches the jobs according to the chosen engine, merging provided engine-specific defaults with the engine_kwargs argument. It manages per-job flags required to request that run_sorter return outputs (by injecting the with_output key into each job dict), creates and submits batch scripts for slurm, and may return the list of Sorting objects produced by run_sorter when supported by the chosen engine.` |
| `spikeinterface_sorters_runsorter_read_sorter_folder` | `spikeinterface.sorters.runsorter.read_sorter_folder` | `spikeinterface/sorters/runsorter.py` | `folder: str, register_recording: bool = True, sorting_info: bool = True, raise_error: bool = True` | `spikeinterface.sorters.runsorter.read_sorter_folder loads a sorting result produced by a SpikeInterface-compatible sorter from a sorter output folder and reconstructs the in-memory sorting object for downstream post-processing, validation, visualization, or export. This function expects the folder to contain a SpikeInterface run log file named "spikeinterface_log.json" that documents the sorter name and the run outcome. It uses the "sorter_name" entry in that log to select the corresponding sorter implementation from the internal sorter registry (sorter_dict) and then delegates reconstruction to that sorter's get_result_from_folder method. This is typically used in the SpikeInterface workflow to programmatically re-load sorting outputs created by run_sorter so they can be inspected, have recordings attached, have sorting metadata re-associated, or be passed to quality metric and comparison tools.` |
| `spikeinterface_sorters_sorterlist_get_default_sorter_params` | `spikeinterface.sorters.sorterlist.get_default_sorter_params` | `spikeinterface/sorters/sorterlist.py` | `sorter_name_or_class: str` | `Returns the default parameter dictionary for a given spike sorter implementation, resolving the sorter either from a registered name or from a Sorter class. This function is part of the SpikeInterface unified framework for spike sorting and is used to obtain canonical runtime configuration values that downstream code (sorting pipelines, benchmarking, GUIs, containerized runs) rely on to run a sorter with its default settings.` |
| `spikeinterface_sorters_sorterlist_get_sorter_description` | `spikeinterface.sorters.sorterlist.get_sorter_description` | `spikeinterface/sorters/sorterlist.py` | `sorter_name_or_class: str` | `spikeinterface.sorters.sorterlist.get_sorter_description: Return the brief parameter description dictionary for a specified sorter used by the SpikeInterface framework. This function looks up and returns the sorter_description attribute defined on a registered sorter class. In the SpikeInterface domain (a unified framework for spike sorting), a "sorter" is a wrapper class that encapsulates a specific spike sorting algorithm and its configurable parameters; the returned description dictionary is used to document sorter parameters, drive GUIs, validate user-supplied parameters, generate help text for automated pipelines, and to assist in containerized or programmatic execution of sorters.` |
| `spikeinterface_sorters_sorterlist_get_sorter_params_description` | `spikeinterface.sorters.sorterlist.get_sorter_params_description` | `spikeinterface/sorters/sorterlist.py` | `sorter_name_or_class: str` | `Returns a description of the parameters supported by a specific spike sorter. This function is part of SpikeInterface, a unified framework for spike sorting (see README). It retrieves the parameter documentation that a given sorter exposes so callers (CLI, GUIs, workflow code, container wrappers, or benchmarking scripts) can present, validate, or programmatically set the parameters needed to run that sorter. The function accepts either the registered sorter name or the Sorter class object itself and returns the sorter-provided parameters description without modifying global state.` |
| `spikeinterface_sorters_utils_misc_get_git_commit` | `spikeinterface.sorters.utils.misc.get_git_commit` | `spikeinterface/sorters/utils/misc.py` | `git_folder: str, shorten: bool = True` | `Get commit to generate sorters version.` |
| `spikeinterface_sortingcomponents_clustering_isosplit_isocut_ensure_continuous_labels` | `spikeinterface.sortingcomponents.clustering.isosplit_isocut.ensure_continuous_labels` | `spikeinterface/sortingcomponents/clustering/isosplit_isocut.py` | `labels: numpy.ndarray` | `Ensure that a set of integer labels is remapped to a contiguous zero-based range [0, N-1].` |
| `spikeinterface_sortingcomponents_clustering_isosplit_isocut_isosplit` | `spikeinterface.sortingcomponents.clustering.isosplit_isocut.isosplit` | `spikeinterface/sortingcomponents/clustering/isosplit_isocut.py` | `X: numpy.ndarray, initial_labels: numpy.ndarray = None, n_init: int = 200, max_iterations_per_pass: int = 500, min_cluster_size: int = 10, isocut_threshold: float = 2.0, seed: int = None` | `spikeinterface.sortingcomponents.clustering.isosplit_isocut.isosplit Performs clustering of feature vectors using a Python/numba implementation of the Isosplit algorithm (Jeremy Magland's isosplit6) tailored for spike sorting components. This function is typically used in spike sorting pipelines to cluster spike waveform features or low-dimensional embeddings so that putative neural units (clusters) are automatically discovered from extracellular recording features. The implementation first optionally initializes cluster assignments with k-means (many centroids) and then iteratively agglomerates clusters using a dip-test based "isocut" criterion to merge similar clusters and enforce a minimum cluster size.` |
| `spikeinterface_sortingcomponents_clustering_merging_tools_merge_peak_labels_from_templates` | `spikeinterface.sortingcomponents.clustering.merging_tools.merge_peak_labels_from_templates` | `spikeinterface/sortingcomponents/clustering/merging_tools.py` | `peaks: numpy.ndarray, peak_labels: numpy.ndarray, unit_ids: list, templates_array: numpy.ndarray, template_sparse_mask: numpy.ndarray, similarity_metric: str = "l1", similarity_thresh: float = 0.8, num_shifts: int = 3, use_lags: bool = False` | `Low-level utility that merges peak labels by comparing template waveforms and grouping templates that exceed a similarity threshold. This function is used in the clustering and merging steps of SpikeInterface sorting components to clean likely oversplits (multiple units that should be a single unit) by computing pairwise template similarities and applying a merge mask to peak labels and templates. The function calls spikeinterface.postprocessing.template_similarity.compute_similarity_with_templates_array to compute a similarity matrix (and optional lag offsets) between templates and then delegates the application of the binary pairwise merge mask to an internal helper that rebuilds cleaned labels, merged template waveforms, and a merged sparsity mask.` |
| `spikeinterface_sortingcomponents_clustering_tools_aggregate_sparse_features` | `spikeinterface.sortingcomponents.clustering.tools.aggregate_sparse_features` | `spikeinterface/sortingcomponents/clustering/tools.py` | `peaks: numpy.ndarray, peak_indices: numpy.ndarray, sparse_feature: numpy.ndarray, sparse_target_mask: numpy.ndarray, target_channels: numpy.ndarray` | `Aggregate sparse features that were computed on per-peak, possibly unaligned channel sets and realign them onto a consistent set of target channels. This function is used in SpikeInterface sorting components to align back per-peak extracted data (for example waveforms, PCA features, or TSVD features) when detections were performed on differing sparse channel sets. Given a global peaks array and a mapping from global channels to sparse feature channel indices, the function builds a new array of features with a fixed last dimension equal to the number of target_channels. The function does not remove peaks that lack the full set of target channels; instead it flags them so downstream code can decide how to handle them.` |
| `spikeinterface_sortingcomponents_clustering_tools_apply_waveforms_shift` | `spikeinterface.sortingcomponents.clustering.tools.apply_waveforms_shift` | `spikeinterface/sortingcomponents/clustering/tools.py` | `waveforms: numpy.ndarray, peak_shifts: numpy.ndarray, inplace: bool = False` | `Apply a per-spike temporal shift to a batch of spike waveforms to realign their troughs before template computation in spike sorting workflows. This function is part of SpikeInterface sorting components (clustering tools) and is used when clusters or templates are merged but their constituent spikes have different trough (peak) time offsets. It shifts each waveform along the time axis so that spikes with early troughs are moved to the right and spikes with late troughs are moved to the left, facilitating correct averaging when computing templates after merge. Border time samples that would fall outside the original buffer are left unchanged. The operation can be performed in-place to avoid allocating a new array or on a copy to preserve the original buffer.` |
| `spikeinterface_sortingcomponents_matching_circus_compress_templates` | `spikeinterface.sortingcomponents.matching.circus.compress_templates` | `spikeinterface/sortingcomponents/matching/circus.py` | `templates_array: numpy.ndarray, approx_rank: int, remove_mean: bool = False, return_new_templates: bool = True` | `spikeinterface.sortingcomponents.matching.circus.compress_templates compresses spike template waveforms using singular value decomposition (SVD) into separable temporal, singular (diagonal/spectral), and spatial components for use in spike sorting workflows. This function is used in the SpikeInterface sorting components to reduce the dimensionality of template waveforms (templates_array), which can speed template matching, storage, and downstream comparisons in sorter pipelines (for example when building or optimizing custom sorters or components such as Circus).` |
| `spikeinterface_sortingcomponents_matching_tdc_peeler_fit_one_amplitude_with_neighbors` | `spikeinterface.sortingcomponents.matching.tdc_peeler.fit_one_amplitude_with_neighbors` | `spikeinterface/sortingcomponents/matching/tdc_peeler.py` | `spike: numpy.ndarray, neighbors_spikes: numpy.ndarray, traces: numpy.ndarray, template_sparsity_mask: numpy.ndarray, sparse_templates_array: numpy.ndarray, template_norms: numpy.ndarray, nbefore: int, nafter: int` | `Fit amplitude for a single spike using its template and optionally fit or subtract contributions from neighboring spikes to account for temporal overlaps in spike sorting deconvolution. This function is used in SpikeInterface's matching/peeler components (tdc_peeler) to estimate the scalar amplitude of one detected spike relative to a stored sparse template. It either computes a simple dot-product projection when there are no neighbors, or builds a local linear model (design matrix) that includes the target spike and selected neighbor regressors, then solves a linear least-squares problem to jointly estimate amplitudes. The estimate is returned as a single float amplitude used downstream by the peeler to scale the template when subtracting predicted waveforms from the raw traces.` |
| `spikeinterface_sortingcomponents_matching_wobble_compress_templates` | `spikeinterface.sortingcomponents.matching.wobble.compress_templates` | `spikeinterface/sortingcomponents/matching/wobble.py` | `templates: numpy.ndarray, approx_rank: int` | `Compress templates using singular value decomposition for use in spike sorting workflows. This function, spikeinterface.sortingcomponents.matching.wobble.compress_templates, computes a reduced-rank representation of input spike template waveforms by applying NumPy's singular value decomposition (numpy.linalg.svd with full_matrices=False) independently to each template matrix. In the SpikeInterface domain this is used to reduce temporal and spatial dimensionality of template waveforms (num_templates, num_samples, num_channels) to accelerate matching, reduce memory use, and enable low-rank operations in downstream sorting and post-processing components.` |
| `spikeinterface_sortingcomponents_matching_wobble_compute_scale_amplitudes` | `spikeinterface.sortingcomponents.matching.wobble.compute_scale_amplitudes` | `spikeinterface/sortingcomponents/matching/wobble.py` | `high_resolution_conv: numpy.ndarray, norm_peaks: numpy.ndarray, scale_min: float, scale_max: float, amplitude_variance: float` | `Compute optimal amplitude scaling factors for spikes and the resulting high-resolution objective used in spike-template matching. This function is used in spike sorting matching components to adjust template amplitudes for each detected spike and to compute a per-timepoint objective (super-resolution objective) within a small time window around each spike peak. It implements a numerically stable form of the closed-form solution for the amplitude that maximizes a quadratic objective with a Gaussian prior on amplitudes (amplitude scaling factor ~ N(1, amplitude_variance)). To avoid overflow from squaring large intermediate values, the computed scaling b/a is clipped to the provided [scale_min, scale_max] interval; the clipped scaling is then substituted into the objective formula implemented here. The outputs are intended for downstream matching/score computations that operate on super-resolved convolution traces and per-spike template norms.` |
| `spikeinterface_sortingcomponents_matching_wobble_compute_template_norm` | `spikeinterface.sortingcomponents.matching.wobble.compute_template_norm` | `spikeinterface/sortingcomponents/matching/wobble.py` | `visible_channels: numpy.ndarray, templates: numpy.ndarray` | `Compute the squared L2 norm of each spike template restricted to its visible channels. This function is used in the spike matching/wobble components of SpikeInterface to produce a per-template magnitude used for normalization and comparison of templates during matching and post-processing. For each template (unit), only channels marked as visible in visible_channels are included in the squared-norm computation; the result is therefore the sum of squared waveform samples across time and the template-specific subset of channels.` |
| `spikeinterface_sortingcomponents_matching_wobble_get_convolution_len` | `spikeinterface.sortingcomponents.matching.wobble.get_convolution_len` | `spikeinterface/sortingcomponents/matching/wobble.py` | `x: int, y: int` | `Returns the length of the linear discrete convolution of two finite-length vectors. This function computes the number of output samples produced when two one-dimensional sequences of lengths x and y are convolved using the standard linear discrete convolution formula. In the SpikeInterface sortingcomponents.matching.wobble context this value is commonly used to determine buffer sizes, allocate arrays, and align or index convolved waveforms or templates used for template matching, cross-correlation, or wobble/alignment corrections in spike sorting pipelines.` |
| `spikeinterface_sortingcomponents_matching_wobble_upsample_and_jitter` | `spikeinterface.sortingcomponents.matching.wobble.upsample_and_jitter` | `spikeinterface/sortingcomponents/matching/wobble.py` | `temporal: numpy.ndarray, jitter_factor: int, num_samples: int` | `spikeinterface.sortingcomponents.matching.wobble.upsample_and_jitter Upsample the temporal components of template SVDs and re-index them to produce jittered, super-resolution temporal templates used for sub-sample template matching in spike sorting. This function is used in SpikeInterface's matching/wobble workflow to generate multiple time-shifted ("jittered") versions of each template's temporal components so matchers can evaluate alignments at fractional-sample offsets. It performs an FFT-based resampling (using scipy.signal.resample) to a higher temporal resolution, selects shifted samples that correspond to different sub-sample alignments, and returns a stack of jittered temporal components suitable for reconstructing jittered templates during template-matching or evaluation in sorting components and custom sorters built with SpikeInterface.` |
| `spikeinterface_sortingcomponents_motion_decentralized_compute_global_displacement` | `spikeinterface.sortingcomponents.motion.decentralized.compute_global_displacement` | `spikeinterface/sortingcomponents/motion/decentralized.py` | `pairwise_displacement: numpy.ndarray, pairwise_displacement_weight: numpy.ndarray = None, sparse_mask: numpy.ndarray = None, temporal_prior: bool = True, spatial_prior: bool = True, soft_weights: bool = False, convergence_method: str = "lsmr", robust_regression_sigma: float = 2, lsqr_robust_n_iter: int = 20, progress_bar: bool = False` | `Compute global displacement across time windows for motion correction in spike sorting. This function estimates a global temporal displacement signal (motion) from pairwise displacement measurements between timepoints or between time windows. It is used in SpikeInterface's sortingcomponents.motion.decentralized pipeline to convert pairwise relative displacements (for example, offsets estimated between pairs of temporal windows of an extracellular recording) into a consistent global displacement per timepoint (and per window when multiple windows/blocks are provided). The algorithm supports multiple numerical solvers (gradient-based, robust least-squares iterations with LSQR, and sparse LSMR with optional temporal and spatial priors), optional pairwise weighting and sparsity masks, and optional robust trimming of outliers. The returned displacement values are in the same numerical units as pairwise_displacement.` |
| `spikeinterface_sortingcomponents_motion_dredge_dredge_online_lfp` | `spikeinterface.sortingcomponents.motion.dredge.dredge_online_lfp` | `spikeinterface/sortingcomponents/motion/dredge.py` | `lfp_recording: int, direction: str = "y", rigid: bool = True, win_shape: str = "gaussian", win_step_um: float = 800, win_scale_um: float = 850, win_margin_um: float = None, chunk_len_s: float = 10.0, max_disp_um: float = 500, time_horizon_s: float = None, mincorr: float = 0.8, mincorr_percentile: float = None, mincorr_percentile_nneighbs: int = 20, soft: bool = False, thomas_kw: dict = None, xcorr_kw: dict = None, extra_outputs: bool = False, device: str = None, progress_bar: bool = True` | `Online registration of a preprocessed LFP recording using windowed cross-correlation and an online tridiagonal solver to estimate time-varying spatial motion along a single axis.` |
| `spikeinterface_sortingcomponents_motion_dredge_laplacian` | `spikeinterface.sortingcomponents.motion.dredge.laplacian` | `spikeinterface/sortingcomponents/motion/dredge.py` | `n: int, wink: bool = True, eps: float = 0.001, lambd: float = 1.0, ridge_mask: numpy.ndarray = None` | `spikeinterface.sortingcomponents.motion.dredge.laplacian constructs a dense discrete 1D Laplacian matrix (with an added small ridge/identity term) used as a regularization / smoothing operator in SpikeInterface sorting components (for example, within motion or drift regularization steps of custom sorters built with the sortingcomponents). The returned matrix is a square numpy.ndarray of shape (n, n) whose main diagonal and immediate off-diagonals encode a second-difference operator scaled by lambd, with an additional eps-scaled identity term to improve numerical stability; an optional ridge_mask allows spatially varying small diagonal augmentation.` |
| `spikeinterface_sortingcomponents_motion_dredge_neg_hessian_likelihood_term` | `spikeinterface.sortingcomponents.motion.dredge.neg_hessian_likelihood_term` | `spikeinterface/sortingcomponents/motion/dredge.py` | `Ub: numpy.ndarray, Ub_prevcur: numpy.ndarray = None, Ub_curprev: numpy.ndarray = None` | `spikeinterface.sortingcomponents.motion.dredge.neg_hessian_likelihood_term computes the negative-Hessian contribution of the non-regularized likelihood term for a nonrigid block and prepares coefficients used in a Newton-step linear system inside SpikeInterface's nonrigid motion estimation components. In the spike sorting domain (see README: SpikeInterface provides sorting components to build custom sorters), this function forms the likelihood part of the coefficient matrix that, together with separate regularization terms, is used to solve for block-wise motion or deformation corrections via a linear solver or Newton update. This function computes the matrix -Ub - Ub.T and then replaces its diagonal with per-row and per-column sums adjusted by optional neighboring-block contributions. The implementation uses a single temporary array (negHUb) to avoid creating an extra full copy of -Ub + -Ub.T; the input Ub itself is not modified.` |
| `spikeinterface_sortingcomponents_motion_dredge_newton_rhs` | `spikeinterface.sortingcomponents.motion.dredge.newton_rhs` | `spikeinterface/sortingcomponents/motion/dredge.py` | `Db: numpy.ndarray, Ub: numpy.ndarray, Pb_prev: numpy.ndarray = None, Db_prevcur: numpy.ndarray = None, Ub_prevcur: numpy.ndarray = None, Db_curprev: numpy.ndarray = None, Ub_curprev: numpy.ndarray = None` | `spikeinterface.sortingcomponents.motion.dredge.newton_rhs computes the right-hand side vector for a Newton update in the "dredge" alignment routine used by SpikeInterface sorting components. This function returns the gradient of the cost function evaluated at P=0 for the batch case, or the gradient augmented with alignment terms for the online (sequential) case where previous/current batch coupling is provided. It is typically used inside a Newton solver that updates a permutation-like matrix P to align clusters or templates across recording batches during spike sorting and motion correction.` |
| `spikeinterface_sortingcomponents_motion_dredge_newton_solve_rigid` | `spikeinterface.sortingcomponents.motion.dredge.newton_solve_rigid` | `spikeinterface/sortingcomponents/motion/dredge.py` | `D: numpy.ndarray, U: numpy.ndarray, Sigma0inv: numpy.ndarray, Pb_prev: numpy.ndarray = None, Db_prevcur: numpy.ndarray = None, Ub_prevcur: numpy.ndarray = None, Db_curprev: numpy.ndarray = None, Ub_curprev: numpy.ndarray = None` | `Solve the rigid Newton step used in the motion-correction dredge component of SpikeInterface. This function builds and solves the linear Newton system for a rigid motion update p used in SpikeInterface's sortingcomponents.motion.dredge workflow. It combines a prior inverse covariance term Sigma0inv with a data-derived negative Hessian of the likelihood (negHU) computed from subsampling/soft-weights, and a right-hand side assembled from the displacement matrix D and optional boundary/adjacent-block contributions. The result p is the Newton update that would be applied to rigid motion parameters in downstream motion-correction steps of the SpikeInterface pipeline.` |
| `spikeinterface_sortingcomponents_motion_dredge_thomas_solve` | `spikeinterface.sortingcomponents.motion.dredge.thomas_solve` | `spikeinterface/sortingcomponents/motion/dredge.py` | `Ds: numpy.ndarray, Us: numpy.ndarray, lambda_t: float = 1.0, lambda_s: float = 1.0, eps: float = 0.001, P_prev: numpy.ndarray = None, Ds_prevcur: numpy.ndarray = None, Us_prevcur: numpy.ndarray = None, Ds_curprev: numpy.ndarray = None, Us_curprev: numpy.ndarray = None, progress_bar: bool = False, bandwidth: int = None` | `spikeinterface.sortingcomponents.motion.dredge.thomas_solve solves a block-tridiagonal linear system for nonrigid motion displacement estimation used in SpikeInterface's nonrigid registration component. It computes displacement estimates for B spatial blocks (nonrigid windows) across T temporal bins by combining blockwise pairwise displacement measurements and their weights with temporal and spatial priors. This function is used during motion correction/registration in spike sorting pipelines to produce per-window, per-time-bin displacements that align recording segments.` |
| `spikeinterface_sortingcomponents_motion_iterative_template_iterative_template_registration` | `spikeinterface.sortingcomponents.motion.iterative_template.iterative_template_registration` | `spikeinterface/sortingcomponents/motion/iterative_template.py` | `spikecounts_hist_images: numpy.ndarray, non_rigid_windows: list = None, num_shifts_global: int = 15, num_iterations: int = 10, num_shifts_block: int = 5, smoothing_sigma: float = 0.5, kriging_sigma: float = 1, kriging_p: float = 2, kriging_d: float = 2` | `spikeinterface.sortingcomponents.motion.iterative_template.iterative_template_registration computes an optimal per-time-bin spatial registration (rigid then non-rigid) of spike count histogram images used in spike sorting motion correction. The function performs an initial iterative rigid integer shift alignment across temporal bins, builds a target spatial histogram, and then computes non-rigid per-block shifts with sub-integer resolution by upsampling blockwise cross-covariance using a kriging kernel. This routine is used in SpikeInterface to estimate and correct probe motion by aligning spike-count histograms across time and spatial bins, producing per-time-block spatial shifts and a target histogram used for downstream template alignment or motion-correction steps.` |
| `spikeinterface_sortingcomponents_motion_motion_cleaner_clean_motion_vector` | `spikeinterface.sortingcomponents.motion.motion_cleaner.clean_motion_vector` | `spikeinterface/sortingcomponents/motion/motion_cleaner.py` | `motion: numpy.ndarray, temporal_bins: numpy.ndarray, bin_duration_s: float, speed_threshold: float = 30, sigma_smooth_s: float = None` | `Clean a 2-D motion estimate time series by removing spurious rapid changes ("bumps") and optionally applying a Gaussian temporal smoothing. This function is used in SpikeInterface sorting components to stabilize probe motion estimates (units: micrometers) over time prior to downstream steps such as drift correction, waveform extraction, quality metrics, and visualization.` |
| `spikeinterface_sortingcomponents_motion_motion_utils_get_rigid_windows` | `spikeinterface.sortingcomponents.motion.motion_utils.get_rigid_windows` | `spikeinterface/sortingcomponents/motion/motion_utils.py` | `spatial_bin_centers: numpy.ndarray` | `spikeinterface.sortingcomponents.motion.motion_utils.get_rigid_windows generates a single rectangular (all-ones) spatial window that covers the full set of provided spatial bins and computes the spatial center of that window. In the SpikeInterface motion-correction and sorting-components context, this function is used for rigid motion models where the entire probe is treated as a single region: the returned window can be used to weight or aggregate signals across all spatial bins when estimating or applying rigid motion corrections.` |
| `spikeinterface_sortingcomponents_motion_motion_utils_get_spatial_windows` | `spikeinterface.sortingcomponents.motion.motion_utils.get_spatial_windows` | `spikeinterface/sortingcomponents/motion/motion_utils.py` | `contact_depths: numpy.ndarray, spatial_bin_centers: numpy.ndarray, rigid: bool = False, win_shape: str = "gaussian", win_step_um: float = 50.0, win_scale_um: float = 150.0, win_margin_um: float = None, zero_threshold: float = None` | `spikeinterface.sortingcomponents.motion.motion_utils.get_spatial_windows Generate spatial tapering windows used for estimating non-rigid motion along a probe depth axis. In the SpikeInterface motion-correction workflow this function converts electrode/contact depths and precomputed spatial bin centers into a set of overlapping spatial windows (tapers) that are used to weight signals or drift estimates at different depths. For rigid motion this function returns a single rectangular window that effectively covers the whole probe (equivalent to estimating only a global shift). Windows may be shaped as Gaussian, rectangular, or triangular and are placed regularly across the probe depth range; window placement, spread, and margins are controlled by the win_step_um, win_scale_um and win_margin_um parameters. When a zero_threshold is provided, values below this threshold are set to zero and each window row is normalized to sum to 1 (see failure modes below).` |
| `spikeinterface_sortingcomponents_peak_selection_select_peak_indices` | `spikeinterface.sortingcomponents.peak_selection.select_peak_indices` | `spikeinterface/sortingcomponents/peak_selection.py` | `peaks: numpy.ndarray, method: str, seed: int, **method_kwargs` | `Select indices of detected peaks to subsample before downstream clustering. This function is used within the SpikeInterface sorting-components pipeline to subsample the set of detected peaks (spike candidates) prior to feature extraction and clustering. It is wrapped by spikeinterface.sortingcomponents.peak_selection.select_peaks and implements several strategies to reduce the number of peaks passed to clustering: uniform random sampling across all peaks or per-channel, and three "smart" sampling strategies that attempt to produce a more homogeneous sampling across amplitudes, spatial locations, or spatial locations combined with time. The function uses numpy's Generator for randomized selection and sklearn.preprocessing.QuantileTransformer to remap distributions when required.` |
| `spikeinterface_sortingcomponents_waveforms_waveform_utils_from_temporal_representation` | `spikeinterface.sortingcomponents.waveforms.waveform_utils.from_temporal_representation` | `spikeinterface/sortingcomponents/waveforms/waveform_utils.py` | `temporal_waveforms: numpy.ndarray, num_channels: int` | `spikeinterface.sortingcomponents.waveforms.waveform_utils.from_temporal_representation converts a temporal (flattened) waveform representation back into a per-waveform, per-time, per-channel numpy array. This function is the inverse operation of to_temporal_representation used in the SpikeInterface sorting components to move between a stacked temporal format (channels concatenated along the first axis) and a structured waveform array used for visualization, feature extraction, quality metrics, and other post-processing steps in spike sorting pipelines.` |
| `spikeinterface_sortingcomponents_waveforms_waveform_utils_to_temporal_representation` | `spikeinterface.sortingcomponents.waveforms.waveform_utils.to_temporal_representation` | `spikeinterface/sortingcomponents/waveforms/waveform_utils.py` | `waveforms: numpy.ndarray` | `spikeinterface.sortingcomponents.waveforms.waveform_utils.to_temporal_representation: Convert a 3D array of extracted spike waveforms into a 2D temporal-only representation by collapsing the channel (spatial) dimension so each output row is the time-series from a single channel of a single waveform. This is used in the SpikeInterface waveform processing and sorting components when downstream algorithms require temporal-only inputs (for example, time-domain feature extraction, PCA over time, or classifier inputs) rather than a per-waveform multichannel spatial representation.` |
| `spikeinterface_widgets_unit_waveforms_get_waveforms_scales` | `spikeinterface.widgets.unit_waveforms.get_waveforms_scales` | `spikeinterface/widgets/unit_waveforms.py` | `templates: numpy.ndarray, channel_locations: numpy.ndarray, nbefore: int, x_offset_units: bool = False, widen_narrow_scale: float = 1.0` | `Return x and amplitude scales used to plot waveform templates arranged by channel location for spike-sorting visualization. This function computes a horizontal coordinate matrix (xvectors) that places each sample of template waveforms along an x-axis offset by channel x-positions, a scalar y_scale that maps waveform amplitude units to spatial vertical units, a per-channel vertical offset (y_offset) derived from channel y-positions, and an estimated horizontal inter-channel interval (delta_x). These outputs are intended for use when drawing template waveforms (e.g., template stacks or unit waveform overlays) on a 2D layout determined by electrode/contact locations, a common step in SpikeInterface visualization and waveform analysis workflows.` |
| `spikeinterface_widgets_utils_array_to_image` | `spikeinterface.widgets.utils.array_to_image` | `spikeinterface/widgets/utils.py` | `data: numpy.ndarray, colormap: str = "RdGy", clim: tuple = None, spatial_zoom: tuple = (0.75, 1.25), num_timepoints_per_row: int = 30000, row_spacing: float = 0.25, scalebar: bool = False, sampling_frequency: float = None` | `spikeinterface.widgets.utils.array_to_image converts a 2D numpy array of time-series data (time x channels) into a 3-channel RGB image array suitable for visualization in SpikeInterface widgets and other plotting/export workflows. This function is intended for visualizing extracellular recording data or preprocessed traces (for example, raw or filtered traces arranged as samples-by-channels produced during spike sorting workflows described in the SpikeInterface README). It applies spatial scaling, color mapping, clamping to color limits, row-wise wrapping (to limit samples per row), and optional scalebar drawing (requires Pillow and a provided sampling frequency).` |
| `spikeinterface_widgets_utils_get_some_colors` | `spikeinterface.widgets.utils.get_some_colors` | `spikeinterface/widgets/utils.py` | `keys: list, color_engine: str = "auto", map_name: str = "gist_ncar", format: str = "RGBA", shuffle: bool = None, seed: int = None, margin: int = None, resample: bool = True` | `get_some_colors returns a dictionary that maps the provided keys to RGBA color tuples suitable for use in SpikeInterface widget visualizations (for example coloring units, channels, or clusters in plots). This function selects a color-generation engine, produces one color per key, optionally resamples a matplotlib colormap, and can shuffle the colors deterministically using a numpy random generator. It is used by spikeinterface.widgets to assign consistent, visually distinct colors when displaying spike-sorting results and recordings.` |

## ‚öñÔ∏è License

Original Code License: MIT

Wrapper Code & Documentation: Apache-2.0

*This file was automatically generated on February 26, 2026.*
