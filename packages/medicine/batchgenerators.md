# batchgenerators

[üîô Back to Main Repo](../../../README.md) | [üîó Original Repo](https://github.com/MIC-DKFZ/batchgenerators)

![Tool Count](https://img.shields.io/badge/Agent_Tools-26-blue?style=flat-square)
![Category](https://img.shields.io/badge/Category-Medicine-green?style=flat-square)
![Status](https://img.shields.io/badge/Import_Test-Passed-success?style=flat-square)

## üìñ Overview

`batchgenerators` is a Python framework for fast, multi-threaded 2D/3D data augmentation pipelines (e.g., spatial, color, noise, cropping, and medical imaging‚Äìspecific transforms) for training machine learning models.

> **Note**: This documentation lists the **agent-ready wrapper functions** generated for this package. These functions have been strictly typed, docstring-enhanced, and tested for import stability within a standardized Apptainer environment.

## üõ†Ô∏è Available Agent Tools

Below is the list of **26** functions optimized for LLM tool-use.

| **Tool Name (Wrapper)**   | **Source**          | **File Path**     | **Arguments (Type)**        | **Description**                |
| ------------------------- | ------------------- | ----------------- | --------------------------- | ------------------------------ |
| `batchgenerators_augmentations_color_augmentations_augment_brightness_additive` | `batchgenerators.augmentations.color_augmentations.augment_brightness_additive` | `batchgenerators/augmentations/color_augmentations.py` | `data_sample: numpy.ndarray, mu: float, sigma: float, per_channel: bool = True, p_per_channel: float = 1.0` | `batchgenerators.augmentations.color_augmentations.augment_brightness_additive adds an additive brightness offset sampled from a Gaussian distribution to each channel of a single sample image tensor. This function implements the "brightness (additive)" color augmentation used in the batchgenerators data augmentation pipeline (see README). It is intended to simulate global or per-channel illumination/brightness shifts for 2D or 3D image data and is applied on a per-sample basis (not per-batch) using numpy's random number generators.` |
| `batchgenerators_augmentations_crop_and_pad_augmentations_crop` | `batchgenerators.augmentations.crop_and_pad_augmentations.crop` | `batchgenerators/augmentations/crop_and_pad_augmentations.py` | `data: numpy.ndarray, seg: numpy.ndarray = None, crop_size: int = 128, margins: tuple = (0, 0, 0), crop_type: str = "center", pad_mode: str = "constant", pad_kwargs: dict = {'constant_values': 0}, pad_mode_seg: str = "constant", pad_kwargs_seg: dict = {'constant_values': 0}` | `Crops a batch of images (and optional segmentation maps) to a target spatial size and pads when the crop exceeds image bounds. This function is used by the batchgenerators data augmentation pipeline for 2D and 3D medical images to produce fixed-size spatial patches (for example to feed into a neural network). It performs either a center crop or a random crop (with an enforceable margin from image borders), preserves the data and segmentation dtypes, and pads using numpy.pad when the requested crop extends beyond the image extent. The function accepts either a numpy array shaped as (b, c, x, y) or (b, c, x, y, z) or a list/tuple of per-sample arrays where each sample has shape (c, x, y(, z)). The segmentation input, if provided, must have matching spatial dimensions and will be transformed identically to the image data so that spatial correspondence is preserved.` |
| `batchgenerators_augmentations_crop_and_pad_augmentations_get_lbs_for_center_crop` | `batchgenerators.augmentations.crop_and_pad_augmentations.get_lbs_for_center_crop` | `batchgenerators/augmentations/crop_and_pad_augmentations.py` | `crop_size: tuple, data_shape: tuple` | `batchgenerators.augmentations.crop_and_pad_augmentations.get_lbs_for_center_crop computes the per-axis lower-bound indices for performing a center crop on spatial data; this is used by the crop-and-pad augmentations in batchgenerators (useful for 2D and 3D medical image augmentation pipelines where data arrays have batch and channel leading dimensions).` |
| `batchgenerators_augmentations_crop_and_pad_augmentations_get_lbs_for_random_crop` | `batchgenerators.augmentations.crop_and_pad_augmentations.get_lbs_for_random_crop` | `batchgenerators/augmentations/crop_and_pad_augmentations.py` | `crop_size: list, data_shape: tuple, margins: list` | `get_lbs_for_random_crop computes integer lower-bound indices for a random spatial crop. It is used by the crop-and-pad augmentations in batchgenerators (a medical image augmentation framework developed at DKFZ) to determine where to place a random crop inside an image (or volume) given desired crop size and safety margins. For each spatial dimension (x, y, and optionally z) the function returns a lower bound index that can be used to slice the input data array. The returned indices are suitable for indexing numpy arrays with shapes that follow the batchgenerators convention (data_shape = (b, c, x, y[, z])).` |
| `batchgenerators_augmentations_crop_and_pad_augmentations_pad_nd_image_and_seg` | `batchgenerators.augmentations.crop_and_pad_augmentations.pad_nd_image_and_seg` | `batchgenerators/augmentations/crop_and_pad_augmentations.py` | `data: numpy.ndarray, seg: numpy.ndarray, new_shape: list = None, must_be_divisible_by: list = None, pad_mode_data: str = "constant", np_pad_kwargs_data: dict = None, pad_mode_seg: str = "constant", np_pad_kwargs_seg: dict = None` | `Pads a data array and its corresponding segmentation array to a target minimum spatial size and/or to sizes that are divisible by given factors. This function is used in the batchgenerators data augmentation pipeline (medical image augmentation for 2D and 3D data) to ensure that images and optional segmentation maps meet minimum size requirements (new_shape) and architectural constraints (must_be_divisible_by, e.g., for UNet downsampling stages). Padding is performed per spatial dimension while preserving batch and channel dimensions and returns new arrays without modifying the inputs in place.` |
| `batchgenerators_augmentations_resample_augmentations_augment_linear_downsampling_scipy` | `batchgenerators.augmentations.resample_augmentations.augment_linear_downsampling_scipy` | `batchgenerators/augmentations/resample_augmentations.py` | `data_sample: numpy.ndarray, zoom_range: list = (0.5, 1), per_channel: bool = True, p_per_channel: float = 1, channels: list = None, order_downsample: int = 1, order_upsample: int = 0, ignore_axes: tuple = None` | `augment_linear_downsampling_scipy(data_sample, zoom_range=(0.5, 1), per_channel=True, p_per_channel=1, channels=None, order_downsample=1, order_upsample=0, ignore_axes=None) Short summary: Per-sample spatial downsampling and upsampling augmentation that simulates lower-resolution acquisitions and subsequent nearest/linear resampling artifacts commonly encountered in medical image preprocessing. This function is used in the batchgenerators augmentation pipeline to degrade each channel of a single sample by a randomly chosen isotropic or per-axis zoom factor and then restore the original voxel grid; it is intended for channel-first image arrays (single sample) used by batchgenerators (for example, shape (C, X, Y) for 2D or (C, X, Y, Z) for 3D). The operation helps train models to be robust against changes in image resolution and partial loss of high-frequency detail.` |
| `batchgenerators_augmentations_spatial_transformations_augment_resize` | `batchgenerators.augmentations.spatial_transformations.augment_resize` | `batchgenerators/augmentations/spatial_transformations.py` | `sample_data: numpy.ndarray, sample_seg: numpy.ndarray, target_size: list, order: int = 3, order_seg: int = 1` | `augment_resize resizes a single-sample image (and optional corresponding segmentation) to a given spatial target size. This function is used in the spatial augmentation / resampling stage of the batchgenerators pipeline to reshape a single example's image data and, if present, its segmentation maps to a new spatial resolution. It computes the spatial dimensionality from the provided sample (by taking len(sample_data.shape) - 1, treating the first axis as the channel axis) and then uses the package's resize utilities (resize_multichannel_image and resize_segmentation) to perform interpolation. This routine returns newly allocated arrays and does not modify the input arrays in-place. It is intended for per-sample usage inside augmentation/data-loading workflows that follow the batchgenerators data conventions (per-sample image shape (c, x, y) for 2D or (c, x, y, z) for 3D, where c is the channel count). Note that segmentation resizing in this package uses the resize_segmentation helper (which, per release notes, operates with 'edge' border handling instead of constant cval borders).` |
| `batchgenerators_augmentations_spatial_transformations_augment_rot90` | `batchgenerators.augmentations.spatial_transformations.augment_rot90` | `batchgenerators/augmentations/spatial_transformations.py` | `sample_data: numpy.ndarray, sample_seg: numpy.ndarray, num_rot: tuple = (1, 2, 3), axes: tuple = (0, 1, 2)` | `Augment a data sample and its segmentation by rotating the spatial axes by a multiple of 90 degrees. This function is used in the spatial augmentation pipeline of batchgenerators (a medical image data augmentation library) to increase training variability for 2D and 3D image data. It selects a rotation amount (an integer count of 90-degree steps) randomly from num_rot and selects two spatial axes randomly from axes to define the rotation plane. The chosen rotation is applied to sample_data and, if present, to sample_seg using numpy.rot90 so that both image data and corresponding segmentation remain spatially aligned. This transform is appropriate for per-sample spatial augmentation of batches that follow the batchgenerators data layout (see README: 'data' should have shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).` |
| `batchgenerators_augmentations_spatial_transformations_augment_spatial_2` | `batchgenerators.augmentations.spatial_transformations.augment_spatial_2` | `batchgenerators/augmentations/spatial_transformations.py` | `data: numpy.ndarray, seg: numpy.ndarray, patch_size: tuple, patch_center_dist_from_border: int = 30, do_elastic_deform: bool = True, deformation_scale: tuple = (0, 0.25), do_rotation: bool = True, angle_x: tuple = (0, 6.283185307179586), angle_y: tuple = (0, 6.283185307179586), angle_z: tuple = (0, 6.283185307179586), do_scale: bool = True, scale: tuple = (0.75, 1.25), border_mode_data: str = "nearest", border_cval_data: float = 0, order_data: int = 3, border_mode_seg: str = "constant", border_cval_seg: int = 0, order_seg: int = 0, random_crop: bool = True, p_el_per_sample: float = 1, p_scale_per_sample: float = 1, p_rot_per_sample: float = 1, independent_scale_for_each_axis: bool = False, p_rot_per_axis: float = 1, p_independent_scale_per_axis: float = 1` | `augment_spatial_2 applies combined spatial augmentations (elastic deformation, rotation, scaling, and cropping) to a batch of images and optionally their corresponding segmentation maps. This function is used in the batchgenerators library (developed for medical image augmentation) to produce augmented training samples from 2D or 3D image batches. It expects inputs that follow the batchgenerators internal data structure: data arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D, and an optional seg array with matching spatial shape. The function returns a new batch cropped to patch_size for model training or further processing. The returned arrays are newly allocated; inputs are not modified in-place.` |
| `batchgenerators_augmentations_spatial_transformations_augment_transpose_axes` | `batchgenerators.augmentations.spatial_transformations.augment_transpose_axes` | `batchgenerators/augmentations/spatial_transformations.py` | `data_sample: numpy.ndarray, seg_sample: numpy.ndarray, axes: tuple = (0, 1, 2)` | `Augment by randomly permuting the specified spatial axes of a single sample array while preserving the channel axis and keeping segmentation aligned. This function is used in the batchgenerators spatial augmentation pipeline (medical image computing/data augmentation) to change image orientation by transposing spatial dimensions (e.g., x,y,(z)) for 2D and 3D samples, increasing variability during training. It operates on single samples where the first axis is the channel axis (shape c,x,y or c,x,y,z as commonly used inside batchgenerators per-sample augmentations).` |
| `batchgenerators_augmentations_spatial_transformations_augment_zoom` | `batchgenerators.augmentations.spatial_transformations.augment_zoom` | `batchgenerators/augmentations/spatial_transformations.py` | `sample_data: numpy.ndarray, sample_seg: numpy.ndarray, zoom_factors: int, order: int = 3, order_seg: int = 1` | `batchgenerators.augmentations.spatial_transformations.augment_zoom Zoom (resample) a single multi-channel image and its optional segmentation by integer scaling factors. This function is part of the batchgenerators spatial augmentation tools used in medical image computing workflows (see repository README). It computes a target spatial size by multiplying the per-channel spatial shape of sample_data by zoom_factors, resamples the image channels with a continuous interpolation order suitable for image intensity data, and resamples each segmentation channel with a discrete interpolation order suitable for label maps. The function is intended to be used on a single sample (not a batch) with shape conventions used across the project: per-sample image arrays are expected to have shape (c, x, y) for 2D or (c, x, y, z) for 3D, where c is the number of channels (modalities). It relies on the repository helpers resize_multichannel_image and resize_segmentation (which in this code base follow the skimage.transform.resize semantics and, per project release notes, use 'edge' mode for segmentation resizing).` |
| `batchgenerators_augmentations_utils_convert_seg_image_to_one_hot_encoding` | `batchgenerators.augmentations.utils.convert_seg_image_to_one_hot_encoding` | `batchgenerators/augmentations/utils.py` | `image: numpy.ndarray, classes: numpy.ndarray = None` | `Convert a segmentation label map to a one-hot encoded array with the class/channel axis first. This function is used in the batchgenerators data-augmentation and preprocessing pipeline to convert a single-sample segmentation label map (a spatial label image) into a one-hot representation where each output channel corresponds to a semantic class. In the context of the README and the medical image augmentation use cases, this is typically applied to a per-sample segmentation (for example a 2D label map of shape (x, y) or a 3D label map of shape (x, y, z)) prior to spatial or intensity augmentations or before feeding the labels into a model that expects channel-first one-hot targets. If classes is None, the set and order of channels is determined from the unique values present in image using numpy.unique. The function performs elementwise equality comparisons (image == c) to generate each class mask; therefore exact value matching semantics apply (this is important for floating-point label maps).` |
| `batchgenerators_augmentations_utils_convert_seg_image_to_one_hot_encoding_batched` | `batchgenerators.augmentations.utils.convert_seg_image_to_one_hot_encoding_batched` | `batchgenerators/augmentations/utils.py` | `image: numpy.ndarray, classes: list = None` | `Convert a batched segmentation label image to a one-hot encoded batch along a new class/channel axis. This function is used in the batchgenerators augmentation pipeline to transform batched segmentation maps (for example the 'seg' entry in the data dictionary used by DataLoaderBase and MultithreadedAugmenter) into a one-hot representation required by many downstream tasks such as multi-class loss computation, network training, or metric calculation. It expects a batch of single-channel label maps and produces an output where each class is represented by a separate channel. If classes is None, the set of classes is inferred from the labels present in the provided batch.` |
| `batchgenerators_augmentations_utils_convert_seg_to_bounding_box_coordinates` | `batchgenerators.augmentations.utils.convert_seg_to_bounding_box_coordinates` | `batchgenerators/augmentations/utils.py` | `data_dict: dict, dim: int, get_rois_from_seg_flag: bool = False, class_specific_seg_flag: bool = False` | `Convert per-pixel segmentation maps into bounding box annotations and per-lesion metadata used by detection/instance tasks (e.g., Mask R-CNN) in the batchgenerators augmentation pipeline. This function is used in the medical-image data-augmentation context (see README) to translate pixel-wise lesion annotations into a set of bounding boxes, per-lesion binary masks and class labels that are easier to consume by object-detection style networks or further augmentation steps. It inspects each sample's segmentation map, extracts connected lesion regions or label-encoded ROIs, computes axis-aligned bounding boxes with a one-voxel margin, and produces outputs placed back into the input data_dict so downstream transforms/augmenters can access them.` |
| `batchgenerators_augmentations_utils_elastic_deform_coordinates_2` | `batchgenerators.augmentations.utils.elastic_deform_coordinates_2` | `batchgenerators/augmentations/utils.py` | `coordinates: numpy.ndarray, sigmas: list, magnitudes: list` | `Elastic deformation of a coordinate grid using smoothed random fields in the Fourier domain, intended for spatial augmentations in the batchgenerators pipeline (e.g., to simulate soft-tissue or other realistic deformations of image sampling grids). The function generates per-dimension random noise, applies a Fourier-domain Gaussian low-pass filter (via fourier_gaussian and FFT/IFFT), scales the resulting deformation fields by the provided magnitudes, normalizes them to the supplied magnitude range, and returns coordinates displaced by these deformation offsets. This is used downstream to map image sampling coordinates for elastic spatial augmentation in 2D/3D data augmentation workflows described in the README.` |
| `batchgenerators_augmentations_utils_get_organ_gradient_field` | `batchgenerators.augmentations.utils.get_organ_gradient_field` | `batchgenerators/augmentations/utils.py` | `organ: numpy.ndarray, spacing_ratio: float = 0.10416666666666667, blur: int = 32` | `Calculates a 3D gradient vector field around a binary organ segmentation to support anatomy-informed data augmentation (used, for example, to simulate soft-tissue deformations in medical image augmentation pipelines).` |
| `batchgenerators_augmentations_utils_mask_random_squares` | `batchgenerators.augmentations.utils.mask_random_squares` | `batchgenerators/augmentations/utils.py` | `img: numpy.ndarray, square_size: int, n_squares: int, n_val: float, channel_wise_n_val: bool = False, square_pos: tuple = None` | `Masks a given number of squares in an image. This utility is part of the batchgenerators augmentation toolkit (used in medical image computing and general image augmentation pipelines) and is intended to simulate localized occlusions or missing regions by repeatedly applying square masks to an input numpy array. The function repeatedly delegates to mask_random_square to place and fill each square and returns the augmented image array for use in training/validation pipelines that expect batchgenerators-style image arrays.` |
| `batchgenerators_augmentations_utils_pad_nd_image` | `batchgenerators.augmentations.utils.pad_nd_image` | `batchgenerators/augmentations/utils.py` | `image: numpy.ndarray, new_shape: list = None, mode: str = "constant", kwargs: dict = None, return_slicer: bool = False, shape_must_be_divisible_by: list = None` | `Pad an N-dimensional numpy image to at least the requested spatial shape, optionally returning a slicer to recover the original region. This function is used in the batchgenerators augmentation pipeline to ensure image arrays (for example the 'data' or 'seg' entries with shapes like (b, c, x, y) or (b, c, x, y, z) described in the README) meet minimum spatial size requirements and/or are adjusted to be divisible by network-friendly values before further processing (e.g., feeding into a CNN). Padding is computed symmetrically where possible and applied only to the last axes of the array if fewer target dimensions are provided than the image has.` |
| `batchgenerators_augmentations_utils_resize_multichannel_image` | `batchgenerators.augmentations.utils.resize_multichannel_image` | `batchgenerators/augmentations/utils.py` | `multichannel_image: numpy.ndarray, new_shape: tuple, order: int = 3` | `Resize a multichannel image by resizing each channel independently and recombining the resized channels into a multichannel array. This function is used in the batchgenerators augmentation pipeline to resample per-sample channel data (for example modalities in a multi-modal medical image) when a new spatial resolution or shape is required. It preserves the input array's channel ordering and numeric dtype while performing interpolation on a floating-point representation.` |
| `batchgenerators_augmentations_utils_resize_segmentation` | `batchgenerators.augmentations.utils.resize_segmentation` | `batchgenerators/augmentations/utils.py` | `segmentation: numpy.ndarray, new_shape: tuple, order: int = 3` | `Resizes a segmentation map for use in spatial augmentations (medical image augmentation pipeline). This function is intended for resizing discrete label maps (segmentation masks) in the batchgenerators augmentation pipeline used by the MIC@DKFZ codebase. To avoid interpolation artifacts that can occur when resizing integer label images with continuous interpolators (for example producing intermediate label values such as [0, 0, 2] -> [0, 1, 2]), this function either (a) applies nearest-neighbor resizing when order==0 or (b) converts the segmentation into a set of binary masks (one-hot / multihot per unique label), resizes each mask with the specified interpolation order, thresholds the resized masks at 0.5, and reconstructs a label map. The reconstructed map is returned with the same numpy.dtype as the input segmentation. This behavior is used in batchgenerators whenever the 'seg' entry of a data dictionary must undergo spatial transforms (see README).` |
| `batchgenerators_augmentations_utils_uniform` | `batchgenerators.augmentations.utils.uniform` | `batchgenerators/augmentations/utils.py` | `low: float, high: float, size: tuple = None` | `Concise wrapper around numpy.random.uniform that ensures well-defined output when the lower and upper bounds are identical.` |
| `batchgenerators_dataloading_data_loader_default_collate` | `batchgenerators.dataloading.data_loader.default_collate` | `batchgenerators/dataloading/data_loader.py` | `batch: list` | `Default collate function used by batchgenerators to assemble a list of samples into a batched structure. This function is heavily inspired by torch.utils.data.default_collate and is intended for use inside the batchgenerators data loading / augmentation pipeline (for example in MultiThreadedAugmenter and custom DataLoaderBase implementations). It converts a list of per-sample objects (the argument `batch`) into a single batched object that can be passed to downstream augmentations or model training. In the context of batchgenerators this is typically used to form arrays with a leading batch dimension consistent with the README data convention (e.g., data/seg arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D). The function handles a small set of concrete element types (numpy arrays, Python numeric types, numpy numeric scalars, dict/OrderedDict, tuple/list and strings) and applies type-specific collating rules described below. Behavior is recursive for nested containers (dicts of tuples of arrays, etc.). The function enforces consistent structure across samples: for dict/OrderedDict inputs every sample must contain the same keys; for arrays the per-sample array shapes must be compatible for stacking along a new leading batch axis.` |
| `batchgenerators_datasets_cifar_maybe_download_and_prepare_cifar` | `batchgenerators.datasets.cifar.maybe_download_and_prepare_cifar` | `batchgenerators/datasets/cifar.py` | `target_dir: str, cifar: int = 10` | `maybe_download_and_prepare_cifar(target_dir: str, cifar: int = 10) Checks for a local CIFAR dataset in target_dir and downloads, extracts, consolidates, and saves it if the expected consolidated files are missing. This function is used by the CIFAR DataLoader in batchgenerators.datasets.cifar to ensure the CIFAR-10 or CIFAR-100 dataset is available in a simple, single-file format suitable for fast loading and downstream data augmentation pipelines described in the repository README (e.g., for training/testing image classification models or for examples that ship with batchgenerators). This function verifies the presence of two compressed NumPy files in target_dir: "cifar{cifar}_training_data.npz" and "cifar{cifar}_test_data.npz". If either is missing, it downloads the official CIFAR tarball from http://www.cs.toronto.edu/~kriz/cifar-{cifar}-python.tar.gz, extracts the original batch files, unpickles and reshapes the raw batch data into image arrays with shape (N, 3, 32, 32) and dtype numpy.uint8, consolidates the five training batches into a single training array, and writes two compressed .npz files containing data, labels and filenames. After successful creation of the .npz files the extracted folder and the downloaded tar.gz archive are removed to clean up disk space. The function prints a brief message ("downloading CIFAR{cifar}...") when it starts a download. Behavior, formats and practical significance: - Training data: the five data_batch_* files are read in order, each block is reshaped from the original flat representation to (samples_per_batch, 3, 32, 32) and cast to numpy.uint8, then vertically stacked to form a single array. For the standard CIFAR releases this yields 5 * 10000 = 50000 training samples for cifar=10 (the function follows the original CIFAR python files layout). The consolidated training file saved is "cifar{cifar}_training_data.npz" and contains: - data: numpy.ndarray of dtype numpy.uint8 and shape (N_train, 3, 32, 32) - labels: numpy.ndarray of integers (converted from the original batch labels) - filenames: numpy.ndarray of strings (converted from the original batch filenames) - Test data: the single test_batch file is reshaped and saved to "cifar{cifar}_test_data.npz" and contains: - data: numpy.ndarray of dtype numpy.uint8 and shape (N_test, 3, 32, 32) - labels: a Python list of integers (created from the test batch labels in the original files) - filenames: a Python list of strings (from the original test batch) - File cleanup: after creating the two .npz files, the temporary extracted directory "cifar-{cifar}-batches-py" and the downloaded tarball "cifar-{cifar}-python.tar.gz" are removed using shutil.rmtree and os.remove respectively to free disk space. - Intended use: the produced .npz files are optimized for quick repeated loading by the CIFAR DataLoader in batchgenerators and for use in augmentation and training pipelines described in the repository README (e.g., examples and DataLoader implementations that expect the consolidated format).` |
| `batchgenerators_datasets_cifar_unpickle` | `batchgenerators.datasets.cifar.unpickle` | `batchgenerators/datasets/cifar.py` | `file: str` | `Load and return the Python object stored in a CIFAR-format pickled file. This function is a minimal utility used by batchgenerators.datasets.cifar to read the batch files provided by the CIFAR-10/CIFAR-100 dataset distribution (see http://www.cs.toronto.edu/~kriz/cifar.html). It opens the given filesystem path in binary mode and uses the Python pickle module to deserialize the stored object using encoding='bytes'. In the context of batchgenerators, the returned object is consumed by CIFAR dataset loaders to obtain image/label data for training, validation, and testing pipelines.` |
| `batchgenerators_utilities_data_splitting_get_split_deterministic` | `batchgenerators.utilities.data_splitting.get_split_deterministic` | `batchgenerators/utilities/data_splitting.py` | `all_keys: list, fold: int = 0, num_splits: int = 5, random_state: int = 12345` | `Deterministically split a list of patient identifiers (or numeric keys) into training and testing sets corresponding to a single fold of a K-fold split. This function is used in the batchgenerators data-loading/augmentation workflow to create reproducible train/test partitions for cross-validation experiments (for example, when evaluating augmentation strategies or model performance on medical imaging datasets where each element of all_keys corresponds to one patient or case).` |
| `batchgenerators_utilities_file_and_folder_operations_split_path` | `batchgenerators.utilities.file_and_folder_operations.split_path` | `batchgenerators/utilities/file_and_folder_operations.py` | `path: str` | `batchgenerators.utilities.file_and_folder_operations.split_path splits the given filesystem path string at every platform-specific path separator (os.sep) and returns the sequence of path components. This function is used in the batchgenerators codebase for file and folder operations where the caller needs all path segments (for example when parsing dataset directory hierarchies, constructing relative paths for data loaders, or comparing individual path components in augmentation pipelines). It differs from os.path.split in that it does not only split at the last separator but returns all components separated by os.sep. This function performs a simple, non-normalizing string split and does not consult the filesystem. It does not resolve symbolic links, collapse '.' or '..' segments, handle alternate separators (for example '/' vs '\' on Windows) or perform OS-specific path normalization beyond using the current os.sep value. Because it calls the standard str.split method, certain corner cases produce empty-string components: a leading separator yields an initial empty string component, a trailing separator yields a final empty string component, and consecutive separators produce empty components between them. An empty input string yields a single-element list containing an empty string. There are no side effects (no file I/O, no modification of the filesystem) and the operation is pure and safe to call repeatedly.` |

## ‚öñÔ∏è License

Original Code License: Apache-2.0

Wrapper Code & Documentation: Apache-2.0

*This file was automatically generated on February 26, 2026.*
