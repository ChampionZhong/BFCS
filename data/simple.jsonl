{"func_name": "Bio_Cluster_distancematrix", "func_desc": "Compute and return a triangular distance matrix from a 2-D NumPy array for use in clustering and other computational-molecular-biology analyses.", "tools": [{"function": {"description": "Compute and return a triangular distance matrix from a 2-D NumPy array for use in clustering and other computational-molecular-biology analyses.\n", "name": "Bio_Cluster_distancematrix", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "nrows x ncolumns 2-D array containing the numeric data values.\nIn the Bio.Cluster context this represents the matrix of observations used to\ncompute pairwise distances (for example rows = samples and columns = features,\nor vice versa when transpose=True). This function calls internal validation\n(__check_data) and requires a numeric, two-dimensional numpy.ndarray.", "default": ""}, "mask": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "nrows x ncolumns array of integers indicating missing\nvalues (same shape as data). If mask[i, j] == 0 then data[i, j] is treated as\nmissing and excluded from pairwise distance calculations. If None (the default),\nno values are treated as missing. The mask must align with data shape exactly;\nsupplying an incorrectly shaped mask will raise an error.", "default": null}, "weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1-D array of weights applied to the data when\ncalculating distances. The length of weight must match the number of items over\nwhich pairwise distances are computed (that is, the number of columns when\ntranspose=False, or the number of rows when transpose=True). If None (the\ndefault) equal weighting is assumed. The function validates weight length via\n__check_weight and will raise an error on length mismatch.", "default": null}, "transpose": {"type": "boolean", "description": "If False (default), distances are computed between rows of data.\nIf True, distances are computed between columns of data. This is provided to\nsupport common analysis patterns in molecular data where either samples or\nmeasured features may be compared.", "default": false}, "dist": {"type": "string", "description": "Single-character code selecting the distance or similarity metric to\nuse (default 'e'). The supported codes and their meanings are:\n'e' -- Euclidean distance\n'b' -- City Block (Manhattan) distance\n'c' -- Pearson correlation\n'a' -- Absolute value of the Pearson correlation\n'u' -- Uncentered correlation\n'x' -- Absolute value of the uncentered correlation\n's' -- Spearman's rank correlation\n'k' -- Kendall's tau\nThe chosen code controls the mathematical form used for pairwise comparisons;\ninvalid or unsupported codes will result in an error from the underlying\nroutine.", "default": "e"}}, "required": ["data", "weight", "transpose", "mask", "dist"], "type": "any"}}, "type": "function"}], "query": "You’re preparing tissue-by-tissue clustering from a small RNA-seq pilot: 4 tissue samples measured across 3 genes, but you realize the wet-lab pipeline produced one clearly non-biological artifact column (a spike-in / barcode metric) that should not contribute to Euclidean distances. Initially you were going to compute the triangular Euclidean distance matrix treating all 3 columns equally, but that would bias the clustering. Instead, compute the triangular distance matrix using Euclidean distance on only the biologically meaningful gene-expression columns: keep the two columns that have values on the same scale as typical expression and drop the remaining column whose values are consistently the largest across all tissues. The matrix is [[5.2, 3.1, 4.8], [4.9, 2.8, 4.5], [6.0, 3.5, 5.1], [5.5, 3.0, 4.9]]. Assume no missing values and do not transpose (rows are tissues). What function call would you use?", "answers": "[{\"name\": \"Bio_Cluster_distancematrix\", \"arguments\": {\"data\": [[3.1, 4.8], [2.8, 4.5], [3.5, 5.1], [3.0, 4.9]], \"mask\": null, \"weight\": null, \"transpose\": false, \"dist\": \"e\"}}]"}
{"func_name": "Bio_Cluster_kmedoids", "func_desc": "Perform k-medoids clustering on a distance matrix.\n    \n    This function performs k-medoids clustering (partitioning around medoids) on a set of items given a distance matrix and returns the best clustering found, the within-cluster sum of distances (objective) for that clustering, and how many times that optimal solution was discovered across repeated restarts. In the context of Biopython and computational molecular biology, this is typically used to cluster biological items (for example, sequences, profiles, or other pairwise comparisons) when a precomputed pairwise distance matrix is available. The routine accepts three different representations of a symmetric distance matrix (full 2D array, condensed 1D array, or a list of lower-triangular rows) and uses only the lower-triangular part when a full 2D NumPy array is provided. The implementation validates the distance input, prepares an initial clustering (either provided or generated randomly), and delegates the core computation to the underlying clustering routine. If an explicit initial clustering is supplied via initialid, the algorithm runs once deterministically using that initialization; otherwise it performs npass independent random restarts and returns the best solution found.", "tools": [{"function": {"description": "Perform k-medoids clustering on a distance matrix.\n\nThis function performs k-medoids clustering (partitioning around medoids) on a set of items given a distance matrix and returns the best clustering found, the within-cluster sum of distances (objective) for that clustering, and how many times that optimal solution was discovered across repeated restarts. In the context of Biopython and computational molecular biology, this is typically used to cluster biological items (for example, sequences, profiles, or other pairwise comparisons) when a precomputed pairwise distance matrix is available. The routine accepts three different representations of a symmetric distance matrix (full 2D array, condensed 1D array, or a list of lower-triangular rows) and uses only the lower-triangular part when a full 2D NumPy array is provided. The implementation validates the distance input, prepares an initial clustering (either provided or generated randomly), and delegates the core computation to the underlying clustering routine. If an explicit initial clustering is supplied via initialid, the algorithm runs once deterministically using that initialization; otherwise it performs npass independent random restarts and returns the best solution found.", "name": "Bio_Cluster_kmedoids", "parameters": {"properties": {"distance": {"type": "array", "items": {"type": "any"}, "description": "The pairwise distance matrix between items. Accepted formats are: (1) a 2D NumPy array (only the left-lower / lower-triangular part is accessed), (2) a 1D NumPy array containing the condensed distances consecutively (as in SciPy/cluster condensed format), or (3) a list of 1D arrays where each element i contains the i-th row of the lower-triangular part (the first element may be empty). This argument is required and is validated by the function; malformed shapes or incompatible dimensions will raise a ValueError. The distances represent domain-specific dissimilarities (for example, sequence edit distances, profile distances, or other biologically meaningful dissimilarity measures) and must be non-negative as expected by k-medoids algorithms.", "default": ""}, "nclusters": {"type": "integer", "description": "The desired number of clusters (the k in k-medoids). Must be a positive integer less than or equal to the number of items represented by distance. Default is 2. If nclusters is larger than the number of items, the routine will raise an error.", "default": 2}, "npass": {"type": "integer", "description": "The number of independent runs (random restarts) of the k-medoids algorithm to perform when no initialid is provided. Each pass uses a different random initial clustering to reduce the chance of converging to a poor local optimum. Default is 1. If initialid is supplied, npass is ignored and the algorithm runs exactly once using the provided initial clustering.", "default": 1}, "initialid": {"type": "any", "nullable": true, "description": "An optional initial clustering assignment used to start the algorithm deterministically. When given, initialid should specify a clustering consistent with the number of items (typically as an array-like of medoid indices or cluster assignments compatible with the internal checks); the routine will perform the EM-style re-assignment/medoid update sequence once starting from this initialization and will not randomize the order in which items are assigned to clusters. Providing initialid makes the run deterministic (useful for reproducible experiments or continuing clustering from a previous result). When initialid is None (the default), the routine will perform npass randomized starts and return the best solution found.", "default": null}}, "required": ["distance", "initialid", "nclusters", "npass"], "type": "any"}}, "type": "function"}], "query": "I’m clustering 5 protein sequences (A–E) based on a precomputed symmetric distance matrix (smaller = more similar). I was going to hand the k-medoids routine the full 5×5 matrix and do 10 random restarts for stability, but I realized this matrix came out of a pipeline that only guarantees the *lower-triangular* values are trustworthy (the upper triangle sometimes has rounding/serialization drift). So please feed the algorithm **only the lower-triangular representation** (include the diagonal zeros as usual in that format) to ensure it ignores the potentially corrupted upper triangle, and still do **the same number of random restarts as originally planned**. Use 2 clusters. The full matrix I have on hand (but do not rely on its upper triangle) is:\n[[0.0, 0.12, 0.55, 0.58, 0.60],\n [0.12, 0.0, 0.57, 0.59, 0.61],\n [0.55, 0.57, 0.0, 0.10, 0.15],\n [0.58, 0.59, 0.10, 0.0, 0.11],\n [0.60, 0.61, 0.15, 0.11, 0.0]].\nProvide the best clustering found, its within-cluster sum of distances, and how many times that optimum was recovered across the restarts.", "answers": "[{\"name\":\"Bio_Cluster_kmedoids\",\"arguments\":{\"distance\":[[0.0],[0.12,0.0],[0.55,0.57,0.0],[0.58,0.59,0.1,0.0],[0.6,0.61,0.15,0.11,0.0]],\"nclusters\":2,\"npass\":10,\"initialid\":null}}]"}
{"func_name": "Bio_Data_CodonTable_list_ambiguous_codons", "func_desc": "Bio.Data.CodonTable.list_ambiguous_codons: Extend a list of codon strings by adding unambiguous and ambiguous codon codes that represent only the provided codons, used for example when constructing codon tables in Biopython (e.g., to extend a list of stop codons with ambiguity codes like 'TAR' or 'URA').\n    \n    This function is used in the Biopython codon table utilities to compute ambiguous codon symbols that are safe to add to an existing set of codons (for example, stop codons). It examines the nucleotides observed at each of the three codon positions in the provided codons and identifies ambiguous nucleotide letters (from ambiguous_nucleotide_values) whose concrete nucleotide meanings are fully represented at that position across the input codons. It then forms candidate ambiguous codons by combining such letters for the three positions and retains only those candidates whose full expansion (all concrete codons implied by the ambiguous letters) are present in the input codons. The function returns a new list containing the original codons (in their original order) followed by any added ambiguous codons (in a deterministic order produced by the algorithm). The function does not modify the input codons list or the ambiguous_nucleotide_values mapping.", "tools": [{"function": {"description": "Bio.Data.CodonTable.list_ambiguous_codons: Extend a list of codon strings by adding unambiguous and ambiguous codon codes that represent only the provided codons, used for example when constructing codon tables in Biopython (e.g., to extend a list of stop codons with ambiguity codes like 'TAR' or 'URA').\n\nThis function is used in the Biopython codon table utilities to compute ambiguous codon symbols that are safe to add to an existing set of codons (for example, stop codons). It examines the nucleotides observed at each of the three codon positions in the provided codons and identifies ambiguous nucleotide letters (from ambiguous_nucleotide_values) whose concrete nucleotide meanings are fully represented at that position across the input codons. It then forms candidate ambiguous codons by combining such letters for the three positions and retains only those candidates whose full expansion (all concrete codons implied by the ambiguous letters) are present in the input codons. The function returns a new list containing the original codons (in their original order) followed by any added ambiguous codons (in a deterministic order produced by the algorithm). The function does not modify the input codons list or the ambiguous_nucleotide_values mapping.", "name": "Bio_Data_CodonTable_list_ambiguous_codons", "parameters": {"properties": {"codons": {"type": "array", "items": {"type": "any"}, "description": "A list of codon strings to extend. Each element is expected to be a three-character nucleotide triplet (DNA or RNA letters such as 'A', 'C', 'G', 'T' or 'U'), for example ['TAG', 'TAA'] or ['UAG', 'UGA']. In Biopython this argument is typically a set of codons with a specific biological meaning (for example, stop codons). The function uses these codons as the authoritative set: ambiguous codons will only be added if every concrete codon they represent is already present in this list. The input list is not modified; it is copied into the returned list.", "default": ""}, "ambiguous_nucleotide_values": {"type": "any", "description": "A mapping from single-letter ambiguous nucleotide codes to the string (or sequence) of concrete nucleotides they represent, for example {'R': 'AG', 'Y': 'CT'} (or using 'U' for RNA). This mapping is used to (1) determine which ambiguous letters cover all observed nucleotides at each codon position, and (2) expand candidate ambiguous codons into their concrete codons for validation. Keys must be single-character strings corresponding to ambiguity codes used in the candidates; values must be iterable strings of concrete nucleotide characters. If a required ambiguity code is missing from this mapping, a KeyError will be raised during expansion.", "default": ""}}, "required": ["codons", "ambiguous_nucleotide_values"], "type": "any"}}, "type": "function"}], "query": "I’m curating stop-codon handling for a messy mixed-format annotation pipeline. I initially planned to extend my stop-codon list using the DNA-style triplets I got from one source, but then I realized the downstream translator expects RNA triplets and will silently mis-handle thymine. So: convert the provided stop codons by applying the standard T→U transcription, then extend that RNA stop-codon set with any ambiguity-coded codons that are *safe* (i.e., every concrete codon they expand to is already in the stop set). Use the standard RNA ambiguity mapping (IUPAC RNA letters) and keep the deterministic ordering behavior of the Biopython utility (original codons first, then any added ambiguous codons). Start from these stop codons: [\"TAA\",\"TAG\",\"TGA\"].", "answers": "[{\"name\":\"Bio_Data_CodonTable_list_ambiguous_codons\",\"arguments\":{\"codons\":[\"UAA\",\"UAG\",\"UGA\"],\"ambiguous_nucleotide_values\":{\"A\":\"A\",\"C\":\"C\",\"G\":\"G\",\"U\":\"U\",\"R\":\"AG\",\"Y\":\"CU\",\"S\":\"GC\",\"W\":\"AU\",\"K\":\"GU\",\"M\":\"AC\",\"B\":\"CGU\",\"D\":\"AGU\",\"H\":\"ACU\",\"V\":\"ACG\",\"N\":\"ACGU\"}}}]"}
{"func_name": "Bio_Data_CodonTable_register_ncbi_table", "func_desc": "Bio.Data.CodonTable.register_ncbi_table registers a single NCBI-style codon table definition with the CodonTable module by converting the provided raw table data into multiple internal codon table objects used throughout Biopython for DNA/RNA translation and ambiguous-base handling. This function is intended for internal (PRIVATE) use when populating the module-level codon table registries used by sequence translation and related utilities in computational molecular biology workflows.", "tools": [{"function": {"description": "Bio.Data.CodonTable.register_ncbi_table registers a single NCBI-style codon table definition with the CodonTable module by converting the provided raw table data into multiple internal codon table objects used throughout Biopython for DNA/RNA translation and ambiguous-base handling. This function is intended for internal (PRIVATE) use when populating the module-level codon table registries used by sequence translation and related utilities in computational molecular biology workflows.\n", "name": "Bio_Data_CodonTable_register_ncbi_table", "parameters": {"properties": {"name": {"type": "string", "description": "The primary name string for the codon table as provided by NCBI. This string may contain multiple synonymous names separated by \"; \", \", \" or the word \" and \". The function parses this string into a list of individual names (splitting on \"; \", converting \" and \" to \"; \", and trimming whitespace) and uses those names when registering the created codon table objects in the module-level name-to-table mappings. The parsed names are important for lookups by human-readable table names in Biopython translation functions.", "default": ""}, "alt_name": {"type": "string", "description": "An alternate or legacy single name for the codon table (may be None). If not None, this alternate name is appended to the parsed names list and also used when registering the table objects. alt_name provides backward compatibility with older naming conventions and is stored with the generated objects and registries.", "default": ""}, "id": {"type": "integer", "description": "The numeric NCBI identifier for the codon table (for example, 1 for the standard code). This integer is used as the key when storing the generated table objects in the module-level id-indexed registries (e.g., unambiguous_dna_by_id, unambiguous_rna_by_id, generic_by_id, ambiguous_*_by_id). If id equals 1, the function also sets the module-level globals standard_dna_table and standard_rna_table to the created DNA and RNA objects respectively. Care should be taken because registering a table with an id that already exists will overwrite the existing entries in these registries.", "default": ""}, "table": {"type": "any", "description": "A mapping of codon strings to encoded amino acid values representing the unambiguous DNA codon-to-amino-acid assignments as provided by NCBI. Keys are expected to be codon strings using the letter \"T\" for thymine (e.g., \"ATG\", \"TAA\"); values are the corresponding amino acid symbol(s) or stop indicators as used by Biopython. The function uses this dictionary to build three variants of unambiguous lookup tables: a DNA-specific table (T-based), an RNA-specific table (T replaced by U), and a generic table that contains both T- and U-based codons for flexible lookup across DNA/RNA sequences.", "default": ""}, "start_codons": {"type": "array", "items": {"type": "any"}, "description": "A list of codon strings (using \"T\" for thymine) that are considered valid translation initiation (start) codons for this genetic code. These are used to populate corresponding start-codon lists for the DNA-specific, RNA-specific (with \"T\" replaced by \"U\"), and generic tables. The implementation takes care to add both T- and U-forms where appropriate but only adds the U-form when the original contains \"T\" to reduce accidental duplicates.", "default": ""}, "stop_codons": {"type": "array", "items": {"type": "any"}, "description": "A list of codon strings (using \"T\" for thymine) that are considered translation termination (stop) codons for this genetic code. These are used to populate the stop-codon lists for the DNA-specific, RNA-specific (with \"T\" replaced by \"U\"), and generic tables in the same manner as start_codons, with both T- and U-forms added where applicable.", "default": ""}}, "required": ["name", "alt_name", "id", "table", "start_codons", "stop_codons"], "type": "any"}}, "type": "function"}], "query": "In our Biopython-based synthetic-genome annotation pipeline we need to register an internal NCBI-style codon table for a prototype chassis organism. Initially I was going to keep the stop-codon list exactly as in the legacy spec, but I just realized one of those triplets is being re-assigned as a sense codon in this design, so it must NOT be treated as a stop anywhere in translation.\n\nPlease register the table using Bio.Data.CodonTable.register_ncbi_table with these constraints:\n- Use the same table id as the draft spec (the two-digit number from the previous version).\n- Keep the human-facing name string exactly: \"Synthetic Code X; SynX and Experimental\".\n- Keep the legacy alternative name exactly: \"SynX-legacy\".\n- Minimal codon mapping: ATG→M, TGG→W, and only the remaining canonical stops should map to \"*\" (i.e., remove the one that was reassigned).\n- Start codons: keep the same two as the draft.\n- Stop codons: include only the canonical stop codons that are still stops after the reassignment.\n\n(Determinism note: the reassigned codon is the one that also acts as a common bacterial alternative start in many systems; remove it from stop_codons and from the \"*\" mappings.)", "answers": "[{\"name\": \"Bio_Data_CodonTable_register_ncbi_table\", \"arguments\": {\"name\": \"Synthetic Code X; SynX and Experimental\", \"alt_name\": \"SynX-legacy\", \"id\": 99, \"table\": {\"ATG\": \"M\", \"TGG\": \"W\", \"TAA\": \"*\", \"TAG\": \"*\"}, \"start_codons\": [\"ATG\", \"TTG\"], \"stop_codons\": [\"TAA\", \"TAG\"]}}]"}
{"func_name": "Bio_ExPASy_get_prosite_entry", "func_desc": "Get a text handle to a PROSITE entry on the ExPASy PROSITE service, returned in HTML format.\n    \n    This function is part of Biopython's ExPASy utilities for computational molecular biology and is used to fetch PROSITE database entries (protein families, domains, and functional site annotations) over the web. It performs an HTTP GET request by constructing the URL f\"{cgi}?{id}\" (concatenating the provided CGI endpoint, a question mark, and the PROSITE entry identifier) and returns a text-mode file-like handle for the HTTP response body. The returned handle supports reading the HTML content (for example, via handle.read()) and can be used as a context manager in a with statement. Typical use is to request a PROSITE accession such as \"PS00001\" to retrieve the human- and machine-readable HTML documentation for that PROSITE pattern or profile, which can then be inspected, archived, or parsed for downstream sequence analysis or annotation tasks.", "tools": [{"function": {"description": "Get a text handle to a PROSITE entry on the ExPASy PROSITE service, returned in HTML format.\n\nThis function is part of Biopython's ExPASy utilities for computational molecular biology and is used to fetch PROSITE database entries (protein families, domains, and functional site annotations) over the web. It performs an HTTP GET request by constructing the URL f\"{cgi}?{id}\" (concatenating the provided CGI endpoint, a question mark, and the PROSITE entry identifier) and returns a text-mode file-like handle for the HTTP response body. The returned handle supports reading the HTML content (for example, via handle.read()) and can be used as a context manager in a with statement. Typical use is to request a PROSITE accession such as \"PS00001\" to retrieve the human- and machine-readable HTML documentation for that PROSITE pattern or profile, which can then be inspected, archived, or parsed for downstream sequence analysis or annotation tasks.", "name": "Bio_ExPASy_get_prosite_entry", "parameters": {"properties": {"id": {"type": "string", "description": "The PROSITE entry identifier to request from the ExPASy PROSITE service. This is the query string portion appended to the CGI endpoint (for example, \"PS00001\"). The function will form the request URL by appending '?' and this id to the cgi parameter exactly as provided, so callers should supply only the identifier (not a leading '?' or additional query parameters).", "default": ""}, "cgi": {"type": "string", "description": "The base ExPASy PROSITE CGI endpoint used to retrieve entries. Defaults to \"https://prosite.expasy.org/cgi-bin/prosite/get-prosite-entry\". Providing a different URL can be used to query mirrors or alternate endpoints; the function will append '?' and the id to this string to form the final request URL.", "default": "https://prosite.expasy.org/cgi-bin/prosite/get-prosite-entry"}}, "required": ["id"], "type": "any"}}, "type": "function"}], "query": "I’m annotating a protein domain and want to archive the official PROSITE HTML page for the Zinc finger C2H2-type signature. Initially I was going to pull the accession I have in my notes, but I just realized our downstream parser rejects entries whose HTML is a pattern/profile *group landing page* rather than a single signature document. So: fetch the PROSITE HTML for the **closest earlier** Zinc finger C2H2-type *signature* accession that is exactly one step lower than the one in my notes (i.e., decrement the numeric part by 1 while keeping the same prefix/format), using the default ExPASy PROSITE endpoint.", "answers": "[{\"name\":\"Bio_ExPASy_get_prosite_entry\",\"arguments\":{\"id\":\"PS00027\"}}]"}
{"func_name": "Bio_Geo_Record_out_block", "func_desc": "Bio.Geo.Record.out_block formats a long text string into fixed-width blocks of up to 80 characters per block (slicing the input text every 80 characters) and returns the assembled lines with an optional prefix prepended to each line. This function is used in the Biopython codebase (for example in record formatting within Bio.Geo.Record) to produce human-readable and file-format-friendly multiline fields when writing out record annotations or other long text fields in computational molecular biology workflows.\n    \n    Behavior: The input text is split by simple character slices of length 80 (text[0:80], text[80:160], ...). Each slice is emitted as a separate line with a trailing newline character. After all blocks are emitted an additional newline is appended. If text is empty the function returns a single newline. Note that the slicing is performed only on the text content; the prefix is prepended to each output line after slicing and therefore can increase the printed line length beyond 80 characters. The function does not perform word-aware wrapping and may split words across block boundaries. This routine does not perform any I/O operations; it builds and returns a string suitable for inclusion in file output or display.", "tools": [{"function": {"description": "Bio.Geo.Record.out_block formats a long text string into fixed-width blocks of up to 80 characters per block (slicing the input text every 80 characters) and returns the assembled lines with an optional prefix prepended to each line. This function is used in the Biopython codebase (for example in record formatting within Bio.Geo.Record) to produce human-readable and file-format-friendly multiline fields when writing out record annotations or other long text fields in computational molecular biology workflows.\n\nBehavior: The input text is split by simple character slices of length 80 (text[0:80], text[80:160], ...). Each slice is emitted as a separate line with a trailing newline character. After all blocks are emitted an additional newline is appended. If text is empty the function returns a single newline. Note that the slicing is performed only on the text content; the prefix is prepended to each output line after slicing and therefore can increase the printed line length beyond 80 characters. The function does not perform word-aware wrapping and may split words across block boundaries. This routine does not perform any I/O operations; it builds and returns a string suitable for inclusion in file output or display.", "name": "Bio_Geo_Record_out_block", "parameters": {"properties": {"text": {"type": "string", "description": "The input text to be formatted into 80-character blocks. In the Biopython context this typically holds long annotation strings or record field content that must be broken into fixed-width lines for textual record output. The function treats this as plain Python text (Unicode string) and slices it by character indices; multi-codepoint or multi-byte characters count as individual Python characters for slicing. Supplying a non-string value may cause a runtime error when slicing is attempted.", "default": ""}, "prefix": {"type": "string", "description": "Optional string to prepend to every output line. By default this is the empty string (prefix = \"\"), meaning no extra characters are added before each block. In practical use within Biopython this prefix can be used to add field labels, indentation, or format-specific line headers. Because prefix is added after slicing the text into 80-character chunks, a non-empty prefix will make the visible line length exceed 80 characters.", "default": ""}}, "required": ["text", "prefix"], "type": "any"}}, "type": "function"}], "query": "I’m exporting a GEO-like flatfile from a metagenomics cruise log, and I need to wrap a long free-text annotation using `Bio.Geo.Record.out_block`. Initially I was going to label each wrapped line with the literal tag I’ve been using in my lab notebook (it ends with a colon and a space), but I just realized the downstream validator rejects that tag because it contains a forbidden substring. Keep the same visual style (all-caps field name, followed by a colon and a single trailing space), but switch to the closest synonym that would be acceptable to a strict GEO parser for an annotation field. Then format this exact annotation text into fixed-width blocks of up to 80 characters per block (character-sliced, not word-wrapped), with that corrected prefix prepended to every emitted line:\n\n\"This coastal sampling campaign focused on microbial communities in surface ocean waters collected along a 500 km transect. Environmental parameters including temperature, salinity, chlorophyll concentration, and nutrient levels were recorded at each station for downstream metagenomic correlation analyses.\"", "answers": "[{\"name\":\"Bio_Geo_Record_out_block\",\"arguments\":{\"text\":\"This coastal sampling campaign focused on microbial communities in surface ocean waters collected along a 500 km transect. Environmental parameters including temperature, salinity, chlorophyll concentration, and nutrient levels were recorded at each station for downstream metagenomic correlation analyses.\",\"prefix\":\"COMMENT: \"}}]"}
{"func_name": "Bio_KEGG_REST_kegg_get", "func_desc": "Bio.KEGG.REST.kegg_get — Retrieve KEGG database entries via the KEGG REST \"get\" endpoint and return a handle to the response. This function is used in the KEGG REST client portion of Biopython to request textual or binary data for one or more KEGG entries (for example genes, pathways, compounds, glycans, reactions, enzymes, or organism-specific entries) and to obtain specific views of those entries (for example amino-acid sequences, nucleotide sequences, molecular files, KCF, pathway images, KGML, or JSON when supported).", "tools": [{"function": {"description": "Bio.KEGG.REST.kegg_get — Retrieve KEGG database entries via the KEGG REST \"get\" endpoint and return a handle to the response. This function is used in the KEGG REST client portion of Biopython to request textual or binary data for one or more KEGG entries (for example genes, pathways, compounds, glycans, reactions, enzymes, or organism-specific entries) and to obtain specific views of those entries (for example amino-acid sequences, nucleotide sequences, molecular files, KCF, pathway images, KGML, or JSON when supported).\n", "name": "Bio_KEGG_REST_kegg_get", "parameters": {"properties": {"dbentries": {"type": "array", "items": {"type": "float"}, "description": "KEGG identifiers to retrieve. This may be a single identifier string (for example \"hsa:10458\", \"cpd:C00031\", or a pathway id like \"path:hsa00010\") or a list of such identifier strings. When passed as a list, the function will join the identifiers with '+' to form the KEGG REST query (the REST API expects multiple entries joined by '+'). The function enforces a maximum of 10 identifiers per request and will raise ValueError if a list longer than 10 is provided. Valid kinds of entries include KEGG database names shown in the code comments (pathway, brite, module, disease, drug, environ, ko, genome, <org>, compound, glycan, reaction, rpair, rclass, enzyme); <org> may be a KEGG organism code or a T number. Practical significance: callers should supply the exact KEGG identifiers or codes required by the KEGG REST API and may combine up to 10 entries in one call to reduce round trips.", "default": ""}, "option": {"type": "string", "nullable": true, "description": "Retrieval option to request a specific representation of the entry. Allowed values (as accepted by the function) are \"aaseq\", \"ntseq\", \"mol\", \"kcf\", \"image\", \"kgml\", and \"json\". If option is provided and valid, it is appended to the REST URL as /<option> (for example requesting protein sequences with \"aaseq\" or pathway XML with \"kgml\"). Semantic constraints enforced by the KEGG REST API and noted in the function: only one pathway entry may be requested when using \"image\" or \"kgml\"; only one compound, glycan, or drug entry may be requested when using \"image\". If an invalid non-empty option string is supplied, the function raises ValueError. If option is None (the default), the canonical text entry is requested.", "default": null}}, "required": ["dbentries", "option"], "type": "any"}}, "type": "function"}], "query": "I’m building a KEGG-based glycolysis comparison, but I just realized my list of pathway IDs was assembled from mixed notes and may include a non-pathway token. Start by attempting to retrieve glycolysis for human, mouse, and yeast in one call using the three organism-specific identifiers I wrote down. Then sanity-check the returned records: keep only entries whose KEGG ENTRY line clearly indicates a PATHWAY record and whose identifier matches the canonical glycolysis map number for that organism. If any of my three inputs isn’t a valid pathway entry under the get endpoint, drop only that one and re-issue the request for the remaining valid IDs (still batched if more than one remains). Return the default plain-text pathway entries only (no KGML/images/JSON).", "answers": "[{\"name\": \"Bio_KEGG_REST_kegg_get\", \"arguments\": {\"dbentries\": [\"path:hsa00010\", \"path:mmu00010\", \"path:sce00010\"], \"option\": null}}]"}
{"func_name": "Bio_Nexus_Nexus_get_start_end", "func_desc": "Return the zero-based start and end indices of the first and last characters\n    in a sequence that are not members of a skiplist. This function is used in\n    Biopython's NEXUS/alignment parsing (Bio.Nexus.Nexus) to locate the ungapped\n    region of an aligned molecular sequence (for example, to trim leading and\n    trailing gap ('-') or unknown ('?') symbols before downstream processing such\n    as consensus building, distance calculations, or writing trimmed alignments).", "tools": [{"function": {"description": "Return the zero-based start and end indices of the first and last characters\nin a sequence that are not members of a skiplist. This function is used in\nBiopython's NEXUS/alignment parsing (Bio.Nexus.Nexus) to locate the ungapped\nregion of an aligned molecular sequence (for example, to trim leading and\ntrailing gap ('-') or unknown ('?') symbols before downstream processing such\nas consensus building, distance calculations, or writing trimmed alignments).", "name": "Bio_Nexus_Nexus_get_start_end", "parameters": {"properties": {"sequence": {"type": "string", "description": "The input sequence to inspect. This should be a Python\nstring representing a biological sequence (for example a DNA, RNA or\nprotein alignment row) that may contain gap or unknown symbols. The\nfunction computes len(sequence) and indexes into sequence, so passing a\nnon-string or a non-indexable object will raise a TypeError. An empty\nstring (length 0) is treated specially and yields (None, None).", "default": ""}, "skiplist": {"type": "any", "description": "A tuple of characters to ignore when scanning from the\nends of sequence. By convention in NEXUS and other alignment contexts,\nthis defaults to ('-', '?') to represent gap and unknown characters.\nElements of skiplist are compared against individual characters of\nsequence using membership (sequence[i] in skiplist). For typical use,\nprovide single-character string elements that match the gap/unknown\nsymbols present in your alignment.", "default": ["-", "?"]}}, "required": ["sequence", "skiplist"], "type": "any"}}, "type": "function"}], "query": "I’m parsing a NEXUS-style aligned DNA row for downstream distance calculations. Initially I was going to treat just gaps and explicit unknowns as skippable, but I realized the export pipeline occasionally pads sequences with a third placeholder character that is lexicographically between '-' and '?' in ASCII, and those padding runs must be trimmed too or the consensus step breaks. For the sequence \"---??ACGTN??--\", return the zero-based start and end indices of the region to keep (i.e., the first and last characters that are not in the skiplist), where the skiplist includes '-' and '?' plus that additional padding character (the one whose codepoint lies strictly between them).", "answers": "[{\"name\":\"Bio_Nexus_Nexus_get_start_end\",\"arguments\":{\"sequence\":\"---??ACGTN??--\",\"skiplist\":[\"-\",\".\",\"?\"]}}]"}
{"func_name": "Bio_PDB_DSSP_ss_to_index", "func_desc": "Bio.PDB.DSSP.ss_to_index converts a single-letter DSSP secondary structure symbol into a small integer index used by Bio.PDB.DSSP and other Biopython code that needs numeric labels for protein secondary structure. In the context of Biopython (tools for computational molecular biology and bioinformatics), this mapping is used to convert DSSP output symbols into compact integer codes for array indexing, statistical summaries, or machine learning feature labels.", "tools": [{"function": {"description": "Bio.PDB.DSSP.ss_to_index converts a single-letter DSSP secondary structure symbol into a small integer index used by Bio.PDB.DSSP and other Biopython code that needs numeric labels for protein secondary structure. In the context of Biopython (tools for computational molecular biology and bioinformatics), this mapping is used to convert DSSP output symbols into compact integer codes for array indexing, statistical summaries, or machine learning feature labels.\n", "name": "Bio_PDB_DSSP_ss_to_index", "parameters": {"properties": {"ss": {"type": "string", "description": "A single-character DSSP secondary structure symbol. This function expects the exact, case-sensitive symbols produced by DSSP: \"H\" for alpha-helix, \"E\" for beta-strand (extended), and \"C\" for coil or other non-regular secondary structure. The argument is compared directly to these literal strings (no trimming or case conversion is performed).", "default": ""}}, "required": ["ss"], "type": "any"}}, "type": "function"}], "query": "I’m building an ML feature matrix from DSSP output and I need the Biopython integer label (via `Bio.PDB.DSSP.ss_to_index`) for each residue’s secondary-structure character. I was going to label the residues marked as the extended beta-type in DSSP, but I just realized my DSSP dump sometimes includes lowercase letters from a post-processing step and occasionally a missing-value placeholder. So: first, sanitize the symbol by stripping whitespace and uppercasing it; if it becomes the DSSP beta extended state after that, convert it to the Biopython DSSP integer index for array indexing.", "answers": "[{\"name\": \"Bio_PDB_DSSP_ss_to_index\", \"arguments\": {\"ss\": \"E\"}}]"}
{"func_name": "Bio_PDB_Polypeptide_index_to_three", "func_desc": "Bio.PDB.Polypeptide.index_to_three maps an integer index to the corresponding three-letter amino acid residue name used in PDB-style representations and by the Bio.PDB.Polypeptide utilities for sequence and structure handling. This function is used in the Biopython PDB module to convert numeric residue indices (for example, indices in the range 0–19 for the 20 standard amino acids) into the conventional three-letter residue codes (e.g., 0 -> 'ALA', 19 -> 'TYR') that appear in PDB files and are required by downstream PDB/structure-processing routines.", "tools": [{"function": {"description": "Bio.PDB.Polypeptide.index_to_three maps an integer index to the corresponding three-letter amino acid residue name used in PDB-style representations and by the Bio.PDB.Polypeptide utilities for sequence and structure handling. This function is used in the Biopython PDB module to convert numeric residue indices (for example, indices in the range 0–19 for the 20 standard amino acids) into the conventional three-letter residue codes (e.g., 0 -> 'ALA', 19 -> 'TYR') that appear in PDB files and are required by downstream PDB/structure-processing routines.\n", "name": "Bio_PDB_Polypeptide_index_to_three", "parameters": {"properties": {"i": {"type": "integer", "description": "Integer index representing an amino acid. In the context of Bio.PDB.Polypeptide, this integer is used as a lookup key into the module-level mapping dindex_to_3 to retrieve the canonical three-letter, uppercase residue name. The value should correspond to an index present in that mapping; typical use is with indices for the twenty standard amino acids.", "default": ""}}, "required": ["i"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing a protein chain from a one-hot model output for a PDB-writing pipeline using Biopython’s `Bio.PDB.Polypeptide.index_to_three`. I was going to directly convert the argmax index from the network for one position, but I realized the model occasionally emits an extra ‘unknown/other’ class at the end, so the valid amino-acid index is the **largest index that is still within the 20 standard residues** (i.e., drop any trailing non-standard class if present). For that corrected index, what three-letter residue name should I write into the PDB file?", "answers": "[{\"name\": \"Bio_PDB_Polypeptide_index_to_three\", \"arguments\": {\"i\": 19}}]"}
{"func_name": "Bio_PDB_Selection_uniqueify", "func_desc": "Bio.PDB.Selection.uniqueify returns a list containing the unique elements from the provided list. This function is used in the Bio.PDB selection and processing code within Biopython (a toolkit for computational molecular biology) to remove duplicate entries when assembling or filtering PDB-related collections (for example, lists of atom identifiers, residue objects, or other selection results). The deduplication is performed via Python's set, so the operation is fast for hashable items but does not preserve the original input order.", "tools": [{"function": {"description": "Bio.PDB.Selection.uniqueify returns a list containing the unique elements from the provided list. This function is used in the Bio.PDB selection and processing code within Biopython (a toolkit for computational molecular biology) to remove duplicate entries when assembling or filtering PDB-related collections (for example, lists of atom identifiers, residue objects, or other selection results). The deduplication is performed via Python's set, so the operation is fast for hashable items but does not preserve the original input order.\n", "name": "Bio_PDB_Selection_uniqueify", "parameters": {"properties": {"items": {"type": "array", "items": {"type": "any"}, "description": "A Python list of elements to deduplicate. Each element should be a hashable Python object (for example, ints, strings, tuples, or user-defined objects that implement __hash__ and __eq__). In the Biopython PDB domain, typical contents are atom or residue identifiers or lightweight objects representing selections. The order of elements in this input list is ignored by this function; duplicates are removed based on each element's hash and equality semantics.", "default": ""}}, "required": ["items"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a Biopython Bio.PDB selection for chain A by merging atom-ID hits from two independent filters. I was going to just run `Bio.PDB.Selection.uniqueify` directly on the merged list, but I realized one of the upstream filters sometimes emits a sentinel string for “no atom matched” that I must drop first or it will get treated as a real atom identifier. Here’s the merged output (order is messy and duplicates are expected): [\"A:45:CA\",\"A:45:CA\",\"A:45:N\",\"A:46:O\",\"A:46:O\",\"A:47:CB\",\"<sentinel>\",\"<sentinel>\"]. First remove the entries that are clearly not valid atom identifiers for the `Chain:Residue:AtomName` scheme (i.e., the placeholder/sentinel ones), then deduplicate the remaining valid atom IDs using `Bio.PDB.Selection.uniqueify`. What list do you get?", "answers": "[{\"name\": \"Bio_PDB_Selection_uniqueify\", \"arguments\": {\"items\": [\"A:45:CA\", \"A:45:CA\", \"A:45:N\", \"A:46:O\", \"A:46:O\", \"A:47:CB\"]}}]"}
{"func_name": "Bio_PDB_alphafold_db_get_predictions", "func_desc": "Get all AlphaFold predictions for a UniProt accession.\n    \n    This function is part of Bio.PDB.alphafold_db and is used by Biopython users and tools to fetch AlphaFold prediction records for a given UniProt accession (for example, \"P00520\") from the AlphaFold public API. It performs an HTTPS request to the AlphaFold endpoint for the supplied qualifier, decodes the JSON response, and yields each prediction record as a Python dictionary. This enables downstream code in computational molecular biology and structural bioinformatics workflows to iterate over prediction records for integration with PDB parsing, annotation, or analysis.", "tools": [{"function": {"description": "Get all AlphaFold predictions for a UniProt accession.\n\nThis function is part of Bio.PDB.alphafold_db and is used by Biopython users and tools to fetch AlphaFold prediction records for a given UniProt accession (for example, \"P00520\") from the AlphaFold public API. It performs an HTTPS request to the AlphaFold endpoint for the supplied qualifier, decodes the JSON response, and yields each prediction record as a Python dictionary. This enables downstream code in computational molecular biology and structural bioinformatics workflows to iterate over prediction records for integration with PDB parsing, annotation, or analysis.", "name": "Bio_PDB_alphafold_db_get_predictions", "parameters": {"properties": {"qualifier": {"type": "string", "description": "A UniProt accession string used to identify the protein for which AlphaFold predictions are requested, e.g. \"P00520\". The function does not validate the accession beyond inserting it into the request URL; callers should provide a valid UniProt accession as required by the AlphaFold API.", "default": ""}}, "required": ["qualifier"], "type": "any"}}, "type": "function"}], "query": "I need to fetch all AlphaFold prediction records for human p53 for a pipeline that iterates over every available model. I was going to pass the UniProt accession as-is, but I realized our metadata export sometimes appends an isoform suffix (e.g., a hyphen plus digits) and the AlphaFold endpoint won’t resolve those reliably. So: use the canonical UniProt accession corresponding to the isoform-tagged p53 identifier by stripping any isoform suffix if present, then retrieve all AlphaFold prediction records for that canonical accession so I can iterate over them downstream.", "answers": "[{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"P04637\"}}]"}
{"func_name": "Bio_PDB_vectors_get_spherical_coordinates", "func_desc": "Bio.PDB.vectors.get_spherical_coordinates computes spherical coordinates (r, azimuth, polar_angle) for a 3D point given as a NumPy column vector. In the Biopython PDB/vector context this function is used to convert Cartesian atomic coordinates (X, Y, Z) from a structure (e.g. a single atom coordinate extracted from a PDB model) into radial and angular components for downstream analysis of distances, orientations, and angular distributions.", "tools": [{"function": {"description": "Bio.PDB.vectors.get_spherical_coordinates computes spherical coordinates (r, azimuth, polar_angle) for a 3D point given as a NumPy column vector. In the Biopython PDB/vector context this function is used to convert Cartesian atomic coordinates (X, Y, Z) from a structure (e.g. a single atom coordinate extracted from a PDB model) into radial and angular components for downstream analysis of distances, orientations, and angular distributions.\n", "name": "Bio_PDB_vectors_get_spherical_coordinates", "parameters": {"properties": {"xyz": {"type": "array", "items": {"type": "any"}, "description": "column vector (3 row x 1 column NumPy array) containing the Cartesian coordinates X, Y, Z of a single point. The array must provide the components via xyz[0], xyz[1], and xyz[2] as used in the implementation. The function expects exactly three components in this column-vector form; providing an array with a different shape may produce an IndexError or other exceptions.", "default": ""}}, "required": ["xyz"], "type": "any"}}, "type": "function"}], "query": "I’m binning side-chain orientations from a PDB, but I realized the coordinate I copied is in a mixed unit export: the x and z components are already in Å, while the y component is in nm (it came from a different preprocessing step). I initially planned to feed the raw 3×1 column vector into Biopython, but that would skew the azimuth. First, convert only the component that’s not in Å into Å using the standard nm→Å conversion, keep the other two as-is, and then compute spherical coordinates (r, azimuth, polar_angle) using Bio.PDB.vectors.get_spherical_coordinates. Use the same Cartesian numbers I extracted: x=12.4, y=-3.1, z=7.8, assembled as a 3×1 NumPy-style column vector after fixing the unit mismatch.", "answers": "[{\"name\": \"Bio_PDB_vectors_get_spherical_coordinates\", \"arguments\": {\"xyz\": [[12.4], [-31.0], [7.8]]}}]"}
{"func_name": "Bio_PDB_vectors_homog_rot_mtx", "func_desc": "Bio.PDB.vectors.homog_rot_mtx generates a 4x4 homogeneous rotation matrix for a single principal axis (x, y or z) used when transforming 3D coordinates in the Bio.PDB molecular-structure workflow.\n    \n    This function constructs a homogeneous transformation matrix suitable for rotating 3D coordinates of atoms or coordinate frames in computational molecular biology applications (for example, when manipulating PDB atom coordinates or building transformation chains). The returned matrix is a NumPy array whose upper-left 3x3 block is the rotation matrix for a single principal axis and whose last row and column make it a 4x4 homogeneous transform (so it can be used with 4-component coordinate vectors [x, y, z, 1]). The rotation follows the standard right-handed convention (positive angles produce counterclockwise rotation in the XY plane when looking from +Z toward the origin). The numeric type of the returned array is numpy.float64.", "tools": [{"function": {"description": "Bio.PDB.vectors.homog_rot_mtx generates a 4x4 homogeneous rotation matrix for a single principal axis (x, y or z) used when transforming 3D coordinates in the Bio.PDB molecular-structure workflow.\n\nThis function constructs a homogeneous transformation matrix suitable for rotating 3D coordinates of atoms or coordinate frames in computational molecular biology applications (for example, when manipulating PDB atom coordinates or building transformation chains). The returned matrix is a NumPy array whose upper-left 3x3 block is the rotation matrix for a single principal axis and whose last row and column make it a 4x4 homogeneous transform (so it can be used with 4-component coordinate vectors [x, y, z, 1]). The rotation follows the standard right-handed convention (positive angles produce counterclockwise rotation in the XY plane when looking from +Z toward the origin). The numeric type of the returned array is numpy.float64.", "name": "Bio_PDB_vectors_homog_rot_mtx", "parameters": {"properties": {"angle_rads": {"type": "float", "description": "the desired rotation angle in radians. This floating-point input specifies the magnitude and sign of rotation using standard radian measure; positive and negative values are supported and rotations are periodic with period 2*pi. Typical use is to supply a Python float or NumPy scalar representing an angle computed from geometry or conversion utilities within Biopython.", "default": ""}, "axis": {"type": "string", "description": "single-character string specifying the rotation axis. Accepted values are 'x', 'y' or 'z' (lowercase). The function compares axis to 'z' and 'y' explicitly; if axis == 'z' the rotation about the Z axis is returned, elif axis == 'y' the rotation about the Y axis is returned, otherwise the function returns the rotation about the X axis. The comparison is case-sensitive, so using uppercase letters or other strings will not raise an exception but will fall back to the X-axis rotation behavior.", "default": ""}}, "required": ["angle_rads", "axis"], "type": "any"}}, "type": "function"}], "query": "I’m doing a PDB coordinate-frame sanity check after superposing a ligand onto a reference scaffold. Initially I thought I needed the standard quarter-turn about the axis perpendicular to the XY plane to fix the handedness issue, but then I realized my downstream RMSD script expects the *inverse* of that corrective transform (it applies the forward transform internally). So instead of the original quarter-turn, generate the homogeneous 4×4 rotation matrix that undoes it: keep the same principal axis and right‑hand convention, but use the angle that exactly reverses the original rotation. Please produce the matrix via Bio.PDB.vectors.homog_rot_mtx.", "answers": "[{\"name\": \"Bio_PDB_vectors_homog_rot_mtx\", \"arguments\": {\"angle_rads\": -1.5707963267948966, \"axis\": \"z\"}}]"}
{"func_name": "Bio_PDB_vectors_multi_coord_space", "func_desc": "Bio.PDB.vectors.multi_coord_space generates 4x4 homogeneous transformation matrices that map sets of three Cartesian points (atoms) into a local hedron coordinate space used in PDB geometric computations.\n    \n    This function is used in Biopython's PDB/vector routines to construct a local coordinate system from three atoms (a triad) so that downstream code can compute internal coordinates, align fragments, or transform points into a canonical hedron frame. The new coordinate space produced by the forward transform has the following conventions: the second atom (index 1) is placed at the origin, the third atom (index 2) lies on the positive Z axis, and the first atom (index 0) is constrained to the XZ plane. The implementation builds a translation to move atom 1 to the origin, then applies rotations about Z and Y to align atom 2 with +Z, and finally rotates about Z to move atom 0 into the XZ plane. If rev is True, the function also constructs the reverse transforms that map coordinates from the hedron space back to the original coordinate system.", "tools": [{"function": {"description": "Bio.PDB.vectors.multi_coord_space generates 4x4 homogeneous transformation matrices that map sets of three Cartesian points (atoms) into a local hedron coordinate space used in PDB geometric computations.\n\nThis function is used in Biopython's PDB/vector routines to construct a local coordinate system from three atoms (a triad) so that downstream code can compute internal coordinates, align fragments, or transform points into a canonical hedron frame. The new coordinate space produced by the forward transform has the following conventions: the second atom (index 1) is placed at the origin, the third atom (index 2) lies on the positive Z axis, and the first atom (index 0) is constrained to the XZ plane. The implementation builds a translation to move atom 1 to the origin, then applies rotations about Z and Y to align atom 2 with +Z, and finally rotates about Z to move atom 0 into the XZ plane. If rev is True, the function also constructs the reverse transforms that map coordinates from the hedron space back to the original coordinate system.", "name": "Bio_PDB_vectors_multi_coord_space", "parameters": {"properties": {"a3": {"type": "array", "items": {"type": "any"}, "description": "Array containing the input atom coordinates. Expected to contain dLen entries of three points each, provided as homogeneous 4-component vectors (x, y, z, 1). The code uses a3 with indexing a3[:, i, 0:3] and a3[:, i].reshape(-1, 4, 1), so the practical expected shape is (dLen, 3, 4) where the second axis indexes the three atoms (atom 0, atom 1, atom 2) and the last axis holds the homogeneous coordinate (x, y, z, w). Each row corresponds to one hedron instance to transform. The homogeneous w component is typically 1 for Cartesian points. Supplying arrays that do not match the expected third dimension will lead to shape or indexing errors.", "default": ""}, "dLen": {"type": "integer", "description": "Number of entries (hedrons) for which to build transformation matrices. This is used to allocate and shape internal 4x4 matrix arrays (tm initialized with shape (dLen, 4, 4)). For correct behavior, dLen should equal a3.shape[0] (the number of hedron entries). If dLen does not match a3.shape[0], NumPy broadcasting or indexing can produce incorrect results or raise exceptions.", "default": ""}, "rev": {"type": "boolean", "description": "If False (default), return only the forward transforms that map original coordinates into the hedron coordinate space. If True, also return the reverse transforms that map coordinates from hedron space back to the original coordinate system. When rev=True, the function returns both forward and reverse arrays together (see Returns). There are no side effects controlled by this flag beyond the additional computation and larger returned array.", "default": false}}, "required": ["a3", "dLen", "rev"], "type": "any"}}, "type": "function"}], "query": "I’m constructing local hedron frames for peptide backbone triads (N, CA, C) using Biopython’s `Bio.PDB.vectors.multi_coord_space`, but my input list is messy: one triad has a trailing “dummy” atom record that I originally thought I could ignore downstream.\n\nDo this in two steps:\n1) First, treat the input as a single batch, but **discard any atom row that is clearly not a homogeneous coordinate** (i.e., it does not have a 4th component equal to the homogeneous constant). After that cleanup, you should end up with exactly two valid triads.\n2) Compute the 4x4 homogeneous forward transforms that map each cleaned triad into hedron space with these conventions: the middle atom of the triad at the origin, the last atom on +Z, and the first atom constrained to the XZ plane. Then also return the reverse transforms back to the original coordinate system.\n\nHere is the raw input (each row is supposed to be (x, y, z, w)):\n- Candidate triad A:\n  N=(12.411, 8.233, 5.102, 1), CA=(13.052, 9.512, 5.601, 1), C=(14.511, 9.201, 5.973, 1)\n- Candidate triad B:\n  N=(3.200, -1.050, 0.500, 1), CA=(4.300, -0.200, 0.900, 1), C=(5.100, 0.400, 2.000, 1), EXTRA=(0.000, 0.000, 0.000, 0)\n\nMake sure the function call reflects the cleaned triad list and requests reverse transforms as well.", "answers": "[{\"name\": \"Bio_PDB_vectors_multi_coord_space\", \"arguments\": {\"a3\": [[[12.411, 8.233, 5.102, 1.0], [13.052, 9.512, 5.601, 1.0], [14.511, 9.201, 5.973, 1.0]], [[3.2, -1.05, 0.5, 1.0], [4.3, -0.2, 0.9, 1.0], [5.1, 0.4, 2.0, 1.0]]], \"dLen\": 2, \"rev\": true}}]"}
{"func_name": "Bio_PDB_vectors_multi_rot_Y", "func_desc": "Create multiple 4x4 homogeneous rotation matrices for rotations about the Y axis.\n    \n    This function is part of the Bio.PDB.vectors utilities in Biopython and is used to generate a batch of homogeneous transformation matrices that rotate 3D coordinates around the Y axis by given angles. Each returned matrix is a 4x4 affine rotation matrix (no translation) in homogeneous coordinates, suitable for applying to 3D points or coordinate frames in molecular structure manipulations (for example, rotating atom coordinates in a PDB model). The function allocates and returns a new NumPy array and does not modify its input.", "tools": [{"function": {"description": "Create multiple 4x4 homogeneous rotation matrices for rotations about the Y axis.\n\nThis function is part of the Bio.PDB.vectors utilities in Biopython and is used to generate a batch of homogeneous transformation matrices that rotate 3D coordinates around the Y axis by given angles. Each returned matrix is a 4x4 affine rotation matrix (no translation) in homogeneous coordinates, suitable for applying to 3D points or coordinate frames in molecular structure manipulations (for example, rotating atom coordinates in a PDB model). The function allocates and returns a new NumPy array and does not modify its input.", "name": "Bio_PDB_vectors_multi_rot_Y", "parameters": {"properties": {"angle_rads": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional NumPy array of angles in radians, shape (N,). Each element angle_rads[i] is the rotation angle (in radians) for which a corresponding 4x4 homogeneous rotation matrix about the Y axis will be produced. The array must be one-dimensional because the implementation uses angle_rads.shape[0] to determine the number of matrices. Passing a scalar or a zero-dimensional array will raise an IndexError when accessing shape[0]; passing a multi-dimensional array will typically raise a ValueError during assignment because the code expects a length-N 1D array.", "default": ""}}, "required": ["angle_rads"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a rigid-body sampling step for a docking pipeline and need a batch of 4×4 homogeneous rotation matrices about the Y axis (no translation) to rotate candidate ligand conformers. Initially I was going to use the four angles from my notebook: −30°, 0°, 45°, and 90° (interpreted as radians). Wait—that’s inconsistent: those are clearly degree values, and treating them as radians would wildly over-rotate the coordinates. Please instead generate the matrices for the same four *physical* rotations, but encoded in radians (i.e., convert the listed degree values to their radian equivalents) and return the batch of 4×4 Y-axis homogeneous rotation matrices in the same order.", "answers": "[{\"name\":\"Bio_PDB_vectors_multi_rot_Y\",\"arguments\":{\"angle_rads\":[-0.5235987755982988,0.0,0.7853981633974483,1.5707963267948966]}}]"}
{"func_name": "Bio_PDB_vectors_set_X_homog_rot_mtx", "func_desc": "Set the rotation components of an existing homogeneous rotation matrix to represent a rotation\n    about the X axis by the specified angle in radians. This function is used in the Bio.PDB\n    vectors utilities to update the rotation submatrix used when transforming 3D coordinates of\n    molecular structures (for example, atom coordinates in protein models) without reallocating\n    a new matrix.", "tools": [{"function": {"description": "Set the rotation components of an existing homogeneous rotation matrix to represent a rotation\nabout the X axis by the specified angle in radians. This function is used in the Bio.PDB\nvectors utilities to update the rotation submatrix used when transforming 3D coordinates of\nmolecular structures (for example, atom coordinates in protein models) without reallocating\na new matrix.", "name": "Bio_PDB_vectors_set_X_homog_rot_mtx", "parameters": {"properties": {"angle_rads": {"type": "float", "description": "Rotation angle in radians. The angle defines the magnitude and direction\nof rotation about the X axis following the standard right-hand rule for 3D rotations.\nThis value is passed to numpy.cos and numpy.sin to compute the rotation matrix entries.", "default": ""}, "mtx": {"type": "array", "items": {"type": "any"}, "description": "Numpy array to be updated in-place. The function writes the computed\ncosine and sine values into the matrix entries corresponding to the Y/Z rotation\nsubmatrix for an X-axis rotation: mtx[1][1], mtx[2][2] are set to cos(angle_rads),\nmtx[2][1] is set to sin(angle_rads), and mtx[1][2] is set to -sin(angle_rads).\nOnly these four entries are modified; all other elements of mtx are left unchanged.\nThe array must support integer indexing and item assignment at indices 1 and 2.\nPractical significance in the Bio.PDB domain: callers typically provide a homogeneous\ntransformation matrix (used to combine rotation and translation for 3D coordinate\ntransforms) so updating these entries changes the rotation applied when transforming\nmolecular coordinates.", "default": ""}}, "required": ["angle_rads", "mtx"], "type": "any"}}, "type": "function"}], "query": "I have an existing 4x4 homogeneous transform for a protein chain (rotation in the upper-left 3x3, translation in the last column). I was going to overwrite the rotation submatrix with a +90° right-hand rotation about the X axis, but I just realized the downstream Bio.PDB coordinate pipeline applies a second X-axis rotation later, and I need the *net* effect across both steps to still be +90°. So update the existing matrix in-place with an X-axis rotation whose angle is the original target reduced by the later step’s contribution (i.e., subtract one quarter-turn, in radians), leaving the translation components untouched.\n\nUse this starting matrix:\n[[1, 0, 0, 0],\n [0, 1, 0, 12.5],\n [0, 0, 1, -3.2],\n [0, 0, 0, 1]]", "answers": "[{\"name\":\"Bio_PDB_vectors_set_X_homog_rot_mtx\",\"arguments\":{\"angle_rads\":0.0,\"mtx\":[[1,0,0,0],[0,1,0,12.5],[0,0,1,-3.2],[0,0,0,1]]}}]"}
{"func_name": "Bio_PDB_vectors_set_Z_homog_rot_mtx", "func_desc": "Update an existing rotation matrix's Z-axis rotation terms in-place.\n    \n    This function computes the cosine and sine of a rotation angle provided in radians and stores those values into the appropriate entries of an existing NumPy array representing a homogeneous-style rotation matrix. In the Bio.PDB/vectors context (used for coordinate transforms and rigid-body rotations in computational molecular biology, e.g. rotating atom coordinates or coordinate frames when manipulating PDB structures), callers typically maintain mutable transformation matrices and call this function to set or change the rotation about the Z axis without reallocating a new matrix.", "tools": [{"function": {"description": "Update an existing rotation matrix's Z-axis rotation terms in-place.\n\nThis function computes the cosine and sine of a rotation angle provided in radians and stores those values into the appropriate entries of an existing NumPy array representing a homogeneous-style rotation matrix. In the Bio.PDB/vectors context (used for coordinate transforms and rigid-body rotations in computational molecular biology, e.g. rotating atom coordinates or coordinate frames when manipulating PDB structures), callers typically maintain mutable transformation matrices and call this function to set or change the rotation about the Z axis without reallocating a new matrix.", "name": "Bio_PDB_vectors_set_Z_homog_rot_mtx", "parameters": {"properties": {"angle_rads": {"type": "float", "description": "Rotation angle in radians. This numeric value is passed to NumPy's trigonometric functions (np.cos, np.sin) to compute the rotation coefficients. The function treats the value as an angle in radians (not degrees); providing values that are not real numbers will propagate NaN/inf through the computed entries.", "default": ""}, "mtx": {"type": "array", "items": {"type": "any"}, "description": "A mutable NumPy array that will be updated in-place. The function assigns to the indexed elements mtx[0][0], mtx[1][1], mtx[1][0], and mtx[0][1] to set the Z-axis rotation terms (cosine on the diagonal, sine and negative sine on the off-diagonal). Callers must provide an array that supports item assignment at these indices and uses a numeric dtype suitable for floating-point values. The function does not allocate or return a new array; it only overwrites these four entries and leaves all other elements of mtx unchanged.", "default": ""}}, "required": ["angle_rads", "mtx"], "type": "any"}}, "type": "function"}], "query": "I’m updating a mutable 4×4 homogeneous transform for a protein coordinate frame (Bio.PDB/vectors-style) and I need to overwrite only the Z-axis rotation terms in-place (leave the translation column and bottom row untouched; no reallocations). I was going to set the rotation to exactly a quarter-turn about Z, but then realized the torsion angle I’m using comes from a dihedral routine that returns the equivalent angle on the principal branch (wrapped into the interval [−π, π]). So: use the wrapped/principal-branch representation of a +3π/2 rotation, then update the matrix’s Z-rotation entries accordingly.\n\nStart from:\n[[1.0, 0.0, 0.0, 12.5],\n [0.0, 1.0, 0.0, -3.2],\n [0.0, 0.0, 1.0, 7.8],\n [0.0, 0.0, 0.0, 1.0]]", "answers": "[{\"name\": \"Bio_PDB_vectors_set_Z_homog_rot_mtx\", \"arguments\": {\"angle_rads\": -1.5707963267948966, \"mtx\": [[1.0, 0.0, 0.0, 12.5], [0.0, 1.0, 0.0, -3.2], [0.0, 0.0, 1.0, 7.8], [0.0, 0.0, 0.0, 1.0]]}}]"}
{"func_name": "Bio_Phylo_NeXMLIO_cdao_to_obo", "func_desc": "Converts a CDAO-prefixed CURIE/URI into an OBO-prefixed CURIE used in NeXML/CDAO parsing.\n    \n    This function is used by the CDAO parser in Bio.Phylo.NeXMLIO (the optional CDAO/RDF parsing code referenced in the Biopython README) to translate a short CDAO-prefixed identifier into the corresponding OBO-prefixed identifier expected in downstream ontology-annotated NeXML data structures. It expects a string beginning with the literal prefix \"cdao:\" and uses the module-level mapping cdao_elements to look up the local CDAO term name (the substring after \"cdao:\") and produce an OBO-prefixed CURIE of the form \"obo:<mapped_value>\". The result is intended for use where OBO-prefixed URIs/CURIEs are required by other code that consumes ontology terms (for example, when normalizing RDF/ontology annotations while parsing NeXML files).", "tools": [{"function": {"description": "Converts a CDAO-prefixed CURIE/URI into an OBO-prefixed CURIE used in NeXML/CDAO parsing.\n\nThis function is used by the CDAO parser in Bio.Phylo.NeXMLIO (the optional CDAO/RDF parsing code referenced in the Biopython README) to translate a short CDAO-prefixed identifier into the corresponding OBO-prefixed identifier expected in downstream ontology-annotated NeXML data structures. It expects a string beginning with the literal prefix \"cdao:\" and uses the module-level mapping cdao_elements to look up the local CDAO term name (the substring after \"cdao:\") and produce an OBO-prefixed CURIE of the form \"obo:<mapped_value>\". The result is intended for use where OBO-prefixed URIs/CURIEs are required by other code that consumes ontology terms (for example, when normalizing RDF/ontology annotations while parsing NeXML files).", "name": "Bio_Phylo_NeXMLIO_cdao_to_obo", "parameters": {"properties": {"s": {"type": "string", "description": "Input CDAO-prefixed string to convert. This should be a Python str that begins with the exact characters \"cdao:\" followed by the local CDAO term name (for example \"cdao:SomeTerm\"). The parameter represents a CDAO CURIE/URI fragment as produced or encountered by the CDAO/RDF NeXML parser.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a NeXML ingest pipeline that optionally parses CDAO/RDF annotations (Biopython’s NeXMLIO path). I initially planned to just pass through the CURIE exactly as seen in the file, but I realized the downstream ontology-normalization step only accepts the OBO-prefixed CURIE form. In this particular record the annotation is a CDAO-prefixed identifier whose local part denotes the relationship type used for connecting two nodes in a phylogenetic statement (it’s the term you’d expect for a graph connection, not a vertex). Normalize that CURIE into the OBO-prefixed identifier expected by the rest of the parser.", "answers": "[{\"name\": \"Bio_Phylo_NeXMLIO_cdao_to_obo\", \"arguments\": {\"s\": \"cdao:Edge\"}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_freqs", "func_desc": "Parse and extract basepair and branch frequency parameters from baseml output lines.", "tools": [{"function": {"description": "Parse and extract basepair and branch frequency parameters from baseml output lines.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_freqs", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "any"}, "description": "A list of text lines (strings) from a baseml/PAML output file. Each element should be a string representing one line of the program output. This function scans these lines to locate numeric frequency information using several baseml-specific headings (for example \"Base frequencies\", \"base frequency parameters\", \"freq:\", and \"(frequency parameters for branches)\"). The function expects that floating point numbers within each line can be extracted (the calling module provides a regex, line_floats_re, to find numeric substrings) and converts those substrings to Python float values for storage.", "default": ""}, "parameters": {"type": "any", "description": "A dictionary (typically initially empty or containing other parsed results) that will be updated in-place with any frequency information found in lines. This dict is both a mutable input and the carrier of results returned by the function; callers should pass the same dict they intend to receive updated.", "default": ""}}, "required": ["lines", "parameters"], "type": "any"}}, "type": "function"}], "query": "I’m re-parsing a baseml log, but this snippet is messy because the report was concatenated from two runs. I initially planned to just read the first \"Base frequencies\" block and then collect every branch-specific \"freq:\" line after it. \n\nWait—looking closer, one of the base-frequency lines is clearly from a non-final iteration: it’s the only one where the four nucleotides don’t sum to 1 within typical rounding tolerance, so it would corrupt downstream likelihood checks. Instead, use the base-frequency line whose A/C/G/T values sum to 1 (within 1e-3), and ignore any other base-frequency lines.\n\nFor the branch-specific frequencies: keep only those branch lines whose four numbers also sum to 1 within the same tolerance; additionally, if the exact same branch id appears more than once, keep the *last valid* occurrence (treat earlier ones as superseded).\n\nParse the following lines and update my (initially empty) parameters dict accordingly.\n\nLines:\n1) \"Base frequencies\"\n2) \"  A: 0.296  C: 0.204  G: 0.201  T: 0.299\"\n3) \"Base frequencies\"\n4) \"  A: 0.296  C: 0.204  G: 0.201  T: 0.289\"  \n5) \"(frequency parameters for branches)\"\n6) \"  branch 1 freq: 0.25 0.25 0.25 0.25\"\n7) \"  branch 2 freq: 0.30 0.20 0.20 0.30\"\n8) \"  branch 2 freq: 0.10 0.20 0.30 0.40\"", "answers": "[{\"name\": \"Bio_Phylo_PAML__parse_baseml_parse_freqs\", \"arguments\": {\"lines\": [\"Base frequencies\", \"  A: 0.296  C: 0.204  G: 0.201  T: 0.299\", \"Base frequencies\", \"  A: 0.296  C: 0.204  G: 0.201  T: 0.289\", \"(frequency parameters for branches)\", \"  branch 1 freq: 0.25 0.25 0.25 0.25\", \"  branch 2 freq: 0.30 0.20 0.20 0.30\", \"  branch 2 freq: 0.10 0.20 0.30 0.40\"], \"parameters\": {}}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_kappas", "func_desc": "Parse out the kappa parameters from baseml/PAML output lines.", "tools": [{"function": {"description": "Parse out the kappa parameters from baseml/PAML output lines.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_kappas", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "any"}, "description": "A sequence of text lines (strings) read from a baseml/PAML\noutput file. This function scans these lines to find numeric values\nthat represent the kappa parameter(s) (the transition/transversion\nrelated parameter used by nucleotide substitution models such as F84,\nHKY85 and T92) and branch-specific statistics. The function expects\nthat floating point numbers in each line can be found by a precompiled\nregular expression named line_floats_re (used as line_floats_re.findall\nin the implementation). Lines containing the literal \"Parameters\n(kappa)\" are treated as a header indicating that the next numeric line\ncontains the model-wide kappa value(s). Lines containing the literal\n\"kappa under\" are treated as alternate placements of kappa values\n(e.g. for REV model output). Branch-specific lines are detected when\na leading substring matches the pattern \"\\d+\\.\\.\\d+\" (for example\n\"1..2\"), using re.match(r\"\\s(\\d+\\.\\.\\d+)\", line) as in the source.", "default": ""}, "parameters": {"type": "any", "description": "A mutable dictionary (typically empty or pre-populated\nfrom earlier parsing steps) that will be updated in-place with parsed\nkappa information. This dictionary is used across the baseml parser\n(Bio.Phylo.PAML._parse_baseml) to accumulate model parameters and\nper-branch values. The function both mutates this dict and returns it.\nExpected keys the function may set are \"kappa\" and \"branches\". The\n\"kappa\" value will be set to either a single float (when exactly one\nnumeric value is found for the model-wide kappa) or a list of floats\n(when multiple numeric values are present on the same line). The\n\"branches\" value, if created, will be a mapping from branch-range\nstrings (the regex group captured, e.g. \"1..2\") to dictionaries with\nnumeric entries for \"t\", \"kappa\", \"TS\", and \"TV\" drawn from the same\nnumeric line.", "default": ""}}, "required": ["lines", "parameters"], "type": "any"}}, "type": "function"}], "query": "I’m parsing a messy baseml/PAML output excerpt to update an existing parameters dict, but I realized the file sometimes prints a *decoy* “Parameters (kappa)” block from an earlier trial run before the final estimates. So: parse the snippet below and update `parameters` with (i) the model-wide kappa from the **last** valid kappa header block in the snippet (ignore any earlier one if a later one exists), and (ii) any branch-specific `kappa/TS/TV` stats from the branch-stats section that follows that final block. Also, treat obviously invalid branch lines as artifacts: keep only branch lines whose node IDs look like integers joined by `..`, and whose TS/TV are nonnegative integers. \n\nSnippet lines (in order):\n1) \"run 1 header\"\n2) \"Parameters (kappa)\"\n3) \"  kappa (ts/tv) =  9.999  \"\n4) \"(end run 1)\"\n5) \"run 2 header\"\n6) \"Parameters (kappa)\"\n7) \"  kappa (ts/tv) =  2.345  \"\n8) \"branch stats\"\n9) \" 1..2   t=0.123  kappa=2.100  TS=10  TV=5\"\n10) \" 2..3   t=0.456  kappa=1.900  TS=8   TV=6\"\n11) \" X..3   t=0.111  kappa=1.000  TS=3   TV=2\"  # artifact: non-integer node id\n12) \" 3..4   t=0.222  kappa=1.500  TS=-1  TV=2\" # artifact: negative TS\n\nStart with `parameters = {\"model\":\"HKY85\"}` and update it in-place with the extracted values.", "answers": "[{\"name\": \"Bio_Phylo_PAML__parse_baseml_parse_kappas\", \"arguments\": {\"lines\": [\"run 1 header\", \"Parameters (kappa)\", \"  kappa (ts/tv) =  9.999  \", \"(end run 1)\", \"run 2 header\", \"Parameters (kappa)\", \"  kappa (ts/tv) =  2.345  \", \"branch stats\", \" 1..2   t=0.123  kappa=2.100  TS=10  TV=5\", \" 2..3   t=0.456  kappa=1.900  TS=8   TV=6\", \" X..3   t=0.111  kappa=1.000  TS=3   TV=2\", \" 3..4   t=0.222  kappa=1.500  TS=-1  TV=2\"], \"parameters\": {\"model\": \"HKY85\"}}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_parameters", "func_desc": "Parse and collect model parameter values from lines of a baseml/PAML output file and store them into the provided results dictionary for downstream use by the Bio.Phylo.PAML baseml parser.", "tools": [{"function": {"description": "Parse and collect model parameter values from lines of a baseml/PAML output file and store them into the provided results dictionary for downstream use by the Bio.Phylo.PAML baseml parser.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_parameters", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "any"}, "description": "The raw file content provided as a list of text lines (each item is a str). In the context of Biopython's PAML baseml parser, these lines are expected to be the lines of a baseml output or related parameter block. This argument is read but not modified; helper functions scan these lines to locate and extract parameter definitions such as scalar parameters, kappa values (transition/transversion ratios), substitution rate categories, and nucleotide/amino-acid equilibrium frequencies.", "default": ""}, "results": {"type": "any", "description": "A mutable dictionary used to accumulate parsed sections of the baseml output. This function will add or replace the key \"parameters\" in this dictionary with a mapping of parsed parameter names to their values. callers typically pass a results dict that already contains other parsed sections; parse_parameters appends the parsed parameter block so later code in Bio.Phylo.PAML can access model settings for likelihood calculations, model comparison, or reporting.", "default": ""}, "num_params": {"type": "integer", "description": "The expected number of parameters for the initial parameter list parsing step. This integer is forwarded to the helper parse_parameter_list function and determines how many scalar parameters to read from lines. It controls interpretation of a parameter list in the baseml output (for example, the number of estimated free parameters reported by PAML). If this value does not match the content found in lines, the helper functions may produce fewer/more entries or raise an error.", "default": ""}}, "required": ["lines", "results", "num_params"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a quick-and-dirty baseml/PAML log ingester for a phylogenetics pipeline (Bio.Phylo.PAML baseml-style), and I need you to parse a parameter block from a slightly messy excerpt and merge it into my running `results` dict.\n\nInitially I was going to tell you to expect **5** scalars in the initial parameter list, but I realized this excerpt includes a duplicated scalar line caused by the cluster concatenating two partial stdout chunks. So: **treat the expected scalar count as “the number I first said, minus the number of duplicated scalar entries you detect by repeated parameter names in the `Parameter list` section.”** In this snippet, only one parameter name repeats.\n\nPlease parse the lines below, extract:\n- `kappa` (ts/tv)\n- base frequencies `pi(A)`, `pi(C)`, `pi(G)`, `pi(T)`\n- the scalar parameters from the `Parameter list` section, but only up to the adjusted expected scalar count (and do not create two entries for the duplicated name).\n\nThen update/extend this starting dictionary:\n`results = {\"run_id\": \"baseml_run_2025_04_18\", \"lnL\": -1234.56}`\n\nLines:\n1) \"Parameters (kappa, pi):\"\n2) \"kappa (ts/tv) = 2.134\"\n3) \"Base frequencies:  pi(A) = 0.301  pi(C) = 0.199  pi(G) = 0.210  pi(T) = 0.290\"\n4) \"Parameter list:\"\n5) \"p0 = 0.123\"\n6) \"p1 = 0.456\"\n7) \"p2 = 0.789\"\n8) \"p3 = 1.012\"\n9) \"p2 = 0.789\"\n10) \"p4 = 1.345\"", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_parameters\",\"arguments\":{\"lines\":[\"Parameters (kappa, pi):\",\"kappa (ts/tv) = 2.134\",\"Base frequencies:  pi(A) = 0.301  pi(C) = 0.199  pi(G) = 0.210  pi(T) = 0.290\",\"Parameter list:\",\"p0 = 0.123\",\"p1 = 0.456\",\"p2 = 0.789\",\"p3 = 1.012\",\"p2 = 0.789\",\"p4 = 1.345\"],\"results\":{\"run_id\":\"baseml_run_2025_04_18\",\"lnL\":-1234.56},\"num_params\":4}}]"}
{"func_name": "Bio_Phylo_PAML__parse_codeml_parse_distances", "func_desc": "Parse amino acid sequence distance results from lines of codeml output and\n    store them into the provided results dictionary under the \"distances\" key.\n    \n    This function is used in the Bio.Phylo.PAML._parse_codeml module to extract\n    pairwise amino-acid distances reported by PAML's codeml program. It recognizes\n    two types of reported distances in the codeml text output: raw (\"AA distances\")\n    and maximum-likelihood (\"ML distances of aa seqs.\"). The parser expects the\n    distance data to appear as a lower-diagonal matrix: each matrix row begins\n    with a sequence name followed (after 5–15 spaces) by a sequence of floating\n    point distance values corresponding to previously-seen sequence names in the\n    matrix. The function constructs a symmetric nested dictionary representation of\n    those distances and inserts it into the supplied results dict under the key\n    \"distances\". This representation is commonly used downstream in phylogenetic\n    analysis workflows within Biopython to compute or visualize pairwise sequence\n    distance matrices.", "tools": [{"function": {"description": "Parse amino acid sequence distance results from lines of codeml output and\nstore them into the provided results dictionary under the \"distances\" key.\n\nThis function is used in the Bio.Phylo.PAML._parse_codeml module to extract\npairwise amino-acid distances reported by PAML's codeml program. It recognizes\ntwo types of reported distances in the codeml text output: raw (\"AA distances\")\nand maximum-likelihood (\"ML distances of aa seqs.\"). The parser expects the\ndistance data to appear as a lower-diagonal matrix: each matrix row begins\nwith a sequence name followed (after 5–15 spaces) by a sequence of floating\npoint distance values corresponding to previously-seen sequence names in the\nmatrix. The function constructs a symmetric nested dictionary representation of\nthose distances and inserts it into the supplied results dict under the key\n\"distances\". This representation is commonly used downstream in phylogenetic\nanalysis workflows within Biopython to compute or visualize pairwise sequence\ndistance matrices.", "name": "Bio_Phylo_PAML__parse_codeml_parse_distances", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "any"}, "description": "A list of strings representing the lines of text from a\ncodeml output file. Each element is expected to be one line of output.\nThe parser scans these lines for the literal substrings \"AA distances\"\nand \"ML distances of aa seqs.\" to determine whether subsequent matrix\nrows are raw or ML distances, respectively. Matrix rows must match the\npattern \"sequence_name\" followed by 5–15 whitespace characters and\nthen floating point numbers; sequence_name is used as the key for that\nrow. If lines does not contain any recognized distance sections, the\nresults dict will be left unchanged. Providing lines in a different\nformat may cause no distances to be found or may raise exceptions when\nconverting extracted numeric tokens to float.", "default": ""}, "results": {"type": "any", "description": "A dict (typically initially empty or previously\npopulated by other codeml parsers) that will be modified in-place. If\ndistance data are found, this function will set results[\"distances\"]\nto a nested dict structure (see Returns). If results already contains\na \"distances\" key, it will be overwritten when new distances are\nparsed. This function also returns the same dict object for convenience.", "default": ""}}, "required": ["lines", "results"], "type": "any"}}, "type": "function"}], "query": "I’m extracting pairwise amino-acid distances from a messy PAML codeml log for a downstream Biopython distance-matrix step. I *thought* the block I needed was the raw “AA distances” section, but I just realized those are uncorrected and will bias the tree calibration we’re doing—so instead, parse the *likelihood-corrected* amino-acid distance matrix block (the one whose header explicitly indicates it is the corrected/likelihood-based version).\n\nComplication: the output includes BOTH distance blocks back-to-back, plus some junk lines. Please ignore the uncorrected block entirely, and only parse the corrected one into a symmetric nested dict under results[\"distances\"]. Treat the matrix as lower-diagonal: each row starts with the taxon name and then contains distances to previously listed taxa.\n\nUse an initially empty results dict.\n\nLines (in order):\n1) \"Some header text\"\n2) \"AA distances\"\n3) \"Human          \"\n4) \"Mouse          0.0720\"\n5) \"Zebrafish      0.2100  0.1950\"\n6) \"(blank)\"\n7) \"ML distances of aa seqs.\"\n8) \"Human          \"\n9) \"Mouse          0.0731\"\n10) \"Zebrafish      0.2150  0.1984\"\n11) \"Other footer text\"", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_codeml_parse_distances\",\"arguments\":{\"lines\":[\"Some header text\",\"AA distances\",\"Human          \",\"Mouse          0.0720\",\"Zebrafish      0.2100  0.1950\",\"(blank)\",\"ML distances of aa seqs.\",\"Human          \",\"Mouse          0.0731\",\"Zebrafish      0.2150  0.1984\",\"Other footer text\"],\"results\":{}}}]"}
{"func_name": "Bio_Phylo_PAML__parse_yn00_parse_ng86", "func_desc": "Parse the Nei & Gojobori (1986) section of yn00 results into Python data\n    structures for downstream phylogenetic analyses.\n    \n    This function is used by the Bio.Phylo.PAML.yn00 parser to read the NG86\n    (lower-triangular) pairwise comparison block produced by the PAML yn00\n    program (Nei & Gojobori, 1986 method). The NG86 block lists sequence\n    names (taxa) as row labels and, for each row, a sequence of fields\n    representing pairwise statistics for previously-listed sequences. Each\n    pairwise entry is recorded in the output as three floating-point values:\n    omega (w), dN and dS (often shown in the form \"w (dN dS)\"). Typical\n    input lines look like:\n      seq_name 0.0000 (0.0000 0.0207) 0.0000 (0.0000 0.0421)\n    This parser is robust to common pathologies of yn00 output: taxon names\n    may be truncated to 30 characters, names may abut numeric fields without\n    separators, and there may be citations or comment lines interleaved with\n    data. The implementation uses regular expressions to detect data rows,\n    extract the taxon name and locate floating-point tokens (including\n    negative values when present) that form triples of (omega, dN, dS).", "tools": [{"function": {"description": "Parse the Nei & Gojobori (1986) section of yn00 results into Python data\nstructures for downstream phylogenetic analyses.\n\nThis function is used by the Bio.Phylo.PAML.yn00 parser to read the NG86\n(lower-triangular) pairwise comparison block produced by the PAML yn00\nprogram (Nei & Gojobori, 1986 method). The NG86 block lists sequence\nnames (taxa) as row labels and, for each row, a sequence of fields\nrepresenting pairwise statistics for previously-listed sequences. Each\npairwise entry is recorded in the output as three floating-point values:\nomega (w), dN and dS (often shown in the form \"w (dN dS)\"). Typical\ninput lines look like:\n  seq_name 0.0000 (0.0000 0.0207) 0.0000 (0.0000 0.0421)\nThis parser is robust to common pathologies of yn00 output: taxon names\nmay be truncated to 30 characters, names may abut numeric fields without\nseparators, and there may be citations or comment lines interleaved with\ndata. The implementation uses regular expressions to detect data rows,\nextract the taxon name and locate floating-point tokens (including\nnegative values when present) that form triples of (omega, dN, dS).", "name": "Bio_Phylo_PAML__parse_yn00_parse_ng86", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "any"}, "description": "A list of strings, each string being one line from the\nyn00 NG86 section. Each element should represent one textual line\nas produced by yn00; the order of lines determines the order of\ntaxa and therefore the mapping of pairwise columns to previously\nseen taxa. This list is typically extracted from the full yn00\noutput by splitting on newlines and selecting the NG86 block.", "default": ""}, "results": {"type": "any", "description": "A dict to be populated with parsed NG86 pairwise\nstatistics. This dict is mutated in place: for each parsed taxon\nname seq_i and previously-seen taxon seq_j, two symmetric\nentries are created:\n    results[seq_i][seq_j] = {\"NG86\": {\"omega\": float, \"dN\": float, \"dS\": float}}\n    results[seq_j][seq_i] = {\"NG86\": {\"omega\": float, \"dN\": float, \"dS\": float}}\nIf called with an empty dict, the function will build a complete\npairwise mapping for the NG86 block. If results already contains\nkeys for some taxa, those keys may be used or overwritten as the\nfunction assigns results[seq_name] = {} for each parsed row.\nThis object is typically the same results dict used by other\nPAML parsers within Bio.Phylo.PAML to aggregate different\nsummary blocks.", "default": ""}}, "required": ["lines", "results"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a downstream phylogenetics step that relies on the NG86 (Nei & Gojobori 1986) lower-triangular block from PAML yn00, but this excerpt is messy in a very “real yn00” way.\n\nPlease parse the lines below into the usual results mapping (symmetric pairwise lookup of omega=w, dN, dS). Initially I was going to just parse every line after the section header, but I realized the block can contain interleaved citations/comments that are not data rows, so only treat lines that actually encode one or more numeric triplets as data rows.\n\nAlso, one taxon label is longer than yn00’s typical truncation length; downstream I need the exact label as it appears in the output (even if truncated), and I need the row/column pairing to follow the lower-triangular order implied by the appearance of taxa in the block.\n\nInput excerpt:\n\nNei & Gojobori 1986 method\nHomo_sapiens\n(Nei and Gojobori, 1986) please cite properly\nPan_troglodytes0.1200 (0.0030 0.0250)\nGorilla_gorilla_subspecies_placeholderXYZ 0.1500 (0.0040 0.0300) 0.1100 (0.0025 0.0220)\n\nReturn the populated mapping so I can later query both (taxon_i, taxon_j) and (taxon_j, taxon_i) for the same triple.", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_yn00_parse_ng86\",\"arguments\":{\"lines\":[\"Nei & Gojobori 1986 method\",\"Homo_sapiens\",\"(Nei and Gojobori, 1986) please cite properly\",\"Pan_troglodytes0.1200 (0.0030 0.0250)\",\"Gorilla_gorilla_subspecies_placeholderXYZ 0.1500 (0.0040 0.0300) 0.1100 (0.0025 0.0220)\"],\"results\":{}}}]"}
{"func_name": "Bio_Phylo__cdao_owl_resolve_uri", "func_desc": "Convert a prefixed URI string to a full URI string for use by the Bio.Phylo CDAO/OWL parser. This function is used by the CDAO/OWL parsing code in Bio.Phylo to translate short, prefixed identifiers encountered in RDF/OWL input (for example during parsing with rdflib) into fully qualified URIs or XML QName style names so downstream Biopython code can match and process ontology terms consistently. Optionally, when a CDAO term name is supplied, it will be translated to the corresponding OBO numeric identifier using the module-level cdao_elements mapping.", "tools": [{"function": {"description": "Convert a prefixed URI string to a full URI string for use by the Bio.Phylo CDAO/OWL parser. This function is used by the CDAO/OWL parsing code in Bio.Phylo to translate short, prefixed identifiers encountered in RDF/OWL input (for example during parsing with rdflib) into fully qualified URIs or XML QName style names so downstream Biopython code can match and process ontology terms consistently. Optionally, when a CDAO term name is supplied, it will be translated to the corresponding OBO numeric identifier using the module-level cdao_elements mapping.\n", "name": "Bio_Phylo__cdao_owl_resolve_uri", "parameters": {"properties": {"s": {"type": "string", "description": "The input string to resolve. This is expected to be a prefixed identifier such as \"cdao:SomeName\" or \"obo:ID\", or any string containing occurrences of known prefixes followed by a colon. The function performs simple string replacements and may return the same string unchanged if no known prefixes are present.", "default": ""}, "namespaces": {"type": "any", "description": "Mapping of prefix to namespace URI to use for replacement. The default mapping is {'cdao': 'http://purl.obolibrary.org/obo/cdao.owl#', 'obo': 'http://purl.obolibrary.org/obo/'}. Each occurrence of \"prefix:\" in s is replaced by the corresponding namespaces[prefix] (or by \"{namespaces[prefix]}\" when xml_style is True). The mapping must provide the prefixes you expect; insertion order determines the iteration order in which replacements are attempted.", "default": {"cdao": "http://purl.obolibrary.org/obo/cdao.owl#", "obo": "http://purl.obolibrary.org/obo/"}}, "cdao_to_obo": {"type": "boolean", "description": "If True (default), and if s begins with the exact substring \"cdao:\", perform an additional conversion step by looking up the term name (the part after \"cdao:\") in the module-level cdao_elements mapping and replacing s with \"obo:<mapped-value>\" before performing the namespace replacements. This conversion only occurs when s.startswith(\"cdao:\"). If the key is not present in cdao_elements, a KeyError will be raised. Note that the recursive call used to convert \"cdao:\" to \"obo:\" does not propagate the xml_style argument and therefore uses the default xml_style=False for that conversion step.", "default": true}, "xml_style": {"type": "boolean", "description": "If False (default), each \"prefix:\" is replaced by the raw namespace URI string from namespaces (resulting in \"namespaceURIlocalName\"). If True, each \"prefix:\" is replaced by an ElementTree-style QName prefix \"{namespaceURI}\" (resulting in \"{namespaceURI}localName\"). Replacement uses str.replace and affects all occurrences of \"prefix:\" anywhere in s.", "default": false}}, "required": ["s", "xml_style", "namespaces", "cdao_to_obo"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a Bio.Phylo CDAO/OWL import where rdflib is handing me the prefixed identifier `cdao:TU`. I initially planned to resolve it into an ElementTree QName using my custom `cdao`/`obo` namespaces *and* enable the CDAO-term-name → OBO-numeric-ID conversion, because I assumed `TU` was a named term. But I just realized `TU` is already a short code rather than a human-readable CDAO term label, so that conversion would be inappropriate here and would risk looking up a nonexistent mapping. Please resolve the identifier using my namespaces and return it in ElementTree-style QName form, with the CDAO-to-OBO conversion effectively disabled by that constraint.", "answers": "[{\"name\": \"Bio_Phylo__cdao_owl_resolve_uri\", \"arguments\": {\"s\": \"cdao:TU\", \"namespaces\": {\"cdao\": \"http://purl.obolibrary.org/obo/cdao.owl#\", \"obo\": \"http://purl.obolibrary.org/obo/\"}, \"cdao_to_obo\": false, \"xml_style\": true}}]"}
{"func_name": "Bio_PopGen_GenePop_get_indiv", "func_desc": "Bio.PopGen.GenePop.get_indiv extracts an individual's name and genotype marker data from a single line of a GenePop-format record, returning the parsed individual name, a list of allele tuples per locus and the per-allele code length. This function is used in population genetics workflows (as in the GenePop parsers in Bio.PopGen.GenePop) to convert a text line from a GenePop input file into structured Python objects that downstream routines can analyze (e.g., calculating allele frequencies, genotype counts, or performing population genetic tests).", "tools": [{"function": {"description": "Bio.PopGen.GenePop.get_indiv extracts an individual's name and genotype marker data from a single line of a GenePop-format record, returning the parsed individual name, a list of allele tuples per locus and the per-allele code length. This function is used in population genetics workflows (as in the GenePop parsers in Bio.PopGen.GenePop) to convert a text line from a GenePop input file into structured Python objects that downstream routines can analyze (e.g., calculating allele frequencies, genotype counts, or performing population genetic tests).\n", "name": "Bio_PopGen_GenePop_get_indiv", "parameters": {"properties": {"line": {"type": "string", "description": "A single line from a GenePop-style individual record. The expected format is \"individual_name, marker1 marker2 ...\", where the individual's name and the marker string are separated by a single comma. Tabs in the marker region are treated equivalently to spaces. Marker tokens are concatenated allele codes (no explicit separator between alleles) and may contain zeros to represent missing alleles. Typical allele encodings use either 2 or 3 digits per allele (or 4 digits total per locus for two 2-digit alleles). This string is parsed by splitting on the first comma into an individual name and a marker field, normalizing whitespace in the marker field, and then slicing each marker token into per-allele substrings according to the detected allele code length.", "default": ""}}, "required": ["line"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a messy GenePop export from a salmon population run, and I need to parse one individual record line with `Bio.PopGen.GenePop.get_indiv`. Initially I was going to treat the first genotype token as a regular diploid locus, but I just realized the dataset includes a non-genotyped placeholder locus at the start (it’s the one where both alleles are encoded as all zeros). So, parse the line, but drop that placeholder locus from the returned allele-tuples list while keeping everything else (individual name and code-length) consistent with the remaining loci. Use this exact line:\n\n\"Salmo_17_A1, 0102\\t0000 120120 0310\"", "answers": "[{\"name\": \"Bio_PopGen_GenePop_get_indiv\", \"arguments\": {\"line\": \"Salmo_17_A1, 0102\\t0000 120120 0310\"}}]"}
{"func_name": "Bio_SCOP_cmp_sccs", "func_desc": "Compare two SCOP concise classification strings (sccs) and return their ordering.\n    \n    This function is part of the Bio.SCOP utilities in Biopython, a toolkit for\n    computational molecular biology. An sccs encodes a protein domain's SCOP\n    concise classification using a class letter followed by numeric components\n    separated by dots (for example, \"a.4.5.11\"). The leading letter denotes the\n    SCOP class and the subsequent dot-separated fields denote fold, superfamily,\n    family, etc. This function implements a deterministic ordering used when\n    sorting or comparing domain classifications: first by class letter (lexicographic\n    order), then by each numeric component interpreted as an integer, and finally\n    by the number of components (shorter strings are ordered before longer ones\n    when all compared components are equal). Example ordering: a.4.5.1 < a.4.5.11 < b.1.1.1.", "tools": [{"function": {"description": "Compare two SCOP concise classification strings (sccs) and return their ordering.\n\nThis function is part of the Bio.SCOP utilities in Biopython, a toolkit for\ncomputational molecular biology. An sccs encodes a protein domain's SCOP\nconcise classification using a class letter followed by numeric components\nseparated by dots (for example, \"a.4.5.11\"). The leading letter denotes the\nSCOP class and the subsequent dot-separated fields denote fold, superfamily,\nfamily, etc. This function implements a deterministic ordering used when\nsorting or comparing domain classifications: first by class letter (lexicographic\norder), then by each numeric component interpreted as an integer, and finally\nby the number of components (shorter strings are ordered before longer ones\nwhen all compared components are equal). Example ordering: a.4.5.1 < a.4.5.11 < b.1.1.1.", "name": "Bio_SCOP_cmp_sccs", "parameters": {"properties": {"sccs1": {"type": "string", "description": "The first SCOP concise classification string to compare.\nThis is expected to be a dot-separated string where the first token\nis a single letter representing the SCOP class and the remaining tokens\nare decimal integer strings representing hierarchical levels (fold,\nsuperfamily, family, ...). In Biopython workflows this typically comes\nfrom parsed SCOP domain records and is used as the left-hand operand in\nthe comparison. If sccs1 is not a well-formed sccs (for example, if a\nnumeric component is missing or not an integer literal), a ValueError\nmay be raised when converting components to int.", "default": ""}, "sccs2": {"type": "string", "description": "The second SCOP concise classification string to compare.\nSame expectations and role as sccs1 but serving as the right-hand\noperand. Both arguments must be of type str; passing another type may\nraise a TypeError if it does not support the string split operation or\na ValueError if numeric conversion fails.", "default": ""}}, "required": ["sccs1", "sccs2"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a SCOP domain table and need a deterministic sort key, but I just realized some of the numeric fields were exported from a spreadsheet and the fold/superfamily indices got their internal digits swapped for a subset of rows. I originally planned to compare the two sccs exactly as written: \"c.1.12.3\" versus \"c.1.2.10\". However, for the one that has the larger two-digit component in the middle position, treat that particular middle component as having its digits reversed (only that component; everything else stays the same) before doing the standard Bio.SCOP ordering comparison (class letter lexicographic, then integer components, then shorter wins if tied). Under that correction rule, which sccs comes first?", "answers": "[{\"name\": \"Bio_SCOP_cmp_sccs\", \"arguments\": {\"sccs1\": \"c.1.21.3\", \"sccs2\": \"c.1.2.10\"}}]"}
{"func_name": "Bio_SearchIO__utils_get_processor", "func_desc": "Return the processor object for a given search result format name by looking it up\n    in a mapping and performing a dynamic import from the Bio.SearchIO subpackage.", "tools": [{"function": {"description": "Return the processor object for a given search result format name by looking it up\nin a mapping and performing a dynamic import from the Bio.SearchIO subpackage.", "name": "Bio_SearchIO__utils_get_processor", "parameters": {"properties": {"format": {"type": "string", "description": "The lower-case name of the file/search format to process. In the\nBio.SearchIO context this is the canonical format identifier provided by the\ncaller (for example when selecting which parser/iterator to use for a\nparticular search program output). This function requires a Python string\nand expects it to already be lower case; if None, not a string, or not\nlower case a descriptive exception will be raised. The function uses this\nexact string as a key into the mapping argument.", "default": ""}, "mapping": {"type": "any", "description": "A dictionary that maps format name strings (lower case) to a\ntwo-item sequence (mod_name, obj_name). mod_name is the name of a module\nlocated under the Bio.SearchIO package (for example the module that\nimplements parsing for that format) and obj_name is the name of an\nattribute in that module (typically a parser class or factory function).\nConcretely each value is unpacked as \"mod_name, obj_name\" and both are\nexpected to be strings. This mapping is typically provided by SearchIO\ninfrastructure code to associate format identifiers with their\nimplementing modules and exported processor names.", "default": ""}}, "required": ["format", "mapping"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a custom Bio.SearchIO loader for a mixed BLAST/HMMER pipeline, but the format id I get from upstream isn’t clean. I *initially* planned to just look up the canonical BLAST tabular id and import its parser, but I realized the upstream config sometimes appends a provenance suffix after a colon (e.g., `:<run_id>`), and passing that raw string into the processor lookup will fail because it won’t match the mapping keys.\n\nSo: take the incoming format string `blast-tab:rerun_2026_02_02` and, before fetching the processor, normalize it by dropping anything after the first colon. Then use the provided mapping (which includes both BLAST tabular and HMMER3 tabular entries) to dynamically import from `Bio.SearchIO` and return the processor object corresponding to the normalized id. Do not try to infer from the suffix; only the normalized key should drive the lookup.\n\nMapping to use:\n- the BLAST tabular entry points to the BLAST I/O submodule and its tabular parser class\n- the HMMER3 tabular entry points to the HMMER I/O submodule and its HMMER3 tabular parser class", "answers": "[{\"name\": \"Bio_SearchIO__utils_get_processor\", \"arguments\": {\"format\": \"blast-tab\", \"mapping\": {\"blast-tab\": [\"BlastIO\", \"BlastTabParser\"], \"hmmer3-tab\": [\"HmmerIO\", \"Hmmer3TabParser\"]}}}]"}
{"func_name": "Bio_SeqIO_QualityIO_solexa_quality_from_phred", "func_desc": "Bio.SeqIO.QualityIO.solexa_quality_from_phred converts a PHRED-style quality score to the equivalent Solexa-style quality score used by some sequencing file formats and older Illumina encodings. It is part of Biopython's SeqIO/QualityIO utilities for handling per-base sequencing quality values when parsing, converting or writing sequence data.", "tools": [{"function": {"description": "Bio.SeqIO.QualityIO.solexa_quality_from_phred converts a PHRED-style quality score to the equivalent Solexa-style quality score used by some sequencing file formats and older Illumina encodings. It is part of Biopython's SeqIO/QualityIO utilities for handling per-base sequencing quality values when parsing, converting or writing sequence data.\n", "name": "Bio_SeqIO_QualityIO_solexa_quality_from_phred", "parameters": {"properties": {"phred_quality": {"type": "float", "description": "PHRED quality score to convert. In sequencing and in this module a PHRED score is the standard log-scaled measure of per-base error probability, defined as phred_quality = -10 * log10(error). Typical practical PHRED values are in the range 0 to about 90. As a special-case documented in the source, passing None is supported as a sentinel for missing values (for example Bio.SeqIO may use None for gaps or unknown qualities); when phred_quality is None the function returns None unchanged. The function expects non-negative numeric PHRED values; negative inputs are treated as invalid (see Failure modes).", "default": ""}}, "required": ["phred_quality"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a mixed-quality pipeline while converting an old Illumina FASTQ: I was going to directly compute the Solexa-style score for a PHRED Q=30, but I realized the lab’s QC sheet reports PHRED values as ASCII-encoded characters (Sanger-style, i.e., offset by 33). The character logged for the base is '?' (question mark). Decode that to the underlying PHRED integer first, then convert that PHRED value to the equivalent Solexa quality using Biopython’s `Bio.SeqIO.QualityIO.solexa_quality_from_phred`. What Solexa quality do I get?", "answers": "[{\"name\": \"Bio_SeqIO_QualityIO_solexa_quality_from_phred\", \"arguments\": {\"phred_quality\": 30}}]"}
{"func_name": "Bio_SeqUtils_GC123", "func_desc": "Bio.SeqUtils.GC123 calculates G+C content for a nucleotide sequence overall and separately for each codon position (first, second, third) in a coding-sequence reading frame starting at the first base. This function is part of Biopython, a toolkit for computational molecular biology, and is used when assessing sequence composition, codon position bias, or GC-related metrics in coding DNA.\n    \n    This function walks the input sequence in codons (triplets) from the first base, counts occurrences of the unambiguous DNA nucleotides A, T, G, and C (case-insensitive) at each codon position, and computes percentages as 100 * (G + C) / n where n is the count of A/T/G/C observed at that position. An incomplete trailing codon (length < 3) is padded internally so that its remaining positions do not match A/T/G/C and therefore do not contribute to counts. Ambiguous nucleotides (for example 'N' or other non-ATGC characters) are not treated as A/T/G/C: they are effectively ignored and reduce the denominator n for the affected positions. The function performs no input sanitization beyond treating the input as an indexable string and does not modify the input sequence.", "tools": [{"function": {"description": "Bio.SeqUtils.GC123 calculates G+C content for a nucleotide sequence overall and separately for each codon position (first, second, third) in a coding-sequence reading frame starting at the first base. This function is part of Biopython, a toolkit for computational molecular biology, and is used when assessing sequence composition, codon position bias, or GC-related metrics in coding DNA.\n\nThis function walks the input sequence in codons (triplets) from the first base, counts occurrences of the unambiguous DNA nucleotides A, T, G, and C (case-insensitive) at each codon position, and computes percentages as 100 * (G + C) / n where n is the count of A/T/G/C observed at that position. An incomplete trailing codon (length < 3) is padded internally so that its remaining positions do not match A/T/G/C and therefore do not contribute to counts. Ambiguous nucleotides (for example 'N' or other non-ATGC characters) are not treated as A/T/G/C: they are effectively ignored and reduce the denominator n for the affected positions. The function performs no input sanitization beyond treating the input as an indexable string and does not modify the input sequence.", "name": "Bio_SeqUtils_GC123", "parameters": {"properties": {"seq": {"type": "string", "description": "A nucleotide sequence supplied as a Python string. The sequence is interpreted as a coding-region sequence where codons are read from the first character in successive groups of three. Mixed upper- and lower-case letters are supported (both 'A' and 'a' are counted as adenine). The function only recognizes the characters 'A', 'T', 'G', and 'C' (and their lower-case equivalents) as valid nucleotides for counting; any other characters are treated as ambiguous and ignored in the GC percentage calculations.", "default": ""}}, "required": ["seq"], "type": "any"}}, "type": "function"}], "query": "I’m profiling codon-position GC bias for a coding-region fragment, and I was going to just run GC123 directly on the raw read. But I realized the sequencer sometimes appends a short 3′ tail that starts at the first lowercase base and isn’t part of the coding frame; keeping it would create an artificial incomplete trailing codon. So: take the sequence \"ATGGCNtaaGCCAT\", trim off the suffix starting at the first lowercase character (keep everything before it), and then compute overall GC% plus GC% at codon positions 1, 2, and 3 (frame starts at the first base). Remember ambiguous bases should be ignored the way GC123 does.", "answers": "[{\"name\":\"Bio_SeqUtils_GC123\",\"arguments\":{\"seq\":\"ATGGCN\"}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_Tm_GC", "func_desc": "Return the estimated melting temperature (Tm) in degrees Celsius for a DNA/RNA primer or oligonucleotide using empirical formulas based on percent GC content and optional salt corrections. This function implements a family of simple, commonly cited Tm approximations (see Marmur & Doty, Wetmur, Primer3Plus, von Ahsen, and QuikChange variants) and is used in Biopython for quick Tm estimates when designing primers, checking primer properties, or as a component of higher-level primer design pipelines described in the Biopython documentation.", "tools": [{"function": {"description": "Return the estimated melting temperature (Tm) in degrees Celsius for a DNA/RNA primer or oligonucleotide using empirical formulas based on percent GC content and optional salt corrections. This function implements a family of simple, commonly cited Tm approximations (see Marmur & Doty, Wetmur, Primer3Plus, von Ahsen, and QuikChange variants) and is used in Biopython for quick Tm estimates when designing primers, checking primer properties, or as a component of higher-level primer design pipelines described in the Biopython documentation.\n", "name": "Bio_SeqUtils_MeltingTemp_Tm_GC", "parameters": {"properties": {"seq": {"type": "string", "description": "Nucleotide sequence for which to calculate Tm. The sequence is converted to str() on entry. Ambiguous bases are allowed but handled specially: gc_fraction is computed with the \"weighted\" option (so e.g. \"X\" counts as 0.5 GC for gc_fraction), and if mismatch is True each \"X\" is treated as an actual mismatch and reduces the calculated %GC and final Tm by the corresponding fraction. Sequence length (len(seq)) is used as N in the empirical formulas and in mismatch percentage calculations. The function may call the internal _check(seq, \"Tm_GC\") when check is True to validate/standardize the sequence (see note on check below).", "default": ""}, "check": {"type": "boolean", "description": "If True (default) perform an internal validity check and possible normalization of seq by calling _check(seq, \"Tm_GC\"). This step is intended to catch or normalize invalid characters early (see Biopython SeqUtils internal checks). If False the input is used as-is after conversion to str().", "default": true}, "strict": {"type": "boolean", "description": "If True (default) raise ValueError when the sequence contains any of the ambiguous bases \"K\", \"M\", \"N\", \"R\", \"Y\", \"B\", \"V\", \"D\", \"H\" (exactly those letters). This enforces unambiguous base usage for calculations where ambiguity is not acceptable. If strict is False these letters are permitted and treated according to the weighted gc_fraction routine.", "default": true}, "valueset": {"type": "integer", "description": "Integer selector (default 7) choosing one of several published empirical Tm formula variants. The implementation provides variants 1–8 (see below). The allowed range is 0–8; valueset > 8 raises ValueError. If userset is provided it overrides valueset. Default valueset 7 corresponds to the Primer3Plus style formula used for product Tm.", "default": 7}, "userset": {"type": "any", "nullable": true, "description": "If provided, a tuple of four numeric values (A, B, C, D) overriding the preset values for the empirical formula Tm = A + B*(%GC) - C/N + salt_correction - D*(%mismatch). Userset must be a tuple with exactly four elements; these values are used directly as the constants in the formula. If userset is supplied it takes precedence over valueset. Incorrect length or non-iterable will result in the usual Python unpacking/type error at runtime.", "default": null}, "Na": {"type": "float", "description": "Sodium ion concentration in mM (default 50). Used directly for salt correction when saltcorr specifies a method that uses [Na+]. If any of K, Tris, Mg or dNTPs are non-zero the function will compute a sodium-equivalent concentration (von Ahsen et al., 2001) and use that value for salt correction according to the selected method.", "default": 50}, "K": {"type": "float", "description": "Potassium ion concentration in mM (default 0). If non-zero it is included when computing a sodium-equivalent concentration for salt correction (see Na).", "default": 0}, "Tris": {"type": "float", "description": "Tris buffer concentration in mM (default 0). If non-zero it contributes to the sodium-equivalent concentration used for salt correction.", "default": 0}, "Mg": {"type": "float", "description": "Magnesium ion concentration in mM (default 0). If non-zero it contributes to the sodium-equivalent concentration used for salt correction; the salt correction routine may account for Mg-dNTP interactions when dNTPs is provided.", "default": 0}, "dNTPs": {"type": "float", "description": "Total deoxynucleotide triphosphates concentration in mM (default 0). If non-zero it is used together with Mg to compute effective free Mg2+ (per von Ahsen) when performing salt correction.", "default": 0}, "saltcorr": {"type": "integer", "description": "Integer code selecting the type of salt correction to apply (default 0). saltcorr == 0 or None means no salt correction is applied. Positive integers select among salt correction methods implemented by salt_correction; note that saltcorr == 5 is explicitly not applicable to Tm_GC and will raise ValueError. If saltcorr is non-zero the function will call salt_correction(Na=Na, K=K, Tris=Tris, Mg=Mg, dNTPs=dNTPs, seq=seq, method=saltcorr) and add the returned correction (in degrees Celsius) to the base empirical Tm.", "default": 0}, "mismatch": {"type": "boolean", "description": "If True (default) every 'X' in seq is treated as a mismatch: first gc_fraction is computed with the \"weighted\" option (where X counts as 0.5 GC) and then percent_gc is reduced by seq.count(\"X\") * 50.0 / len(seq) before applying the formula; finally the Tm is decreased by D * (seq.count(\"X\") * 100.0 / len(seq)) to account for the percent mismatch. If mismatch is False the function does not apply the explicit mismatch penalty D*(%mismatch) and leaves the weighted gc_fraction value as-is.", "default": true}}, "required": ["seq", "mismatch", "K", "Mg", "saltcorr", "strict", "valueset", "userset", "check", "Tris", "Na", "dNTPs"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick-and-dirty PCR primer sanity check before ordering. Use the same 20-mer primer sequence (ATGCCGTTGACCTGATCGTA) and start with the Primer3Plus-style GC-based Tm approximation with salt correction using the buffer I wrote down: 50 mM Na+, 10 mM K+, 20 mM Tris, 1.5 mM Mg2+, and 0.2 mM dNTPs. Wait—my LIMS export revealed the Tris number is actually reported in molar (not millimolar), which would make the ionic strength absurd and would completely dominate any correction. Don’t use that raw Tris value; instead, treat Tris as the common PCR-buffer concentration that is exactly one-thousandth of what the export suggests, and keep everything else the same. Use the same salt-correction scheme as originally intended (the one indexed as the first option in that function), and keep strict sequence checking enabled. Calculate the Tm for that primer under these corrected assumptions.", "answers": "[{\"name\": \"Bio_SeqUtils_MeltingTemp_Tm_GC\", \"arguments\": {\"seq\": \"ATGCCGTTGACCTGATCGTA\", \"check\": true, \"strict\": true, \"valueset\": 7, \"userset\": null, \"Na\": 50.0, \"K\": 10.0, \"Tris\": 0.02, \"Mg\": 1.5, \"dNTPs\": 0.2, \"saltcorr\": 1, \"mismatch\": true}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_Tm_Wallace", "func_desc": "Bio.SeqUtils.MeltingTemp.Tm_Wallace calculates the melting temperature (Tm) of a DNA oligonucleotide using the Wallace rule, a simple rule-of-thumb estimator commonly used in PCR primer design and other basic oligonucleotide annealing estimates.", "tools": [{"function": {"description": "Bio.SeqUtils.MeltingTemp.Tm_Wallace calculates the melting temperature (Tm) of a DNA oligonucleotide using the Wallace rule, a simple rule-of-thumb estimator commonly used in PCR primer design and other basic oligonucleotide annealing estimates.\n", "name": "Bio_SeqUtils_MeltingTemp_Tm_Wallace", "parameters": {"properties": {"seq": {"type": "string", "description": "The input nucleic acid sequence for which to estimate Tm. In the Biopython context this is typically a DNA primer sequence string possibly containing whitespace or non-DNA characters; the function treats the sequence as a string and counts nucleotides to compute the Tm. The method ignores non-DNA characters (for example digits or punctuation) when computing the temperature (see examples in the source: spaces and digits do not change the result). When check=True (the default) the sequence is passed to the module's internal _check routine which performs validation and normalization (for example, converting to upper case and removing or handling whitespace) before counting bases; _check may raise an exception for unacceptable input.", "default": ""}, "check": {"type": "boolean", "description": "If True (default), perform sequence validation/normalization by calling the module's _check function prior to counting bases. This typically normalizes the sequence (e.g., uppercasing, removing or handling whitespace) and may raise a ValueError or other exception for badly formed input. If False, the function uses the seq string as provided (after conversion via str(seq)) and counts characters directly, still ignoring characters that are not counted as A, T, G, C, or recognised ambiguous IUPAC codes used below.", "default": true}, "strict": {"type": "boolean", "description": "If True (default), disallow ambiguous IUPAC nucleotide codes B, D, H, K, M, N, R, V, Y in the sequence and raise a ValueError if any of these ambiguous bases are present. If False, ambiguous bases are handled by adding intermediate fractional contributions to the Tm (see detailed behavior below). Use strict=True when you require an exact base composition (typical when assessing a specific primer sequence), and strict=False when you want a rough estimate that accounts for ambiguous positions.", "default": true}}, "required": ["seq", "check", "strict"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick PCR primer sanity-check and want a Wallace-rule Tm estimate, but the sequence I got from our lab sheet may include mixed-case letters, spaces, and an in-band annotation that shouldn’t be treated as a base. Start by cleaning the string to a contiguous oligo sequence by removing whitespace and any non-IUPAC characters. I initially thought we should reject anything with ambiguity codes to avoid overestimating, but then I realized the ordering primer sometimes contains standard IUPAC degeneracy and we still need a number for triage. So: validate the cleaned sequence, and if it contains any ambiguous IUPAC bases, run the calculation in the permissive mode that still accepts them; otherwise, keep the stricter validation. Use the cleaned version of this raw input: \"agtc cgATGCttACGGAATC\".", "answers": "[{\"name\": \"Bio_SeqUtils_MeltingTemp_Tm_Wallace\", \"arguments\": {\"seq\": \"AGTCCGATGCTTACGGAATC\", \"check\": true, \"strict\": true}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_chem_correction", "func_desc": "Bio.SeqUtils.MeltingTemp.chem_correction corrects a melting temperature (Tm) estimate for the presence of chemical denaturants DMSO and formamide, producing an adjusted Tm used in oligonucleotide design, PCR primer planning, and nucleic acid hybridization predictions within the Biopython computational molecular biology toolkit.\n    \n    This function applies simple, literature-based linear corrections to a provided melting temperature. These corrections are approximate and intended to give a pragmatic adjustment to Tm values computed elsewhere in Biopython or supplied by the user. The function does not mutate inputs and returns a new float value. The default numeric factors are those coded into the function; alternative reported values from the literature are noted below for user awareness.", "tools": [{"function": {"description": "Bio.SeqUtils.MeltingTemp.chem_correction corrects a melting temperature (Tm) estimate for the presence of chemical denaturants DMSO and formamide, producing an adjusted Tm used in oligonucleotide design, PCR primer planning, and nucleic acid hybridization predictions within the Biopython computational molecular biology toolkit.\n\nThis function applies simple, literature-based linear corrections to a provided melting temperature. These corrections are approximate and intended to give a pragmatic adjustment to Tm values computed elsewhere in Biopython or supplied by the user. The function does not mutate inputs and returns a new float value. The default numeric factors are those coded into the function; alternative reported values from the literature are noted below for user awareness.", "name": "Bio_SeqUtils_MeltingTemp_chem_correction", "parameters": {"properties": {"melting_temp": {"type": "float", "description": "Input melting temperature to correct. This is the Tm value in whatever temperature units the caller is using (typically degrees Celsius) produced by melting temperature calculations or experimental estimates. The function returns a corrected value in the same units.", "default": ""}, "DMSO": {"type": "float", "description": "Percent (by volume) DMSO present in the sample (e.g., 3 for 3% DMSO). If non-zero, the function decreases melting_temp by DMSOfactor * DMSO. A value of 0 means no DMSO correction is applied.", "default": 0}, "fmd": {"type": "float", "description": "Formamide concentration. Interpreted according to fmdmethod: when fmdmethod == 1, fmd is percent formamide (e.g., 5 for 5%); when fmdmethod == 2, fmd is molar concentration (e.g., 1.25 for 1.25 M). If fmd is 0, no formamide correction is applied.", "default": 0}, "DMSOfactor": {"type": "float", "description": "Per-percent decrease in Tm applied for DMSO (multiplier used as DMSOfactor * DMSO). The function default is 0.75 (as set in the function signature). Literature reports a range of values (e.g., 0.5, 0.6, 0.65 — von Ahsen et al. 2001 — and 0.675); users can override this parameter to reflect their preferred empirical factor for their experimental conditions.", "default": 0.75}, "fmdfactor": {"type": "float", "description": "Per-percent decrease in Tm applied when fmdmethod == 1 (i.e., degrees per percent formamide). The function default is 0.65. Literature reports factors between approximately 0.6 and 0.72; set this parameter to match the empirical correction appropriate for your data if needed.", "default": 0.65}, "fmdmethod": {"type": "integer", "description": "Method for applying the formamide correction. Two methods are supported:\n1: Linear percent method (default). The function subtracts fmdfactor * fmd from melting_temp where fmd is percent formamide.\n2: GC-dependent molar method. The function uses the formula Tm += (0.453 * f(GC) - 2.88) * fmd, where f(GC) is the fraction (not percent) of GC (GC / 100.0) and fmd is given in molar. This method is taken from Blake & Delcourt (1996).", "default": 1}, "GC": {"type": "float", "nullable": true, "description": "GC content expressed in percent (e.g., 50 for 50% GC). Required only when fmdmethod == 2. If fmdmethod == 2 and GC is None or negative, the function raises a ValueError. If fmdmethod == 1, GC is ignored and may be left as None.", "default": null}}, "required": ["melting_temp", "fmd", "GC", "DMSO", "DMSOfactor", "fmdfactor", "fmdmethod"], "type": "any"}}, "type": "function"}], "query": "I’ve got an oligo Tm estimate of 62.8\u0000C and I was going to apply the usual chemical correction assuming the formamide input is in percent (v/v). Wait—our lab notebook records formamide as a **fraction of total reaction volume** (so the number is on a 0–1 scale), while DMSO is still recorded in percent. The mix will contain 4% DMSO and 0.06 formamide (fraction). Adjust the Tm using Biopython’s default correction factors, and use the formamide method that corresponds to fraction-based input (not percent).", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_chem_correction\",\"arguments\":{\"melting_temp\":62.8,\"DMSO\":4.0,\"fmd\":0.06,\"DMSOfactor\":0.75,\"fmdfactor\":0.65,\"fmdmethod\":2,\"GC\":null}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_make_table", "func_desc": "Return a dictionary table of thermodynamic parameters used by DNA melting\n    temperature calculations.\n    \n    This function is used within the Bio.SeqUtils.MeltingTemp module to build or\n    customize the lookup table of nearest-neighbor and initiation thermodynamic\n    parameters (commonly enthalpy and entropy pairs) employed by melting\n    temperature routines. If no existing table is provided, a default table is\n    constructed with a set of standard parameter names initialized to (0, 0).\n    A user can supply a pre-existing table and/or a dictionary of new or updated\n    values to modify that table. This is intended for practical use cases such as\n    replacing initiation parameters from one published dataset (for example,\n    Sugimoto '96, stored in DNA_NN2) with values from another dataset (for\n    example, Allawi & SantaLucia '97), as shown in the original example usage.", "tools": [{"function": {"description": "Return a dictionary table of thermodynamic parameters used by DNA melting\ntemperature calculations.\n\nThis function is used within the Bio.SeqUtils.MeltingTemp module to build or\ncustomize the lookup table of nearest-neighbor and initiation thermodynamic\nparameters (commonly enthalpy and entropy pairs) employed by melting\ntemperature routines. If no existing table is provided, a default table is\nconstructed with a set of standard parameter names initialized to (0, 0).\nA user can supply a pre-existing table and/or a dictionary of new or updated\nvalues to modify that table. This is intended for practical use cases such as\nreplacing initiation parameters from one published dataset (for example,\nSugimoto '96, stored in DNA_NN2) with values from another dataset (for\nexample, Allawi & SantaLucia '97), as shown in the original example usage.", "name": "Bio_SeqUtils_MeltingTemp_make_table", "parameters": {"properties": {"oldtable": {"type": "any", "nullable": true, "description": "An existing dictionary of thermodynamic parameters to use\nas the starting point. The keys are parameter name strings (for\nexample 'AA/TT', 'init_A/T', 'sym') and the values are two-number\ntuples (commonly interpreted as enthalpy and entropy). If None, the\nfunction builds and returns a fresh default table. The default table\ncreated when oldtable is None contains these keys initialized to\n(0, 0): \"init\", \"init_A/T\", \"init_G/C\", \"init_oneG/C\",\n\"init_allA/T\", \"init_5T/A\", \"sym\", \"AA/TT\", \"AT/TA\", \"TA/AT\",\n\"CA/GT\", \"GT/CA\", \"CT/GA\", \"GA/CT\", \"CG/GC\", \"GC/CG\", \"GG/CC\".\nThe function makes a shallow copy of oldtable (via oldtable.copy())\nbefore applying any updates, so passing a mutable mapping will not\nbe mutated by this function (the returned object is a separate\ndictionary). If oldtable is not a mapping with a copy() method, a\nruntime error may be raised when attempting to copy it.", "default": null}, "values": {"type": "any", "nullable": true, "description": "A dictionary of parameter updates to apply to the table.\nKeys should be parameter name strings matching those used by the\nMeltingTemp routines (for example 'init_A/T'). Values should be\ntwo-number tuples consistent with the table's value format (for\nexample (enthalpy, entropy) as floats or ints). If values is None or\nempty, no updates are applied. The function applies updates using\nthe standard dict.update(values) operation; it does not validate keys\nor the numeric nature of tuple elements. If values is not a mapping\nwith an update-compatible interface, a runtime error may be raised.\nNote that because the function only checks truthiness before calling\nupdate (if values: table.update(values)), an empty dict will result\nin no update call but yields the same final table as updating with an\nempty dict would.", "default": null}}, "required": ["oldtable", "values"], "type": "any"}}, "type": "function"}], "query": "I’m calibrating a DNA melting temperature workflow (nearest-neighbor thermodynamics) and I want to start from the module’s default thermodynamic lookup table (i.e., no pre-existing table passed in). Initially I was going to overwrite the initiation terms and the symmetry correction using my lab’s reference pairs: init = (-10.8, -28.4), init_G/C = (-0.1, -2.8), init_A/T = (2.3, 4.1), and sym = (0.0, -1.4). Wait—our downstream sanity check flags any *positive initiation enthalpy* as a transcription error (initiation enthalpies should not be > 0 in our pipeline). So keep everything the same except: for the A/T initiation enthalpy, use the same magnitude but flip its sign (leave its entropy exactly as written). Build the resulting table for me.", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_make_table\",\"arguments\":{\"oldtable\":null,\"values\":{\"init\":[-10.8,-28.4],\"init_G/C\":[-0.1,-2.8],\"init_A/T\":[-2.3,4.1],\"sym\":[0.0,-1.4]}}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_salt_correction", "func_desc": "Calculate a term to correct nucleic acid melting temperature (Tm) or entropy\n    for the ionic environment. This function computes a scalar correction term\n    based on supplied millimolar concentrations of common ions (Na+, K+, Tris,\n    Mg2+) and dNTPs, and on a selected empirical method (1-7) drawn from the\n    literature (Schildkraut & Lifson 1965; Wetmur 1991; SantaLucia 1996/1998;\n    Owczarzy 2004/2008, and von Ahsen 2001 for Na-equivalent). The computed\n    correction is intended to be applied to a previously calculated Tm or to\n    deltaS according to the method semantics described below; the function does\n    not itself compute Tm or deltaS, only the ionic correction term.", "tools": [{"function": {"description": "Calculate a term to correct nucleic acid melting temperature (Tm) or entropy\nfor the ionic environment. This function computes a scalar correction term\nbased on supplied millimolar concentrations of common ions (Na+, K+, Tris,\nMg2+) and dNTPs, and on a selected empirical method (1-7) drawn from the\nliterature (Schildkraut & Lifson 1965; Wetmur 1991; SantaLucia 1996/1998;\nOwczarzy 2004/2008, and von Ahsen 2001 for Na-equivalent). The computed\ncorrection is intended to be applied to a previously calculated Tm or to\ndeltaS according to the method semantics described below; the function does\nnot itself compute Tm or deltaS, only the ionic correction term.", "name": "Bio_SeqUtils_MeltingTemp_salt_correction", "parameters": {"properties": {"Na": {"type": "float", "description": "Millimolar concentration of sodium ions [Na+]. This is the\nprimary input for a simple salt correction: pass only Na to apply a\nsodium-only correction. Units are millimolar (mM). Typical practical\nuse: buffer [Na+] expressed in mM derived from experimental conditions.", "default": 0}, "K": {"type": "float", "description": "Millimolar concentration of potassium ions [K+]. When non-zero,\nK contributes to a sodium-equivalent concentration according to the\nvon Ahsen et al. (2001) prescription used here: it is added directly\n(in mM) to the effective monovalent contribution. Defaults to 0.0 mM.", "default": 0}, "Tris": {"type": "float", "description": "Millimolar concentration of Tris base (Tris buffer). In the\nsodium-equivalent calculation Tris contributes half its molar amount\n(Tris/2) because Tris is a weak base that partially provides monovalent\ncations. Units are millimolar (mM). Defaults to 0.0 mM.", "default": 0}, "Mg": {"type": "float", "description": "Millimolar concentration of magnesium ions [Mg2+]. Mg2+ is a\ndivalent cation and affects salt correction differently: it is used\nto compute a sodium-equivalent term 120*sqrt([Mg2+] - [dNTPs]) (mM)\nwhen appropriate (see behavior). Mg is also converted internally to\nmolar units (Mg * 1e-3) for formulae that require molar concentrations.\nDefaults to 0.0 mM.", "default": 0}, "dNTPs": {"type": "float", "description": "Millimolar total concentration of dNTPs (deoxynucleotide\ntriphosphates). dNTPs strongly bind Mg2+ and therefore reduce free\nMg2+. When dNTPs >= Mg the code treats free Mg2+ as negligible in the\nsodium-equivalent calculation. Units are millimolar (mM). Defaults to\n0.0 mM.", "default": 0}, "method": {"type": "integer", "description": "Integer selecting the empirical correction method to apply.\nValid values are 0 and 1-7. Default is 1. Semantic effect of the\nreturned correction depends on method:\n- method == 0: No correction; the function returns 0.0 (useful to\n  disable salt correction programmatically).\n- methods 1-4: The returned correction corr is intended to be added to\n  an existing Tm value: Tm(new) = Tm(old) + corr. These methods use\n  simple log([Na+]) or modified log formulae (units consistent with\n  Tm scales used by the cited authors).\n- method 5: The returned correction is an entropy correction (deltaS).\n  It should be added to an existing deltaS: deltaS(new) = deltaS(old)\n  + corr. Formula is proportional to (N-1)*ln[Na+] where N is sequence\n  length.\n- methods 6 and 7: The returned correction is for the reciprocal of\n  Tm; the corrected Tm is computed as Tm(new) = 1/(1/Tm(old) + corr).\nDetailed definitions and literature sources:\n1: 16.6 * log10([Na+]) (Schildkraut & Lifson 1965)\n2: 16.6 * log10([Na+]/(1 + 0.7*[Na+])) (Wetmur 1991)\n3: 12.5 * log10([Na+]) (SantaLucia et al. 1996)\n4: 11.7 * log10([Na+]) (SantaLucia 1998)\n5: 0.368 * (N - 1) * ln([Na+]) (SantaLucia 1998) — entropy correction\n6: (4.29*%GC - 3.95)*1e-5*ln([Na+]) + 9.40e-6*ln([Na+])^2\n   (Owczarzy et al. 2004) — correction applied to 1/Tm\n7: Complex empirical formula with decision tree and empirical\n   constants, includes explicit Mg2+ correction for dNTP binding and\n   additional regimes based on the ratio sqrt([Mg2+])/[Na_eq]\n   (Owczarzy et al. 2008). Note: method 7 applies a specific Mg2+\n   treatment and a dissociation constant for Mg:dNTP interactions.\nUse this parameter to select which empirical model best matches your\nexperimental protocol or literature source.", "default": 1}, "seq": {"type": "string", "nullable": true, "description": "DNA sequence string used only for methods that require\nsequence-dependent quantities: methods 5, 6 and 7. For method 5 the\nsequence length (len(seq)) is used; for methods 6 and 7 the GC\nfraction (percentage GC) is used via SeqUtils.gc_fraction(seq,\n\"ignore\"). If seq is required by the selected method but is None or\nempty, a ValueError is raised. The function does not validate or\ncanonicalize sequence letters beyond what SeqUtils.gc_fraction\nperforms; pass the nucleotide string you used when computing Tm or\ndeltaS.", "default": null}}, "required": ["K", "Mg", "method", "seq", "Tris", "Na", "dNTPs"], "type": "any"}}, "type": "function"}], "query": "I’m doing an ionic-environment correction for a PCR-like buffer, but I just noticed our LIMS exports Tris as the *free base* concentration, and the salt-correction routine assumes the *protonated buffer species* contributes to ionic strength. So: start from the same 20-mer duplex sequence (ATGCGTACGTTAGCGTACGA) and the same measured concentrations (50.0 mM Na+, 10.0 mM K+, 20.0 mM Tris-base, 1.5 mM Mg2+, 0.2 mM total dNTPs). First convert Tris to the contributing ionic form using the rule that, at our working pH near Tris pKa, exactly half of the reported Tris-base is protonated. Then compute the Owczarzy 2008 salt correction term using the method variant that accounts for both monovalent ions and Mg2+ competition with dNTP chelation (i.e., the most comprehensive option in that paper’s family as implemented in this function).", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_salt_correction\",\"arguments\":{\"Na\":50.0,\"K\":10.0,\"Tris\":10.0,\"Mg\":1.5,\"dNTPs\":0.2,\"method\":7,\"seq\":\"ATGCGTACGTTAGCGTACGA\"}}]"}
{"func_name": "Bio_SeqUtils_lcc_lcc_mult", "func_desc": "Calculate Local Composition Complexity (LCC) values over a sliding window for an unambiguous DNA sequence.\n    \n    This function implements an optimized, incremental computation of the Local Composition Complexity (LCC) over a sequence using a fixed-size sliding window. LCC is a per-window measure of nucleotide composition heterogeneity based on the Shannon entropy of nucleotide frequencies normalized by log(4) (natural logarithm), producing values in the range [0.0, 1.0]. A value of 0.0 indicates a window composed of a single nucleotide (minimum complexity); values near 1.0 indicate maximal composition diversity (approximately equal counts of A, C, G, T). This implementation is used in the Biopython project for genomic sequence analysis to detect low-complexity regions and survey local sequence composition. It is optimized relative to a naive recomputation by updating counts and entropy terms incrementally when the window slides by one base.", "tools": [{"function": {"description": "Calculate Local Composition Complexity (LCC) values over a sliding window for an unambiguous DNA sequence.\n\nThis function implements an optimized, incremental computation of the Local Composition Complexity (LCC) over a sequence using a fixed-size sliding window. LCC is a per-window measure of nucleotide composition heterogeneity based on the Shannon entropy of nucleotide frequencies normalized by log(4) (natural logarithm), producing values in the range [0.0, 1.0]. A value of 0.0 indicates a window composed of a single nucleotide (minimum complexity); values near 1.0 indicate maximal composition diversity (approximately equal counts of A, C, G, T). This implementation is used in the Biopython project for genomic sequence analysis to detect low-complexity regions and survey local sequence composition. It is optimized relative to a naive recomputation by updating counts and entropy terms incrementally when the window slides by one base.", "name": "Bio_SeqUtils_lcc_lcc_mult", "parameters": {"properties": {"seq": {"type": "string", "description": "The input DNA sequence as a Python string. semantically, this should be an unambiguous DNA sequence composed of the nucleotide letters A, C, G, and T (case-insensitive). The function converts the sequence to upper case internally. This parameter represents the biological nucleotide sequence to be scanned; providing non-ACGT characters may produce incorrect or undefined LCC values because the algorithm only updates counts for A, C, G and T.", "default": ""}, "wsize": {"type": "integer", "description": "Window size as a positive integer. This controls the length of the sliding window used to compute each LCC value and thus the spatial resolution of the complexity measure. Typical usage provides 1 <= wsize <= len(seq). The function precomputes per-count entropy contributions for indices from 0 up to wsize and uses these to update the LCC efficiently as the window moves by one base.", "default": ""}}, "required": ["seq", "wsize"], "type": "any"}}, "type": "function"}], "query": "I need to scan a short DNA fragment for low-complexity runs using the Biopython-style Local Composition Complexity (LCC) sliding-window calculation. Start by running it on the sequence \"AAACCCGGGTTTACGTACGTAAACCC\" with what I initially thought was a 12-base window. But wait: for the downstream comparison step, the window length must be an exact multiple of the repeat unit length in the leading homopolymer-like blocks (those blocks are length 3 here), and I still want the window as close as possible to my original choice while satisfying that constraint. Recompute the LCC using that adjusted window size (sequence remains unambiguous; do not treat any characters as ambiguous).", "answers": "[{\"name\": \"Bio_SeqUtils_lcc_lcc_mult\", \"arguments\": {\"seq\": \"AAACCCGGGTTTACGTACGTAAACCC\", \"wsize\": 12}}]"}
{"func_name": "Bio_SeqUtils_six_frame_translations", "func_desc": "Return a formatted, human-readable string that shows the six-frame translations of a nucleotide sequence together with nucleotide counts and GC content.\n    \n    This function is part of Bio.SeqUtils in Biopython and is intended for use in computational molecular biology and bioinformatics workflows where a quick, printable overview of all six translation frames (three forward frames and three reverse/complement frames) and basic sequence composition (A, T, G, C counts and GC fraction) is useful. The visual layout and behavior were adapted from xbbtools and are similar to the DNA Strider six-frame translation output: a header with nucleotide counts and a short sequence summary, then blocks of up to 60 nucleotides with corresponding translated amino acids for each frame. The function detects RNA input by checking for the presence of \"u\" (case-insensitive) in the sequence and uses an RNA-specific reverse-complement routine when appropriate. It delegates codon translation to the translate() implementation used elsewhere in Biopython and computes GC fraction via gc_fraction(..., ambiguous=\"ignore\").", "tools": [{"function": {"description": "Return a formatted, human-readable string that shows the six-frame translations of a nucleotide sequence together with nucleotide counts and GC content.\n\nThis function is part of Bio.SeqUtils in Biopython and is intended for use in computational molecular biology and bioinformatics workflows where a quick, printable overview of all six translation frames (three forward frames and three reverse/complement frames) and basic sequence composition (A, T, G, C counts and GC fraction) is useful. The visual layout and behavior were adapted from xbbtools and are similar to the DNA Strider six-frame translation output: a header with nucleotide counts and a short sequence summary, then blocks of up to 60 nucleotides with corresponding translated amino acids for each frame. The function detects RNA input by checking for the presence of \"u\" (case-insensitive) in the sequence and uses an RNA-specific reverse-complement routine when appropriate. It delegates codon translation to the translate() implementation used elsewhere in Biopython and computes GC fraction via gc_fraction(..., ambiguous=\"ignore\").", "name": "Bio_SeqUtils_six_frame_translations", "parameters": {"properties": {"seq": {"type": "string", "description": "The input nucleotide sequence to analyze. This should be a Python string containing the sequence characters (for example, \"ATGCGT...\" or an RNA sequence containing \"U\"). The function uses seq.lower() to detect RNA (\"u\" presence) and uses seq.count(nt.upper()) to compute the header counts for \"A\", \"T\", \"G\", and \"C\" (note: counts are computed by counting the uppercase letters in the provided string using nt.upper(), so providing an all-lowercase sequence will result in zero counts in the header unless the sequence contains uppercase letters). The sequence is sliced into 60-nt windows for display and into codon-aligned fragments for translation. If the sequence length is greater than 20, a shortened summary with the first 10 and last 10 bases is shown in the header; otherwise the full sequence is shown.", "default": ""}, "genetic_code": {"type": "integer", "description": "An integer specifying the genetic code (translation table) to use for codon-to-amino-acid translation. This corresponds to the NCBI translation table identifier used by Biopython's translate implementation; the default value is 1 (the standard genetic code). Invalid or unsupported genetic_code values may cause the underlying translate() function to raise an exception.", "default": 1}}, "required": ["seq", "genetic_code"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick ORF sanity-check on a short mitochondrial fragment, but I just realized this contig came out of a mixed pipeline and might actually be RNA (some assemblers silently output U’s) — and that matters because I need the reverse/complement computed with the RNA-aware routine if any U is present anywhere in the input. Start by generating the same six-frame translation overview (with nucleotide counts and GC content) for the fragment below, using the vertebrate mitochondrial genetic code (the table used for animal mitochondria, not the standard nuclear one). One more constraint: before you run it, fix exactly one obvious transcription artifact in the string (a single base that violates the DNA/RNA consistency of the rest of the fragment) by converting it to the corresponding canonical base so the sequence is internally consistent, but do not otherwise edit the sequence.\n\nSequence: ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG", "answers": "[{\"name\":\"Bio_SeqUtils_six_frame_translations\",\"arguments\":{\"seq\":\"ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG\",\"genetic_code\":2}}]"}
{"func_name": "Bio_Seq_transcribe", "func_desc": "Bio.Seq.transcribe transcribes a DNA sequence into an RNA sequence (T -> U),\n    using the convention that the provided sequence represents the coding strand\n    of the DNA double helix. This function is intended for use in computational\n    molecular biology and bioinformatics workflows (for example, preparing RNA\n    sequences for downstream analysis such as translation or RNA structure\n    prediction), as provided by the Biopython project.\n    \n    Transcription is performed by replacing thymine bases ('T' and 't') with\n    uracil ('U' and 'u') without performing any complement or reverse-complement\n    operation. Ambiguous or non-thymine characters (for example 'N') are left\n    unchanged. The function preserves the original object: it does not modify a\n    MutableSeq or string in place but returns a new object containing the\n    transcribed sequence.", "tools": [{"function": {"description": "Bio.Seq.transcribe transcribes a DNA sequence into an RNA sequence (T -> U),\nusing the convention that the provided sequence represents the coding strand\nof the DNA double helix. This function is intended for use in computational\nmolecular biology and bioinformatics workflows (for example, preparing RNA\nsequences for downstream analysis such as translation or RNA structure\nprediction), as provided by the Biopython project.\n\nTranscription is performed by replacing thymine bases ('T' and 't') with\nuracil ('U' and 'u') without performing any complement or reverse-complement\noperation. Ambiguous or non-thymine characters (for example 'N') are left\nunchanged. The function preserves the original object: it does not modify a\nMutableSeq or string in place but returns a new object containing the\ntranscribed sequence.", "name": "Bio_Seq_transcribe", "parameters": {"properties": {"dna": {"type": "string", "description": "Input DNA sequence to transcribe. In typical use this is a\nPython string containing IUPAC nucleotide codes (for example \"ACTG\").\nPer the implementation in Bio.Seq, this function also accepts objects\nfrom the Biopython sequence classes Seq and MutableSeq: if the runtime\nobject is an instance of Bio.Seq.Seq the method dna.transcribe() is\ncalled and a new Seq object is returned; if it is an instance of\nBio.Seq.MutableSeq the object is first converted to Seq and then\ntranscribed. The str annotation in the function signature indicates\nthe common case of passing a plain Python string, but Seq and\nMutableSeq instances are explicitly handled by the function. If an\nobject without a .replace method (and which is not an instance of\nSeq/MutableSeq) is supplied, an AttributeError will be raised by the\nunderlying code path.", "default": ""}}, "required": ["dna"], "type": "any"}}, "type": "function"}], "query": "I’m prepping an RNA sequence for an in‑vitro transcription spike‑in. Please transcribe the *coding-strand* DNA sequence to RNA (so just replace thymine with uracil; no complementing, no reverse-complement). Initially I was going to paste the sequence exactly as-is, but I just realized our LIMS sometimes appends a trailing 3′ primer artifact that’s a short run of the same nucleotide and should be dropped if present. So: remove that terminal artifact only if it matches that description, then transcribe the remaining sequence, preserving case and leaving ambiguous bases unchanged. Sequence: \"aTgCCtNnTAAGt\".", "answers": "[{\"name\": \"Bio_Seq_transcribe\", \"arguments\": {\"dna\": \"aTgCCtNnTAAGt\"}}]"}
{"func_name": "Bio_TogoWS_entry", "func_desc": "Bio.TogoWS.entry fetches a single or multiple records from the TogoWS \"entry\" endpoint for molecular biology resources (for example NCBI, KEGG, DDBj, EBI databases). It validates the requested database, optional field extraction and output format against cached metadata (populated on first use), constructs the proper TogoWS URL (quoting and joining identifiers as required), and returns the opened network resource produced by the module's internal _open helper. This function is used within Biopython to retrieve sequence, annotation, and other bioinformatics records programmatically (similar in purpose to NCBI Entrez EFetch, but with optional per-record field extraction supported by TogoWS).", "tools": [{"function": {"description": "Bio.TogoWS.entry fetches a single or multiple records from the TogoWS \"entry\" endpoint for molecular biology resources (for example NCBI, KEGG, DDBj, EBI databases). It validates the requested database, optional field extraction and output format against cached metadata (populated on first use), constructs the proper TogoWS URL (quoting and joining identifiers as required), and returns the opened network resource produced by the module's internal _open helper. This function is used within Biopython to retrieve sequence, annotation, and other bioinformatics records programmatically (similar in purpose to NCBI Entrez EFetch, but with optional per-record field extraction supported by TogoWS).\n", "name": "Bio_TogoWS_entry", "parameters": {"properties": {"db": {"type": "string", "description": "The target database name as expected by TogoWS for the \"entry\" service. Practical examples include \"pubmed\", \"nucleotide\", \"protein\", \"uniprot\", \"compound\" (KEGG), \"ddbj\", etc. The function first ensures this name is among the supported databases returned by the internal cache _entry_db_names (fetched from TogoWS on demand). If the database is not supported, a ValueError is raised. The database parameter determines which record namespace, available fields, and output formats will be validated and used when constructing the request URL.", "default": ""}, "id": {"type": "array", "items": {"type": "any"}, "description": "One or more record identifiers for the target database. This may be a single identifier string or a list of identifier strings; if a list is supplied the identifiers are joined with commas to form a single TogoWS request. The identifier(s) are URL-quoted before being appended to the constructed entry URL. Typical identifiers are accession numbers, database-specific IDs, or PubMed IDs depending on the chosen db.", "default": ""}, "format": {"type": "string", "nullable": true, "description": "Desired output format for the returned record, for example \"xml\", \"json\", \"gff\", \"fasta\", or \"ttl\" (RDF Turtle), where supported by the chosen db. When provided, the function validates this format against the formats advertised by the TogoWS entry metadata for the chosen database (cached in _entry_db_formats and fetched on demand via _get_entry_formats). If the format is not supported for that database, a ValueError is raised. If omitted or None, the default format as provided by the TogoWS service for that database is used.", "default": null}, "field": {"type": "string", "nullable": true, "description": "A database-specific field name to extract from within the returned record (for example \"au\" or \"authors\" for PubMed records). When provided, the function validates the field against the fields advertised by the TogoWS entry metadata for the chosen database (cached in _entry_db_fields and fetched on demand via _get_entry_fields). For backward compatibility, a call requesting db=\"pubmed\" and field=\"ti\" is translated to \"title\" and emits a warnings.warn message instructing callers to use \"title\" instead. If the field is not supported for that database, a ValueError is raised. If omitted or None, the entire record is requested.", "default": null}}, "required": ["db", "id", "field", "format"], "type": "any"}}, "type": "function"}], "query": "I need to programmatically pull the amino-acid FASTA sequences for two globin proteins from UniProt for a quick alignment. Start by fetching them as usual. But I just realized my downstream parser chokes on any UniProt *isoform* accessions (the ones that include a suffix separated by a dash) and it also rejects anything that isn’t exactly 6 characters long. So: from this messy list of candidate identifiers — [\"P69905\", \"P68871-2\", \"P68871\", \"Q9XXXX\", \"P69905 \", \"p69905\"] — fetch FASTA only for the entries that are valid UniProt accessions under those constraints (case-sensitive, trimmed of obvious whitespace, no isoform suffix, length exactly like canonical UniProt accessions). Use the TogoWS entry endpoint, requesting the UniProt database, and return the sequences in FASTA with no field extraction.", "answers": "[{\"name\":\"Bio_TogoWS_entry\",\"arguments\":{\"db\":\"uniprot\",\"id\":[\"P69905\",\"P68871\"],\"format\":\"fasta\",\"field\":null}}]"}
{"func_name": "Bio_motifs_create", "func_desc": "Create a Motif object from a collection of aligned motif occurrences.\n    \n    This convenience function constructs an Alignment from the provided instances and then constructs and returns a Motif using that alignment and the provided alphabet. In the Biopython project, a Motif object represents a conserved sequence pattern (for example, a transcription factor binding site) derived from multiple aligned occurrences and is used in downstream analyses such as computing position-specific counts, position-specific scoring matrices (PSSMs), consensus sequences, and motif visualization.", "tools": [{"function": {"description": "Create a Motif object from a collection of aligned motif occurrences.\n\nThis convenience function constructs an Alignment from the provided instances and then constructs and returns a Motif using that alignment and the provided alphabet. In the Biopython project, a Motif object represents a conserved sequence pattern (for example, a transcription factor binding site) derived from multiple aligned occurrences and is used in downstream analyses such as computing position-specific counts, position-specific scoring matrices (PSSMs), consensus sequences, and motif visualization.", "name": "Bio_motifs_create", "parameters": {"properties": {"instances": {"type": "array", "items": {"type": "any"}, "description": "A list of sequence instances that represent individual occurrences of the motif to be modeled. These elements together form the multiple alignment passed to Alignment(instances). In practical use within computational molecular biology, each element corresponds to one observed occurrence of the motif (for example, a short DNA sequence from a set of binding-site hits). Typically all instances should have the same length (i.e., be already aligned); if they are not compatible with Alignment, the Alignment constructor may raise an exception (for example ValueError or a more specific error from Alignment). The exact element types accepted are those accepted by the Alignment constructor; this function does not perform additional type conversion beyond calling Alignment(instances).", "default": ""}, "alphabet": {"type": "string", "description": "A string of characters defining the alphabet used to interpret the sequences (default \"ACGT\"). The default value corresponds to the canonical DNA nucleotide alphabet and is used to initialize the Motif's internal representations (counts, frequency matrices, etc.). The alphabet string is forwarded to the Motif constructor, which may validate it and raise an exception if it is invalid for the Motif implementation being used.", "default": "ACGT"}}, "required": ["instances", "alphabet"], "type": "any"}}, "type": "function"}], "query": "I’m building a motif for a bacterial TF, starting from these 6 *supposedly aligned* 6-mers: [\"TATAAT\",\"TACAAT\",\"TATACT\",\"TATAAT\",\"TATGAT\",\"TATAAT\"]. Initially I was going to just use the standard DNA alphabet, but I realized our sequencer sometimes calls an ambiguous base at the SNP position, and downstream I need the motif to preserve ambiguity rather than forcing everything into the 4-letter alphabet. Please create the Motif object from the same instances, but using the DNA alphabet variant that includes ambiguous IUPAC nucleotide symbols so I can compute a consensus later without losing that information.", "answers": "[{\"name\": \"Bio_motifs_create\", \"arguments\": {\"instances\": [\"TATAAT\", \"TACAAT\", \"TATACT\", \"TATAAT\", \"TATGAT\", \"TATAAT\"], \"alphabet\": \"ACGTRYSWKMBDHVN\"}}]"}
{"func_name": "DeepPurpose_pybi__lper_GetSequenceOrderCorrelationFactorForAPAAC", "func_desc": "DeepPurpose.pybiomed_helper.GetSequenceOrderCorrelationFactorForAPAAC computes the sequence-order correlation factors used in the Amphiphilic Pseudo Amino Acid Composition (APAAC, type II PseAAC) encoding for proteins. The function calculates the average correlation between amino acid pairs separated by a gap of k residues using two physicochemical properties: hydrophobicity and hydrophilicity. These two averaged correlation values are commonly appended to composition-based protein descriptors in DeepPurpose for downstream tasks such as drug–target interaction (DTI) prediction, protein–protein interaction (PPI) prediction, and protein function prediction.", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.GetSequenceOrderCorrelationFactorForAPAAC computes the sequence-order correlation factors used in the Amphiphilic Pseudo Amino Acid Composition (APAAC, type II PseAAC) encoding for proteins. The function calculates the average correlation between amino acid pairs separated by a gap of k residues using two physicochemical properties: hydrophobicity and hydrophilicity. These two averaged correlation values are commonly appended to composition-based protein descriptors in DeepPurpose for downstream tasks such as drug–target interaction (DTI) prediction, protein–protein interaction (PPI) prediction, and protein function prediction.\n", "name": "DeepPurpose_pybi__lper_GetSequenceOrderCorrelationFactorForAPAAC", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein sequence given as a Python string of one-letter amino acid codes. This sequence is the input whose sequence-order correlation factors are computed. The function expects standard amino-acid single-letter symbols; non-standard characters or gaps may cause the underlying correlation lookup (_GetCorrelationFunctionForAPAAC) to fail or raise an error.", "default": ""}, "k": {"type": "integer", "description": "Gap (sequence separation) between residue pairs for which correlations are computed. The default value is 1, meaning adjacent residue pairs. The parameter controls the sequence-order distance used by APAAC (type II PseAAC) to capture local and semi-local physicochemical interactions; larger k measures correlations over longer sequence distances.", "default": 1}}, "required": ["ProteinSequence", "k"], "type": "any"}}, "type": "function"}], "query": "I’m generating APAAC (type II PseAAC) features for a kinase-domain fragment for a DTI baseline, and I want the sequence-order correlation factors computed from the same raw sequence as before: \"EGVAIKAIHLEEKKKHKRVDPVNFKLLSHCLSVMP\". Initially I planned to use a gap of 3, but then I realized the downstream APAAC feature vector in our pipeline is defined using the largest gap that still yields at least 30 amino-acid pairs contributing to the average (otherwise the estimate is too noisy for this short fragment). So compute the two APAAC sequence-order correlation factors using that gap value (i.e., the maximum k satisfying that pair-count constraint) for this sequence.", "answers": "[{\"name\": \"DeepPurpose_pybi__lper_GetSequenceOrderCorrelationFactorForAPAAC\", \"arguments\": {\"ProteinSequence\": \"EGVAIKAIHLEEKKKHKRVDPVNFKLLSHCLSVMP\", \"k\": 5}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_CalculateAAComposition", "func_desc": "DeepPurpose.pybiomed_helper.CalculateAAComposition computes the percent composition of the 20 standard amino acids for a given protein sequence. This function is a simple, per-sequence featurization used in DeepPurpose for protein encoding and preprocessing steps that support downstream tasks such as drug-target interaction (DTI) prediction, protein-protein interaction (PPI) prediction, and protein function prediction. The returned composition can be used as a lightweight input feature or baseline encoding in model training, evaluation, repurposing, and virtual screening workflows described in the DeepPurpose README.", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.CalculateAAComposition computes the percent composition of the 20 standard amino acids for a given protein sequence. This function is a simple, per-sequence featurization used in DeepPurpose for protein encoding and preprocessing steps that support downstream tasks such as drug-target interaction (DTI) prediction, protein-protein interaction (PPI) prediction, and protein function prediction. The returned composition can be used as a lightweight input feature or baseline encoding in model training, evaluation, repurposing, and virtual screening workflows described in the DeepPurpose README.\n", "name": "DeepPurpose_pybiomed_helper_CalculateAAComposition", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A protein sequence provided as a Python string of single-letter amino acid codes. The function expects a \"pure\" protein sequence composed of the standard one-letter amino acid codes present in the module-level AALetter list (20 amino acids). ProteinSequence is treated literally (case-sensitive): occurrences are counted using str.count for each code in AALetter and the denominator is the total length len(ProteinSequence). Therefore, passing lowercase letters, whitespace, digits, symbols, or non-standard letters will alter counts and produce skewed percentage values because they contribute to the sequence length but are not matched to AALetter entries. Passing an empty string (zero-length ProteinSequence) will lead to a division-by-zero failure (ZeroDivisionError). There are no implicit conversions or side effects; if inputs might be lowercase or contain non-amino-acid characters, normalize or validate them before calling this function.", "default": ""}}, "required": ["ProteinSequence"], "type": "any"}}, "type": "function"}], "query": "I need a quick baseline protein featurization for a DTI sanity-check run using DeepPurpose’s `pybiomed_helper.CalculateAAComposition`, but the insulin sequence I got from a collaborator is messy. Start by computing the amino-acid percent composition for the *same human insulin precursor* as below, **however**: I initially assumed it was already cleaned, but I just noticed the string contains whitespace and other non-residue separators. So first, **remove every character that is not one of the 20 standard amino-acid one-letter codes**, then **uppercase** what remains, and only then compute the 20-AA percent composition. Use this exact raw input string:\n\n“MALWMRLLPLLALLALWGPDPAAA FVNQHLCGSHLVEALYLVCGERGFFYTPKT RAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN”\n\nReturn the percent composition output from `CalculateAAComposition` for the cleaned sequence.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_CalculateAAComposition\",\"arguments\":{\"ProteinSequence\":\"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition", "func_desc": "CalculateAADipeptideComposition computes a fixed-length numerical feature vector that encodes the amino acid (AA) composition, dipeptide composition, and 3-mer (tripeptide/spectrum) composition for a given protein primary sequence. In the DeepPurpose toolkit (used for drug-target interaction prediction, protein property/function prediction, PPI, and related molecular modeling tasks), this function is used to transform a raw protein sequence (single-letter amino acid codes) into a deterministic numeric representation (feature vector) suitable for machine learning models, virtual screening, and repurposing workflows.", "tools": [{"function": {"description": "CalculateAADipeptideComposition computes a fixed-length numerical feature vector that encodes the amino acid (AA) composition, dipeptide composition, and 3-mer (tripeptide/spectrum) composition for a given protein primary sequence. In the DeepPurpose toolkit (used for drug-target interaction prediction, protein property/function prediction, PPI, and related molecular modeling tasks), this function is used to transform a raw protein sequence (single-letter amino acid codes) into a deterministic numeric representation (feature vector) suitable for machine learning models, virtual screening, and repurposing workflows.\n", "name": "DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A protein primary sequence provided as a string of single-letter amino acid codes (e.g., \"MSEQ...\"). In the DeepPurpose context, this sequence represents the target protein whose composition-based encoding will be used as input features for models (DTI, PPI, ProteinPred). The function expects a \"pure\" protein sequence; sequences containing non-standard characters, whitespace, or gaps are not guaranteed to be handled and may cause errors or incorrect features. The caller is responsible for providing an appropriately preprocessed sequence (uppercase or lowercase single-letter codes as accepted by the helper composition functions in this module).", "default": ""}}, "required": ["ProteinSequence"], "type": "any"}}, "type": "function"}], "query": "I’m encoding the human BRCA1 N-terminal domain for a DeepPurpose-style protein–ligand interaction pipeline. First, compute the AA composition + dipeptide + 3-mer spectrum vector for this sequence: \"MDLSALRVEEVQNVINAMQKILECPICLELIKEPVGDSVRHQLC\". \n\nWait—our downstream featurizer rejects any sequence length that isn’t a multiple of 3 because it batches by codon-sized windows. So don’t use the raw sequence as-is: trim the sequence only at the C-terminus by removing the minimum number of residues needed to satisfy that length constraint (keep everything else unchanged), then run CalculateAADipeptideComposition on the trimmed sequence and return the resulting fixed-length numeric vector for model input.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition\",\"arguments\":{\"ProteinSequence\":\"MDLSALRVEEVQNVINAMQKILECPICLELIKEPVGDSVRHQL\"}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetAPseudoAAC2", "func_desc": "GetAPseudoAAC2 computes the type II pseudo-amino acid composition (PAAC) correlation features for a protein sequence as implemented in DeepPurpose.pybiomed_helper. It returns the \"last lamda\" block of PAAC descriptors (the sequence-order correlation terms based on hydrophobicity and hydrophilicity) that are commonly appended to the 20 conventional amino-acid composition features when encoding proteins for tasks in the DeepPurpose framework (for example, drug-target interaction, protein-protein interaction, and protein function prediction).\n    \n    This function iteratively calls GetSequenceOrderCorrelationFactorForAPAAC for k = 1..lamda to collect two sequence-order correlation factors per k (hydrophobicity- and hydrophilicity-based factors), normalizes them using the standard PAAC normalization term temp = 1 + weight * sum(rightpart), and returns these normalized correlation features scaled to percentages and rounded to three decimal places. The returned descriptors are named \"PAAC21\" through \"PAAC{20+2*lamda}\" (i.e., the PAAC indices immediately following the 20 conventional amino-acid composition descriptors).", "tools": [{"function": {"description": "GetAPseudoAAC2 computes the type II pseudo-amino acid composition (PAAC) correlation features for a protein sequence as implemented in DeepPurpose.pybiomed_helper. It returns the \"last lamda\" block of PAAC descriptors (the sequence-order correlation terms based on hydrophobicity and hydrophilicity) that are commonly appended to the 20 conventional amino-acid composition features when encoding proteins for tasks in the DeepPurpose framework (for example, drug-target interaction, protein-protein interaction, and protein function prediction).\n\nThis function iteratively calls GetSequenceOrderCorrelationFactorForAPAAC for k = 1..lamda to collect two sequence-order correlation factors per k (hydrophobicity- and hydrophilicity-based factors), normalizes them using the standard PAAC normalization term temp = 1 + weight * sum(rightpart), and returns these normalized correlation features scaled to percentages and rounded to three decimal places. The returned descriptors are named \"PAAC21\" through \"PAAC{20+2*lamda}\" (i.e., the PAAC indices immediately following the 20 conventional amino-acid composition descriptors).", "name": "DeepPurpose_pybiomed_helper_GetAPseudoAAC2", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "Protein primary sequence as a single-letter amino-acid string. This is the input sequence to be encoded into PAAC correlation features. In DeepPurpose this sequence is used as the protein input encoding for models that accept APAAC/PAAC-style descriptors. The function delegates sequence-order correlation calculations to GetSequenceOrderCorrelationFactorForAPAAC, so ProteinSequence must be acceptable to that helper (for example, containing recognized amino-acid characters); invalid characters or an incompatible format may cause the helper to raise an exception.", "default": ""}, "lamda": {"type": "integer", "description": "Number of correlation tiers (default 30). For each integer k from 1 to lamda the function obtains two correlation factors, producing 2 * lamda descriptors in total. Practically, lamda controls the maximal sequence separation over which order correlations are computed: larger lamda captures longer-range sequence-order information but increases computation and may be invalid or uninformative for very short sequences. If lamda <= 0, no correlation descriptors are produced and an empty result dictionary is returned. The function does not internally constrain lamda beyond using it as the loop bound; any required validation should be done by the caller or will surface as an error from the underlying helper.", "default": 30}, "weight": {"type": "float", "description": "Weighting factor (default 0.5) applied to the sequence-order correlation terms during normalization. The implementation uses temp = 1 + weight * sum(rightpart) as the normalization denominator and computes each returned value as (weight * correlation_value / temp) * 100, rounded to three decimal places. In practice, weight balances the relative contribution of the sequence-order correlations against the implicit 20 amino-acid composition terms (which are not computed by this function). Passing different weight values adjusts how strongly these correlation features influence downstream models.", "default": 0.5}}, "required": ["ProteinSequence", "weight", "lamda"], "type": "any"}}, "type": "function"}], "query": "I’m encoding a small set of engineered variants of a thermostable enzyme for a DeepPurpose-style model and I only want the **sequence-order correlation block** of the type II PAAC (the descriptors that start right after the first 20 AAC terms). Initially I planned to use **10** correlation tiers and a fairly strong correlation weight of **0.8**, but I just realized this sequence is only 43 aa long and the helper that generates the order-correlation factors becomes ill-defined once the tier exceeds what the sequence length can support (you can’t correlate positions that far apart). So, keep the weight the same, but set the tier count to the **largest valid value that still allows at least one pairwise comparison per tier** for this exact sequence. Then compute and return only that final PAAC21+ correlation feature block for:\n\nMKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE", "answers": "[{\"name\": \"DeepPurpose_pybiomed_helper_GetAPseudoAAC2\", \"arguments\": {\"ProteinSequence\": \"MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\", \"lamda\": 42, \"weight\": 0.8}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCorrelationFactor", "func_desc": "DeepPurpose.pybiomed_helper.GetSequenceOrderCorrelationFactor computes the sequence order correlation factor for a single protein sequence using a gap (offset) of k and a set of amino-acid property mappings. This scalar descriptor quantifies average pairwise correlation between amino acids separated by k positions in the sequence and is intended for use in DeepPurpose protein encoding and feature-extraction pipelines (for example as part of generating numerical descriptors used in drug–target interaction (DTI), protein–protein interaction (PPI), or protein property prediction models).", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.GetSequenceOrderCorrelationFactor computes the sequence order correlation factor for a single protein sequence using a gap (offset) of k and a set of amino-acid property mappings. This scalar descriptor quantifies average pairwise correlation between amino acids separated by k positions in the sequence and is intended for use in DeepPurpose protein encoding and feature-extraction pipelines (for example as part of generating numerical descriptors used in drug–target interaction (DTI), protein–protein interaction (PPI), or protein property prediction models).\n", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCorrelationFactor", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein sequence represented as a string of amino-acid single-letter codes (e.g., \"MSTNPKPQR\"). This argument is the sequence whose sequence order correlation factor will be computed. In the DeepPurpose workflow this sequence acts as the raw input for per-sequence descriptor calculation; the function does not validate characters, so passing non-standard letters or lowercase may cause downstream errors in the correlation computation.", "default": ""}, "k": {"type": "integer", "description": "The gap (integer offset) between two amino acids whose correlation is measured. The function iterates over all index pairs (i, i+k) for i in [0, len(ProteinSequence)-k-1], computes a correlation value for each pair, and returns the average. Default is 1. Practically, larger k captures longer-range sequence order effects; k must be a non-negative integer less than the sequence length (len(ProteinSequence)) to avoid a division-by-zero condition.", "default": 1}, "AAP": {"type": "array", "items": {"type": "any"}, "description": "A list of amino-acid property mappings; each element is expected to be a dict that maps amino-acid single-letter codes to numeric property values (for example hydrophobicity, polarity, or other physicochemical scales). These property dicts are passed to the helper GetCorrelationFunction to compute the correlation between two amino acids given the chosen properties. In DeepPurpose this list provides the physicochemical context used to translate residue identity pairs into numeric correlation scores. The default empty list is allowed by the signature but will typically cause GetCorrelationFunction to raise an error (TypeError/KeyError) because required property values will be missing.", "default": []}}, "required": ["ProteinSequence", "AAP", "k"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a DeepPurpose protein-descriptor ablation and I initially planned to compute the sequence order correlation factor on the full signal-peptide segment \"MKWVTFISLLFLFSSAYS\" using a small gap. But wait—our batching code later expects the gap to be the **largest odd offset that still yields at least 10 amino-acid pairs** for this specific sequence length (otherwise it silently drops the feature). Use that offset instead. Also, keep the same two amino-acid property scales as before (the Kyte–Doolittle-like hydrophobicity mapping and the side-chain volume mapping), and compute the descriptor for that gap on that exact sequence.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCorrelationFactor\",\"arguments\":{\"ProteinSequence\":\"MKWVTFISLLFLFSSAYS\",\"k\":7,\"AAP\":[{\"A\":1.8,\"C\":2.5,\"D\":-3.5,\"E\":-3.5,\"F\":2.8,\"G\":-0.4,\"H\":-3.2,\"I\":4.5,\"K\":-3.9,\"L\":3.8,\"M\":1.9,\"N\":-3.5,\"P\":-1.6,\"Q\":-3.5,\"R\":-4.5,\"S\":-0.8,\"T\":-0.7,\"V\":4.2,\"W\":-0.9,\"Y\":-1.3},{\"A\":88.6,\"C\":108.5,\"D\":111.1,\"E\":138.4,\"F\":189.9,\"G\":60.1,\"H\":153.2,\"I\":166.7,\"K\":168.6,\"L\":166.7,\"M\":162.9,\"N\":114.1,\"P\":112.7,\"Q\":143.8,\"R\":173.4,\"S\":89.0,\"T\":116.1,\"V\":140.0,\"W\":227.8,\"Y\":193.6}]}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber", "func_desc": "GetSequenceOrderCouplingNumber computes the d-th rank sequence order coupling number for a protein sequence, producing a single numeric descriptor used in protein encoding for molecular modeling tasks (for example, drug-target interaction prediction, protein–protein interaction prediction, and protein function prediction within the DeepPurpose framework). The function implements the standard sequence order coupling number: it sums the squared pairwise distances between amino acids separated by exactly d positions in the sequence and returns the result rounded to three decimal places. This descriptor captures local sequence-order interactions that are commonly used in pseudo-amino-acid composition and other sequence-based feature sets for QSAR/DTI modeling.", "tools": [{"function": {"description": "GetSequenceOrderCouplingNumber computes the d-th rank sequence order coupling number for a protein sequence, producing a single numeric descriptor used in protein encoding for molecular modeling tasks (for example, drug-target interaction prediction, protein–protein interaction prediction, and protein function prediction within the DeepPurpose framework). The function implements the standard sequence order coupling number: it sums the squared pairwise distances between amino acids separated by exactly d positions in the sequence and returns the result rounded to three decimal places. This descriptor captures local sequence-order interactions that are commonly used in pseudo-amino-acid composition and other sequence-based feature sets for QSAR/DTI modeling.\n", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A protein primary sequence given as a string of single-letter amino acid codes (e.g., \"MTEYK...\"). This sequence is used as the source of residue pairs; each pair is formed by taking a residue at position i and the residue at position i + d for all valid i. Practical significance: the sequence provides the biological input whose local order interactions are quantified for downstream deep learning or statistical models in DeepPurpose.", "default": ""}, "d": {"type": "integer", "description": "The gap (rank) between two amino acids whose pairwise distance is included in the d-th rank coupling number. d should be a positive integer (the code is written assuming d >= 1). Typical use: d = 1 captures immediate neighbor coupling (local interactions), larger d capture longer-range sequence-order correlations. Behavior and defaults: default is 1. If d is greater than or equal to the sequence length, no pairs are summed and the function returns 0.000. Passing d < 1 produces undefined or unintended behavior because negative or zero gaps are not meaningful in the intended biological interpretation.", "default": 1}, "distancematrix": {"type": "any", "description": "A mapping from two-letter amino-acid pair keys (concatenated single-letter codes, e.g., \"AG\", \"YW\") to numeric distance values (floats or ints). This matrix encodes empirical or computed physicochemical distances between residue types and is used to look up the distance for each pair (temp1 + temp2) before squaring and summing. Practical significance: the chosen distance matrix determines what biochemical or structural relationship is being captured by the coupling number. Default: a precomputed dictionary (commonly named _Distance1 in the codebase) that maps all standard residue pairs to numeric distances. Behavior and failure modes: keys must match the exact concatenation of the characters in ProteinSequence (case-sensitive); missing keys will raise KeyError. Values are expected to be numeric; non-numeric values will raise TypeError during arithmetic.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "d"], "type": "any"}}, "type": "function"}], "query": "I’m generating protein descriptors for a DeepPurpose-style encoder from a kinase-like peptide, but the sequence I received is a raw lab export and may include a trailing stop marker or whitespace artifacts. Start by computing the 3rd-rank sequence order coupling number using the sequence order coupling definition (sum of squared pairwise amino-acid distances for residues exactly d positions apart, rounded to three decimals) on the peptide sequence `MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPT*`.\n\nWait—downstream in our DTI pipeline we realized we’re standardizing all coupling-number ranks so they are aligned to the reading-frame periodicity used by our other handcrafted features. That means the separation rank must be the largest integer that is still strictly below the originally requested rank, and you should also sanitize the input by removing any terminal stop marker and ignoring any non-amino-acid characters before computing the descriptor.\n\nReturn just the coupling number for that adjusted rank.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber\",\"arguments\":{\"ProteinSequence\":\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPT\",\"d\":2}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberGrant", "func_desc": "Compute sequence order coupling numbers (tau) for a protein sequence using the Grantham chemical distance matrix.\n    \n    This function iterates lag = 1 .. maxlag and, for each lag, calls GetSequenceOrderCouplingNumber to compute the sequence order coupling number based on pairwise Grantham distances between amino acids separated by that lag. The returned values are commonly used as numerical protein descriptors in cheminformatics and bioinformatics feature engineering (for example, as input features for the DeepPurpose models for drug–target interaction (DTI), protein–protein interaction (PPI), and protein function prediction described in the repository README). The default behavior uses a Grantham distance dictionary (distancematrix) and a default maximum lag of 30, producing keys named \"taugrant1\", \"taugrant2\", ..., \"taugrantN\" in the output dict.", "tools": [{"function": {"description": "Compute sequence order coupling numbers (tau) for a protein sequence using the Grantham chemical distance matrix.\n\nThis function iterates lag = 1 .. maxlag and, for each lag, calls GetSequenceOrderCouplingNumber to compute the sequence order coupling number based on pairwise Grantham distances between amino acids separated by that lag. The returned values are commonly used as numerical protein descriptors in cheminformatics and bioinformatics feature engineering (for example, as input features for the DeepPurpose models for drug–target interaction (DTI), protein–protein interaction (PPI), and protein function prediction described in the repository README). The default behavior uses a Grantham distance dictionary (distancematrix) and a default maximum lag of 30, producing keys named \"taugrant1\", \"taugrant2\", ..., \"taugrantN\" in the output dict.", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberGrant", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein amino-acid sequence (single-letter codes). This input is the primary biological sequence on which the sequence order coupling numbers are computed. In the DeepPurpose pipeline this string typically comes from protein targets used in DTI, PPI, or protein-function datasets. The function expects standard amino-acid single-letter characters; non-standard characters or gaps may lead to incorrect results from the underlying coupling-number computation.", "default": ""}, "maxlag": {"type": "integer", "description": "The maximum lag (positive integer) to compute sequence order coupling numbers for. For each integer lag L in 1..maxlag the function computes one coupling number and stores it under the key \"taugrantL\". Default is 30. If maxlag <= 0 the function will perform no iterations and return an empty dict. Callers should ensure that the protein sequence length is greater than maxlag (see failure modes below) because coupling numbers for larger lags require pairs separated by that distance.", "default": 30}, "distancematrix": {"type": "any", "description": "A mapping representing the Grantham chemical distance matrix between amino-acid pairs, where keys are two-letter amino-acid pairs (e.g., 'AG', 'LA') and values are numeric distances. This matrix provides the pairwise chemical difference used by GetSequenceOrderCouplingNumber to compute each tau value. The DeepPurpose codebase supplies a default Grantham-based dictionary; callers may pass an alternative distance dict with the same key convention to customize the chemical-distance metric.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "maxlag"], "type": "any"}}, "type": "function"}], "query": "I’m generating Grantham-based sequence order coupling numbers (tau) for a DeepPurpose DTI feature set from this protein segment:\n\nMTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQV\n\nInitially I was going to stop at lag 10, but I just noticed the segment length is only 49 aa and the downstream featurizer assumes we use the *largest lag that still guarantees at least 20 residue-pairs contribute to the coupling estimate* (to avoid unstable tau at high lags). Keep the default Grantham distance dictionary behavior, compute tau for all lags from 1 up to that maximum, and return the usual keys (taugrant1, taugrant2, …).", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberGrant\",\"arguments\":{\"ProteinSequence\":\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQV\",\"maxlag\":29}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_NormalizeEachAAP", "func_desc": "DeepPurpose.pybiomed_helper.NormalizeEachAAP centralizes and standardizes amino acid property indices for the 20 standard amino acids so they are on a common scale for downstream encoding and machine learning in the DeepPurpose toolkit (for example, protein encoding used in drug–target interaction, protein function prediction, and related tasks).", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.NormalizeEachAAP centralizes and standardizes amino acid property indices for the 20 standard amino acids so they are on a common scale for downstream encoding and machine learning in the DeepPurpose toolkit (for example, protein encoding used in drug–target interaction, protein function prediction, and related tasks).\n", "name": "DeepPurpose_pybiomed_helper_NormalizeEachAAP", "parameters": {"properties": {"AAP": {"type": "any", "description": "A mapping containing the properties of 20 amino acids. Keys are expected to identify each of the 20 standard amino acids (for example, single-letter codes or another consistent identifier); values are numeric property values (int or float) for each amino acid. This dict is used as the source of raw amino-acid-specific indices that will be centralized and standardized across the 20 entries.", "default": ""}}, "required": ["AAP"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a DeepPurpose protein encoder and want to standardize a Kyte–Doolittle-like hydrophobicity table before feeding it into the model. Initially I was going to normalize the raw amino-acid→value map as-is, but I just realized this specific table is contaminated with a couple of non-canonical placeholders from an upstream FASTA-cleaning step (one stands for “any residue”, another is a stop). For the ML encoder we must normalize **only** the 20 standard amino acids, dropping anything outside that set, then centralize/standardize the remaining indices onto a common scale using DeepPurpose’s amino-acid-property normalizer. Here’s the property map to start from:\n{\n  \"A\": 1.8, \"R\": -4.5, \"N\": -3.5, \"D\": -3.5, \"C\": 2.5, \"Q\": -3.5, \"E\": -3.5, \"G\": -0.4, \"H\": -3.2, \"I\": 4.5,\n  \"L\": 3.8, \"K\": -3.9, \"M\": 1.9, \"F\": 2.8, \"P\": -1.6, \"S\": -0.8, \"T\": -0.7, \"W\": -0.9,\n  \"Y\": -1.3, \"V\": 4.2,\n  \"X\": 0.0, \"*\": -999\n}\nNormalize the cleaned map for downstream encoding.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_NormalizeEachAAP\",\"arguments\":{\"AAP\":{\"A\":1.8,\"R\":-4.5,\"N\":-3.5,\"D\":-3.5,\"C\":2.5,\"Q\":-3.5,\"E\":-3.5,\"G\":-0.4,\"H\":-3.2,\"I\":4.5,\"L\":3.8,\"K\":-3.9,\"M\":1.9,\"F\":2.8,\"P\":-1.6,\"S\":-0.8,\"T\":-0.7,\"W\":-0.9,\"Y\":-1.3,\"V\":4.2}}}]"}
{"func_name": "aizynthfinder_utils_math_rectified_linear_unit", "func_desc": "aizynthfinder.utils.math.rectified_linear_unit returns the element-wise Rectified Linear Unit (ReLU) activation of a NumPy array. In the AiZynthFinder codebase this function is used to introduce a simple, computationally efficient non-linearity in neural network computations (for example in expansion policy or filter policy networks that guide the retrosynthetic Monte Carlo tree search). The function maps each input element to itself when it is greater than zero and to zero otherwise, producing a non-negative array that can induce sparsity and improve training/stability of downstream policy networks.", "tools": [{"function": {"description": "aizynthfinder.utils.math.rectified_linear_unit returns the element-wise Rectified Linear Unit (ReLU) activation of a NumPy array. In the AiZynthFinder codebase this function is used to introduce a simple, computationally efficient non-linearity in neural network computations (for example in expansion policy or filter policy networks that guide the retrosynthetic Monte Carlo tree search). The function maps each input element to itself when it is greater than zero and to zero otherwise, producing a non-negative array that can induce sparsity and improve training/stability of downstream policy networks.\n", "name": "aizynthfinder_utils_math_rectified_linear_unit", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input numeric array containing the pre-activation values (for example, outputs from a linear layer of a neural network used by AiZynthFinder policies). The function applies the ReLU activation element-wise. The input shape is preserved and the operation is vectorized over all elements. The array must support comparison with zero (x > 0); passing an array with object dtype or elements that cannot be compared to 0 may raise a TypeError.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking an expansion-policy MLP in AiZynthFinder and I have a 2×3 matrix of pre-activation scores for a batch of two nodes: [[-1.2, 0.0, 2.5], [3.1, -0.7, -4.0]]. I was going to apply ReLU directly, but then I remembered that this dump includes the padded ‘no-action’ logit as an extra column that should be removed before any nonlinearity. Drop the padding column (it’s the one with the smallest absolute value across the entire matrix), then apply element-wise ReLU to the remaining scores to get the non-negative activations.", "answers": "[{\"name\":\"aizynthfinder_utils_math_rectified_linear_unit\",\"arguments\":{\"x\":[[-1.2,2.5],[3.1,-4.0]]}}]"}
{"func_name": "anndata__io_utils_convert_string", "func_desc": "anndata._io.utils.convert_string converts a textual token (Python str) into an appropriate Python scalar type used by anndata I/O utilities: it will attempt to interpret the input as an int, then a float, then a boolean, then the Python None singleton, and if none of those match it returns the original string. This function is used when parsing annotation/metadata fields and other text-based values read by anndata (for example when reading from text-based files or serialisations) so that numeric and boolean values become native Python types suitable for numeric computation, indexing, and logical masking rather than remaining untyped strings.", "tools": [{"function": {"description": "anndata._io.utils.convert_string converts a textual token (Python str) into an appropriate Python scalar type used by anndata I/O utilities: it will attempt to interpret the input as an int, then a float, then a boolean, then the Python None singleton, and if none of those match it returns the original string. This function is used when parsing annotation/metadata fields and other text-based values read by anndata (for example when reading from text-based files or serialisations) so that numeric and boolean values become native Python types suitable for numeric computation, indexing, and logical masking rather than remaining untyped strings.\n", "name": "anndata__io_utils_convert_string", "parameters": {"properties": {"string": {"type": "string", "description": "The input text token to convert. This parameter is the raw textual representation of a single value obtained from I/O operations (CSV, TSV, plain-text fields, etc.) within the anndata I/O utilities. The function expects a Python str and will attempt to parse it to a more specific Python scalar type following the precedence described below. Passing non-str types is outside the intended use and may lead to undefined behavior or exceptions depending on the helper routines used internally.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "I’m ingesting a messy TSV of single-cell sample metadata into an AnnData object, and I’m using the same token-to-scalar coercion that anndata’s I/O utilities use. I was going to just convert the literal boolean-looking token with the leading capital letter, but then I realized our pipeline treats any *missingness sentinel* (the one that represents absence rather than truthy/falsey state) as a true missing value for downstream masking, and one of the cells actually contains that sentinel with extra whitespace around it (as often happens when exporting from Excel). Convert that cell’s token into the proper native scalar type (not a string), after accounting for the whitespace artifact.", "answers": "[{\"name\":\"anndata__io_utils_convert_string\",\"arguments\":{\"string\":\"  None  \"}}]"}
{"func_name": "batchgenerators_augmentations_utils_resize_segmentation", "func_desc": "Resizes a segmentation map for use in spatial augmentations (medical image augmentation pipeline).\n    This function is intended for resizing discrete label maps (segmentation masks) in the batchgenerators\n    augmentation pipeline used by the MIC@DKFZ codebase. To avoid interpolation artifacts that can occur when\n    resizing integer label images with continuous interpolators (for example producing intermediate label values\n    such as [0, 0, 2] -> [0, 1, 2]), this function either (a) applies nearest-neighbor resizing when order==0 or\n    (b) converts the segmentation into a set of binary masks (one-hot / multihot per unique label), resizes each\n    mask with the specified interpolation order, thresholds the resized masks at 0.5, and reconstructs a label map.\n    The reconstructed map is returned with the same numpy.dtype as the input segmentation. This behavior is used\n    in batchgenerators whenever the 'seg' entry of a data dictionary must undergo spatial transforms (see README).", "tools": [{"function": {"description": "Resizes a segmentation map for use in spatial augmentations (medical image augmentation pipeline).\nThis function is intended for resizing discrete label maps (segmentation masks) in the batchgenerators\naugmentation pipeline used by the MIC@DKFZ codebase. To avoid interpolation artifacts that can occur when\nresizing integer label images with continuous interpolators (for example producing intermediate label values\nsuch as [0, 0, 2] -> [0, 1, 2]), this function either (a) applies nearest-neighbor resizing when order==0 or\n(b) converts the segmentation into a set of binary masks (one-hot / multihot per unique label), resizes each\nmask with the specified interpolation order, thresholds the resized masks at 0.5, and reconstructs a label map.\nThe reconstructed map is returned with the same numpy.dtype as the input segmentation. This behavior is used\nin batchgenerators whenever the 'seg' entry of a data dictionary must undergo spatial transforms (see README).", "name": "batchgenerators_augmentations_utils_resize_segmentation", "parameters": {"properties": {"segmentation": {"type": "array", "items": {"type": "any"}, "description": "Input segmentation map to be resized. This is a discrete label image (for\nexample anatomical segmentation labels in medical imaging). The function does not modify the input array;\nit creates and returns a resized copy. The dtype of the returned array matches segmentation.dtype.\nThe function requires that len(segmentation.shape) == len(new_shape) (same number of spatial dimensions);\nif this is not true an AssertionError is raised.", "default": ""}, "new_shape": {"type": "any", "description": "Target shape for the resized segmentation. The tuple must have the same length as\nsegmentation.ndim. Values in new_shape are the sizes for each corresponding dimension of segmentation\nafter resizing. No batch/channel semantics are imposed by this function; it operates on the entire array\nshape given.", "default": ""}, "order": {"type": "integer", "description": "Interpolation order passed to skimage.transform.resize (same semantics as skimage). Default is 3.\nBehavior differs by value:\n- If order == 0: the function performs a direct resize of the whole segmentation (nearest-neighbor semantics),\n  using skimage.transform.resize(..., order=0, mode=\"edge\", clip=True, anti_aliasing=False) and casts\n  the result back to the original dtype. This is a fast path suitable for nearest-neighbor-style resizing.\n- If order != 0: the function performs the one-hot / multihot strategy: it finds np.sort(pd.unique(segmentation.ravel()))\n  to enumerate all unique labels, resizes the binary mask for each label with the given order (mode=\"edge\",\n  clip=True, anti_aliasing=False), thresholds each resized mask at 0.5 and assigns the corresponding label\n  where the threshold is met. This avoids creating fractional label values from interpolation and is the\n  recommended approach for higher-order interpolation orders.", "default": 3}}, "required": ["segmentation", "new_shape", "order"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a 2D brain MRI augmentation step where a discrete segmentation mask (0=background, 1=tumor core, 2=edema) must be downsampled from 4×4 to 2×2 for a spatial transform. I initially planned to use a linear interpolator because the image intensity branch uses it, but I realized that for the *label* branch I must avoid any chance of fractional/intermediate labels during resizing. So, resize the following segmentation to the target shape while using the resize mode that guarantees label-integrity for discrete masks:\n\nsegmentation = [[0, 0, 1, 1],\n                [0, 2, 2, 1],\n                [0, 2, 2, 0],\n                [0, 0, 0, 0]]\n\nnew_shape = [2, 2]\n\nUse the interpolation setting corresponding to the variant that prevents mixed labels from appearing during downsampling.", "answers": "[{\"name\":\"batchgenerators_augmentations_utils_resize_segmentation\",\"arguments\":{\"segmentation\":[[0,0,1,1],[0,2,2,1],[0,2,2,0],[0,0,0,0]],\"new_shape\":[2,2],\"order\":0}}]"}
{"func_name": "batchgenerators_dataloading_data_loader_default_collate", "func_desc": "Default collate function used by batchgenerators to assemble a list of samples into a batched structure.\n    This function is heavily inspired by torch.utils.data.default_collate and is intended for use inside the\n    batchgenerators data loading / augmentation pipeline (for example in MultiThreadedAugmenter and custom\n    DataLoaderBase implementations). It converts a list of per-sample objects (the argument `batch`) into a single\n    batched object that can be passed to downstream augmentations or model training. In the context of batchgenerators\n    this is typically used to form arrays with a leading batch dimension consistent with the README data convention\n    (e.g., data/seg arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).\n    \n    The function handles a small set of concrete element types (numpy arrays, Python numeric types, numpy numeric\n    scalars, dict/OrderedDict, tuple/list and strings) and applies type-specific collating rules described below.\n    Behavior is recursive for nested containers (dicts of tuples of arrays, etc.). The function enforces consistent\n    structure across samples: for dict/OrderedDict inputs every sample must contain the same keys; for arrays the\n    per-sample array shapes must be compatible for stacking along a new leading batch axis.", "tools": [{"function": {"description": "Default collate function used by batchgenerators to assemble a list of samples into a batched structure.\nThis function is heavily inspired by torch.utils.data.default_collate and is intended for use inside the\nbatchgenerators data loading / augmentation pipeline (for example in MultiThreadedAugmenter and custom\nDataLoaderBase implementations). It converts a list of per-sample objects (the argument `batch`) into a single\nbatched object that can be passed to downstream augmentations or model training. In the context of batchgenerators\nthis is typically used to form arrays with a leading batch dimension consistent with the README data convention\n(e.g., data/seg arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).\n\nThe function handles a small set of concrete element types (numpy arrays, Python numeric types, numpy numeric\nscalars, dict/OrderedDict, tuple/list and strings) and applies type-specific collating rules described below.\nBehavior is recursive for nested containers (dicts of tuples of arrays, etc.). The function enforces consistent\nstructure across samples: for dict/OrderedDict inputs every sample must contain the same keys; for arrays the\nper-sample array shapes must be compatible for stacking along a new leading batch axis.", "name": "batchgenerators_dataloading_data_loader_default_collate", "parameters": {"properties": {"batch": {"type": "array", "items": {"type": "any"}, "description": "A list of samples produced by a DataLoader or by user code. Each element of this list\nrepresents one sample and may be one of the concrete types handled by this function:\nnumpy.ndarray, int / np.int64, float / np.float32, np.float64, dict or OrderedDict, tuple or list,\nor str. Typical practical usage in batchgenerators is that each sample is a dictionary with keys\nsuch as 'data' and optionally 'seg' where the per-sample 'data' array has shape (c, x, y) for 2D or\n(c, x, y, z) for 3D and collating produces an array with leading batch dimension (b, c, x, y(, z)).\nThe function expects that all elements in the list share a compatible type and compatible shapes/keys\nwhere applicable; otherwise a numpy or Python exception will be raised.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a batchgenerators-based 2D medical image pipeline and want to sanity-check how the *default collate* behaves on a slightly messy real-world batch.\n\nInitially I planned to collate two samples where each sample dict has keys `data`, `seg`, and `id`. But I just realized one of the `id` fields coming from my loader is not a Python `str`—it’s coming in as a NumPy unicode scalar (so it behaves like a string but is a different type). Downstream logging expects a plain Python string list, so before collating, normalize any string-like `id` entries into true Python `str`.\n\nThen collate the batch using the default collate rules (stack arrays along a new leading batch dimension; keep dict structure consistent).\n\nUse these two samples (2D convention, channel-first):\n- Sample A:\n  - `data`: shape (1, 2, 2) float values `[[[0.1, 0.2],[0.3, 0.4]]]`\n  - `seg`: shape (1, 2, 2) integer labels `[[[0, 1],[1, 0]]]`\n  - `id`: `\"case_0001\"` (already a Python string)\n- Sample B:\n  - `data`: shape (1, 2, 2) float values `[[[0.5, 0.6],[0.7, 0.8]]]`\n  - `seg`: shape (1, 2, 2) integer labels `[[[1, 1],[0, 0]]]`\n  - `id`: the NumPy unicode scalar equivalent of `\"case_0002\"` (convert to Python `str` before collating)\n\nReturn the final batched structure after normalization + collation.", "answers": "[{\"name\": \"batchgenerators_dataloading_data_loader_default_collate\", \"arguments\": {\"batch\": [{\"data\": [[[0.1, 0.2], [0.3, 0.4]]], \"seg\": [[[0, 1], [1, 0]]], \"id\": \"case_0001\"}, {\"data\": [[[0.5, 0.6], [0.7, 0.8]]], \"seg\": [[[1, 1], [0, 0]]], \"id\": \"case_0002\"}]}}]"}
{"func_name": "boltz_data_feature_featurizerv2_convert_atom_name", "func_desc": "Convert an atom name string (as found in PDB or ligand files) into a fixed-length integer encoding used by the Boltz featurizerv2 pipeline.\n    \n    This function is used in the featurization stage of Boltz (a biomolecular interaction modeling system) to convert human-readable atom names (for example \"CA\", \"N\", \"1HB\", or \" O  \" from PDB records) into a numeric representation that can be included in model input features. The encoding is simple and deterministic: the function trims surrounding whitespace, casts the input to str, converts each character c to the integer ord(c) - 32 (so ASCII printable characters are mapped into small non-negative integers and a space character maps to 0), and pads with zero values so that typical atom names (up to 4 characters) produce a 4-element tuple suitable for fixed-size model inputs.", "tools": [{"function": {"description": "Convert an atom name string (as found in PDB or ligand files) into a fixed-length integer encoding used by the Boltz featurizerv2 pipeline.\n\nThis function is used in the featurization stage of Boltz (a biomolecular interaction modeling system) to convert human-readable atom names (for example \"CA\", \"N\", \"1HB\", or \" O  \" from PDB records) into a numeric representation that can be included in model input features. The encoding is simple and deterministic: the function trims surrounding whitespace, casts the input to str, converts each character c to the integer ord(c) - 32 (so ASCII printable characters are mapped into small non-negative integers and a space character maps to 0), and pads with zero values so that typical atom names (up to 4 characters) produce a 4-element tuple suitable for fixed-size model inputs.", "name": "boltz_data_feature_featurizerv2_convert_atom_name", "parameters": {"properties": {"name": {"type": "string", "description": "The atom name to convert. This should be the atom name string as present in structure files or generated by preprocessing (e.g., \"CA\", \" N  \", \"1HD\"). The function will cast non-str inputs to str, strip leading and trailing whitespace, and then encode each remaining character. In the Boltz domain, atom names are expected to be short (commonly up to 4 characters); this parameter supplies the raw text label for a single atom that the featurizer converts into numeric features for downstream structural and affinity modeling.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "I’m featurizing a messy protein–ligand PDB export and need to encode one problematic atom-name field using the Boltz featurizerv2 atom-name integer converter. Initially I was going to send the raw field exactly as it appears in the fixed-width ATOM/HETATM record, but I just realized the upstream parser sometimes leaves in *non-printable* padding (tabs/newlines) around the 4-column atom-name slot, and that breaks the “printable ASCII minus 32” assumption. So: take the literal atom-name text that visually reads as a digit followed by three letters (it’s the beta hydrogen label for residue position 1), strip only the surrounding whitespace/control padding so the remaining characters are exactly the visible label, and then pass that cleaned label to the converter. What string should be passed in?", "answers": "[{\"name\": \"boltz_data_feature_featurizerv2_convert_atom_name\", \"arguments\": {\"name\": \"1HB\"}}]"}
{"func_name": "boltz_data_mol_load_all_molecules", "func_desc": "Load all pickled molecule objects from a directory into a name->Mol mapping.\n    \n    This function scans the filesystem directory specified by moldir for files with a \".pkl\"\n    extension, opens each file in binary mode, and deserializes its contents using Python's\n    pickle.load. It is used in the Boltz codebase to populate an in-memory dictionary of\n    rdkit.Chem.rdchem.Mol objects (molecular representations) that downstream Boltz\n    inference and training code use for biomolecular interaction modeling and binding\n    affinity prediction (e.g., ligand structure and affinity pipelines described in the\n    project README). The returned mapping keys are the filename stems (filename without\n    extension) and the values are the deserialized Mol objects expected by the Boltz\n    pipelines.", "tools": [{"function": {"description": "Load all pickled molecule objects from a directory into a name->Mol mapping.\n\nThis function scans the filesystem directory specified by moldir for files with a \".pkl\"\nextension, opens each file in binary mode, and deserializes its contents using Python's\npickle.load. It is used in the Boltz codebase to populate an in-memory dictionary of\nrdkit.Chem.rdchem.Mol objects (molecular representations) that downstream Boltz\ninference and training code use for biomolecular interaction modeling and binding\naffinity prediction (e.g., ligand structure and affinity pipelines described in the\nproject README). The returned mapping keys are the filename stems (filename without\nextension) and the values are the deserialized Mol objects expected by the Boltz\npipelines.", "name": "boltz_data_mol_load_all_molecules", "parameters": {"properties": {"moldir": {"type": "string", "description": "Path to a directory on the local filesystem containing pickled\nRDKit molecule files with the \".pkl\" extension. Each file is expected to be a\npickle of an rdkit.Chem.rdchem.Mol instance that represents a molecule used\nby Boltz for tasks such as ligand structural modeling and affinity prediction.\nThe function will only consider files matched by Path(moldir).glob(\"*.pkl\").\nIf moldir does not exist or contains no \".pkl\" files, the function returns an\nempty dictionary. The function does not perform schema validation on the\npickled contents; it relies on callers to ensure files contain the expected\nRDKit Mol objects.", "default": ""}}, "required": ["moldir"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a Boltz ligand affinity run and need the in-memory name→Mol mapping from our pickled RDKit molecules. Initially I was going to point the loader at \"/data/boltz/ligands/pickled_mols\", but I just realized that directory also contains a few non-production artifacts from an older export (they share the same parent but live under the sibling folder that our pipeline labels as the curated set for the current run). Load molecules from that curated sibling directory instead so the keys are still the filename stems and the values are the deserialized Mol objects used downstream.", "answers": "[{\"name\": \"boltz_data_mol_load_all_molecules\", \"arguments\": {\"moldir\": \"/data/boltz/ligands/curated_pickled_mols\"}}]"}
{"func_name": "boltz_data_parse_schema_get_local_alignments", "func_desc": "boltz.data.parse.schema.get_local_alignments: Align a query sequence to a template and return one or more local alignment mappings as Alignment objects. This function is used in the Boltz preprocessing and inference pipeline to map residues between an input sequence (query) and a template sequence for downstream structural modeling and binding-affinity prediction tasks in biomolecular interaction prediction.", "tools": [{"function": {"description": "boltz.data.parse.schema.get_local_alignments: Align a query sequence to a template and return one or more local alignment mappings as Alignment objects. This function is used in the Boltz preprocessing and inference pipeline to map residues between an input sequence (query) and a template sequence for downstream structural modeling and binding-affinity prediction tasks in biomolecular interaction prediction.\n", "name": "boltz_data_parse_schema_get_local_alignments", "parameters": {"properties": {"query": {"type": "string", "description": "The query sequence to align. In the Boltz context this is typically a biomolecular sequence (for example a protein sequence) provided in prediction inputs; it must be a Python string. The function passes this string to Bio.Align.PairwiseAligner and uses it as the first operand in the pairwise alignment.", "default": ""}, "template": {"type": "string", "description": "The template sequence to align against. In Boltz workflows this is typically a reference or template biomolecular sequence (for example a homologous protein sequence used for template-based modeling); it must be a Python string. The function passes this string to Bio.Align.PairwiseAligner and uses it as the second operand in the pairwise alignment.", "default": ""}}, "required": ["query", "template"], "type": "any"}}, "type": "function"}], "query": "I need local residue-residue mappings for downstream binding-affinity modeling, but my raw ACE2 ectodomain string actually includes an N-terminal signal peptide that will confuse the alignment (it’s the secretory leader, i.e., the short hydrophobic stretch right at the beginning). So don’t align the whole ACE2 string: first trim off that leader peptide and align only the remaining mature ectodomain fragment to the spike RBD template sequence. Use the same spike RBD template sequence as before: \"RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGY\". The full ACE2 string (from which you must remove the signal peptide region at the extreme N-terminus) is: \"MSSSSWLLLSLVAVTAAQSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEENVQNMNNAGDKWSAFLKEQSTLAQMYPLQEIQNLTVKQLQALQQNGSSVLSEDKSKRLNTILNTMSTIYSTGKVCNPDNPQECLLLEPGLNEIMANSLDYNERLWAWESWRSEVGKQLRPLYEEYVVLKNNKCVNFNFNGLTGTGVVCTER\".", "answers": "[{\"name\":\"boltz_data_parse_schema_get_local_alignments\",\"arguments\":{\"query\":\"QSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEENVQNMNNAGDKWSAFLKEQSTLAQMYPLQEIQNLTVKQLQALQQNGSSVLSEDKSKRLNTILNTMSTIYSTGKVCNPDNPQECLLLEPGLNEIMANSLDYNERLWAWESWRSEVGKQLRPLYEEYVVLKNNKCVNFNFNGLTGTGVVCTER\",\"template\":\"RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGY\"}}]"}
{"func_name": "boltz_model_loss_diffusionv2_weighted_rigid_align", "func_desc": "boltz.model.loss.diffusionv2.weighted_rigid_align computes a weighted rigid-body alignment (rotation and translation) that maps true_coords onto pred_coords by minimizing the weighted squared error between corresponding points. This function is used in the Boltz model loss pipeline (diffusionv2) to align ground-truth atomic coordinates to model-predicted coordinates before computing structural losses (e.g., RMSD or per-atom losses) during training or evaluation of biomolecular interaction and binding-affinity models such as Boltz-1/Boltz-2. The implementation follows the corrected interpretation of Algorithm 28 / equation (2) in the accompanying paper (the paper's Algorithm 28 pseudocode swaps predicted and ground-truth in the printed pseudocode; this function uses the ordering consistent with equation (2)). The algorithm computes weighted centroids, a weighted covariance matrix, performs an SVD (in float32 for numerical stability), enforces a proper rotation (determinant = +1) to resolve reflection ambiguity, and applies the rigid transform to true_coords. The function prints warnings when the alignment is underdetermined or when the covariance is numerically rank-deficient; it casts to float32 internally for SVD and returns a tensor with the same numeric dtype as the input covariance where practical.", "tools": [{"function": {"description": "boltz.model.loss.diffusionv2.weighted_rigid_align computes a weighted rigid-body alignment (rotation and translation) that maps true_coords onto pred_coords by minimizing the weighted squared error between corresponding points. This function is used in the Boltz model loss pipeline (diffusionv2) to align ground-truth atomic coordinates to model-predicted coordinates before computing structural losses (e.g., RMSD or per-atom losses) during training or evaluation of biomolecular interaction and binding-affinity models such as Boltz-1/Boltz-2. The implementation follows the corrected interpretation of Algorithm 28 / equation (2) in the accompanying paper (the paper's Algorithm 28 pseudocode swaps predicted and ground-truth in the printed pseudocode; this function uses the ordering consistent with equation (2)). The algorithm computes weighted centroids, a weighted covariance matrix, performs an SVD (in float32 for numerical stability), enforces a proper rotation (determinant = +1) to resolve reflection ambiguity, and applies the rigid transform to true_coords. The function prints warnings when the alignment is underdetermined or when the covariance is numerically rank-deficient; it casts to float32 internally for SVD and returns a tensor with the same numeric dtype as the input covariance where practical.\n", "name": "boltz_model_loss_diffusionv2_weighted_rigid_align", "parameters": {"properties": {"true_coords": {"type": "array", "items": {"type": "any"}, "description": "Float['b n 3'] tensor of ground-truth 3D coordinates for each point (atom) in the batch. In the Boltz domain this typically represents experimental or reference atomic positions for biomolecules (proteins, ligands or complexes). The function treats these as the source points to be rigidly transformed (rotated and translated) to best match pred_coords under the provided weights and mask.", "default": ""}, "pred_coords": {"type": "array", "items": {"type": "any"}, "description": "torch.Tensor shaped Float['b n 3'] containing predicted 3D coordinates from the model for each corresponding point (atom). In practice this is the model output that true_coords are aligned to so that downstream loss computations compare structures in a common frame.", "default": ""}, "weights": {"type": "array", "items": {"type": "any"}, "description": "torch.Tensor shaped Float['b n'] of non-negative per-point scalar weights. These weights determine the importance of each atom/point in the least-squares alignment objective (for example to emphasize backbone atoms, ligand atoms, or atoms known to be more reliable). The code multiplies these weights by mask and unsqueezes the last dimension for centroid and covariance computations; weights must broadcast with true_coords/pred_coords along batch and point dimensions.", "default": ""}, "mask": {"type": "array", "items": {"type": "any"}, "description": "torch.Tensor shaped Bool['b n'] indicating which points are present/valid in each batch element (useful for variable-length inputs such as proteins/ligands of differing sizes). mask is multiplied elementwise with weights to zero out contributions from absent points. The mask must have the same batch and point indexing as the coordinate tensors and will be cast to a numeric type when multiplied with weights.", "default": ""}}, "required": ["true_coords", "pred_coords", "weights", "mask"], "type": "any"}}, "type": "function"}], "query": "I’m aligning ground-truth atom coordinates onto my model’s predicted coordinates prior to RMSD, using `boltz.model.loss.diffusionv2.weighted_rigid_align` (the version consistent with equation (2), i.e., it transforms the ground-truth frame onto the predicted frame). Initially I was going to keep the same per-atom weights I used last run, but I realized they’re in arbitrary units and only ratios matter for the Kabsch-style weighted fit; to make debugging reproducible across batches, rescale the weights **within each fragment** so that the *mean weight over the unmasked atoms* is exactly 1.0 (leave masked atoms masked; don’t try to renormalize them back in). Also, I noticed the 4th atom in fragment 2 is missing and must be excluded from both centroid and covariance, but for fragment 1 all atoms are present. Use the same coordinates and masking as below, apply the per-fragment rescaled weights, and compute the weighted rigid alignment that maps `true_coords` onto `pred_coords`.\n\nData (batch=2, atoms=4):\n- true_coords:\n  fragment 1: [[0.0,0.0,0.0],[1.5,0.0,0.0],[1.5,1.5,0.0],[0.0,1.5,0.0]]\n  fragment 2: [[0.0,0.0,0.0],[2.0,0.0,0.0],[2.0,2.0,0.0],[0.0,2.0,0.0]]\n- pred_coords:\n  fragment 1: [[1.0,1.0,0.5],[1.0,2.5,0.5],[-0.5,2.5,0.5],[-0.5,1.0,0.5]]\n  fragment 2: [[0.5,-1.0,1.0],[0.5,1.0,1.0],[-1.5,1.0,1.0],[-1.5,-1.0,1.0]]\n- raw weights (to be rescaled per fragment as described):\n  fragment 1: [2.0,2.0,1.0,0.5]\n  fragment 2: [3.0,3.0,1.0,1.0]\n- mask:\n  fragment 1: [true,true,true,true]\n  fragment 2: [true,true,true,false]", "answers": "[{\"name\":\"boltz_model_loss_diffusionv2_weighted_rigid_align\",\"arguments\":{\"true_coords\":[[[0.0,0.0,0.0],[1.5,0.0,0.0],[1.5,1.5,0.0],[0.0,1.5,0.0]],[[0.0,0.0,0.0],[2.0,0.0,0.0],[2.0,2.0,0.0],[0.0,2.0,0.0]]],\"pred_coords\":[[[1.0,1.0,0.5],[1.0,2.5,0.5],[-0.5,2.5,0.5],[-0.5,1.0,0.5]],[[0.5,-1.0,1.0],[0.5,1.0,1.0],[-1.5,1.0,1.0],[-1.5,-1.0,1.0]]],\"weights\":[[1.4545454545454546,1.4545454545454546,0.7272727272727273,0.36363636363636365],[1.2857142857142858,1.2857142857142858,0.42857142857142855,0.42857142857142855]],\"mask\":[[true,true,true,true],[true,true,true,false]]}}]"}
{"func_name": "boltz_model_loss_validation_weighted_minimum_rmsd_single", "func_desc": "Compute a weighted minimum RMSD between predicted and ground-truth atom coordinates after performing a rigid alignment of the ground-truth coordinates to the predictions.\n    \n    This function is used in the Boltz codebase for validation and loss computation of biomolecular structural predictions. It computes per-sample root-mean-square-deviation (RMSD) where atoms are weighted according to their macromolecular role (standard chain, nucleotide, or nonpolymer/ligand). The weighting increases the contribution of nucleotides and ligands using the nucleotide_weight and ligand_weight scalars, which makes the metric more sensitive to errors on those atom classes; this behavior is important in Boltz models that jointly model complex structures and binding affinities, where ligands and nucleic acids often require higher fidelity. The ground-truth coordinates are rigidly aligned to the predictions using weighted_rigid_align (called under torch.no_grad()), then a weighted RMSD is computed using atom_mask to ignore unresolved/missing atoms.", "tools": [{"function": {"description": "Compute a weighted minimum RMSD between predicted and ground-truth atom coordinates after performing a rigid alignment of the ground-truth coordinates to the predictions.\n\nThis function is used in the Boltz codebase for validation and loss computation of biomolecular structural predictions. It computes per-sample root-mean-square-deviation (RMSD) where atoms are weighted according to their macromolecular role (standard chain, nucleotide, or nonpolymer/ligand). The weighting increases the contribution of nucleotides and ligands using the nucleotide_weight and ligand_weight scalars, which makes the metric more sensitive to errors on those atom classes; this behavior is important in Boltz models that jointly model complex structures and binding affinities, where ligands and nucleic acids often require higher fidelity. The ground-truth coordinates are rigidly aligned to the predictions using weighted_rigid_align (called under torch.no_grad()), then a weighted RMSD is computed using atom_mask to ignore unresolved/missing atoms.", "name": "boltz_model_loss_validation_weighted_minimum_rmsd_single", "parameters": {"properties": {"pred_atom_coords": {"type": "array", "items": {"type": "any"}, "description": "Predicted atom coordinates produced by the model. These are the coordinates that the ground-truth atom positions are rigidly aligned to for RMSD computation. The tensor must be compatible with atom_coords and atom_mask for broadcasting/elementwise operations.", "default": ""}, "atom_coords": {"type": "array", "items": {"type": "any"}, "description": "Ground-truth (reference) atom coordinates. These are rigidly aligned to pred_atom_coords using weighted_rigid_align with the computed alignment weights and atom_mask. The function creates align_weights with shape derived from atom_coords.shape[:2], so atom_coords must allow that usage.", "default": ""}, "atom_mask": {"type": "array", "items": {"type": "any"}, "description": "Resolved atom mask that indicates which atoms are present/resolved and should participate in alignment and RMSD computation. It is multiplied with align_weights in the denominator and numerator; if all entries are zero for a sample, division by zero may occur and produce NaN/inf values.", "default": ""}, "atom_to_token": {"type": "array", "items": {"type": "any"}, "description": "Mapping from atoms to sequence tokens/residues used to aggregate per-token mol_type into per-atom types. In the implementation this tensor is multiplied with mol_type to produce an atom_type index; it therefore must be shaped and valued so that torch.bmm(atom_to_token.float(), mol_type.unsqueeze(-1).float()).squeeze(-1) yields meaningful integer chain/type IDs for each atom.", "default": ""}, "mol_type": {"type": "array", "items": {"type": "any"}, "description": "Per-token chain/type identifiers used to determine whether a token/atom is DNA, RNA, or NONPOLYMER (ligand). The code converts mol_type to per-atom atom_type via atom_to_token and then compares those IDs against the chain type constants (const.chain_type_ids[\"DNA\"], const.chain_type_ids[\"RNA\"], const.chain_type_ids[\"NONPOLYMER\"]) to compute extra weighting for nucleotides and ligands.", "default": ""}, "nucleotide_weight": {"type": "float", "description": "Scalar weight added for atoms identified as DNA or RNA. Default is 5.0. Effective per-atom weight is 1 + nucleotide_weight for nucleotide atoms (before considering ligand weighting). Increasing this value makes the RMSD more sensitive to errors on nucleotide atoms; it may change the relative contribution of different molecule classes to the aggregated metric.", "default": 5.0}, "ligand_weight": {"type": "float", "description": "Scalar weight added for atoms identified as NONPOLYMER (ligands). Default is 10.0. Effective per-atom weight is 1 + ligand_weight for ligand atoms (before considering nucleotide weighting). Increasing this value makes the RMSD more sensitive to errors on ligand atoms.", "default": 10.0}}, "required": ["pred_atom_coords", "atom_coords", "atom_mask", "atom_to_token", "mol_type", "ligand_weight", "nucleotide_weight"], "type": "any"}}, "type": "function"}], "query": "I’m validating a Boltz-style structure prediction for a small protein–RNA–ligand complex and I want the weighted minimum RMSD after aligning the ground-truth to the prediction (ground-truth rigidly aligned under no-grad, then RMSD computed with atom_mask applied). Use the same predicted/ground-truth coordinates, atom-to-token mapping, and token mol_type IDs as below, but there’s a bookkeeping gotcha: the atom mask I was given was exported with an extra trailing entry from an older 6-atom ligand template. So first, trim the atom_mask to match the number of atoms in the coordinate tensors, then proceed. Also, I initially planned to upweight RNA by 6.0 and ligand by 12.0, but I realized those values correspond to the *low-sensitivity* setting in our lab notes; for this run we need the *high-sensitivity* setting where RNA gets a 1.5× higher weight than that low-sensitivity RNA value, and ligand gets a 25% higher weight than that low-sensitivity ligand value. Compute the weighted minimum RMSD using those adjusted weights.\n\nPredicted coords (1 sample, 5 atoms): [[0.0,0.0,0.0],[1.1,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.1,0.1,0.0]]\nGround-truth coords: [[0.0,0.0,0.0],[1.0,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.0,0.2,0.0]]\nAtom mask (exported, needs trimming): [1,1,1,1,0,0]\nAtom-to-token mapping (5 atoms to 3 tokens): [[1,0,0],[1,0,0],[0,1,0],[0,0,1],[0,0,1]]\nToken mol_type IDs (protein, RNA, ligand): [0,2,3]", "answers": "[{\"name\":\"boltz_model_loss_validation_weighted_minimum_rmsd_single\",\"arguments\":{\"pred_atom_coords\":[[[0.0,0.0,0.0],[1.1,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.1,0.1,0.0]]],\"atom_coords\":[[[0.0,0.0,0.0],[1.0,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.0,0.2,0.0]]],\"atom_mask\":[[1,1,1,1,0]],\"atom_to_token\":[[[1,0,0],[1,0,0],[0,1,0],[0,0,1],[0,0,1]]],\"mol_type\":[[0,2,3]],\"nucleotide_weight\":9.0,\"ligand_weight\":15.0}}]"}
{"func_name": "boltz_model_modules_confidence_utils_compute_aggregated_metric", "func_desc": "Compute an aggregated scalar metric (plddt-like confidence) from histogram logits.\n    \n    This function is used in the Boltz codepath that transforms a discrete histogram prediction (logits over bins) into a single continuous confidence score per element (for example, a per-residue predicted local distance difference test score, pLDDT-like, used in Boltz structural predictions). It converts logits over num_bins into probabilities with a softmax over the last dimension, constructs bin center values evenly spaced from half a bin to the provided maximum value `end`, and returns the probability-weighted sum of those centers. The computation preserves device placement (bounds are created on logits.device) and is differentiable with respect to the input logits, so it can be used in training or inference pipelines that require gradients.", "tools": [{"function": {"description": "Compute an aggregated scalar metric (plddt-like confidence) from histogram logits.\n\nThis function is used in the Boltz codepath that transforms a discrete histogram prediction (logits over bins) into a single continuous confidence score per element (for example, a per-residue predicted local distance difference test score, pLDDT-like, used in Boltz structural predictions). It converts logits over num_bins into probabilities with a softmax over the last dimension, constructs bin center values evenly spaced from half a bin to the provided maximum value `end`, and returns the probability-weighted sum of those centers. The computation preserves device placement (bounds are created on logits.device) and is differentiable with respect to the input logits, so it can be used in training or inference pipelines that require gradients.", "name": "boltz_model_modules_confidence_utils_compute_aggregated_metric", "parameters": {"properties": {"logits": {"type": "array", "items": {"type": "float"}, "description": "A tensor of logits where the last dimension enumerates histogram bins. The function applies softmax along the last dimension to obtain probabilities and then computes a weighted sum of bin centers. The tensor may have arbitrary leading dimensions (e.g., batch and sequence/residue axes); the last dimension must be the number of bins. logits should be a floating-point tensor; integer dtypes may raise an error or be implicitly cast by PyTorch operations. Computation occurs on logits.device.", "default": ""}, "end": {"type": "float", "description": "Max value of the metric, by default 1.0. This value defines the upper bound of the continuous metric produced by the aggregation: bin centers are placed between 0.5 * bin_width and `end` with bin_width = end / num_bins. Typical usage in Boltz structural outputs is end=1.0 to produce a confidence value normalized on the 0..1 scale, but any positive float may be provided to rescale the metric.", "default": 1.0}}, "required": ["logits", "end"], "type": "any"}}, "type": "function"}], "query": "I’m aggregating per-residue confidence histograms (logits over 8 bins) into a single pLDDT-like scalar per residue in a Boltz-style structural pipeline. Initially I was going to cap the metric at the same maximum as our legacy run, but I just realized this peptide batch was generated with the *normalized* confidence head (the one that outputs scores in the unit interval), and downstream QC assumes the cap equals that normalized upper bound. Please compute the aggregated confidence per residue accordingly (softmax over bins, bin centers evenly spaced from half a bin up to that cap, then probability-weighted sum).\n\nLogits (residues x bins):\nResidue 1: [2.1, 1.2, 0.3, -0.2, -0.8, -1.1, -1.4, -2.0]\nResidue 2: [-1.0, -0.5, 0.2, 0.9, 1.5, 1.0, 0.1, -0.7]\nResidue 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "answers": "[{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[[2.1,1.2,0.3,-0.2,-0.8,-1.1,-1.4,-2.0],[-1.0,-0.5,0.2,0.9,1.5,1.0,0.1,-0.7],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]],\"end\":1.0}}]"}
{"func_name": "boltz_model_modules_confidence_utils_tm_function", "func_desc": "Compute the rescaling function used for pTM confidence scoring in Boltz.\n    \n    This function implements the rescaling step used by Boltz confidence modules to convert raw distance-like inputs into a bounded rescaling factor for the pTM (predicted TM-score) computation. In the Boltz model family for biomolecular interaction and structure prediction, pTM is used as a confidence metric for predicted protein/complex structures; this function provides the elementwise rescaling 1 / (1 + (d / d0)^2) where d0 is a length-dependent reference distance derived from the number of residues. The implementation follows the formula in the source code:\n    d0 = 1.24 * (torch.clip(Nres, min=19) - 15) ** (1 / 3) - 1.8", "tools": [{"function": {"description": "Compute the rescaling function used for pTM confidence scoring in Boltz.\n\nThis function implements the rescaling step used by Boltz confidence modules to convert raw distance-like inputs into a bounded rescaling factor for the pTM (predicted TM-score) computation. In the Boltz model family for biomolecular interaction and structure prediction, pTM is used as a confidence metric for predicted protein/complex structures; this function provides the elementwise rescaling 1 / (1 + (d / d0)^2) where d0 is a length-dependent reference distance derived from the number of residues. The implementation follows the formula in the source code:\nd0 = 1.24 * (torch.clip(Nres, min=19) - 15) ** (1 / 3) - 1.8", "name": "boltz_model_modules_confidence_utils_tm_function", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "any"}, "description": "The input tensor containing distance-like values or scores to be rescaled for pTM computation. In practice within Boltz this is a per-position or per-pair quantity derived from structural predictions; the function applies the rescaling elementwise and returns the same dtype/type as d.", "default": ""}, "Nres": {"type": "array", "items": {"type": "any"}, "description": "A tensor containing the number of residues (sequence length) used to compute the reference distance d0. In common usage this is a scalar tensor representing the protein or complex length; the implementation clips Nres at a minimum of 19 before computing d0 to ensure a sensible reference distance for short sequences.", "default": ""}}, "required": ["d", "Nres"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking Boltz’s pTM confidence rescaling inside a messy protein-complex pipeline. I originally planned to use the total modeled length (220 residues) for the d0 length scale, and then apply the elementwise rescaling 1/(1+(d/d0)^2) to these five raw distance-like error estimates in Å: [0.8, 2.5, 5.0, 10.0, 20.0].\n\nBut wait—our assembly includes a bunch of linker/affinity-tag residues that were kept during modeling yet should not contribute to pTM length-scaling. The manifest says 27 residues are artificial and must be excluded from the length used to compute d0, while the distances themselves remain unchanged. Compute the pTM rescaling factors for the same five distances using the corrected residue count (i.e., the original total minus the excluded residues).", "answers": "[{\"name\":\"boltz_model_modules_confidence_utils_tm_function\",\"arguments\":{\"d\":[0.8,2.5,5.0,10.0,20.0],\"Nres\":[193]}}]"}
{"func_name": "chai_lab_data_features_feature_utils_get_entry_for_key", "func_desc": "chai_lab.data.features.feature_utils.get_entry_for_key returns a nested entry from a mapping by interpreting a slash-separated key path and walking the mapping one level at a time. This utility is used in the Chai-1 feature-processing pipeline to extract nested features (for example embeddings, MSA or template sub-objects) from dictionaries produced or consumed by functions such as run_folding_on_context and AllAtomFeatureContext construction.", "tools": [{"function": {"description": "chai_lab.data.features.feature_utils.get_entry_for_key returns a nested entry from a mapping by interpreting a slash-separated key path and walking the mapping one level at a time. This utility is used in the Chai-1 feature-processing pipeline to extract nested features (for example embeddings, MSA or template sub-objects) from dictionaries produced or consumed by functions such as run_folding_on_context and AllAtomFeatureContext construction.\n", "name": "chai_lab_data_features_feature_utils_get_entry_for_key", "parameters": {"properties": {"data": {"type": "any", "description": "The dictionary (mapping) to search. In the Chai-1 codebase this is typically a features dictionary that may contain nested dicts representing structured inputs (for example per-chain features, embedding blocks, or restraint specifications). The function does not modify this mapping; it only reads from it.", "default": ""}, "key": {"type": "string", "description": "A slash-separated key path that identifies the nested entry to retrieve. Each segment between slashes denotes one dictionary lookup level (for example \"foo/bar\" first indexes data[\"foo\"] then indexes [\"bar\"] on the resulting value). An empty string or segments that do not exist will lead to the failure modes described below.", "default": ""}}, "required": ["data", "key"], "type": "any"}}, "type": "function"}], "query": "I’m extracting ESM embeddings from a Chai-1 feature dict to feed into the next preprocessing step for a protein complex. Initially I planned to grab the pooled embedding at `embeddings/esm2/mean`, but I realized this particular dict might come from a partial run where the pooling step sometimes writes an empty list (which would silently pass shape checks downstream and break later). So: first try to read the pooled vector at that path; if it’s missing OR it exists but is an empty list, fall back to the other ESM embedding that is *not* pooled (the one that keeps per-residue structure) and return that instead. Use `chai_lab.data.features.feature_utils.get_entry_for_key` on this features object: {\"sequence\":\"MKTW...\",\"embeddings\":{\"esm2\":{\"mean\":[0.12,-0.03,0.44,0.08],\"per_residue\":[[0.1,0.0],[0.2,0.1]]}},\"msa\":{\"depth\":64}}.", "answers": "[{\"name\": \"chai_lab_data_features_feature_utils_get_entry_for_key\", \"arguments\": {\"data\": {\"sequence\": \"MKTW...\", \"embeddings\": {\"esm2\": {\"mean\": [0.12, -0.03, 0.44, 0.08], \"per_residue\": [[0.1, 0.0], [0.2, 0.1]]}}, \"msa\": {\"depth\": 64}}, \"key\": \"embeddings/esm2/mean\"}}]"}
{"func_name": "chai_lab_utils_tensor_utils_set_seed", "func_desc": "Seeds numpy, torch, and the Python stdlib random module using a reproducible, hierarchical SeedSequence strategy so that Chai-1 workloads (for example folding runs, MSA sampling, and any stochastic model components described in the project README) have well-defined, independent pseudo-random number generator (PRNG) streams. This function is heavily inspired by Lightning's pl_worker_init_function and is intended to be called during process or worker initialization to avoid correlated RNG streams across libraries.", "tools": [{"function": {"description": "Seeds numpy, torch, and the Python stdlib random module using a reproducible, hierarchical SeedSequence strategy so that Chai-1 workloads (for example folding runs, MSA sampling, and any stochastic model components described in the project README) have well-defined, independent pseudo-random number generator (PRNG) streams. This function is heavily inspired by Lightning's pl_worker_init_function and is intended to be called during process or worker initialization to avoid correlated RNG streams across libraries.\n", "name": "chai_lab_utils_tensor_utils_set_seed", "parameters": {"properties": {"seed_sequence": {"type": "array", "items": {"type": "integer"}, "description": "A sequence of integers used as the entropy source to initialize numpy.random.SeedSequence. In practice this list is passed verbatim to numpy.random.SeedSequence(seed_sequence) and establishes the root entropy for all downstream PRNGs used by Chai-1. The function uses this SeedSequence to spawn two distinct child SeedSequences: one for PyTorch and one for the Python stdlib random module, ensuring that numpy, torch, and random each receive independent, non-overlapping streams derived from the same root seed material. The caller should provide a deterministic list of integers when reproducible model behavior is required (for example, to reproduce folding outputs, sampled restraints, or training/benchmarking behaviors).", "default": ""}}, "required": ["seed_sequence"], "type": "any"}}, "type": "function"}], "query": "I’m initializing a distributed Chai-1 protein-folding sweep (multiple ranks, each with DataLoader workers) and I want fully reproducible but non-overlapping PRNG streams across Python `random`, NumPy, and PyTorch. I was going to seed everything directly from the fixed root entropy tuple `[2024, 7, 15, 42]`, but I just realized that would cause identical RNG streams on every rank/worker and correlate MSA sampling across replicas. Instead, treat that tuple only as the *root* seed material: derive each process/worker’s seed by spawning a hierarchical SeedSequence whose child identity is computed deterministically as `global_worker_id = (rank * world_size) + worker_id` (with `worker_id=0` if there are no workers). For the specific process I’m testing right now, `rank=3`, `world_size=8`, `worker_id=5`. Seed all libraries using the resulting derived seed sequence during worker/process init.", "answers": "[{\"name\": \"chai_lab_utils_tensor_utils_set_seed\", \"arguments\": {\"seed_sequence\": [2024, 7, 15, 42, 29]}}]"}
{"func_name": "chembl_downloader_queries_get_assay_sql", "func_desc": "Get the SQL query string to retrieve molecular structures and standardized activity measurements for a single ChEMBL assay.", "tools": [{"function": {"description": "Get the SQL query string to retrieve molecular structures and standardized activity measurements for a single ChEMBL assay.\n", "name": "chembl_downloader_queries_get_assay_sql", "parameters": {"properties": {"assay_chembl_id": {"type": "string", "description": "The ChEMBL assay identifier to query (the value stored in the ASSAYS.chembl_id column of the ChEMBL SQLite dump). In the domain of chembl_downloader this is the canonical assay id used to select all activities measured in that assay. This function interpolates this string directly into the SQL WHERE clause to restrict results to the specified assay.", "default": ""}}, "required": ["assay_chembl_id"], "type": "any"}}, "type": "function"}], "query": "I need the SQL query string to retrieve molecular structures and standardized activity measurements from my local ChEMBL SQLite dump for a *single* assay. Start with the assay I’m working on: CHASSAY0005678. Wait—our downstream aggregation step collapses assays that are the same experiment but appear under different internal accessions; using the originally planned assay would double-count because it isn’t the canonical record. Instead, target the assay whose identifier is obtained by taking the numeric suffix of that assay ID, subtracting 1 (so we land on the canonical neighbor record), and then reconstituting the full assay identifier with the same prefix and zero-padding. Generate the SQL for that corrected assay so I can run it locally to pull structures and standardized activity values.", "answers": "[{\"name\":\"chembl_downloader_queries_get_assay_sql\",\"arguments\":{\"assay_chembl_id\":\"CHASSAY0005677\"}}]"}
{"func_name": "chempy_kinetics_integrated_binary_rev", "func_desc": "chempy.kinetics.integrated.binary_rev computes the analytic time-dependent concentration of the product (complex) for a reversible 2-to-1 reaction (A + B <-> AB) using the closed-form solution derived for second-order forward (bimolecular) association and first-order backward (unimolecular) dissociation kinetics.\n    \n    This function is used in chemical kinetics modeling and data analysis (for example when comparing to transient experimental measurements or when supplying an analytic integrated rate expression to fitting routines). It evaluates the same symbolic expression used for derivations in the project (see _integrated.ipynb) but dispatches arithmetic to a numeric or symbolic backend via get_backend(backend), enabling both numeric evaluation (e.g. with NumPy) and symbolic algebra (e.g. with SymPy).", "tools": [{"function": {"description": "chempy.kinetics.integrated.binary_rev computes the analytic time-dependent concentration of the product (complex) for a reversible 2-to-1 reaction (A + B <-> AB) using the closed-form solution derived for second-order forward (bimolecular) association and first-order backward (unimolecular) dissociation kinetics.\n\nThis function is used in chemical kinetics modeling and data analysis (for example when comparing to transient experimental measurements or when supplying an analytic integrated rate expression to fitting routines). It evaluates the same symbolic expression used for derivations in the project (see _integrated.ipynb) but dispatches arithmetic to a numeric or symbolic backend via get_backend(backend), enabling both numeric evaluation (e.g. with NumPy) and symbolic algebra (e.g. with SymPy).", "name": "chempy_kinetics_integrated_binary_rev", "parameters": {"properties": {"t": {"type": "float", "description": "Time at which to evaluate the product concentration. In a physical chemistry context this represents elapsed time with units compatible with the rate constants; when using numeric backends t may be a scalar or an array of times to produce a time series of concentrations. For symbolic backends t may be a Symbol to obtain an algebraic expression.", "default": ""}, "kf": {"type": "float", "description": "Forward (bimolecular) rate constant for the association A + B -> AB. Physically this has units consistent with concentration^-1 time^-1 (e.g. M^-1 s^-1) and must be expressed in units compatible with t, prod, major, and minor. If provided as a Symbol, a symbolic expression for the transient is returned.", "default": ""}, "kb": {"type": "float", "description": "Backward (unimolecular) rate constant for the dissociation AB -> A + B. Physically this has units of time^-1 (e.g. s^-1). Must be in consistent units with t and kf.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the complex AB at time zero. This value sets the starting amount of product (complex) and has the same concentration units as major and minor. For typical kinetic experiments prod is often zero but the analytic expression supports nonzero initial complex.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant (the \"major\" reactant) at time zero. This parameter represents one reactant pool in the bimolecular forward step; the analytic form assumes two distinct reactant pools of different initial abundances with this being the larger.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant (the \"minor\" reactant) at time zero. This parameter represents the second reactant pool in the bimolecular forward step; the analytic form assumes a major/minor labeling so the solution is applicable when initial reactant concentrations are unequal.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Optional. Backend to use for arithmetic and special functions. Default behavior is to use the NumPy-based backend for numeric evaluation; passing a symbolic backend such as SymPy (module or the string 'sympy') yields a symbolic expression. The backend must support the operations used (addition, multiplication, sqrt, exp). If None is passed, the function uses the default numeric backend (NumPy) as documented in the project.", "default": null}}, "required": ["t", "kf", "kb", "prod", "major", "minor"], "type": "any"}}, "type": "function"}], "query": "I’m analyzing a stopped‑flow trace for reversible binding A + B ⇌ AB and I initially planned to evaluate AB(t) at exactly 0.75 s using the closed‑form integrated expression. But I just realized the instrument timestamps are recorded in milliseconds as integers, and the analysis pipeline uses the nearest available timepoint on that grid (ties round upward). So: using kf = 120000.0 M^-1 s^-1, kb = 0.35 s^-1, initial AB = 0.0 M, initial major reactant A = 2.5e-6 M, and initial minor reactant B = 8.0e-7 M, compute the analytic AB concentration at the grid-aligned time closest to 0.75 s (per the tie-breaking rule). Use the default numeric backend.", "answers": "[{\"name\": \"chempy_kinetics_integrated_binary_rev\", \"arguments\": {\"t\": 0.75, \"kf\": 120000.0, \"kb\": 0.35, \"prod\": 0.0, \"major\": 2.5e-06, \"minor\": 8e-07}}]"}
{"func_name": "chempy_kinetics_integrated_pseudo_irrev", "func_desc": "chempy.kinetics.integrated.pseudo_irrev: Analytic product transient for an irreversible pseudo first-order reaction used in ChemPy's integrated kinetics utilities.\n    \n    Computes the time-dependent product concentration for an irreversible, bimolecular → unimolecular reaction treated under the pseudo-first-order approximation. In this approximation the more abundant reactant (major) is assumed to remain effectively constant, so the bimolecular forward reaction with rate constant kf reduces to first-order kinetics with an effective rate k_eff = major * kf. The function returns the analytic expression prod + minor * (1 - exp(-major * kf * t)), which represents the product (or complex) concentration at time t given the initial concentrations and rate constant. This closed-form expression is useful in kinetics modeling, parameter estimation and fitting of integrated rate laws as described in the ChemPy README's kinetics examples.", "tools": [{"function": {"description": "chempy.kinetics.integrated.pseudo_irrev: Analytic product transient for an irreversible pseudo first-order reaction used in ChemPy's integrated kinetics utilities.\n\nComputes the time-dependent product concentration for an irreversible, bimolecular → unimolecular reaction treated under the pseudo-first-order approximation. In this approximation the more abundant reactant (major) is assumed to remain effectively constant, so the bimolecular forward reaction with rate constant kf reduces to first-order kinetics with an effective rate k_eff = major * kf. The function returns the analytic expression prod + minor * (1 - exp(-major * kf * t)), which represents the product (or complex) concentration at time t given the initial concentrations and rate constant. This closed-form expression is useful in kinetics modeling, parameter estimation and fitting of integrated rate laws as described in the ChemPy README's kinetics examples.", "name": "chempy_kinetics_integrated_pseudo_irrev", "parameters": {"properties": {"t": {"type": "float", "description": "Time at which to evaluate the product concentration. In practical use this may be a scalar float for a single time point, a sequence/array of time points for waveform evaluation (numpy arrays are supported by the default backend), or a symbolic time variable (Symbol) when constructing analytic expressions with e.g. sympy. Units are not interpreted by this function; pass unitless numeric values or use compatible unit handling externally (see ChemPy units utilities).", "default": ""}, "kf": {"type": "float", "description": "Forward bimolecular rate constant for the reaction. In the pseudo-first-order reduction this combines with the major reactant concentration to form an effective first-order rate. Typical units are (concentration^-1 time^-1) for bimolecular kf; when combined with major (concentration) the product major * kf has units of time^-1. kf may be a numeric value for numerical evaluation or a Symbol for symbolic algebra.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the product/complex (the species whose transient is reported) at time t = 0. This is the baseline concentration to which the generated product from the minor reactant adds. Use a numeric value for numeric evaluation or a Symbol for symbolic manipulation.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant. In the pseudo-first-order approximation this concentration is treated as effectively constant and multiplies kf to give the effective first-order rate constant (major * kf). This parameter should be positive in physical models; the function does not enforce sign constraints.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant that is consumed to form product. The term minor * (1 - exp(-major * kf * t)) represents the fraction of this initial minor pool converted to product over time. Use a numeric value for numeric evaluation or a Symbol for symbolic manipulation.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Backend providing basic mathematical operations used to evaluate the expression. Default is 'numpy' when backend is None (i.e., get_backend(backend) will return the numpy-backed API), but can be e.g. the sympy module when constructing symbolic expressions. The backend must provide an exp function (called as backend.exp) and support arithmetic between the provided inputs; an incompatible backend or one lacking exp will raise an AttributeError or similar when called.", "default": null}}, "required": ["t", "kf", "prod", "major", "minor"], "type": "any"}}, "type": "function"}], "query": "I’m modeling an irreversible pseudo-first-order binding assay (L in large excess) and I want the complex concentration at the 2-minute mark using ChemPy’s analytic pseudo-first-order integrated expression. Initially I was going to plug in the ligand concentration directly, but I just realized the 0.0008 M value I have is the *prepared stock* and the assay mix is made by combining it 1:1 with buffer right before starting the timer, so the effective ‘major’ concentration during the run is the diluted value implied by that mixing step. Keep kf = 2500 (1/(M·s)), initial product = 0.0 M, and minor reactant = 0.000002 M. Evaluate at t = 120 s with the default numeric backend.", "answers": "[{\"name\": \"chempy_kinetics_integrated_pseudo_irrev\", \"arguments\": {\"t\": 120, \"kf\": 2500, \"prod\": 0.0, \"major\": 0.0004, \"minor\": 2e-06}}]"}
{"func_name": "chempy_symmetry_representations_print_table", "func_desc": "chempy.symmetry.representations.print_table prints a formatted character table for a molecular point group given in Schoenflies notation.\n    \n    This function is used in the ChemPy symmetry utilities to display the character table (symmetry operation headers and Mulliken irreducible-representation labels with their characters) for a point group commonly encountered in molecular and inorganic chemistry. The output is intended for human inspection (e.g., in interactive sessions, documentation, or debugging) and helps chemists and computational chemistry users reason about orbital symmetries, selection rules, and spectroscopic transitions. The function looks up pre-defined module-level dictionaries (headers, mulliken, column_coeffs, row_coeffs, tables) to build the printed table.", "tools": [{"function": {"description": "chempy.symmetry.representations.print_table prints a formatted character table for a molecular point group given in Schoenflies notation.\n\nThis function is used in the ChemPy symmetry utilities to display the character table (symmetry operation headers and Mulliken irreducible-representation labels with their characters) for a point group commonly encountered in molecular and inorganic chemistry. The output is intended for human inspection (e.g., in interactive sessions, documentation, or debugging) and helps chemists and computational chemistry users reason about orbital symmetries, selection rules, and spectroscopic transitions. The function looks up pre-defined module-level dictionaries (headers, mulliken, column_coeffs, row_coeffs, tables) to build the printed table.", "name": "chempy_symmetry_representations_print_table", "parameters": {"properties": {"group": {"type": "string", "description": "Point group in Schoenflies notation (for example, \"C2v\"). This argument is case-insensitive: the function immediately lowercases the input internally. The string must match one of the keys present in the module-level dictionaries used by the function (headers, mulliken, column_coeffs, tables); otherwise a KeyError will be raised.", "default": ""}}, "required": ["group"], "type": "any"}}, "type": "function"}], "query": "I’m classifying vibrational mode symmetries for a planar trigonal molecule. I was going to print the character table for the obvious trigonal-planar group, but I just realized the real molecule is slightly puckered out of plane, so the horizontal mirror plane is not a true symmetry element anymore while the threefold principal axis is still a good approximation. For reference, print the formatted character table for the point group that results from removing that horizontal mirror operation from the original trigonal-planar group (use Schoenflies notation).", "answers": "[{\"name\":\"chempy_symmetry_representations_print_table\",\"arguments\":{\"group\":\"D3\"}}]"}
{"func_name": "chempy_units_logspace_from_lin", "func_desc": "Logarithmically spaced data points with units preserved.\n    \n    This function is part of chempy.units, which wraps the quantities package for\n    unit-aware numerical work in chemistry (e.g. concentrations, amounts, rates).\n    logspace_from_lin produces an array of values that are evenly spaced in\n    base-2 logarithmic space between two endpoint values. The unit of the\n    returned array is taken from start; stop is converted to that unit before\n    computing the logarithmic spacing. This is useful when sampling a physical\n    quantity (for example, concentration, activity, or pressure) across orders\n    of magnitude while keeping the values' units consistent with the rest of a\n    chempy workflow.", "tools": [{"function": {"description": "Logarithmically spaced data points with units preserved.\n\nThis function is part of chempy.units, which wraps the quantities package for\nunit-aware numerical work in chemistry (e.g. concentrations, amounts, rates).\nlogspace_from_lin produces an array of values that are evenly spaced in\nbase-2 logarithmic space between two endpoint values. The unit of the\nreturned array is taken from start; stop is converted to that unit before\ncomputing the logarithmic spacing. This is useful when sampling a physical\nquantity (for example, concentration, activity, or pressure) across orders\nof magnitude while keeping the values' units consistent with the rest of a\nchempy workflow.", "name": "chempy_units_logspace_from_lin", "parameters": {"properties": {"start": {"type": "float", "description": "Left endpoint of the interval expressed in a unit-aware\nnumeric value. The unit of start determines the unit of the returned\narray. In the context of chempy.units this is typically a quantities\nobject or a numeric value associated with a unit; unit_of(start) is\nused internally to extract the unit. The returned values are spaced\nso that log2(value) is linearly spaced between log2(start) and\nlog2(stop_in_start_unit). start must be finite.", "default": ""}, "stop": {"type": "float", "description": "Right endpoint of the interval expressed in a unit-aware\nnumeric value. stop is converted to the unit of start using\nchempy.units.to_unitless(start_unit) semantics (i.e., the function\ncomputes log2(to_unitless(stop, unit_of(start)))). If stop has\nincompatible units such that conversion to the unit of start fails,\na conversion error (for example ValueError) from the underlying units\nhandling code will be propagated. stop must be finite after\nconversion.", "default": ""}, "num": {"type": "integer", "description": "Number of samples to generate. Defaults to 50. num is passed\ndirectly to numpy.linspace to create num evenly spaced points in the\nexponent (log2) domain and therefore must be a positive integer\naccepted by numpy.linspace (typical use is an integer >= 1).", "default": 50}}, "required": ["start", "stop", "num"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a kinetics parameter sweep for an aqueous reaction where the concentration grid needs to be **evenly spaced in base-2 log space** and remain **unit-aware** for downstream chempy.units calculations. Start from 1e-9 mol/L and go up to 1e-3 mol/L, but I just realized the lab notebook spec says I need **twelve experimental levels total including both endpoints**, and our DOE template also includes a blank/control row that I’m generating separately—so the sweep itself must contain **one fewer point than I first thought**. Generate the unit-preserving base-2 log-spaced concentration values accordingly, with the returned array carrying concentration units compatible with chempy.units (stop should be converted to the start unit before spacing).", "answers": "[{\"name\":\"chempy_units_logspace_from_lin\",\"arguments\":{\"start\":1e-09,\"stop\":0.001,\"num\":11}}]"}
{"func_name": "chempy_units_unit_registry_to_human_readable", "func_desc": "chempy.units.unit_registry_to_human_readable converts an internal unit registry used by ChemPy's units subsystem into a human-readable, serializable mapping suitable for JSON-style storage or display. It is used when persisting or inspecting the set of base unit definitions (the SI base keys listed in the module-level SI_base_registry) so that numeric multipliers and unit symbols can be represented as simple Python primitives.", "tools": [{"function": {"description": "chempy.units.unit_registry_to_human_readable converts an internal unit registry used by ChemPy's units subsystem into a human-readable, serializable mapping suitable for JSON-style storage or display. It is used when persisting or inspecting the set of base unit definitions (the SI base keys listed in the module-level SI_base_registry) so that numeric multipliers and unit symbols can be represented as simple Python primitives.\n", "name": "chempy_units_unit_registry_to_human_readable", "parameters": {"properties": {"unit_registry": {"type": "any", "description": "A mapping representing the unit registry used by ChemPy's units module. In the domain of ChemPy this registry is expected to contain entries for each key in the module-level SI_base_registry; each value is either the integer 1 (the literal integer object used here to denote a pure dimensionless base entry) or an object from the underlying units/quantities package with a .dimensionality attribute. The .dimensionality attribute is iterated to produce a list of unit descriptor objects; each unit descriptor must expose a u_symbol attribute containing the unit symbol string. This function does not accept inputs other than a dict or None, and it will attempt to read unit_registry[k] for every k in SI_base_registry, so missing keys will raise a KeyError.", "default": ""}}, "required": ["unit_registry"], "type": "any"}}, "type": "function"}], "query": "I’m trying to persist our ChemPy unit settings to JSON for a multi-lab kinetics pipeline, and I need a human-readable dump of the SI base registry. I was going to export the registry exactly as currently used, but then I realized one of our upstream parsers accidentally inserted a non-base key during a merge (it’s the only entry whose value is not equal to the identity multiplier used by the true SI bases). Please convert the registry to the human-readable, serializable mapping **after excluding that stray entry**, keeping only the genuine SI base keys (the ones that all share the same identity multiplier). Here’s the merged mapping: {\"length\": 1, \"mass\": 1, \"time\": 1, \"current\": 1, \"temperature\": 1, \"amount\": 1, \"luminous_intensity\": 1, \"pressure\": 101325}.", "answers": "[{\"name\": \"chempy_units_unit_registry_to_human_readable\", \"arguments\": {\"unit_registry\": {\"length\": 1, \"mass\": 1, \"time\": 1, \"current\": 1, \"temperature\": 1, \"amount\": 1, \"luminous_intensity\": 1}}}]"}
{"func_name": "chempy_util__expr_create_Piecewise", "func_desc": "create_Piecewise creates a parameterized piecewise expression factory for use in ChemPy expression trees and symbolic/numeric backends.\n    \n    This function returns an Expr factory (produced by Expr.from_callback) that constructs a piecewise expression which selects one of several sub-expressions based on the value of a single runtime parameter. It is intended for use in ChemPy contexts where expressions depend on a single named parameter (for example 'x' in concentration- or position-dependent expressions used in kinetics, equilibria, or property functions). The factory expects a specific sequence encoding alternating interval bounds and expressions (see behavior below). The implementation supports two execution modes: if the provided backend exposes a Piecewise constructor (e.g. a symbolic backend such as SymPy) a symbolic backend.Piecewise expression is returned (with evaluate=False to avoid eager simplification); otherwise a simple numeric selection is performed by iterating the supplied bounds and returning the matching expression. If the runtime parameter is a quantity, bounds are converted to unitless values using the parameter's unit so comparisons are meaningful in unit-aware contexts.", "tools": [{"function": {"description": "create_Piecewise creates a parameterized piecewise expression factory for use in ChemPy expression trees and symbolic/numeric backends.\n\nThis function returns an Expr factory (produced by Expr.from_callback) that constructs a piecewise expression which selects one of several sub-expressions based on the value of a single runtime parameter. It is intended for use in ChemPy contexts where expressions depend on a single named parameter (for example 'x' in concentration- or position-dependent expressions used in kinetics, equilibria, or property functions). The factory expects a specific sequence encoding alternating interval bounds and expressions (see behavior below). The implementation supports two execution modes: if the provided backend exposes a Piecewise constructor (e.g. a symbolic backend such as SymPy) a symbolic backend.Piecewise expression is returned (with evaluate=False to avoid eager simplification); otherwise a simple numeric selection is performed by iterating the supplied bounds and returning the matching expression. If the runtime parameter is a quantity, bounds are converted to unitless values using the parameter's unit so comparisons are meaningful in unit-aware contexts.", "name": "chempy_util__expr_create_Piecewise", "parameters": {"properties": {"parameter_name": {"type": "string", "description": "Name of the single runtime parameter used to select which branch of the piecewise expression to evaluate. This exact string is passed as the parameter key when creating the Expr via Expr.from_callback, so callers and downstream code must use the same name to provide the parameter value (for example, 'x' to indicate the independent variable in a spatial or concentration-dependent expression). The returned Expr factory therefore produces expressions that expect a mapping or value for this parameter at evaluation time.", "default": ""}, "nan_fallback": {"type": "boolean", "description": "When True, the constructed symbolic Piecewise will include a final fallback branch that returns a backend Symbol called 'NAN' with an unconditional (True) predicate; this ensures the Piecewise is always defined in symbolic backends even when the parameter lies outside all provided intervals. When False (the default), no unconditional fallback is added for the symbolic Piecewise; in numeric/backends-without-Piecewise mode, omission of a matching interval causes a ValueError to be raised. Use nan_fallback=True when you need a defined symbolic placeholder for out-of-range values and are prepared to handle the backend-specific 'NAN' symbol.", "default": false}}, "required": ["parameter_name", "nan_fallback"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a ChemPy expression tree for a plug-flow reactor and I need a piecewise temperature profile along the axial coordinate. Initially I was going to key the factory on the parameter name I used in my notes (a single-letter coordinate), but I just realized my mechanism file and downstream rate expressions already reserve that symbol for something else, so the runtime parameter has to be the reactor coordinate symbol used everywhere else in the model (the one in my T(·) profile). Also, for debugging I want out-of-domain positions to propagate as an explicit symbolic undefined value rather than silently clamping or returning the last segment—so please enable the option that makes the factory yield a symbolic NaN placeholder whenever the parameter falls outside all specified intervals. Create the Piecewise expression factory accordingly (no need to build the actual bounds/expressions yet—just the factory).", "answers": "[{\"name\": \"chempy_util__expr_create_Piecewise\", \"arguments\": {\"parameter_name\": \"z\", \"nan_fallback\": true}}]"}
{"func_name": "chempy_util_parsing_formula_to_composition", "func_desc": "Parse a chemical formula string and return its composition as a mapping from atomic number to multiplicity.\n    \n    This function is used in ChemPy to convert human-readable chemical formulae (as found in the README examples and reaction/solution specifications) into a machine-friendly composition dictionary. The composition keys are integer atomic numbers (0 reserved for net charge) and the values are numeric multiplicities aggregated across possible dot-separated parts (hydrates, adducts). The parser ignores specified textual prefixes and physical-state suffixes, supports both the Unicode middle dot (·, U+00B7) and the legacy double-dot (\"..\") hydrate notation, and handles leading integer multipliers on subsequent parts (e.g., \"Na2CO3..7H2O\" treats \"7\" as a multiplier for the water part).", "tools": [{"function": {"description": "Parse a chemical formula string and return its composition as a mapping from atomic number to multiplicity.\n\nThis function is used in ChemPy to convert human-readable chemical formulae (as found in the README examples and reaction/solution specifications) into a machine-friendly composition dictionary. The composition keys are integer atomic numbers (0 reserved for net charge) and the values are numeric multiplicities aggregated across possible dot-separated parts (hydrates, adducts). The parser ignores specified textual prefixes and physical-state suffixes, supports both the Unicode middle dot (·, U+00B7) and the legacy double-dot (\"..\") hydrate notation, and handles leading integer multipliers on subsequent parts (e.g., \"Na2CO3..7H2O\" treats \"7\" as a multiplier for the water part).", "name": "chempy_util_parsing_formula_to_composition", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula to parse. Typical inputs are molecular formulae or species names that encode charge and stoichiometry, for example 'H2O', 'Fe+3', 'Cl-', '.NHO-(aq)', 'Na2CO3..7H2O', or 'UO2.3'. The string may include a leading prefix (to be ignored), a trailing suffix indicating phase (to be ignored), charge tokens (e.g. \"+3\", \"-\"), and dot-separated hydrate/adduct parts. This argument is required and must be a text string; a non-string will result in a TypeError in downstream parsing.", "default": ""}, "prefixes": {"type": "any", "nullable": true, "description": "Iterable of prefix strings to ignore before parsing the core formula. Examples are '.' or 'alpha-'. If prefixes is None (the default), the function uses the internal LaTeX-to-text mapping keys (i.e., a predefined set of LaTeX prefixes known to ChemPy) as the prefixes to strip. Passing an explicit iterable restricts which leading substrings are stripped from the input formula prior to parsing.", "default": null}, "suffixes": {"type": "any", "description": "Tuple of suffix strings to ignore after parsing the core formula. The default is ('(s)', '(l)', '(g)', '(aq)') to remove common physical-state annotations like solid, liquid, gas and aqueous. Any matching suffix found at the end of the input is removed before the chemical composition is parsed.", "default": ["(s)", "(l)", "(g)", "(aq)"]}}, "required": ["formula"], "type": "any"}}, "type": "function"}], "query": "I’m standardizing some legacy LIMS entries for copper(II) sulfate hydrate. Start by parsing the label that uses the old hydrate delimiter and includes a physical-state suffix (same one you’d see on bottle labels). I was going to keep the state annotation, but then I realized our downstream stoichiometry normalizer rejects any non-formula trailing tokens—so strip the state suffix entirely. Also, the delimiter in our dataset is inconsistent: sometimes it’s the legacy double-dot and sometimes it’s the Unicode middle dot. For this specific entry, use the version that’s compatible with the README examples (i.e., the delimiter variant that’s *not* two literal periods), and treat the trailing water part as a hydrate with a leading integer multiplier applied only to that dot-separated part. Return the composition as a mapping from atomic number to multiplicity, with charge (0) omitted since none is specified.", "answers": "[{\"name\":\"chempy_util_parsing_formula_to_composition\",\"arguments\":{\"formula\":\"CuSO4·5H2O\"}}]"}
{"func_name": "chempy_util_pyutil_defaultnamedtuple", "func_desc": "Generates and returns a new named tuple subclass (a lightweight immutable record type) with configurable default values for its trailing fields. This helper wraps collections.namedtuple and then adjusts the generated class's __new__.__defaults__ so that instances can be created with omitted trailing fields supplied from defaults. In the ChemPy codebase this is useful for defining compact, tuple-backed data containers used in chemical modelling (for example, simple spatial/property records such as the Body example in the original docstring: Body(x, y, z, density)), where some attributes commonly have sensible defaults (e.g., density).", "tools": [{"function": {"description": "Generates and returns a new named tuple subclass (a lightweight immutable record type) with configurable default values for its trailing fields. This helper wraps collections.namedtuple and then adjusts the generated class's __new__.__defaults__ so that instances can be created with omitted trailing fields supplied from defaults. In the ChemPy codebase this is useful for defining compact, tuple-backed data containers used in chemical modelling (for example, simple spatial/property records such as the Body example in the original docstring: Body(x, y, z, density)), where some attributes commonly have sensible defaults (e.g., density).\n", "name": "chempy_util_pyutil_defaultnamedtuple", "parameters": {"properties": {"typename": {"type": "string", "description": "The name to assign to the generated class. This becomes the class __name__ and appears in the class __doc__ (for example 'Body'). The name must be a valid Python identifier acceptable to collections.namedtuple; if it is not, namedtuple will raise ValueError.", "default": ""}, "field_names": {"type": "string", "description": "The field names for the record, given either as a space/comma-separated string or as an iterable of strings. These determine the order of fields in the tuple, the Tuple._fields sequence, and the order used when converting instances to dictionaries via _asdict(). Invalid field names or malformed input will cause namedtuple to raise ValueError.", "default": ""}, "defaults": {"type": "any", "description": "Default values to assign to the last N fields, where N is len(defaults). The function accepts either: (a) a mapping (e.g., dict) where keys are field names and values are the defaults to apply to those fields (the mapping is used to construct a temporary instance Tuple(**defaults) and its values are used), or (b) a sequence/iterable of default values which are applied to the final fields in order. If fewer defaults than fields are provided, the earlier (leading) fields receive None as their default. If defaults is omitted or empty (the default ()), all fields will default to None. The defaults are applied by setting Tuple.__new__.__defaults__, which affects how the class can be instantiated with positional and keyword arguments.", "default": []}}, "required": ["typename", "field_names", "defaults"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a ChemPy-style reaction-network toy model and I need a lightweight immutable record for per-species thermodynamic bookkeeping. Please create a namedtuple subclass called \"SpeciesState\" with fields in this exact order: name, charge, phase, temperature.\n\nInitially I was going to make only the last two fields default (aqueous phase at room temperature), but then I realized a bunch of my parsing code sometimes omits the ionic charge too, and I want those records to still construct cleanly. So: make it so that if I provide only the name, the remaining fields are filled in automatically; if I provide name and charge, then only the last two are filled in automatically; and if I provide name, charge, and phase, then only temperature is filled in automatically. Use the conventional defaults: neutral charge, aqueous phase, and standard ambient temperature (in kelvin). Generate the namedtuple class with these trailing defaults.", "answers": "[{\"name\":\"chempy_util_pyutil_defaultnamedtuple\",\"arguments\":{\"typename\":\"SpeciesState\",\"field_names\":\"name charge phase temperature\",\"defaults\":[0,\"aq\",298.15]}}]"}
{"func_name": "chempy_util_stoich_get_coeff_mtx", "func_desc": "chempy.util.stoich.get_coeff_mtx computes the net stoichiometry coefficient matrix for a set of chemical reactions. It converts a sequence of reaction stoichiometries (each given as a pair of reactant and product dictionaries mapping substance keys to stoichiometric coefficients) into a 2-D integer NumPy array where each row corresponds to a substance and each column corresponds to a reaction. This matrix is the standard coefficient matrix used in balancing stoichiometric equations and in forming linear systems for chemical-equilibrium and reaction-network calculations (for example, as an intermediate in balance_stoichiometry and related routines in ChemPy).", "tools": [{"function": {"description": "chempy.util.stoich.get_coeff_mtx computes the net stoichiometry coefficient matrix for a set of chemical reactions. It converts a sequence of reaction stoichiometries (each given as a pair of reactant and product dictionaries mapping substance keys to stoichiometric coefficients) into a 2-D integer NumPy array where each row corresponds to a substance and each column corresponds to a reaction. This matrix is the standard coefficient matrix used in balancing stoichiometric equations and in forming linear systems for chemical-equilibrium and reaction-network calculations (for example, as an intermediate in balance_stoichiometry and related routines in ChemPy).\n", "name": "chempy_util_stoich_get_coeff_mtx", "parameters": {"properties": {"substances": {"type": "array", "items": {"type": "any"}, "description": "Ordered list of substance keys. Each entry identifies a substance (for example a chemical formula string or any hashable key used consistently in the stoichiometry dictionaries). The order of this list defines the row order of the returned matrix: row i corresponds to substances[i]. If a substance does not appear in a particular reaction pair, it is treated as having coefficient zero for that reaction.", "default": ""}, "stoichs": {"type": "array", "items": {"type": "any"}, "description": "List of reaction stoichiometries, one element per reaction, in the same column order as the desired output. Each element must be a pair (reactant_dict, product_dict), where each dict maps the same kind of substance keys (as used in substances) to stoichiometric coefficients (integers are expected). The function iterates over this sequence and uses each pair to compute the net production minus consumption for each substance.", "default": ""}}, "required": ["substances", "stoichs"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a small reaction-network Jacobian for a combustion + water–gas-shift toy model, but the input stoichiometries I got from a collaborator are a bit messy.\n\nStart by building the net stoichiometry coefficient matrix with rows ordered exactly as: CH4, O2, CO2, H2O, CO, H2.\n\nReactions (as provided):\n1) CH4 + 2 O2 → CO2 + 2 H2O (this one is fine)\n2) CO + H2O → CO2 + H2 (also fine)\n\nWait—downstream I’m coupling this to a separate ODE module that assumes **every column corresponds to an oxidation step consuming O2**, so I can’t keep the water–gas-shift as the second column. Instead, replace the second reaction by the **only reaction you can form by adding the two given reactions together** (i.e., the net overall conversion when both steps proceed once each), and then compute the coefficient matrix for the resulting two-reaction set.\n\nUse chempy.util.stoich.get_coeff_mtx and keep the row order fixed as above.", "answers": "[{\"name\": \"chempy_util_stoich_get_coeff_mtx\", \"arguments\": {\"substances\": [\"CH4\", \"O2\", \"CO2\", \"H2O\", \"CO\", \"H2\"], \"stoichs\": [[{\"CH4\": 1, \"O2\": 2}, {\"CO2\": 1, \"H2O\": 2}], [{\"CH4\": 1, \"O2\": 2, \"CO\": 1, \"H2O\": 1}, {\"CO2\": 2, \"H2O\": 2, \"H2\": 1}]]}}]"}
{"func_name": "chempy_util_terminal_limit_logging", "func_desc": "chempy.util.terminal.limit_logging provides a context manager that temporarily raises the global logging threshold to suppress logging messages up to and including a specified numeric level. It is intended for use in ChemPy workflows where noisy log output from numerical integrators, equilibrium solvers, or optional backend libraries (see README: ODE solver front-end, equilibria solvers, and optional backends) should be silenced for clarity during tests, example runs, or user scripts.", "tools": [{"function": {"description": "chempy.util.terminal.limit_logging provides a context manager that temporarily raises the global logging threshold to suppress logging messages up to and including a specified numeric level. It is intended for use in ChemPy workflows where noisy log output from numerical integrators, equilibrium solvers, or optional backend libraries (see README: ODE solver front-end, equilibria solvers, and optional backends) should be silenced for clarity during tests, example runs, or user scripts.\n", "name": "chempy_util_terminal_limit_logging", "parameters": {"properties": {"max_lvl": {"type": "integer", "description": "Numeric logging threshold to disable. This integer is forwarded to logging.disable and therefore follows the semantics of the standard Python logging levels (for example, logging.CRITICAL == 50). The default value in the function signature is 50, which corresponds to logging.CRITICAL and will suppress all messages at that level and below while the context is active. The parameter controls the global logging state for the process; passing a lower value will allow fewer messages through and a higher value will suppress more.", "default": 50}}, "required": ["max_lvl"], "type": "any"}}, "type": "function"}], "query": "I’m running a ChemPy batch that couples equilibrium solvers with an ODE front-end, and the optional backend libraries are spamming the console. I initially planned to silence *everything* (even the most severe messages) so my report logs only show the final tabulated results. But I just realized I still need the top-severity messages to remain visible in case a run hard-fails, while suppressing everything at or below the next rung down in the standard Python logging ladder. Configure `chempy.util.terminal.limit_logging` accordingly so that only the highest-severity records can still break through during the batch.\n\n(Use the numeric level that corresponds to that “next rung down” threshold; the goal is: show only the very highest-severity logs, hide the rest.)", "answers": "[{\"name\": \"chempy_util_terminal_limit_logging\", \"arguments\": {\"max_lvl\": 40}}]"}
{"func_name": "cirpy_construct_api_url", "func_desc": "Construct and return the CIR (Chemical Identifier Resolver) API URL for a desired resolution request.\n    \n    This function is used by CIRpy to build the exact HTTP request URL that will be sent to the CIR web service (the NCI/NIH Chemical Identifier Resolver). Given a chemical identifier (for example, a chemical name or registry number) and the desired output representation (for example, a SMILES string, InChI, or a file format), this function encodes the identifier, selects the appropriate path and query parameters accepted by the CIR API, and returns a single percent-encoded URL string. The function does not perform any network I/O; it only composes the URL. It respects module-level constants such as FILE_FORMATS (which cause a representation to be sent as representation=file with a format=... query parameter) and API_BASE (the CIR endpoint base path).", "tools": [{"function": {"description": "Construct and return the CIR (Chemical Identifier Resolver) API URL for a desired resolution request.\n\nThis function is used by CIRpy to build the exact HTTP request URL that will be sent to the CIR web service (the NCI/NIH Chemical Identifier Resolver). Given a chemical identifier (for example, a chemical name or registry number) and the desired output representation (for example, a SMILES string, InChI, or a file format), this function encodes the identifier, selects the appropriate path and query parameters accepted by the CIR API, and returns a single percent-encoded URL string. The function does not perform any network I/O; it only composes the URL. It respects module-level constants such as FILE_FORMATS (which cause a representation to be sent as representation=file with a format=... query parameter) and API_BASE (the CIR endpoint base path).", "name": "cirpy_construct_api_url", "parameters": {"properties": {"input": {"type": "string", "description": "Chemical identifier to resolve. This is the raw identifier provided by the caller (for example \"Aspirin\" or \"50-78-2\"). The value will be percent-encoded (quoted) for safe inclusion in a URL path; if non-string values are passed, the underlying quote call may raise a TypeError. If tautomers is True, the function will prepend the literal prefix \"tautomers:\" to this string before encoding to instruct CIR to return all tautomers for the identifier.", "default": ""}, "representation": {"type": "string", "description": "Desired output representation requested from CIR. This specifies the CIR resource path segment (for example \"smiles\", \"inchi\", or other representation names). If this value is a member of the module FILE_FORMATS set, the function will instead set the path representation to \"file\" and add a query parameter format=<representation> so CIR returns the requested file format. The returned URL path will include this representation (or \"file\" when a file format is requested).", "default": ""}, "resolvers": {"type": "any", "nullable": true, "description": "Optional ordered list of resolver names to pass to the CIR service. When provided, the list elements are joined with commas and added as the resolver query parameter (resolver=name1,name2,...). The order in this list indicates the order in which CIR should attempt resolution. Elements must be strings; non-string elements will cause a TypeError during the join operation.", "default": null}, "get3d": {"type": "boolean", "description": "Optional flag indicating whether to request 3D coordinates from CIR when applicable. When True, the function adds get3d=True to the query string. Default is False. This flag is meaningful for representations and resolver combinations that can return 3D coordinate data.", "default": false}, "tautomers": {"type": "boolean", "description": "Optional flag indicating whether to request all tautomers of the given identifier. When True, the function prepends the literal prefix \"tautomers:\" to the input identifier (before percent-encoding) which tells the CIR service to return alternate tautomeric forms. Default is False.", "default": false}, "xml": {"type": "boolean", "description": "Optional flag indicating whether to request the CIR XML wrapper for the response. When True (the default), the function appends the literal path segment \"/xml\" to the constructed path so the CIR service returns its full XML response wrapper. When False, the \"/xml\" suffix is omitted and the raw resource endpoint is returned.", "default": true}, "kwargs": {"type": "dict", "additionalProperties": {"type": "any"}, "description": "Additional optional query parameters to include in the URL as a query string. Keys and values are encoded with urllib.parse.urlencode. Common uses include specifying format (for file outputs), page or detail options supported by CIR, or any other query parameters accepted by the CIR API. Values should be types accepted by urlencode (strings or sequences as appropriate); unsupported types may cause an exception during encoding.", "default": ""}}, "required": ["input", "representation", "get3d", "kwargs", "resolvers", "xml", "tautomers"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a structure-standardization pipeline for a small-molecule docking benchmark and need you to compose (only compose; no network calls) the exact CIR request URL for aspirin starting from its registry identifier `50-78-2`. Initially I was going to ask for an SDF, but I realized the downstream docking stage rejects 2D-only SDFs unless they are explicitly requested with 3D coordinates and (for file-like outputs) must be routed through CIR’s file-representation pathway with the correct format query parameter. So: request the file-based output corresponding to the SDF family, with 3D coordinates enabled, do NOT request tautomer enumeration, and return the raw endpoint rather than the XML-wrapped response. Also, resolver preference should remain ‘PubChem first, then ChemSpider’. Build and return the single percent-encoded CIR URL string that CIRpy would send.", "answers": "[{\"name\": \"cirpy_construct_api_url\", \"arguments\": {\"input\": \"50-78-2\", \"representation\": \"sdf\", \"resolvers\": [\"pubchem\", \"chemspider\"], \"get3d\": true, \"tautomers\": false, \"xml\": false, \"kwargs\": {}}}]"}
{"func_name": "cirpy_resolve", "func_desc": "Resolve a chemical identifier to a specified output representation using the Chemical Identifier Resolver (CIR) web service.\n    \n    This function is the high-level convenience wrapper used in CIRpy to convert one form of chemical identifier (for example a common name such as \"Aspirin\") into another representation (for example a SMILES string). It issues a network query to the CIR service via the internal cirpy.query function, takes the first result from the CIR XML response, and returns that result's value as a string. If CIR returns no results the function returns None. This function therefore encapsulates the practical workflow described in the project README: no manual URL construction or XML parsing is required — cirpy.resolve performs the request, parsing, and selection of the first returned representation.", "tools": [{"function": {"description": "Resolve a chemical identifier to a specified output representation using the Chemical Identifier Resolver (CIR) web service.\n\nThis function is the high-level convenience wrapper used in CIRpy to convert one form of chemical identifier (for example a common name such as \"Aspirin\") into another representation (for example a SMILES string). It issues a network query to the CIR service via the internal cirpy.query function, takes the first result from the CIR XML response, and returns that result's value as a string. If CIR returns no results the function returns None. This function therefore encapsulates the practical workflow described in the project README: no manual URL construction or XML parsing is required — cirpy.resolve performs the request, parsing, and selection of the first returned representation.", "name": "cirpy_resolve", "parameters": {"properties": {"input": {"type": "string", "description": "Chemical identifier to resolve. This is the input token you want translated (for example a trivial/common name, InChI, CAS number, etc.) and is passed directly to the CIR web service. Its practical role is to identify the chemical entity to be looked up.", "default": ""}, "representation": {"type": "string", "description": "Desired output representation. This string names the CIR output type you require (for example \"smiles\", \"inchi\", etc.). The function requests this representation from CIR and returns the first matching value when available.", "default": ""}, "resolvers": {"type": "any", "nullable": true, "description": "(Optional) Ordered list of resolvers to use. When provided, this list constrains and orders which backend resolvers CIR should attempt. If None (the default), resolver selection is left to CIR's default behavior. The list items are resolver identifiers accepted by the CIR service.", "default": null}, "get3d": {"type": "boolean", "description": "(Optional) Whether to request 3D coordinates where applicable. If True, the request will ask CIR for three-dimensional coordinate output when the chosen representation and CIR backend support it. Default is False. Note that not all representations or resolver backends can provide 3D coordinates; support is dependent on the CIR service.", "default": false}, "kwargs": {"type": "dict", "additionalProperties": {"type": "any"}, "description": "Additional keyword arguments forwarded to cirpy.query and ultimately to the CIR web service client. These keyword arguments are not interpreted by cirpy.resolve itself but are passed through to the underlying query/HTTP layer (for example to influence network behavior or parser options supported by cirpy.query). Users should consult cirpy.query documentation for the set of accepted extra options.", "default": ""}}, "required": ["input", "representation", "kwargs"], "type": "any"}}, "type": "function"}], "query": "I’m annotating an LC–MS run and need a stable identifier for the analyte I labeled in the sample sheet as “caffeine”. Initially I was going to store the full InChI string, but I realized the LIMS field I’m pasting into only accepts the fixed-length hashed variant that CIR can return (the one typically used as a compact lookup key). So: resolve “caffeine” via the CIR high-level resolver wrapper and return that compact key representation. Keep it as the default request (no custom resolver list and don’t request 3D). If CIR returns multiple candidates, just use the first one as usual; if nothing comes back, return None.", "answers": "[{\"name\": \"cirpy_resolve\", \"arguments\": {\"input\": \"caffeine\", \"representation\": \"inchikey\", \"kwargs\": {}}}]"}
{"func_name": "datamol_convert_from_selfies", "func_desc": "datamol.convert.from_selfies converts a SELFIES representation of a molecule into a SMILES string or into an RDKit Mol object usable in Datamol pipelines.", "tools": [{"function": {"description": "datamol.convert.from_selfies converts a SELFIES representation of a molecule into a SMILES string or into an RDKit Mol object usable in Datamol pipelines.\n", "name": "datamol_convert_from_selfies", "parameters": {"properties": {"selfies": {"type": "string", "description": "SELFIES string encoding a molecular graph. In the datamol context this is the serialized, robust molecular representation produced or consumed by the selfies library; passing None is tolerated by this implementation and will cause the function to return None immediately. This parameter is the primary molecular input for conversion and is expected to be a textual SELFIES sequence.", "default": ""}, "as_mol": {"type": "boolean", "description": "Whether to return an RDKit Mol object instead of a SMILES string. Defaults to False. When True the function will first decode SELFIES to a SMILES string and then convert that SMILES to an rdkit.Chem.rdchem.Mol using datamol.to_mol, producing an RDKit Mol which Datamol and downstream RDKit-based operations expect.", "default": false}}, "required": ["selfies", "as_mol"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a small reaction-feeds dataset where the substrate field is stored as SELFIES. One record is supposed to be “ethanol”, and the SELFIES token stream I received is:\n\n\"[C][C][O][H]\"\n\nInitially I was going to decode it straight into a SMILES string for quick logging, and then separately decode it into an RDKit Mol for the Datamol pipeline. But I realized our downstream standardization step rejects explicit terminal hydrogens in the textual form, so don’t keep the representation that encodes the hydrogen explicitly; instead, decode a version that corresponds to the same heavy-atom connectivity but omits that explicit hydrogen token. \n\nPlease do this in two steps:\n1) Output the decoded SMILES for the hydrogen-suppressed variant.\n2) Output the RDKit Mol object (Datamol-compatible) for that same hydrogen-suppressed variant.", "answers": "[{\"name\":\"datamol_convert_from_selfies\",\"arguments\":{\"selfies\":\"[C][C][O]\",\"as_mol\":false}},{\"name\":\"datamol_convert_from_selfies\",\"arguments\":{\"selfies\":\"[C][C][O]\",\"as_mol\":true}}]"}
{"func_name": "datamol_fragment__assemble_build", "func_desc": "Build a super molecule from lists of fragment pools by applying retrosynthetic/assembly reactions and yielding assembled RDKit Mol objects.", "tools": [{"function": {"description": "Build a super molecule from lists of fragment pools by applying retrosynthetic/assembly reactions and yielding assembled RDKit Mol objects.\n", "name": "datamol_fragment__assemble_build", "parameters": {"properties": {"ll_mols": {"type": "array", "items": {"type": "any"}, "description": "A list of fragment pools where each element is an iterable (commonly a list) of fragment objects. In Datamol (RDKit-first design), each fragment is expected to be an rdkit.Chem.Mol-like object representing a molecular fragment that will be used as building blocks. The function enumerates Cartesian products across the fragment pools via itertools.product(*ll_mols) so ll_mols defines which combinations of fragments are attempted for assembly.", "default": ""}, "max_n_mols": {"type": "float", "description": "Maximum number of unique assembled molecules to produce. The default is infinity (float(\"inf\")), meaning no artificial upper bound. The function maintains an internal set of seen canonical/molecular identifiers (mSmi strings returned by the internal reaction runner) and stops yielding once len(seen) >= max_n_mols. Use this to limit runtime and memory when exploring large combinatorial fragment spaces.", "default": ""}, "mode": {"type": "string", "description": "Selection of the predefined reaction sets to apply. Accepts \"brics\" to use the library's BRICS retrosynthetic/assembly reactions, \"rxn\" to use the non-BRICS reaction set, or None to use both sets combined. The default is \"brics\". The chosen mode determines which module-level reaction collections (e.g., ALL_BRICS_RETRO, ALL_RXNS_RETRO and their associated type lists) are iterated and applied to fragment pairs.", "default": "brics"}, "frag_rxn": {"type": "string", "nullable": true, "description": "Optional filter to restrict assembly to a single reaction type. If provided, frag_rxn is compared (after stripping surrounding double-quote characters) against the reaction type labels (elements of the module-level CUR_RXNS_TYPE list). When a match is found, only the corresponding reaction from the selected reaction set is used for all assembly attempts. Use this to reproduce or debug a single reaction’s behavior instead of trying the full reaction collection.", "default": null}, "ADD_RNXS": {"type": "array", "items": {"type": "any"}, "description": "Additional reaction objects or a mapping of reaction-type-label -> reaction object to append to the selected reaction set. If a list is given (default []), synthetic type labels \"RXN-0\", \"RXN-1\", ... are generated for those values. If a dict is given, its keys are used as reaction type labels and its values as reaction objects. If ADD_RNXS is None, no extra reactions are appended. Note: the function signature uses a mutable default (empty list); to avoid surprises from shared mutable defaults across calls, explicitly pass a freshly created list or None when calling.", "default": []}}, "required": ["ll_mols", "max_n_mols", "frag_rxn", "mode", "ADD_RNXS"], "type": "any"}}, "type": "function"}], "query": "I’m assembling candidate ligands from three RDKit-fragment pools using BRICS, but I realized my downstream scoring workflow can’t handle duplicate constitutional isomers coming from symmetric fragments. So, enumerate assemblies as usual, then **only count unique products by canonical SMILES** toward the cap.\n\nAlso, I initially thought to cap at 25 total enumerations, but that includes one “control” molecule I need to reserve capacity for in the batch. **So set the maximum number of returned unique assembled molecules to one less than 25.**\n\nUse these fragment pools (SMILES): pool1=[\"c1ccccc1\",\"CC(=O)O\"], pool2=[\"NCCN\",\"OCCO\"], pool3=[\"C1CCCCC1\",\"COC\"]. Use the standard BRICS reaction set to build the super molecules, returning up to the adjusted cap of unique assembled RDKit Mol objects.", "answers": "[{\"name\": \"datamol_fragment__assemble_build\", \"arguments\": {\"ll_mols\": [[\"c1ccccc1\", \"CC(=O)O\"], [\"NCCN\", \"OCCO\"], [\"C1CCCCC1\", \"COC\"]], \"max_n_mols\": 24, \"mode\": \"brics\", \"frag_rxn\": \"\", \"ADD_RNXS\": []}}]"}
{"func_name": "datamol_reactions__reactions_rxn_from_smarts", "func_desc": "datamol.reactions._reactions.rxn_from_smarts: Create and initialize an RDKit ChemicalReaction from a reaction SMARTS string for use in Datamol reaction processing pipelines.", "tools": [{"function": {"description": "datamol.reactions._reactions.rxn_from_smarts: Create and initialize an RDKit ChemicalReaction from a reaction SMARTS string for use in Datamol reaction processing pipelines.\n", "name": "datamol_reactions__reactions_rxn_from_smarts", "parameters": {"properties": {"rxn_smarts": {"type": "string", "description": "Reaction SMARTS string describing the chemical transformation using RDKit reaction SMARTS syntax. This string is passed verbatim to rdkit.Chem.rdChemReactions.ReactionFromSmarts(SMARTS=...), so it must follow RDKit's SMARTS conventions for reactant and product patterns, atom mapping, and bond specifications. In the Datamol context this SMARTS is used to define transformation rules that can be applied to rdkit.Chem.Mol objects (for example via ChemicalReaction.RunReactants) during reaction enumeration, retrosynthesis workflows, or virtual chemical transformations.", "default": ""}}, "required": ["rxn_smarts"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a Datamol/RDKit virtual reaction enumeration step and I initially wrote a reaction SMARTS for an SN2 on bromoethane with hydroxide. But I realized my downstream bookkeeping assumes **products do not contain any explicit hydrogens**, and it also assumes the leaving group is represented as the **separated anion** (not neutral bromine). Please create the RDKit ChemicalReaction via `datamol.reactions._reactions.rxn_from_smarts`, using an atom-mapped reaction SMARTS that satisfies those constraints while still representing bromoethane + hydroxide giving ethanol (as the corresponding alkoxide form, i.e., no explicit H on oxygen) + bromide. Keep the carbon atom maps consistent between reactant and product, and keep the nucleophile’s oxygen map consistent too.", "answers": "[{\"name\": \"datamol_reactions__reactions_rxn_from_smarts\", \"arguments\": {\"rxn_smarts\": \"[CH3:1][CH2:2][Br:3].[OH-:4]>>[CH3:1][CH2:2][O-:4].[Br-:3]\"}}]"}
{"func_name": "deepchem_data_datasets_pad_batch", "func_desc": "Pads a minibatch of examples to exactly batch_size by repeating the provided examples in tiled fashion.\n    \n    This function is used by DeepChem data-loading and training code to ensure each minibatch passed to a model has a fixed size (batch_size) even when the dataset size or the last batch of an epoch is smaller. It takes arrays of features, labels, sample weights, and identifiers that all represent the same short batch (length <= batch_size) and produces new arrays of length batch_size suitable for input to neural network training or evaluation routines in molecular machine learning, drug discovery, materials science, and related life-science applications.", "tools": [{"function": {"description": "Pads a minibatch of examples to exactly batch_size by repeating the provided examples in tiled fashion.\n\nThis function is used by DeepChem data-loading and training code to ensure each minibatch passed to a model has a fixed size (batch_size) even when the dataset size or the last batch of an epoch is smaller. It takes arrays of features, labels, sample weights, and identifiers that all represent the same short batch (length <= batch_size) and produces new arrays of length batch_size suitable for input to neural network training or evaluation routines in molecular machine learning, drug discovery, materials science, and related life-science applications.", "name": "deepchem_data_datasets_pad_batch", "parameters": {"properties": {"batch_size": {"type": "integer", "description": "The target number of datapoints required in the output batch. In training and evaluation loops this is the minibatch dimension expected by model code and loss computations. Must be a positive integer.", "default": ""}, "X_b": {"type": "array", "items": {"type": "any"}, "description": "Array of input features for the current (possibly short) batch. Must have length len(X_b) <= batch_size and must share a consistent feature shape across examples. If X_b.ndim > 1 then the output feature array will have shape (batch_size,) + X_b.shape[1:], preserving the per-example feature shape and the numpy dtype of X_b. If X_b.ndim == 1 then the output feature array will have shape (batch_size,) and the same dtype as X_b. X_b provides the concrete example tensors that will be tiled into X_out to reach batch_size.", "default": ""}, "y_b": {"type": "array", "items": {"type": "any"}, "description": "Array of labels/targets corresponding to X_b; must have length len(y_b) == len(X_b) and len(y_b) <= batch_size. If y_b is None, no label array will be produced (y_out will be None). If y_b is a 1-D array the returned y_out will have shape (batch_size,); if y_b has additional per-example dimensions (y_b.ndim >= 2) the returned y_out will have shape (batch_size,) + y_b.shape[1:], and dtype preserved. y_b supplies supervised targets used by loss functions and evaluation.", "default": ""}, "w_b": {"type": "array", "items": {"type": "any"}, "description": "Array of sample weights corresponding to X_b; must have length len(w_b) == len(X_b) and len(w_b) <= batch_size. If w_b is None, w_out will be None. If w_b is 1-D the returned w_out will have shape (batch_size,); if w_b has extra per-example dimensions (w_b.ndim >= 2) the returned w_out will have shape (batch_size,) + w_b.shape[1:], and dtype preserved. Note: when padding occurs this implementation assigns the original w_b values only to the first len(X_b) positions of w_out (see behavior below); remaining weight entries remain zero.", "default": ""}, "ids_b": {"type": "array", "items": {"type": "any"}, "description": "1-D array of identifiers (e.g., example IDs, indices, or string/object dtype allowed by numpy) for the examples in X_b; must have length len(ids_b) == len(X_b) and len(ids_b) <= batch_size. ids_out will be a numpy array of length batch_size with the same dtype as ids_b; ids from ids_b are copied repeatedly to fill ids_out.", "default": ""}}, "required": ["batch_size", "X_b", "y_b", "w_b", "ids_b"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a DeepChem molecular property prediction training loop where the final minibatch is shorter than the model’s required fixed minibatch length. Initially I was going to pad it up to the same length as the model’s configured batch_size, but then I realized the evaluation callback concatenates predictions assuming the padded minibatch length is the **smallest power-of-two that is at least the configured batch_size** (to match an older XLA-friendly path). So: pad this short batch by tiling/repeating the provided examples until the minibatch length reaches that power-of-two target. Use these arrays for the short batch: X_b is a 3x4 feature matrix [[0.12,1.5,-0.3,2.2],[0.05,1.2,-0.1,2.0],[0.20,1.8,-0.4,2.5]], y_b is a length-3 regression target [7.1,6.8,7.5], w_b is sample weights [1.0,0.5,1.0], and ids_b are molecule IDs [\"mol-001\",\"mol-002\",\"mol-003\"]. (Configured batch_size is 8.)", "answers": "[{\"name\":\"deepchem_data_datasets_pad_batch\",\"arguments\":{\"batch_size\":8,\"X_b\":[[0.12,1.5,-0.3,2.2],[0.05,1.2,-0.1,2.0],[0.2,1.8,-0.4,2.5]],\"y_b\":[7.1,6.8,7.5],\"w_b\":[1.0,0.5,1.0],\"ids_b\":[\"mol-001\",\"mol-002\",\"mol-003\"]}}]"}
{"func_name": "deepchem_dock_pose_scoring_cutoff_filter", "func_desc": "deepchem.dock.pose_scoring.cutoff_filter applies a distance-based cutoff to a pairwise feature matrix used in molecular docking and pose scoring. In docking workflows within DeepChem, d typically encodes pairwise distances (for example atom–atom distances in Angstroms) and x encodes the corresponding per-pair contributions (for example interaction energies, contact indicators, or other features). This function zeroes out entries in x whose corresponding distance in d is greater than or equal to the cutoff, producing a filtered (N, M) array suitable for downstream scoring or featurization.", "tools": [{"function": {"description": "deepchem.dock.pose_scoring.cutoff_filter applies a distance-based cutoff to a pairwise feature matrix used in molecular docking and pose scoring. In docking workflows within DeepChem, d typically encodes pairwise distances (for example atom–atom distances in Angstroms) and x encodes the corresponding per-pair contributions (for example interaction energies, contact indicators, or other features). This function zeroes out entries in x whose corresponding distance in d is greater than or equal to the cutoff, producing a filtered (N, M) array suitable for downstream scoring or featurization.\n", "name": "deepchem_dock_pose_scoring_cutoff_filter", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "any"}, "description": "Pairwise distances matrix. A numpy array of shape (N, M) containing distances in Angstroms between two sets of atomic or molecular elements. Each element d[i, j] is the distance used to decide whether the corresponding value x[i, j] is kept. The function expects d to be a numpy.ndarray; if d does not have a shape compatible with x, numpy broadcasting rules apply and a ValueError may be raised.", "default": ""}, "x": {"type": "array", "items": {"type": "any"}, "description": "Matrix of shape (N, M) containing values associated with each pairwise distance in d (for example interaction scores, weights, or binary contact indicators). Values in x are retained only when the corresponding entry in d is strictly less than the cutoff. The function does not modify x in place; it constructs and returns a new numpy.ndarray with the same shape and dtype as x.", "default": ""}, "cutoff": {"type": "float", "description": "Cutoff distance in Angstroms (default 8.0). Pairs with d[i, j] < cutoff are considered proximal and their corresponding x[i, j] values are preserved; pairs with d[i, j] >= cutoff are considered too distant and will be replaced with zeros. The comparison is strict (<), so distances exactly equal to cutoff are treated as too large and thresholded to zero.", "default": 8.0}}, "required": ["d", "x", "cutoff"], "type": "any"}}, "type": "function"}], "query": "I’m rechecking my docking pose-scoring features before feeding them into the next DeepChem featurizer. I initially planned to use the same strict geometric cutoff I’ve been using (the one that blanks anything at or beyond six angstroms), but then I realized the downstream stage expects the *slightly more permissive* convention where pairs exactly on the boundary are still kept (only pairs strictly beyond the boundary get zeroed). Using the same 3×4 distance matrix d (Å) and 3×4 interaction contribution matrix x as below, apply the distance-based cutoff_filter using that boundary rule, and return the filtered contribution matrix.\n\nDistances d:\n[[2.3, 5.9, 6.0, 8.1],\n [1.2, 6.5, 4.4, 10.0],\n [5.99, 6.01, 3.3, 7.8]]\n\nContributions x:\n[[-0.8, -1.1, -0.4, -0.2],\n [-2.3, -0.7, -1.5, -0.1],\n [-0.9, -0.6, -2.0, -0.3]]", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_cutoff_filter\",\"arguments\":{\"d\":[[2.3,5.9,6.0,8.1],[1.2,6.5,4.4,10.0],[5.99,6.01,3.3,7.8]],\"x\":[[-0.8,-1.1,-0.4,-0.2],[-2.3,-0.7,-1.5,-0.1],[-0.9,-0.6,-2.0,-0.3]],\"cutoff\":6.01}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_gaussian_first", "func_desc": "Computes Autodock Vina's first Gaussian interaction term.\n    \n    This function implements the first Gaussian term from the Autodock Vina scoring function (Jain, 1996) that is commonly used in molecular docking to estimate a component of protein–ligand binding affinity. The computation is performed elementwise as exp(-(d / 0.5)**2), where the constant 0.5 is the Gaussian width parameter used by Autodock Vina. In practical workflows within DeepChem, this function converts a matrix of surface distances between ligand and protein atoms or grid points into a matrix of Gaussian-shaped interaction contributions that can be summed or combined with other terms to produce a docking score.", "tools": [{"function": {"description": "Computes Autodock Vina's first Gaussian interaction term.\n\nThis function implements the first Gaussian term from the Autodock Vina scoring function (Jain, 1996) that is commonly used in molecular docking to estimate a component of protein–ligand binding affinity. The computation is performed elementwise as exp(-(d / 0.5)**2), where the constant 0.5 is the Gaussian width parameter used by Autodock Vina. In practical workflows within DeepChem, this function converts a matrix of surface distances between ligand and protein atoms or grid points into a matrix of Gaussian-shaped interaction contributions that can be summed or combined with other terms to produce a docking score.", "name": "deepchem_dock_pose_scoring_vina_gaussian_first", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of shape `(N, M)` containing surface distances as defined in Jain 1996. Each element represents a distance measure used by the Vina scoring model (typically non-negative distances between ligand and protein surface points). The array should have a numeric dtype (e.g., float32 or float64). The function expects a full (N, M) array; providing inputs of a different shape will result in an output whose shape follows numpy's broadcasting rules or may raise an exception if broadcasting is not possible.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a docking run and need the Vina first-Gaussian interaction term computed elementwise, but the distance matrix I exported is messy: [[0.0, 0.25, 0.5], [0.75, 1.0, 1.5], [2.0, 2.5, 3.0]]. I initially planned to feed these values directly into the Gaussian term, then realized the scoring code expects the same distance unit as the Gaussian width parameter (the one hard-coded in Vina), while my table is in nanometers. Convert the distances into the unit consistent with that width parameter, and then compute the first Gaussian interaction term matrix (exp(-(d/width)^2) using the Vina width constant) for the converted distances.", "answers": "[{\"name\": \"deepchem_dock_pose_scoring_vina_gaussian_first\", \"arguments\": {\"d\": [[0.0, 2.5, 5.0], [7.5, 10.0, 15.0], [20.0, 25.0, 30.0]]}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_gaussian_second", "func_desc": "Computes Autodock Vina's second Gaussian interaction term used in docking pose scoring.\n    \n    This function implements the second Gaussian term from the Autodock Vina scoring function (Jain 1996) and is intended for use within DeepChem's docking and pose_scoring utilities to model a short-range attractive component of protein-ligand noncovalent interactions. Given an array of surface distances d (as defined in the Vina formulation), the function applies the elementwise transformation exp(-((d - 3) / 2)**2) to produce the corresponding Gaussian interaction contributions that are later combined with other scoring terms to estimate binding affinity.", "tools": [{"function": {"description": "Computes Autodock Vina's second Gaussian interaction term used in docking pose scoring.\n\nThis function implements the second Gaussian term from the Autodock Vina scoring function (Jain 1996) and is intended for use within DeepChem's docking and pose_scoring utilities to model a short-range attractive component of protein-ligand noncovalent interactions. Given an array of surface distances d (as defined in the Vina formulation), the function applies the elementwise transformation exp(-((d - 3) / 2)**2) to produce the corresponding Gaussian interaction contributions that are later combined with other scoring terms to estimate binding affinity.", "name": "deepchem_dock_pose_scoring_vina_gaussian_second", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of shape `(N, M)` containing the set of surface distances for which the second Gaussian interaction term should be computed. Each element represents a distance value used in the Autodock Vina continuous scoring function (see Jain, 1996). The array must be a numpy.ndarray as required by the implementation; the function performs the calculation elementwise and does not perform explicit validation of contents or shape beyond relying on numpy operations.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a DeepChem docking pose_scoring workflow and need the Autodock Vina *second Gaussian* interaction term for a small batch of protein–ligand surface distances. I started by planning to feed the full 2×4 distance matrix straight into the Gaussian transform, but I just realized one entry is an obvious geometric artifact from a bad contact assignment: it’s the only value that’s *below the typical hard-contact cutoff* for nonbonded heavy-atom surfaces in Vina-style preprocessing, so it should be excluded before scoring.\n\nTake this raw 2×4 matrix of distances (Å):\n[[2.0, 2.5, 3.0, 3.5],\n [1.2, 2.8, 3.3, 4.0]]\n\nFilter out the single artifact distance described above, keep all remaining distances in their original row/column order (with the artifact removed), and compute the Vina second Gaussian term elementwise as exp(-((d - 3)/2)^2). Return the resulting array so I can pass it downstream in the pose scoring pipeline.", "answers": "[{\"name\": \"deepchem_dock_pose_scoring_vina_gaussian_second\", \"arguments\": {\"d\": [[2.0, 2.5, 3.0, 3.5], [2.8, 3.3, 4.0]]}}]"}
{"func_name": "deepchem_feat_graph_features_get_intervals", "func_desc": "deepchem.feat.graph_features.get_intervals computes multiplicative stride intervals for a list of discrete option lists used in DeepChem graph featurization.\n    \n    This function is used in DeepChem's graph feature utilities to compute cumulative product \"intervals\" (mixed-radix strides) when enumerating combinations of categorical choices across multiple feature slots (for example, different atom or edge feature option lists when flattening categorical feature combinations in molecular/graph featurization for drug discovery and related computational chemistry tasks). The implementation adds 1 to every inner list length to ensure an empty option list does not force a zero product; the first interval is initialized to 1 and subsequent intervals are produced by multiplying by (len(inner_list) + 1).", "tools": [{"function": {"description": "deepchem.feat.graph_features.get_intervals computes multiplicative stride intervals for a list of discrete option lists used in DeepChem graph featurization.\n\nThis function is used in DeepChem's graph feature utilities to compute cumulative product \"intervals\" (mixed-radix strides) when enumerating combinations of categorical choices across multiple feature slots (for example, different atom or edge feature option lists when flattening categorical feature combinations in molecular/graph featurization for drug discovery and related computational chemistry tasks). The implementation adds 1 to every inner list length to ensure an empty option list does not force a zero product; the first interval is initialized to 1 and subsequent intervals are produced by multiplying by (len(inner_list) + 1).", "name": "deepchem_feat_graph_features_get_intervals", "parameters": {"properties": {"l": {"type": "array", "items": {"type": "any"}, "description": "A list of lists. Each element of l is expected to be a sequence (typically a list) representing the set of discrete options for one feature slot. The function treats each inner element by taking its length via len(inner). This parameter is the core input that defines the per-slot radices used to compute cumulative stride intervals for indexing or flattening combinations of options.", "default": ""}}, "required": ["l"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a DeepChem atom featurizer where I originally planned to enumerate four categorical slots in this order: (1) formal charges [-1, 0, +1], (2) hybridizations [\"sp\", \"sp2\", \"sp3\", \"sp3d\"], (3) an optional chirality list that is currently empty, and (4) ring sizes [3, 4, 5, 6, 7]. Compute the mixed-radix stride intervals for flattening these combinations. \n\nWait—downstream I realized we can’t keep the chirality slot where it is, because another component assumes the *empty* slot comes last so that missingness only affects the final radix. Don’t drop any slot; instead, reorder the feature slots so that the one with no options is placed at the end, while preserving the relative order of the remaining non-empty slots. Using that corrected slot ordering, compute the stride intervals exactly as DeepChem’s get_intervals does (first interval 1; each next multiplies by (len(inner_list)+1)).", "answers": "[{\"name\": \"deepchem_feat_graph_features_get_intervals\", \"arguments\": {\"l\": [[-1, 0, 1], [\"sp\", \"sp2\", \"sp3\", \"sp3d\"], [3, 4, 5, 6, 7], []]}}]"}
{"func_name": "deepchem_feat_graph_features_id_to_features", "func_desc": "deepchem.feat.graph_features.id_to_features converts a single flattened feature-vector index into the original set of discrete feature indices used by DeepChem graph featurizers. This function reverses the mixed-radix encoding produced when a multi-feature categorical state was flattened to a single integer index (for example, when enumerating or indexing combinations of atom/bond categorical features during featurization). It is typically used in DeepChem's molecular graph featurization pipeline to map an index back to the list of per-feature category indices as returned by get_feature_list().", "tools": [{"function": {"description": "deepchem.feat.graph_features.id_to_features converts a single flattened feature-vector index into the original set of discrete feature indices used by DeepChem graph featurizers. This function reverses the mixed-radix encoding produced when a multi-feature categorical state was flattened to a single integer index (for example, when enumerating or indexing combinations of atom/bond categorical features during featurization). It is typically used in DeepChem's molecular graph featurization pipeline to map an index back to the list of per-feature category indices as returned by get_feature_list().\n", "name": "deepchem_feat_graph_features_id_to_features", "parameters": {"properties": {"id": {"type": "integer", "description": "The 1-based index in a flattened feature vector that encodes a combination of discrete features. In the featurization workflow, this integer represents a position in the flattened space produced by combining multiple categorical feature dimensions. The function internally subtracts 1 from this value to correct for a null/one-based indexing convention used by the caller; therefore callers should pass the same indexing convention used when the flat index was created. If id is negative after correction or otherwise outside the expected encoded range, the returned feature components may be negative or otherwise invalid.", "default": ""}, "intervals": {"type": "array", "items": {"type": "float"}, "description": "List of interval sizes as returned by get_intervals(). Each element in this list is used as the radix (stride) for a corresponding categorical feature when decoding the flattened index and is expected to be an integer-like value that supports integer floor division (//). This list should follow the same ordering and length expected by the featurizer (the implementation assumes six interval entries and will index intervals[0] through intervals[5]). Supplying a shorter list will raise an IndexError; supplying zero-valued entries will raise a ZeroDivisionError when they are used as divisors.", "default": ""}}, "required": ["id", "intervals"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a DeepChem molecular graph featurization run where a 6-way categorical atom state was flattened via mixed-radix encoding. I initially thought the stored state ID was already suitable to feed into `deepchem.feat.graph_features.id_to_features`, but I realized it’s logged as a **1-based** index for human readability, while the featurizer’s internal encoding/decoding expects the **corresponding 0-based** index. Using the same six categorical feature radices (intervals) `[2, 3, 4, 5, 6, 7]`, decode the logged index `250` by first converting it to the index that `id_to_features` actually expects, then return the per-feature category indices.", "answers": "[{\"name\":\"deepchem_feat_graph_features_id_to_features\",\"arguments\":{\"id\":249,\"intervals\":[2,3,4,5,6,7]}}]"}
{"func_name": "deepchem_feat_mol_graphs_cumulative_sum_minus_last", "func_desc": "cumulative_sum_minus_last returns cumulative sums for a sequence of integer counts, omitting the final total. It is intended for reindexing tasks in molecular graph construction and other DeepChem workflows where per-item counts (for example, number of atoms or bonds per molecule) are converted into starting offsets into a flat concatenated array.\n    \n    This function computes the cumulative sum of the input list l with numpy dtype np.int32, inserts an initial 0 so that the first returned value is 0, removes the final element (the overall total), and then adds the integer offset value to every element. Practically, the returned values represent the starting index for each block when concatenating blocks of sizes given by l. Example: l = [3, 2, 4] -> cumulative sums [3, 5, 9], insert initial 0 -> [0, 3, 5, 9], drop last element -> [0, 3, 5], then add offset (default 0) -> [0, 3, 5].", "tools": [{"function": {"description": "cumulative_sum_minus_last returns cumulative sums for a sequence of integer counts, omitting the final total. It is intended for reindexing tasks in molecular graph construction and other DeepChem workflows where per-item counts (for example, number of atoms or bonds per molecule) are converted into starting offsets into a flat concatenated array.\n\nThis function computes the cumulative sum of the input list l with numpy dtype np.int32, inserts an initial 0 so that the first returned value is 0, removes the final element (the overall total), and then adds the integer offset value to every element. Practically, the returned values represent the starting index for each block when concatenating blocks of sizes given by l. Example: l = [3, 2, 4] -> cumulative sums [3, 5, 9], insert initial 0 -> [0, 3, 5, 9], drop last element -> [0, 3, 5], then add offset (default 0) -> [0, 3, 5].", "name": "deepchem_feat_mol_graphs_cumulative_sum_minus_last", "parameters": {"properties": {"l": {"type": "array", "items": {"type": "float"}, "description": "List of integers representing counts for consecutive blocks. In DeepChem this is typically a small list of per-molecule or per-component counts such as numbers of atoms, bonds, or features. Each element is cast to numpy.int32 for the cumulative sum. The length of l determines the length of the returned array; the function does not validate that elements are non-negative, but negative values will affect the computed offsets in the usual arithmetic way.", "default": ""}, "offset": {"type": "integer", "description": "Integer offset to add to every returned cumulative-sum value. Default is 0. In molecular batching workflows this is commonly used to add a base index when building global indexing arrays across multiple molecules or batches.", "default": 0}}, "required": ["l", "offset"], "type": "any"}}, "type": "function"}], "query": "I’m flattening a mini-batch of molecular graphs into a single global atom array for a DeepChem-style featurization pass. I initially planned to treat the three molecules as having 12, 8, and 15 atoms and to use the running global atom count from earlier batches as the offset. But I just realized the third structure includes exactly one dummy/placeholder atom (it’s only there to anchor a coordinate frame) that must NOT be counted toward indexing in the concatenated atom tensor. Keep the first two molecules’ counts unchanged, adjust only the last molecule’s count accordingly, and then compute the starting atom indices for each molecule using an offset equal to the number of atoms already present from prior batches (which is 100).", "answers": "[{\"name\": \"deepchem_feat_mol_graphs_cumulative_sum_minus_last\", \"arguments\": {\"l\": [12, 8, 14], \"offset\": 100}}]"}
{"func_name": "deepchem_feat_sequence_featurizer___matrix_featurizer_PFM_to_PPM", "func_desc": "deepchem.feat.sequence_featurizers.position_frequency_matrix_featurizer.PFM_to_PPM converts a position frequency matrix (PFM) into a position probability matrix (PPM) by normalizing each column so the entries per column represent relative probabilities. In the DeepChem sequence featurizers context, a PFM is typically a 2-D numpy.ndarray of counts (for example counts of nucleotides or amino acids at each alignment position) and the resulting PPM is used by downstream featurizers and models to represent motif or positional composition as probabilities rather than raw counts.", "tools": [{"function": {"description": "deepchem.feat.sequence_featurizers.position_frequency_matrix_featurizer.PFM_to_PPM converts a position frequency matrix (PFM) into a position probability matrix (PPM) by normalizing each column so the entries per column represent relative probabilities. In the DeepChem sequence featurizers context, a PFM is typically a 2-D numpy.ndarray of counts (for example counts of nucleotides or amino acids at each alignment position) and the resulting PPM is used by downstream featurizers and models to represent motif or positional composition as probabilities rather than raw counts.\n", "name": "deepchem_feat_sequence_featurizer___matrix_featurizer_PFM_to_PPM", "parameters": {"properties": {"pfm": {"type": "array", "items": {"type": "float"}, "description": "A 2-D array representing a position frequency matrix. Rows correspond to alphabet symbols (for example nucleotides or amino acids) and columns correspond to sequence positions. Each element pfm[i, j] is the count or weight for symbol i at position j. This function expects pfm to be indexable as pfm[:, col] and to have at least two dimensions; passing a non-2-D array will raise an IndexError when attempting to access shape[1].", "default": ""}}, "required": ["pfm"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a DNA motif for a probabilistic scanner in DeepChem. I was going to feed the raw PFM straight into `deepchem.feat.sequence_featurizers.position_frequency_matrix_featurizer.PFM_to_PPM`, but I just realized one alignment position is clearly an artifact: its entire column sums to zero (no reads contributed at that position due to a trimming bug). That will blow up a naive normalization.\n\nSo: first, drop the motif position(s) whose column total is exactly zero (keep the row order A,C,G,T, and preserve the left-to-right order of the remaining positions). Then convert the resulting cleaned PFM into a PPM by normalizing each remaining column so it sums to 1.\n\nUse this PFM (rows A,C,G,T; columns are motif positions 1–4):\nPFM = [[10, 2, 0, 1],\n       [0,  3, 5, 1],\n       [0,  5, 5, 8],\n       [0,  0, 0, 0]]", "answers": "[{\"name\":\"deepchem_feat_sequence_featurizer___matrix_featurizer_PFM_to_PPM\",\"arguments\":{\"pfm\":[[10,2,1],[0,3,1],[0,5,8],[0,0,0]]}}]"}
{"func_name": "deepchem_models_losses_log_sum_exp", "func_desc": "deepchem.models.losses.log_sum_exp computes a numerically stable log-sum-exp reduction over a specified axis. This function is intended for use in DeepChem loss and model computations where one needs to aggregate scores or logits across a dimension (for example computing log-partition functions, stable denominators for softmax, or reductions used in cross-entropy and likelihood calculations in molecular machine learning tasks).", "tools": [{"function": {"description": "deepchem.models.losses.log_sum_exp computes a numerically stable log-sum-exp reduction over a specified axis. This function is intended for use in DeepChem loss and model computations where one needs to aggregate scores or logits across a dimension (for example computing log-partition functions, stable denominators for softmax, or reductions used in cross-entropy and likelihood calculations in molecular machine learning tasks).\n", "name": "deepchem_models_losses_log_sum_exp", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input tensor containing raw scores, logits, or real-valued quantities produced by a model or intermediate computation. In the DeepChem context, x is typically a floating-point tensor representing per-example or per-class values (for example, model outputs for each class or energy scores over a set of states). The function performs no in-place modification of x; it reads x to compute a new tensor y.", "default": ""}, "axis": {"type": "integer", "nullable": true, "description": "Axis (dimension) along which to perform the log-sum-exp reduction. This should be an integer indexing a valid dimension of x. The implementation applies a numerically stable algorithm by first taking the maximum along axis, subtracting that maximum from x, exponentiating, summing over axis, taking the logarithm, and finally adding the maximum back. The signature provides a default value of None, but passing axis=None will cause torch.max and the subsequent reduction calls in the implementation to raise an error (TypeError or IndexError) because an integer dimension is required. Supplying an integer outside the valid range for x will similarly raise an IndexError. The caller is responsible for providing an appropriate integer axis.", "default": null}}, "required": ["x", "axis"], "type": "any"}}, "type": "function"}], "query": "I’m aggregating per-molecule class logits into a numerically stable log-partition term for a multitask molecular classifier, but I just realized our training pipeline concatenates a per-molecule “calibration/bias” channel as the last column. I initially planned to reduce over all columns, but that would incorrectly treat the bias channel as a class. Using deepchem.models.losses.log_sum_exp, compute the log-sum-exp per molecule across only the true class dimension (i.e., exclude the appended bias channel). Use the 2D tensor x=[[1.2, -0.7, 3.5, -9.9], [0.0, 2.1, -1.3, -9.9], [4.2, 3.3, 0.5, -9.9]]. Reduce along the axis corresponding to classes and return the resulting 1D tensor (one value per molecule).", "answers": "[{\"name\":\"deepchem_models_losses_log_sum_exp\",\"arguments\":{\"x\":[[1.2,-0.7,3.5],[0.0,2.1,-1.3],[4.2,3.3,0.5]],\"axis\":1}}]"}
{"func_name": "deepchem_trans_transformers_get_cdf_values", "func_desc": "get_cdf_values computes per-column empirical cumulative distribution function (CDF) quantile values for a 1D or 2D numeric array. This helper is used in DeepChem transformer code to map raw feature values (rows = examples, columns = features) to discretized CDF-like values by splitting the sorted data into a fixed number of bins and assigning each sample a bin-based quantile. It is intended for preprocessing and normalization steps in molecular machine-learning workflows where consistent, column-wise percentile mapping is required.", "tools": [{"function": {"description": "get_cdf_values computes per-column empirical cumulative distribution function (CDF) quantile values for a 1D or 2D numeric array. This helper is used in DeepChem transformer code to map raw feature values (rows = examples, columns = features) to discretized CDF-like values by splitting the sorted data into a fixed number of bins and assigning each sample a bin-based quantile. It is intended for preprocessing and normalization steps in molecular machine-learning workflows where consistent, column-wise percentile mapping is required.\n", "name": "deepchem_trans_transformers_get_cdf_values", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "any"}, "description": "Input data to be mapped to CDF-like values. Must be either a 1-D array of shape (n_rows,) or a 2-D array of shape (n_rows, n_cols). Rows are interpreted as independent samples (examples) and columns as separate features/channels. If a 1-D array is provided, it is internally reshaped to shape (n_rows, 1) so the output always has two dimensions. Arrays with more than two dimensions are not supported. The function does not modify the input array in place; it returns a new numpy.ndarray.", "default": ""}, "bins": {"type": "integer", "description": "Number of bins to split the data into when computing the quantile mapping. This integer controls the discretization resolution: larger values produce finer-grained quantile steps. The implementation requires bins to be an integer greater than 1 (bins <= 1 will cause division-by-zero or invalid partitioning). If bins is odd the code normalizes by (bins - 1) producing values that can reach 1.0; if bins is even the code normalizes by bins producing values with maximum (bins - 1) / bins (< 1.0). Non-integer or non-positive inputs for this parameter will raise an error (TypeError for non-int types, or runtime errors such as ZeroDivisionError for invalid values).", "default": ""}}, "required": ["array", "bins"], "type": "any"}}, "type": "function"}], "query": "I’m preprocessing a small molecular descriptor table (rows = molecules; columns = [logP, TPSA, MW]) for a DeepChem-style CDF discretization step. Start by computing per-column CDF-like quantile-bin values using the same helper as before on this 6×3 matrix:\n[[1.2, 35.0, 250.5],\n [0.7, 42.1, 310.2],\n [2.1, 28.4, 180.0],\n [1.2, 35.0, 400.0],\n [3.4, 55.2, 290.1],\n [0.1, 20.0, 210.3]].\n\nWait—our downstream model exports the bin index into a fixed-width integer feature, and it only supports a number of bins that is an exact power of two *and* does not exceed the smallest feature dimension we have here (the number of molecules). I initially planned to use 8 bins, but that violates the “not exceeding the sample count” constraint. Recompute the per-column CDF-mapped values using the largest allowed bin count under those constraints.", "answers": "[{\"name\": \"deepchem_trans_transformers_get_cdf_values\", \"arguments\": {\"array\": [[1.2, 35.0, 250.5], [0.7, 42.1, 310.2], [2.1, 28.4, 180.0], [1.2, 35.0, 400.0], [3.4, 55.2, 290.1], [0.1, 20.0, 210.3]], \"bins\": 4}}]"}
{"func_name": "deepchem_utils_dft_utils_grid_radial_grid_get_xw_integration", "func_desc": "Return n quadrature points and corresponding integration weights on the interval [-1, 1] using a chosen one-dimensional quadrature rule.\n    \n    This function is used by DeepChem's DFT utilities (deepchem.utils.dft_utils.grid.radial_grid) to build one-dimensional quadrature rules that serve as the radial component of numerical integration grids in quantum chemistry and density functional theory workflows. The produced points and weights are suitable for numerically approximating integrals of functions defined on [-1, 1], for example the radial part of atomic orbitals or density components after a coordinate transformation. Three integrator choices are provided: two Chebyshev-based nonuniform grids (useful for concentrating points near boundaries) and a uniform/trapezoidal grid (simple, evenly spaced points).", "tools": [{"function": {"description": "Return n quadrature points and corresponding integration weights on the interval [-1, 1] using a chosen one-dimensional quadrature rule.\n\nThis function is used by DeepChem's DFT utilities (deepchem.utils.dft_utils.grid.radial_grid) to build one-dimensional quadrature rules that serve as the radial component of numerical integration grids in quantum chemistry and density functional theory workflows. The produced points and weights are suitable for numerically approximating integrals of functions defined on [-1, 1], for example the radial part of atomic orbitals or density components after a coordinate transformation. Three integrator choices are provided: two Chebyshev-based nonuniform grids (useful for concentrating points near boundaries) and a uniform/trapezoidal grid (simple, evenly spaced points).", "name": "deepchem_utils_dft_utils_grid_radial_grid_get_xw_integration", "parameters": {"properties": {"n": {"type": "integer", "description": "Number of grid points to generate. This integer controls the resolution of the quadrature: larger n increases quadrature accuracy at increased computational cost. For the Chebyshev-based integrators ('chebyshev' and 'chebyshev2') values n >= 1 are supported by the implementation. For the 'uniform' integrator n must be at least 2 because the routine computes the spacing using x[1] and will raise an IndexError otherwise. The function does not validate types beyond relying on NumPy operations, so passing a non-integer may produce NumPy/TypeErrors.", "default": ""}, "s0": {"type": "string", "description": "Name of the grid integrator to use; comparison is case-insensitive because the implementation lower-cases this string. Available options are 'chebyshev', 'chebyshev2', and 'uniform'. 'chebyshev' implements the specific Chebyshev-based node and weight formula used in the referenced DFT/radial-grid literature (non-uniform clustering with weights computed from trigonometric expressions). 'chebyshev2' returns Chebyshev-type nodes defined by cos(ipn1) and their associated weights. 'uniform' returns an evenly spaced grid on [-1, 1] with trapezoidal rule weights (endpoints half-weighted). Use the Chebyshev options when endpoint clustering or specific polynomial-based quadrature behavior is required; use 'uniform' for simple, equally spaced sampling.", "default": ""}}, "required": ["n", "s0"], "type": "any"}}, "type": "function"}], "query": "I’m building a toy DFT radial grid (after mapping r\\in[0,\\infty) to x\\in[-1,1]) and I initially thought to use the simple evenly spaced / trapezoidal-style 1D quadrature on [-1,1] with 8 nodes. But I just realized my endpoint values are numerically unstable for the transformed integrand (it behaves badly right at x=\\u00b11), so I need a quadrature rule that avoids placing nodes exactly at the boundaries while still clustering nodes toward the boundaries (to resolve near-nucleus and asymptotic features). Keep the node count the same as before and generate the points and weights using the Chebyshev-based option that excludes the endpoints (i.e., the nonuniform variant that uses interior extrema rather than including x=\\u00b11).", "answers": "[{\"name\":\"deepchem_utils_dft_utils_grid_radial_grid_get_xw_integration\",\"arguments\":{\"n\":8,\"s0\":\"chebyshev2\"}}]"}
{"func_name": "deepchem_utils_differentiation_utils_misc_set_default_option", "func_desc": "deepchem.utils.differentiation_utils.misc.set_default_option returns a merged options dictionary by taking a shallow copy of a dictionary of default options and updating it with user-provided options so that keys in opt override those in defopt. This function is used in DeepChem (a library for deep learning in drug discovery, materials science, quantum chemistry, and biology) to construct concrete configuration dictionaries for components such as models, training loops, data preprocessors, and differentiation utilities where sensible defaults must be combined with user overrides.\n    \n    This function makes a shallow copy of defopt to detach the returned mapping object from the original default mapping, then updates that copy in-place with opt. Because the copy is shallow, mutable values stored in defopt (for example lists, dicts, or objects representing hyperparameters or preprocessors) remain the same objects in the returned dictionary unless they are overridden by entries in opt. Keys present in opt will replace the corresponding keys from defopt; keys absent from opt will retain the values from defopt.", "tools": [{"function": {"description": "deepchem.utils.differentiation_utils.misc.set_default_option returns a merged options dictionary by taking a shallow copy of a dictionary of default options and updating it with user-provided options so that keys in opt override those in defopt. This function is used in DeepChem (a library for deep learning in drug discovery, materials science, quantum chemistry, and biology) to construct concrete configuration dictionaries for components such as models, training loops, data preprocessors, and differentiation utilities where sensible defaults must be combined with user overrides.\n\nThis function makes a shallow copy of defopt to detach the returned mapping object from the original default mapping, then updates that copy in-place with opt. Because the copy is shallow, mutable values stored in defopt (for example lists, dicts, or objects representing hyperparameters or preprocessors) remain the same objects in the returned dictionary unless they are overridden by entries in opt. Keys present in opt will replace the corresponding keys from defopt; keys absent from opt will retain the values from defopt.", "name": "deepchem_utils_differentiation_utils_misc_set_default_option", "parameters": {"properties": {"defopt": {"type": "any", "description": "Default options. A dictionary containing canonical default configuration values (for example, default hyperparameters like learning rate, batch size, optimizer settings, or preprocessing flags). The function will make a shallow copy of this dictionary to form the basis of the returned options mapping.", "default": ""}, "opt": {"type": "any", "description": "Options. A dictionary of user-specified option values that should override or extend defaults in defopt. Keys in this dictionary take precedence and will replace the corresponding entries from defopt in the returned dictionary.", "default": ""}}, "required": ["defopt", "opt"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a DeepChem training loop config using `deepchem.utils.differentiation_utils.misc.set_default_option`, starting from these defaults:\n\n- optimizer=\"adam\"\n- learning_rate=0.001\n- batch_size=64\n- max_epochs=50\n- callbacks=[\"early_stopping\",\"lr_scheduler\"]\n\nInitially I was going to override `batch_size` to 128 and set `learning_rate` to 0.0003, plus add `gradient_clip_norm=1.0`.\n\nWait—because `set_default_option` only does a **shallow** copy, if I just merge and then later append an extra callback during an ablation sweep, I’ll accidentally mutate the original default `callbacks` list too. So instead, before merging, treat `callbacks` in the defaults as immutable by replacing it with a new list object containing the same two strings (everything else in defaults stays the same).\n\nThen perform the merge where:\n- `batch_size` becomes twice the original default `batch_size`\n- `learning_rate` becomes 30% of the original default `learning_rate`\n- add `gradient_clip_norm=1.0`\n\nReturn the merged options dictionary produced by calling `set_default_option` on the adjusted defaults and these overrides.", "answers": "[{\"name\": \"deepchem_utils_differentiation_utils_misc_set_default_option\", \"arguments\": {\"defopt\": {\"optimizer\": \"adam\", \"learning_rate\": 0.001, \"batch_size\": 64, \"max_epochs\": 50, \"callbacks\": [\"early_stopping\", \"lr_scheduler\"]}, \"opt\": {\"batch_size\": 128, \"learning_rate\": 0.0003, \"gradient_clip_norm\": 1.0}}}]"}
{"func_name": "deepchem_utils_genomics_utils_seq_one_hot_encode", "func_desc": "One-hot encodes a collection of genomic sequences into an image-like NumPy array for use in convolutional or other models that expect channeled image inputs in genomics and computational biology workflows (for example, converting DNA sequences to inputs for CNNs used in regulatory element prediction or sequence-based models in drug discovery). The function maps each character in the input sequences to a channel index according to the provided letters string and returns a 4D array where the second axis indexes letters and the final axis is a singleton channel dimension so the result can be treated as an image with one color channel.", "tools": [{"function": {"description": "One-hot encodes a collection of genomic sequences into an image-like NumPy array for use in convolutional or other models that expect channeled image inputs in genomics and computational biology workflows (for example, converting DNA sequences to inputs for CNNs used in regulatory element prediction or sequence-based models in drug discovery). The function maps each character in the input sequences to a channel index according to the provided letters string and returns a 4D array where the second axis indexes letters and the final axis is a singleton channel dimension so the result can be treated as an image with one color channel.\n", "name": "deepchem_utils_genomics_utils_seq_one_hot_encode", "parameters": {"properties": {"sequences": {"type": "array", "items": {"type": "any"}, "description": "Iterable container of genetic sequences to encode. If a numpy.ndarray is provided, the function expects an array-like sequence of equal-length sequence objects (e.g., strings, Bio.SeqRecord objects whose string representation yields the sequence) and will index into the array to obtain the first element then process the remainder via slicing (sequences[1:]). If an iterator is provided, the function will call next(sequences) to obtain the first sequence and then iterate the remainder from the same iterator; therefore the iterator will be partially consumed (the first element is removed) as a side effect. All sequences must support len() and iteration over characters and must have the same length; otherwise a ValueError is raised. This parameter is the primary input representing the DNA/RNA sequences to convert to a neural-network-ready numeric representation used in DeepChem genomics utilities.", "default": ""}, "letters": {"type": "string", "description": "String containing the ordered set of possible letters (alphabet) present in the sequences. Default is \"ATCGN\". The order of characters in this string determines the ordering of channels in the output (the second axis of the returned array). The function builds an internal mapping from each character in letters to a channel index; any character in the input sequences that is not present in letters will cause a KeyError during encoding. Use this parameter to control which nucleotides or ambiguous symbols are recognized and their channel ordering for downstream models.", "default": "ATCGN"}}, "required": ["sequences", "letters"], "type": "any"}}, "type": "function"}], "query": "I’m prepping inputs for a CNN that treats one-hot DNA as an image tensor, but I just realized my sequencing pipeline produces IUPAC ambiguity codes and some occasional junk characters from a bad demultiplexing step. Start by one-hot encoding these 3 equal-length 12bp sequences: [\"ACGTNACGTNAC\",\"NNNNACGTACGT\",\"TGCANNTGCANN\"]. Initially I was going to keep the channel order with the ambiguous base first (so my first channel is the unknown/ambiguous bucket), but then I noticed my model’s first channel is actually reserved for the canonical base that comes alphabetically before the others. So keep the same 5-letter alphabet and characters as before, but rotate/reorder it so the ambiguous base ends up in the last channel while preserving the relative order of the four canonical bases. Use that adjusted alphabet when you call the one-hot encoder.", "answers": "[{\"name\":\"deepchem_utils_genomics_utils_seq_one_hot_encode\",\"arguments\":{\"sequences\":[\"ACGTNACGTNAC\",\"NNNNACGTACGT\",\"TGCANNTGCANN\"],\"letters\":\"ACGTN\"}}]"}
{"func_name": "deepchem_utils_geometry_utils_compute_centroid", "func_desc": "Compute the (x, y, z) centroid of the provided atomic coordinates.\n    \n    This utility computes the arithmetic mean position (centroid) over the set of 3D coordinates supplied. In the DeepChem molecular machine-learning context (drug discovery, materials science, quantum chemistry, biology), this centroid is commonly used as the geometric center of a molecule's atom positions for preprocessing tasks such as translating a molecule to the origin, centering coordinate-based features, or as a reference point for alignment and distance calculations. This function computes the center of geometry (simple mean of coordinates) and is distinct from a mass-weighted center of mass.", "tools": [{"function": {"description": "Compute the (x, y, z) centroid of the provided atomic coordinates.\n\nThis utility computes the arithmetic mean position (centroid) over the set of 3D coordinates supplied. In the DeepChem molecular machine-learning context (drug discovery, materials science, quantum chemistry, biology), this centroid is commonly used as the geometric center of a molecule's atom positions for preprocessing tasks such as translating a molecule to the origin, centering coordinate-based features, or as a reference point for alignment and distance calculations. This function computes the center of geometry (simple mean of coordinates) and is distinct from a mass-weighted center of mass.", "name": "deepchem_utils_geometry_utils_compute_centroid", "parameters": {"properties": {"coordinates": {"type": "array", "items": {"type": "any"}, "description": "A numpy array of shape (N, 3), where N is the number of atoms and each row is an (x, y, z) coordinate for an atom. This argument represents the 3D positions of atoms in a molecule or structure; providing correctly shaped coordinates is required for the returned centroid to correspond to (x, y, z) geometry used in downstream DeepChem featurizers, alignments, and models. The function does not modify this array in-place.", "default": ""}}, "required": ["coordinates"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a docked ligand pose for coordinate-based featurization. I was going to compute the geometric centroid of all 5 atoms below, but I just noticed one atom is a spurious “ghost” from an upstream conformer merge: it’s the only atom with a negative x-coordinate *and* a positive z-coordinate, which shouldn’t be included when centering the real ligand heavy-atom cloud. Compute the **geometric centroid (simple mean, not mass-weighted)** using only the remaining atoms. Coordinates (Å): A1 (1.20, -0.50, 0.00), A2 (0.80, 1.10, -0.40), A3 (-1.00, 0.30, 0.90), A4 (0.50, -1.20, 1.10), A5 (-0.70, 0.40, -0.60).", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_compute_centroid\",\"arguments\":{\"coordinates\":[[1.2,-0.5,0.0],[0.8,1.1,-0.4],[0.5,-1.2,1.1],[-0.7,0.4,-0.6]]}}]"}
{"func_name": "deepchem_utils_geometry_utils_compute_protein_range", "func_desc": "Compute the protein range of provided coordinates.\n    \n    This function computes the axis-aligned extent (size) of a protein (or any set of atoms)\n    along the three Cartesian axes by taking the element-wise difference between the\n    maximum and minimum coordinates across all atoms. In DeepChem workflows (for example,\n    preparing inputs for voxelization, bounding-box cropping, or spatial featurizers in\n    drug discovery and structural biology), this range vector is used to determine the\n    size of the protein along x, y, and z and to guide grid sizing, padding, or scale\n    normalization. The returned values are in the same units as the input coordinates\n    (typically Angstroms when coordinates are read from PDB files).", "tools": [{"function": {"description": "Compute the protein range of provided coordinates.\n\nThis function computes the axis-aligned extent (size) of a protein (or any set of atoms)\nalong the three Cartesian axes by taking the element-wise difference between the\nmaximum and minimum coordinates across all atoms. In DeepChem workflows (for example,\npreparing inputs for voxelization, bounding-box cropping, or spatial featurizers in\ndrug discovery and structural biology), this range vector is used to determine the\nsize of the protein along x, y, and z and to guide grid sizing, padding, or scale\nnormalization. The returned values are in the same units as the input coordinates\n(typically Angstroms when coordinates are read from PDB files).", "name": "deepchem_utils_geometry_utils_compute_protein_range", "parameters": {"properties": {"coordinates": {"type": "array", "items": {"type": "any"}, "description": "A numpy array of shape (N, 3), where N is the\nnumber of atoms and the three columns correspond to Cartesian x, y, z\ncoordinates. The array must be numeric and finite for meaningful results.\nThe function treats rows as independent atom positions and computes the\nper-axis maxima and minima across rows. Providing an empty array, an array\nwith an unexpected shape (not 2D with size-3 second dimension), or arrays\ncontaining only NaNs will result in errors or propagate NaNs from numpy\noperations (see failure modes below).", "default": ""}}, "required": ["coordinates"], "type": "any"}}, "type": "function"}], "query": "I’m voxelizing a binding-site fragment from a PDB, but I just realized one of the coordinate triplets in my extracted list is a parse artifact (it has the most negative value along the y-axis and came from a line with a missing occupancy, so it shouldn’t define the bounding box). Compute the axis-aligned protein range (extent) in Å using all the *remaining* atoms. Coordinates: [[12.4, -3.1, 7.0], [15.9, -1.8, 10.2], [11.2, -4.5, 9.6], [14.0, -2.9, 6.3], [13.5, -5.0, 8.8]].", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_compute_protein_range\",\"arguments\":{\"coordinates\":[[12.4,-3.1,7.0],[15.9,-1.8,10.2],[14.0,-2.9,6.3],[13.5,-5.0,8.8]]}}]"}
{"func_name": "deepchem_utils_geometry_utils_is_angle_within_cutoff", "func_desc": "deepchem.utils.geometry_utils.is_angle_within_cutoff: Determine whether two 3-D vectors are within a specified angular deviation (in degrees) of being exactly 180 degrees apart. This function is used in DeepChem geometry utilities to detect anti-parallel or nearly anti-parallel directions (for example, opposing bond directions or opposite normals) by computing the angle between vectors in degrees and testing whether that angle lies strictly within the symmetric interval (180 - angle_cutoff, 180 + angle_cutoff).", "tools": [{"function": {"description": "deepchem.utils.geometry_utils.is_angle_within_cutoff: Determine whether two 3-D vectors are within a specified angular deviation (in degrees) of being exactly 180 degrees apart. This function is used in DeepChem geometry utilities to detect anti-parallel or nearly anti-parallel directions (for example, opposing bond directions or opposite normals) by computing the angle between vectors in degrees and testing whether that angle lies strictly within the symmetric interval (180 - angle_cutoff, 180 + angle_cutoff).\n", "name": "deepchem_utils_geometry_utils_is_angle_within_cutoff", "parameters": {"properties": {"vector_i": {"type": "array", "items": {"type": "any"}, "description": "A 1-D numpy array of shape (3,) representing a 3D vector with components (x, y, z). In molecular and materials contexts (per the DeepChem project), this typically encodes a bond direction, displacement, or normal vector. The function expects a real-valued vector of length 3; providing arrays of other shapes may lead to incorrect results or exceptions.", "default": ""}, "vector_j": {"type": "array", "items": {"type": "any"}, "description": "A 1-D numpy array of shape (3,) representing a second 3D vector with components (x, y, z). As with vector_i, this usually represents a geometric direction in molecular structures. The relative orientation of vector_i and vector_j is tested against the 180-degree criterion.", "default": ""}, "angle_cutoff": {"type": "float", "description": "The allowed deviation from exactly 180 degrees, expressed in degrees. For example, angle_cutoff = 5.0 means the function returns True when the angle between vector_i and vector_j is strictly greater than 175.0 degrees and strictly less than 185.0 degrees. The comparison is strict (uses > and <), so exact boundary values equal to 180 +/- angle_cutoff do not return True.", "default": ""}}, "required": ["vector_i", "vector_j", "angle_cutoff"], "type": "any"}}, "type": "function"}], "query": "I’m running a quick post-docking geometry sanity check in DeepChem to see whether the ligand’s putative exit-direction is essentially pointing opposite the receptor pocket normal. Start by using deepchem.utils.geometry_utils.is_angle_within_cutoff on the vectors [1.0, 0.2, -0.1] (ligand exit-direction) and [-1.0, -0.15, 0.05] (pocket normal) with the initially planned angular tolerance. However, I just realized our downstream pose filter flags borderline anti-parallel cases unless we use the *stricter* tolerance that is exactly half of what we first intended. Use that stricter tolerance and report whether the vectors pass the anti-parallel check.", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_is_angle_within_cutoff\",\"arguments\":{\"vector_i\":[1.0,0.2,-0.1],\"vector_j\":[-1.0,-0.15,0.05],\"angle_cutoff\":5.0}}]"}
{"func_name": "deepchem_utils_geometry_utils_unit_vector", "func_desc": "deepchem.utils.geometry_utils.unit_vector returns the unit (normalized) vector of a three-dimensional Cartesian vector. This utility is part of DeepChem's geometry utilities used in molecular machine learning and computational chemistry workflows (for example, to normalize bond direction vectors, compute molecular orientation axes, or prepare direction features for models).", "tools": [{"function": {"description": "deepchem.utils.geometry_utils.unit_vector returns the unit (normalized) vector of a three-dimensional Cartesian vector. This utility is part of DeepChem's geometry utilities used in molecular machine learning and computational chemistry workflows (for example, to normalize bond direction vectors, compute molecular orientation axes, or prepare direction features for models).\n", "name": "deepchem_utils_geometry_utils_unit_vector", "parameters": {"properties": {"vector": {"type": "array", "items": {"type": "any"}, "description": "A numpy array of shape `(3,)` representing a 3D Cartesian vector (x, y, z). This argument is the input vector to be normalized. The function expects exactly three components as used in DeepChem geometry operations; providing a different shape is not supported by this implementation and may lead to incorrect results or runtime errors.", "default": ""}}, "required": ["vector"], "type": "any"}}, "type": "function"}], "query": "I’m engineering orientation features for a molecular ML model and started by taking the bond direction from atom A to atom B as the raw Cartesian displacement (in Å): (3.0, -4.0, 12.0). I was going to normalize that directly, but then I realized my featurizer assumes the **axis points from B back to A** (otherwise my chirality-dependent features flip sign between conformers). So please normalize the **same bond direction but with the opposite sign** (i.e., reverse the displacement before normalization) using DeepChem’s geometry utility, and return the resulting 3D unit vector.", "answers": "[{\"name\": \"deepchem_utils_geometry_utils_unit_vector\", \"arguments\": {\"vector\": [-3.0, 4.0, -12.0]}}]"}
{"func_name": "deepchem_utils_graph_utils_fourier_encode_dist", "func_desc": "deepchem.utils.graph_utils.fourier_encode_dist performs a Fourier-style encoding of a tensor of scalar values (typically inter-node or inter-atomic distances used in graph-based molecular models) by applying sinusoidal basis functions at multiple scales. The function divides the input values by powers-of-two scales (2**i for i in range(num_encodings)), computes sine and cosine for each scaled value, and concatenates these trigonometric features. Optionally, the original input values can be appended to the encoded features to preserve the absolute magnitude information. This encoding is commonly used in graph neural networks and other geometric deep learning models within DeepChem to provide models with multi-scale, periodic features that help represent distance-based relationships in molecules and materials.", "tools": [{"function": {"description": "deepchem.utils.graph_utils.fourier_encode_dist performs a Fourier-style encoding of a tensor of scalar values (typically inter-node or inter-atomic distances used in graph-based molecular models) by applying sinusoidal basis functions at multiple scales. The function divides the input values by powers-of-two scales (2**i for i in range(num_encodings)), computes sine and cosine for each scaled value, and concatenates these trigonometric features. Optionally, the original input values can be appended to the encoded features to preserve the absolute magnitude information. This encoding is commonly used in graph neural networks and other geometric deep learning models within DeepChem to provide models with multi-scale, periodic features that help represent distance-based relationships in molecules and materials.\n", "name": "deepchem_utils_graph_utils_fourier_encode_dist", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Input tensor containing scalar values to encode (for example, pairwise distances or edge attributes in a molecular graph). The function uses x.device and x.dtype to allocate intermediate tensors on the same device and with the same dtype; x must therefore be a torch.Tensor. The function will internally unsqueeze x at the final axis before encoding and will squeeze singleton dimensions before returning, so the leading (batch) dimensions are preserved while the final axis is replaced by the encoding features.", "default": ""}, "num_encodings": {"type": "integer", "description": "Number of Fourier encodings (scales) to apply. For each i in range(num_encodings) the function uses a scale factor 2**i and computes both sin(x / 2**i) and cos(x / 2**i). Defaults to 4. Must be a positive integer; providing zero or a negative value will result in an error during the computation (the implementation assumes at least one encoding).", "default": 4}, "include_self": {"type": "boolean", "description": "If True (default), the original input values (after the temporary unsqueeze) are concatenated to the trigonometric encodings so that the output retains the raw scalar in addition to the Fourier features. If False, only the sine and cosine features are returned.", "default": true}}, "required": ["x", "include_self", "num_encodings"], "type": "any"}}, "type": "function"}], "query": "I’m assembling edge features for a toy molecular graph, and I was going to Fourier-encode this 2×3 tensor of interatomic distances (Å): [[1.09, 1.43, 2.35],[0.96, 1.54, 3.12]] using six dyadic scales and without concatenating the raw distances. But I just realized our training pipeline expects the *full* number of channels produced by the encoding step, including the absolute-distance channel for calibration against a separate bond-length prior. So keep the same distance tensor and the same number of dyadic scales, but modify the encoding so that the output includes the original distances appended to the sinusoidal features.", "answers": "[{\"name\": \"deepchem_utils_graph_utils_fourier_encode_dist\", \"arguments\": {\"x\": [[1.09, 1.43, 2.35], [0.96, 1.54, 3.12]], \"num_encodings\": 6, \"include_self\": true}}]"}
{"func_name": "deepchem_utils_graph_utils_scale_attenuation", "func_desc": "deepchem.utils.graph_utils.scale_attenuation scales node feature tensors by a degree-dependent attenuation factor computed from training-set averages. This function implements the elementwise operation h * (avg_d[\"log\"] / np.log(D + 1)), which realizes the attenuation described in the source as (log(D + 1))^-1 re-scaled by the training-set average of (log(D + 1))^-1. In DeepChem this is used to normalize attention or message-passing updates in graph neural network models for molecular and biological graphs so that nodes with different connectivity (degree) are attenuated according to statistics observed during training.", "tools": [{"function": {"description": "deepchem.utils.graph_utils.scale_attenuation scales node feature tensors by a degree-dependent attenuation factor computed from training-set averages. This function implements the elementwise operation h * (avg_d[\"log\"] / np.log(D + 1)), which realizes the attenuation described in the source as (log(D + 1))^-1 re-scaled by the training-set average of (log(D + 1))^-1. In DeepChem this is used to normalize attention or message-passing updates in graph neural network models for molecular and biological graphs so that nodes with different connectivity (degree) are attenuated according to statistics observed during training.\n", "name": "deepchem_utils_graph_utils_scale_attenuation", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "float"}, "description": "Input feature tensor representing node features or per-node messages produced by a graph neural network layer. In the DeepChem graph modeling context, h contains the values that will be attenuated by node degree so that high- and low-degree nodes are scaled consistently with training-set statistics.", "default": ""}, "D": {"type": "array", "items": {"type": "float"}, "description": "Degree tensor. Each element contains the degree (connectivity) value for the corresponding node or graph element. The function computes np.log(D + 1) from this tensor and uses it in the denominator of the attenuation factor. D is expected to contain values for which np.log(D + 1) is defined (typically non-negative degrees in molecular/graph data). Supplying values that make np.log(D + 1) equal to zero or undefined will lead to infinities or NaNs.", "default": ""}, "avg_d": {"type": "any", "description": "Dictionary containing averages computed over the training set. This function reads the key \"log\" from avg_d and expects avg_d[\"log\"] to be a numeric scalar (or a tensor/array-like broadcastable with h) that represents the training-set average of (log(D + 1))^-1. The value avg_d[\"log\"] is used as the numerator in the re-scaling factor. If the key \"log\" is missing, a KeyError will be raised.", "default": ""}}, "required": ["h", "D", "avg_d"], "type": "any"}}, "type": "function"}], "query": "I’m re-checking our node-wise attention normalization for a molecular GNN, and I realized I can’t just run degree attenuation on the raw degree tensor as-is because the batch has a padding artifact: some entries of D are 0 (these correspond to padded nodes/phantom edges), and the upstream code guarantees *true* node degrees are never below 1 for this dataset split. So: take the given h and D, but first sanitize D by replacing any non-physical degree values with the smallest physically allowed degree for this dataset, and then apply DeepChem’s degree-dependent attenuation using the training-set statistic stored in avg_d under the key \"log\" (0.72). Use the exact elementwise rule implemented by scale_attenuation: h * (avg_d[\"log\"] / np.log(D + 1)).\n\nData:\n- h (3 nodes × 4 dims): [[0.5, 1.2, -0.3, 2.0], [1.0, -0.5, 0.8, 0.0], [2.1, 0.3, -1.2, 1.5]]\n- D (same shape): [[1.0, 2.0, 3.0, 1.0], [4.0, 1.0, 2.0, 0.0], [3.0, 3.0, 1.0, 2.0]]\n- avg_d: {\"log\": 0.72}\n\nPlease output the attenuated node features after this correction.", "answers": "[{\"name\":\"deepchem_utils_graph_utils_scale_attenuation\",\"arguments\":{\"h\":[[0.5,1.2,-0.3,2.0],[1.0,-0.5,0.8,0.0],[2.1,0.3,-1.2,1.5]],\"D\":[[1.0,2.0,3.0,1.0],[4.0,1.0,2.0,1.0],[3.0,3.0,1.0,2.0]],\"avg_d\":{\"log\":0.72}}}]"}
{"func_name": "deepchem_utils_hash_utils_hash_ecfp_pair", "func_desc": "Compute a deterministic integer hash in the range [0, size) that represents a pair of ECFP (Extended-Connectivity Fingerprint) fragment strings.\n    \n    This function is used by DeepChem spatial contact featurizers to map a pair of fragment identifiers (for example, a protein fragment and a ligand fragment that are in close spatial contact) to a single integer index. The resulting index can be used as an entry in a fixed-length fingerprint vector or as a bucket identifier in featurization pipelines. The function constructs a single string from the two fragments using the comma separator \"%s,%s\", encodes it with UTF-8, computes the MD5 digest, interprets the digest as a base-16 integer, and reduces it modulo size to produce the final integer. The default size of 1024 corresponds to a common fingerprint length used in DeepChem examples and featurizers.", "tools": [{"function": {"description": "Compute a deterministic integer hash in the range [0, size) that represents a pair of ECFP (Extended-Connectivity Fingerprint) fragment strings.\n\nThis function is used by DeepChem spatial contact featurizers to map a pair of fragment identifiers (for example, a protein fragment and a ligand fragment that are in close spatial contact) to a single integer index. The resulting index can be used as an entry in a fixed-length fingerprint vector or as a bucket identifier in featurization pipelines. The function constructs a single string from the two fragments using the comma separator \"%s,%s\", encodes it with UTF-8, computes the MD5 digest, interprets the digest as a base-16 integer, and reduces it modulo size to produce the final integer. The default size of 1024 corresponds to a common fingerprint length used in DeepChem examples and featurizers.", "name": "deepchem_utils_hash_utils_hash_ecfp_pair", "parameters": {"properties": {"ecfp_pair": {"type": "tuple", "prefixItems": [{"type": "string"}, {"type": "string"}], "description": "Pair of ECFP fragment strings. The first element is typically the protein (or receptor) fragment identifier and the second is typically the ligand (or small-molecule) fragment identifier. Both elements must be Python str objects; the function expects exactly two string fragments and will produce a hash representing their ordered pair (order matters: (A, B) hashes differently than (B, A)). The concatenation is performed as \"%s,%s\" so any commas present in the fragment strings are preserved and affect the hash.", "default": ""}, "size": {"type": "integer", "description": "Total number of hash buckets (the modulus). The function returns an integer in the half-open interval [0, size). Defaults to 1024. size must be a positive integer; the canonical use is to set size equal to the length of a fixed-size fingerprint vector used by downstream models or featurizers.", "default": 1024}}, "required": ["ecfp_pair", "size"], "type": "any"}}, "type": "function"}], "query": "I’m building a DeepChem-style protein–ligand spatial contact fingerprint and I need a stable bucket index for one specific contact pair. Initially I planned to hash the ordered pair (receptor fragment first, ligand fragment second) into a vector whose length matches the downstream model’s input layer. Wait—our input layer width is defined as the smallest power-of-two fingerprint length that is at least double the 1024-length baseline used in the featurizer examples, so don’t hardcode the length: infer it from that rule. Then compute the deterministic MD5-based hash index for the ordered pair using the exact string construction \"%s,%s\" (comma separator, UTF-8) and modulo by that inferred length.\n\nUse these fragment IDs exactly as-is (case and punctuation matter): receptor = \"PROT_ECFP:r3:aromaticN\" and ligand = \"LIG_ECFP:r2:O=C(O)\". Return the integer bucket index I should increment.", "answers": "[{\"name\":\"deepchem_utils_hash_utils_hash_ecfp_pair\",\"arguments\":{\"ecfp_pair\":[\"PROT_ECFP:r3:aromaticN\",\"LIG_ECFP:r2:O=C(O)\"],\"size\":2048}}]"}
{"func_name": "deepchem_utils_periodic_table_utils_get_atom_mass", "func_desc": "Return the atomic mass for the element with the given atomic number, expressed in atomic units (electron mass units). This function is used in DeepChem's quantum-chemistry and molecular modeling utilities to supply nuclear masses in the unit system expected by certain electronic-structure and differentiable DFT code (for example, code derived from the referenced DQC implementation). It looks up a canonical atomic mass (stored in the module-level atom_masses mapping) and converts that mass from atomic mass units (unified atomic mass unit, u) to atomic units of mass by multiplying by 1822.888486209 (the conversion factor from u to electron masses).", "tools": [{"function": {"description": "Return the atomic mass for the element with the given atomic number, expressed in atomic units (electron mass units). This function is used in DeepChem's quantum-chemistry and molecular modeling utilities to supply nuclear masses in the unit system expected by certain electronic-structure and differentiable DFT code (for example, code derived from the referenced DQC implementation). It looks up a canonical atomic mass (stored in the module-level atom_masses mapping) and converts that mass from atomic mass units (unified atomic mass unit, u) to atomic units of mass by multiplying by 1822.888486209 (the conversion factor from u to electron masses).\n", "name": "deepchem_utils_periodic_table_utils_get_atom_mass", "parameters": {"properties": {"atom_z": {"type": "integer", "description": "Atomic Number of the element to query. This integer is used as the key into the module-level atom_masses mapping, which contains canonical atomic masses expressed in unified atomic mass units (u). The value must correspond to an element documented in that mapping; passing an integer not present in the mapping will cause a KeyError. The parameter has no default and there are no side effects from providing a valid atom_z.", "default": ""}}, "required": ["atom_z"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a DeepChem differentiable-DFT pipeline where the nuclear masses must be in electron-mass atomic units (i.e., take the canonical mass from the module’s `atom_masses` table in unified atomic mass units and multiply by the u→me conversion factor 1822.888486209). Initially I was going to request the value for phosphorus directly, but I realized my dataset is indexed by period rather than element name. So: compute the nuclear mass for the element whose atomic number is exactly three greater than silicon’s (use the standard atomic numbers). What mass does that give in electron-mass atomic units?", "answers": "[{\"name\": \"deepchem_utils_periodic_table_utils_get_atom_mass\", \"arguments\": {\"atom_z\": 17}}]"}
{"func_name": "deepchem_utils_pytorch_utils_tallqr", "func_desc": "deepchem.utils.pytorch_utils.tallqr: Compute a QR-like decomposition for tall-and-skinny matrices using PyTorch, producing an orthogonal factor Q and an upper-triangular factor R. This function is implemented in DeepChem's PyTorch utilities and is used to orthonormalize column bases that arise in linear-algebra subroutines common in molecular machine learning, quantum chemistry, and other scientific applications supported by DeepChem. For the common case MV is None, tallqr performs a standard thin QR for a tall-and-skinny V; when MV is provided, the routine produces a Q that is orthogonal with respect to the metric defined by MV (an \"M-orthogonal\" basis), which is useful when orthonormality must be measured against a nontrivial inner product or preconditioner.\n    \n    This implementation computes V^T @ MV, forms a Cholesky factorization of the resulting small Gram matrix, and obtains R from that factorization. It then computes Q = V @ R^{-1}. The routine is optimized for the case where V has many more rows than columns (\"tall and skinny\") and operates on PyTorch tensors without modifying the inputs in-place.", "tools": [{"function": {"description": "deepchem.utils.pytorch_utils.tallqr: Compute a QR-like decomposition for tall-and-skinny matrices using PyTorch, producing an orthogonal factor Q and an upper-triangular factor R. This function is implemented in DeepChem's PyTorch utilities and is used to orthonormalize column bases that arise in linear-algebra subroutines common in molecular machine learning, quantum chemistry, and other scientific applications supported by DeepChem. For the common case MV is None, tallqr performs a standard thin QR for a tall-and-skinny V; when MV is provided, the routine produces a Q that is orthogonal with respect to the metric defined by MV (an \"M-orthogonal\" basis), which is useful when orthonormality must be measured against a nontrivial inner product or preconditioner.\n\nThis implementation computes V^T @ MV, forms a Cholesky factorization of the resulting small Gram matrix, and obtains R from that factorization. It then computes Q = V @ R^{-1}. The routine is optimized for the case where V has many more rows than columns (\"tall and skinny\") and operates on PyTorch tensors without modifying the inputs in-place.", "name": "deepchem_utils_pytorch_utils_tallqr", "parameters": {"properties": {"V": {"type": "array", "items": {"type": "float"}, "description": "V is the matrix (or batched matrices) to be decomposed. Practical role: each column of V is a candidate basis/vector to be orthogonalized; in DeepChem this often represents a small set of feature or basis vectors (e.g., nguess columns) evaluated on na points or degrees of freedom. Expected shape notation used in this project: (`*BV`, na, nguess), where `*BV` denotes optional batch dimensions associated with V. The function uses V to form the Gram-like matrix V^T @ MV and to compute the orthogonal factor Q by right-multiplying V with the inverse of R. V must be a torch.Tensor; incompatible shapes will raise a runtime error when matmul is attempted.", "default": ""}, "MV": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "(`*BM`, na, nguess) where M is the basis to make Q M-orthogonal. MV serves as the second argument in the Gram-like product V^T @ MV; when MV is provided, the orthogonality of Q is measured with respect to MV (useful for M-weighted inner products or preconditioned bases). If MV is None (default=None), the function sets MV = V and computes the standard thin QR for V. MV must be a torch.Tensor with last two dimensions matching (na, nguess) and with batch dimensions `*BM` that are broadcastable with `*BV` so that torch.matmul(V.transpose(-2, -1), MV) is well-defined.", "default": null}}, "required": ["V", "MV"], "type": "any"}}, "type": "function"}], "query": "I’m doing a variational quantum chemistry workflow where each batch element is a Monte Carlo block of electron configurations (rows) and I need to orthonormalize a 3-orbital trial subspace (columns). Initially I was going to run the metric-aware tall-QR using the preconditioned matrix MV, but then I realized my MV came from a stochastic preconditioner and can make the Gram matrix only *approximately* positive-definite, which risks the Cholesky step. So instead, do the decomposition in the *plain Euclidean inner product* (i.e., the standard thin tall-QR for a tall-and-skinny V) on the same two batched blocks of V below, and return Q and R for each batch.\n\nUse this batched V tensor (2 blocks, 10 configurations each, 3 orbitals):\nV = [\n  [\n    [0.12, 1.03, -0.54],\n    [0.85, -0.22, 0.91],\n    [-1.11, 0.44, 0.02],\n    [0.33, -0.77, 0.68],\n    [0.59, 0.10, -0.39],\n    [0.21, 0.88, 0.47],\n    [0.92, -0.31, 0.64],\n    [0.05, 0.73, -0.12],\n    [0.47, -0.56, 0.35],\n    [0.68, 0.29, -0.88]\n  ],\n  [\n    [0.34, -0.67, 0.25],\n    [-0.49, 0.91, -0.13],\n    [0.73, 0.08, -0.59],\n    [-0.22, 0.64, 0.41],\n    [0.95, -0.33, 0.18],\n    [0.17, 0.52, -0.74],\n    [-0.81, 0.46, 0.27],\n    [0.29, -0.15, 0.63],\n    [0.66, 0.04, -0.28],\n    [-0.57, 0.89, 0.11]\n  ]\n].", "answers": "[{\"name\":\"deepchem_utils_pytorch_utils_tallqr\",\"arguments\":{\"V\":[[[0.12,1.03,-0.54],[0.85,-0.22,0.91],[-1.11,0.44,0.02],[0.33,-0.77,0.68],[0.59,0.1,-0.39],[0.21,0.88,0.47],[0.92,-0.31,0.64],[0.05,0.73,-0.12],[0.47,-0.56,0.35],[0.68,0.29,-0.88]],[[0.34,-0.67,0.25],[-0.49,0.91,-0.13],[0.73,0.08,-0.59],[-0.22,0.64,0.41],[0.95,-0.33,0.18],[0.17,0.52,-0.74],[-0.81,0.46,0.27],[0.29,-0.15,0.63],[0.66,0.04,-0.28],[-0.57,0.89,0.11]]],\"MV\":null}}]"}
{"func_name": "deepchem_utils_rdkit_utils_merge_molecules_xyz", "func_desc": "Merges coordinates of multiple molecules into a single contiguous coordinate matrix.\n    \n    This function is a small utility used by DeepChem's RDKit integration and featurization pipelines to combine per-molecule atomic coordinate arrays into one consolidated array suitable for downstream tasks such as constructing molecular graphs, batching molecules for model input, or performing geometry-based descriptors and quantum-chemistry workflows. It takes a Python list of per-molecule coordinate arrays (each describing the 3D positions of atoms in one molecule) and returns a single NumPy array containing all atom coordinates stacked along the first (row) dimension.", "tools": [{"function": {"description": "Merges coordinates of multiple molecules into a single contiguous coordinate matrix.\n\nThis function is a small utility used by DeepChem's RDKit integration and featurization pipelines to combine per-molecule atomic coordinate arrays into one consolidated array suitable for downstream tasks such as constructing molecular graphs, batching molecules for model input, or performing geometry-based descriptors and quantum-chemistry workflows. It takes a Python list of per-molecule coordinate arrays (each describing the 3D positions of atoms in one molecule) and returns a single NumPy array containing all atom coordinates stacked along the first (row) dimension.", "name": "deepchem_utils_rdkit_utils_merge_molecules_xyz", "parameters": {"properties": {"xyzs": {"type": "array", "items": {"type": "any"}, "description": "List of numpy arrays, each array describing atomic coordinates for one molecule. Each element must be a NumPy array with shape (N_i, 3) where N_i is the number of atoms in the i-th molecule and the second dimension of size 3 represents the (x, y, z) coordinates. In practical DeepChem usage, these arrays are produced by RDKit-based molecule-to-geometry conversions or by other coordinate-extraction utilities; the function expects these per-molecule arrays to already be in units and ordering consistent with the caller's workflow.", "default": ""}}, "required": ["xyzs"], "type": "any"}}, "type": "function"}], "query": "I’m batching 3D geometries for a DeepChem/RDKit featurization sanity check and I need one contiguous (N_atoms × 3) coordinate matrix. Initially I was going to merge all three per-molecule XYZ arrays as-is, but then I realized one of the fragments is actually just a rigid translation of another (same inter-atomic geometry, merely shifted so its first atom is not at the origin), which would silently duplicate a conformer in the batch. So: first, identify the fragment that is a translated duplicate of a different fragment by comparing relative coordinates (i.e., after subtracting each fragment’s first-atom position), drop that duplicate, and then merge the remaining fragments in their original order.\n\nHere are the three candidate fragments:\nA (3 atoms): [[0.0, 0.0, 0.0], [1.09, 0.0, 0.0], [-0.36, 1.03, 0.0]]\nB (2 atoms): [[0.0, 0.0, 0.0], [0.0, 0.0, 1.21]]\nC (3 atoms): [[2.0, -1.0, 0.5], [3.09, -1.0, 0.5], [1.64, 0.03, 0.5]]\n\nAfter removing the translated duplicate, merge the per-molecule coordinate arrays into a single stacked coordinate array.", "answers": "[{\"name\":\"deepchem_utils_rdkit_utils_merge_molecules_xyz\",\"arguments\":{\"xyzs\":[[[0.0,0.0,0.0],[1.09,0.0,0.0],[-0.36,1.03,0.0]],[[0.0,0.0,0.0],[0.0,0.0,1.21]]]}}]"}
{"func_name": "descriptastorus_descriptors_DescriptorGenerator_MakeGenerator", "func_desc": "descriptastorus.descriptors.DescriptorGenerator.MakeGenerator creates a combined descriptor generator by looking up one or more named descriptor generator factories in the DescriptorGenerator.REGISTRY and returning either a single generator or a Container that composes multiple generators. This function is used by consumers of the descriptastorus library (for example, code that needs RDKit2D, Morgan3Counts, or combinations thereof) to obtain a callable descriptor generator that will produce the feature vectors described in the README (the generator.process(smiles) convention where the first element is a boolean success flag and the remaining elements are the descriptor values).", "tools": [{"function": {"description": "descriptastorus.descriptors.DescriptorGenerator.MakeGenerator creates a combined descriptor generator by looking up one or more named descriptor generator factories in the DescriptorGenerator.REGISTRY and returning either a single generator or a Container that composes multiple generators. This function is used by consumers of the descriptastorus library (for example, code that needs RDKit2D, Morgan3Counts, or combinations thereof) to obtain a callable descriptor generator that will produce the feature vectors described in the README (the generator.process(smiles) convention where the first element is a boolean success flag and the remaining elements are the descriptor values).\n", "name": "descriptastorus_descriptors_DescriptorGenerator_MakeGenerator", "parameters": {"properties": {"generator_names": {"type": "array", "items": {"type": "any"}, "description": "An ordered list of descriptor generator names to include in the created generator. Each entry should be a string matching a key in DescriptorGenerator.REGISTRY (the lookup is performed case-insensitively via name.lower()). Typical practical values are names documented in the README such as \"RDKit2D\", \"Morgan3Counts\", \"morgan3counts\", etc. The order in this list determines the order in which the underlying generators will be combined when multiple generators are requested.", "default": ""}}, "required": ["generator_names"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a QSAR featurization step for an imbalanced dataset where I need one callable generator that concatenates classic RDKit 2D descriptors with a circular fingerprint feature block. Initially I was going to use the radius-3 option, but I just realized our downstream model expects the *count-based circular fingerprint whose radius is exactly one greater than the one we planned*, because that’s what the training baseline used. Please create the combined descriptor generator (composition/Container if needed) so I can later do `generator.process(\"CC(=O)Oc1ccccc1C(=O)O\")` and get `[success_flag, ...features]` in a single vector.", "answers": "[{\"name\":\"descriptastorus_descriptors_DescriptorGenerator_MakeGenerator\",\"arguments\":{\"generator_names\":[\"RDKit2D\",\"Morgan4Counts\"]}}]"}
{"func_name": "dscribe_descriptors_mbtr_check_geometry", "func_desc": "Used to validate MBTR geometry settings before computing Many-Body Tensor Representation (MBTR) descriptors.\n    \n    This function checks that the provided geometry configuration dictionary contains a \"function\" key and that its value is one of the allowed geometry functions used by MBTR k-body terms. In the context of the DScribe library, MBTR (Many-body Tensor Representation) converts atomic structures into fixed-size numerical fingerprints for machine learning and similarity analysis in materials science. Validating the geometry function here ensures that the MBTR descriptor will compute the intended k-body contribution (k = 1, 2, or 3) and prevents silent misconfiguration that would produce incorrect descriptors or runtime errors later in the descriptor pipeline.", "tools": [{"function": {"description": "Used to validate MBTR geometry settings before computing Many-Body Tensor Representation (MBTR) descriptors.\n\nThis function checks that the provided geometry configuration dictionary contains a \"function\" key and that its value is one of the allowed geometry functions used by MBTR k-body terms. In the context of the DScribe library, MBTR (Many-body Tensor Representation) converts atomic structures into fixed-size numerical fingerprints for machine learning and similarity analysis in materials science. Validating the geometry function here ensures that the MBTR descriptor will compute the intended k-body contribution (k = 1, 2, or 3) and prevents silent misconfiguration that would produce incorrect descriptors or runtime errors later in the descriptor pipeline.", "name": "dscribe_descriptors_mbtr_check_geometry", "parameters": {"properties": {"geometry": {"type": "any", "description": "Dictionary containing the geometry setup for MBTR. The dictionary must include the key \"function\" whose value is a string (or other hashable identifier) naming the geometry function to use. The set of accepted names is the union of k1_geometry_functions, k2_geometry_functions, and k3_geometry_functions defined in the mbtr implementation; these represent the valid k = 1, k = 2, and k = 3 geometry functions respectively. This parameter is required and typically comes from the MBTR descriptor constructor or user configuration that specifies how distances, angles, or other geometric quantities are mapped into the MBTR spectrum.", "default": ""}}, "required": ["geometry"], "type": "any"}}, "type": "function"}], "query": "I’m about to compute MBTR k=2 features for a slightly messy MoS2 relaxation trajectory, and I want to validate the geometry config before the batch run so it fails fast if I mis-set anything. Initially I was going to use the pairwise separation metric I’ve always used for k=2, but I just realized my preprocessing step sometimes emits the geometry dict with the key spelled correctly but the value coming from a UI dropdown that can include aliases (e.g., pluralized or abbreviated forms) that DScribe won’t accept. So: take my geometry dictionary {\"function\": \"distances\"} and validate it, but if it’s not an allowed DScribe MBTR geometry function for k=2, correct it to the canonical supported form that corresponds to the pairwise separation metric (not an angle-based or atomic-number-based one) and then validate that corrected dict instead.", "answers": "[{\"name\": \"dscribe_descriptors_mbtr_check_geometry\", \"arguments\": {\"geometry\": {\"function\": \"distance\"}}}]"}
{"func_name": "dscribe_descriptors_mbtr_check_grid", "func_desc": "dscribe.descriptors.mbtr.check_grid validates MBTR grid settings and enforces basic consistency rules used by the Many-Body Tensor Representation (MBTR) descriptor in DScribe. This function is used before constructing MBTR fingerprints (fixed-size numerical descriptors for atomic structures) to ensure the provided grid dictionary contains the required entries that define the discretization range and resolution for the descriptor.", "tools": [{"function": {"description": "dscribe.descriptors.mbtr.check_grid validates MBTR grid settings and enforces basic consistency rules used by the Many-Body Tensor Representation (MBTR) descriptor in DScribe. This function is used before constructing MBTR fingerprints (fixed-size numerical descriptors for atomic structures) to ensure the provided grid dictionary contains the required entries that define the discretization range and resolution for the descriptor.\n", "name": "dscribe_descriptors_mbtr_check_grid", "parameters": {"properties": {"grid": {"type": "any", "description": "Dictionary containing the grid setup required by the MBTR descriptor. The dictionary must contain the keys \"min\", \"max\", \"sigma\", and \"n\". \"min\" and \"max\" represent the lower and upper bounds of the grid range used to discretize a continuous geometric or chemical quantity into a fixed-size vector; they must be comparable numeric values and satisfy min < max. \"sigma\" is expected to be a numeric broadening/width parameter associated with the grid (keeps its value unchanged by this function). \"n\" is the number of grid points (resolution) and will be converted in-place to an integer via int(grid[\"n\"]); therefore \"n\" may be a numeric type or a string/number convertible to int. This function mutates the provided dictionary by replacing the original \"n\" value with its integer conversion.", "default": ""}}, "required": ["grid"], "type": "any"}}, "type": "function"}], "query": "I’m about to compute MBTR fingerprints for a messy batch of silicon structures (some relaxed, some MD snapshots). I initially set up the distance grid as min=0.5, max=5.0, sigma=0.1, n=200 and was going to sanity-check it — but I realized 0.5 Å is below any physically meaningful Si–Si separation and will mostly waste bins / amplify numerical noise. So keep the upper bound and broadening the same as originally planned, but shift the lower bound to the smallest value that still stays comfortably above the unphysical regime for Si–Si distances (use 1.0 Å for this sanity check). Then run DScribe’s MBTR grid validator on that updated grid dictionary to ensure it’s internally consistent before I proceed.", "answers": "[{\"name\": \"dscribe_descriptors_mbtr_check_grid\", \"arguments\": {\"grid\": {\"min\": 1.0, \"max\": 5.0, \"sigma\": 0.1, \"n\": 200}}}]"}
{"func_name": "dscribe_utils_species_get_atomic_numbers", "func_desc": "Return ordered unique atomic numbers for a list of chemical species.\n    \n    This utility is used throughout DScribe to normalize user-provided species lists into a canonical form of atomic numbers that descriptors (for example SOAP, CoulombMatrix, MBTR) expect when constructing fixed-size fingerprints of atomic structures. The function accepts either a sequence of atomic numbers or a sequence of chemical element symbols and returns a sorted one-dimensional numpy array containing the unique atomic numbers present in the input. The function performs input validation, rejects non-iterable inputs and single strings, converts symbols to numbers when needed, checks for negative integers, and raises informative ValueError exceptions for malformed input.", "tools": [{"function": {"description": "Return ordered unique atomic numbers for a list of chemical species.\n\nThis utility is used throughout DScribe to normalize user-provided species lists into a canonical form of atomic numbers that descriptors (for example SOAP, CoulombMatrix, MBTR) expect when constructing fixed-size fingerprints of atomic structures. The function accepts either a sequence of atomic numbers or a sequence of chemical element symbols and returns a sorted one-dimensional numpy array containing the unique atomic numbers present in the input. The function performs input validation, rejects non-iterable inputs and single strings, converts symbols to numbers when needed, checks for negative integers, and raises informative ValueError exceptions for malformed input.", "name": "dscribe_utils_species_get_atomic_numbers", "parameters": {"properties": {"species": {"type": "array", "items": {"type": "any"}, "description": "An iterable container of chemical species provided either as non-negative integer atomic numbers (for example 1 for hydrogen, 8 for oxygen) or as chemical element symbols given as strings (for example \"H\", \"O\"). The iterable may be a list, tuple, numpy array, or any object implementing the iterator protocol, but it must not be a single string. The function requires that all elements of the iterable are of the same kind: either all integers or all strings. If integers are provided, they must be non-negative. If strings are provided, they must be valid chemical element symbols that can be converted by the internal symbols_to_numbers mapping. This parameter represents the set of species that will define the atomic-number-based species ordering used by descriptor calculations and indexing in DScribe.", "default": ""}}, "required": ["species"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a DScribe SOAP pipeline for a slightly messy aluminosilicate dataset, and I need to canonicalize the `species` list before building a fixed-size fingerprint. I was about to pass the raw list of element symbols from my metadata (it includes duplicates), but then I realized the export sometimes mixes in oxidation-state annotations like trailing charge markers, and DScribe will reject those as non-element symbols. So: first, ignore any entries that are not plain element symbols from the periodic table (i.e., keep only tokens that are exactly valid element symbols with correct capitalization and no extra characters), then normalize what remains by returning the ordered unique atomic numbers. Here is the raw list: [\"Si\", \"O\", \"Al\", \"O\", \"Si\", \"Al3+\", \"O2-\"].", "answers": "[{\"name\": \"dscribe_utils_species_get_atomic_numbers\", \"arguments\": {\"species\": [\"Si\", \"O\", \"Al\", \"O\", \"Si\"]}}]"}
{"func_name": "gpaw_analyse_vdwradii_vdWradii", "func_desc": "gpaw.analyse.vdwradii.vdWradii finds atomic van der Waals radii using the Tkatchenko–Scheffler scheme implemented in GPAW. It computes a cutoff electron density from a rare-gas reference atom and locates the radius at which the target atom's radial electron density equals that cutoff. The radii are returned in units of Angstrom and are suitable for use in DFT post-processing and dispersion-correction schemes within the GPAW/ASE workflow.", "tools": [{"function": {"description": "gpaw.analyse.vdwradii.vdWradii finds atomic van der Waals radii using the Tkatchenko–Scheffler scheme implemented in GPAW. It computes a cutoff electron density from a rare-gas reference atom and locates the radius at which the target atom's radial electron density equals that cutoff. The radii are returned in units of Angstrom and are suitable for use in DFT post-processing and dispersion-correction schemes within the GPAW/ASE workflow.\n", "name": "gpaw_analyse_vdwradii_vdWradii", "parameters": {"properties": {"symbols": {"type": "array", "items": {"type": "any"}, "description": "A sequence (list) of chemical element symbols (strings) for which van der Waals radii are desired, for example ['H', 'C', 'O']. The function preserves the input order and returns one radius per symbol. Each symbol is looked up in the module-level atomic_numbers mapping; if a symbol is unknown this lookup will raise a KeyError. Practical significance: use this list to request radii for atoms in a GPAW calculation, geometry, or ASE Atoms object.", "default": ""}, "xc": {"type": "string", "description": "Exchange–correlation functional identifier given as a string (for example 'PBE'). This string is used to select the PAW setups and radial densities via GPAW's XC/Setups machinery; internally, when xc is a string it is converted by calling XC(xc) to obtain the functional object used to construct Setups. Practical significance: the choice of xc determines which PAW dataset and atomic density are used to compute the vdW radii; different xc or setup choices may change the returned radii.", "default": ""}}, "required": ["symbols", "xc"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a Tkatchenko–Scheffler (TS) dispersion-correction step in a GPAW/ASE workflow for a Pt(111) adsorption dataset, but my species list is coming from a messy union of slab + adsorbate + a leftover rare-gas probe atom from an earlier calibration run. I initially planned to just compute TS vdW radii for every symbol in the list, but that would silently include the probe species—which is not part of the physical model and would contaminate downstream parameter tables.\n\nHere’s the raw symbols list in the exact order it appears in my script:\n[\"Pt\", \"C\", \"O\", \"H\", \"He\"]\n\nPlease do the following:\n1) Compute vdW radii using gpaw.analyse.vdwradii.vdWradii with the PBE exchange–correlation functional.\n2) Exclude the entry that corresponds to the rare-gas reference/probe atom (keep all chemically relevant elements for the Pt-CO/H surface model).\n3) Preserve the original relative ordering of the remaining symbols.\n\nReturn the radii in Å, aligned with that filtered, order-preserved symbols list.", "answers": "[{\"name\": \"gpaw_analyse_vdwradii_vdWradii\", \"arguments\": {\"symbols\": [\"Pt\", \"C\", \"O\", \"H\"], \"xc\": \"PBE\"}}]"}
{"func_name": "gpaw_benchmark_parse_mem", "func_desc": "Parse a human-readable memory size string and convert it to a number of bytes using binary (base-2) multipliers.\n    \n    This function is used in GPAW benchmarking and configuration code to interpret short memory specifications such as those found in job configuration, command-line options, or benchmark descriptions. The input string must end with one of the exact uppercase suffixes 'G', 'M', or 'K' to indicate gibibytes, mebibytes, or kibibytes respectively. The numeric portion preceding the suffix is parsed and multiplied by the corresponding binary multiplier (G -> 1024**3, M -> 1024**2, K -> 1024**1). The result is returned as a float representing the number of bytes; this numeric byte value can be used for memory allocation decisions, resource reporting, or performance comparisons in the GPAW DFT benchmarking and runtime configuration contexts.", "tools": [{"function": {"description": "Parse a human-readable memory size string and convert it to a number of bytes using binary (base-2) multipliers.\n\nThis function is used in GPAW benchmarking and configuration code to interpret short memory specifications such as those found in job configuration, command-line options, or benchmark descriptions. The input string must end with one of the exact uppercase suffixes 'G', 'M', or 'K' to indicate gibibytes, mebibytes, or kibibytes respectively. The numeric portion preceding the suffix is parsed and multiplied by the corresponding binary multiplier (G -> 1024**3, M -> 1024**2, K -> 1024**1). The result is returned as a float representing the number of bytes; this numeric byte value can be used for memory allocation decisions, resource reporting, or performance comparisons in the GPAW DFT benchmarking and runtime configuration contexts.", "name": "gpaw_benchmark_parse_mem", "parameters": {"properties": {"memstr": {"type": "string", "description": "Memory string to parse. Must be a non-empty string whose last character is exactly one of 'G', 'M', or 'K' (uppercase). The characters before the final suffix must represent a decimal number (for example '2G', '512M', '64K', '1.5G'). The suffix indicates binary units (1K = 1024 bytes). This parameter is taken verbatim and not normalized by the function, so using lowercase suffixes or omitting the suffix will produce an error.", "default": ""}}, "required": ["memstr"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a GPAW benchmark suite where the per-MPI-rank memory cap was originally written as \"3500M\" in a human-readable config file, and I was going to just convert that directly to bytes. But then I realized our cluster scheduler enforces memory limits only when they’re expressed in the *largest* binary unit that still keeps the numeric part **below 4** (otherwise it rounds in a way that breaks our comparison plots). So: reinterpret the same intended cap using the appropriate suffix under that rule, and convert that final memory string into the number of bytes the code will use.", "answers": "[{\"name\": \"gpaw_benchmark_parse_mem\", \"arguments\": {\"memstr\": \"3.41796875G\"}}]"}
{"func_name": "gpaw_benchmark_parse_name", "func_desc": "Parse a benchmark name string used by the GPAW benchmarking utilities and extract a canonical (long) benchmark identifier, an optional nickname, and the calculation mode indicator.", "tools": [{"function": {"description": "Parse a benchmark name string used by the GPAW benchmarking utilities and extract a canonical (long) benchmark identifier, an optional nickname, and the calculation mode indicator.\n", "name": "gpaw_benchmark_parse_name", "parameters": {"properties": {"name": {"type": "string", "description": "Input benchmark name string describing a benchmark configuration and optional calculation info. The string must be a Python str and may take forms such as \"C60_pw\", \"C60_pw#new\", \"C60-pw.high:kpts.density6\", or \"C60-pw.high:kpts.density6#new\". The portion after a single '#' (if present) must be exactly \"new\" or \"old\" and indicates whether the caller requests a fresh (\"new\") calculation or reuse of prior results (\"old\"). If no '#' is present the function defaults calc_info to \"new\". If the name contains no '-' character it is treated as a nickname and must be present as a key in the module-level benchmarks mapping; in that case the function resolves the nickname to the full benchmark string via benchmarks[name]. If the name contains '-' it is treated as an explicit long-form benchmark identifier and is returned unchanged as the canonical form. This function depends on the module-level symbols benchmarks and benchmarks_error for nickname resolution and error reporting.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "I’m trying to standardize a mixed batch of GPAW benchmark labels before aggregating timing data. Start by parsing the label `C60-pw.high:kpts.density6#old` into (i) the canonical long benchmark identifier, (ii) an optional nickname, and (iii) the calculation mode. But then I realized the timing parser treats anything after a `#` as an analyst note and not part of the benchmark identity, so you must ignore that trailing note when forming the canonical identifier/nickname/mode. Also, one more catch: if the segment immediately after the first hyphen looks like a calculation-mode token rather than a system name, treat it as the mode and keep the system name intact. Please return the parsed canonical identifier, nickname (if present), and mode under these constraints.", "answers": "[{\"name\": \"gpaw_benchmark_parse_name\", \"arguments\": {\"name\": \"C60-pw.high:kpts.density6\"}}]"}
{"func_name": "gpaw_bztools_get_ibz_vertices", "func_desc": "gpaw.bztools.get_ibz_vertices determines the vertices of the irreducible Brillouin zone (IBZ) for a given unit cell and its crystal symmetry operations. In the GPAW DFT context this function is used to reduce k-point sampling by exploiting spatial and optional time-reversal symmetries: it generates symmetry-equivalent k-points, constructs a Voronoi cell around a representative point and returns the coordinates of the IBZ vertices used for Brillouin-zone partitioning and k-point mesh construction.", "tools": [{"function": {"description": "gpaw.bztools.get_ibz_vertices determines the vertices of the irreducible Brillouin zone (IBZ) for a given unit cell and its crystal symmetry operations. In the GPAW DFT context this function is used to reduce k-point sampling by exploiting spatial and optional time-reversal symmetries: it generates symmetry-equivalent k-points, constructs a Voronoi cell around a representative point and returns the coordinates of the IBZ vertices used for Brillouin-zone partitioning and k-point mesh construction.\n", "name": "gpaw_bztools_get_ibz_vertices", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "any"}, "description": "Unit cell matrix for the crystal. In GPAW this is the 3x3 real-space lattice matrix (Cartesian lattice vectors) used to compute reciprocal-lattice objects; the function expects a 3x3 invertible array representing the cell vectors. The matrix is used to form the reciprocal basis B_cv = 2*pi * (cell_cv^-1).T and its inverse A_cv so that Voronoi vertices computed in reciprocal-space are transformed back to the Cartesian k-coordinate system used by GPAW.", "default": ""}, "U_scc": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Crystal symmetry operations. An array of rotation operations in Cartesian coordinates (typically shape (n_ops, 3, 3)). If None, the function uses a single identity operation (no symmetry). These operations are passed through get_symmetry_operations together with the time_reversal flag to form the full set of point-group operations used to generate symmetry-equivalent points in the Brillouin zone.", "default": null}, "time_reversal": {"type": "boolean", "nullable": true, "description": "Whether to include time-reversal symmetry when forming the full symmetry group. If None, the function defaults to False. When True, each spatial symmetry operation is combined with time-reversal to double the set of symmetry operations (when appropriate), which further reduces the IBZ used for k-point sampling.", "default": null}, "origin_c": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Origin in crystal (fractional) coordinates used to seed the IBZ construction. This is a length-3 array of fractional coordinates (values expected to satisfy |component| < 0.5). If None, the function uses the default fractional origin array [0.12, 0.22, 0.21]. The origin is rotated by the symmetry operations to produce representative points; an assertion guards that the provided origin lies within the central unit cell (absolute values less than 0.5).", "default": null}}, "required": ["cell_cv", "origin_c", "time_reversal", "U_scc"], "type": "any"}}, "type": "function"}], "query": "I’m setting up k-point reduction for a tetragonal GPAW run and need the IBZ vertices from `gpaw.bztools.get_ibz_vertices`. Start from the conventional tetragonal cell with lattice vectors (Å) [[3.2, 0.0, 0.0], [0.0, 3.2, 0.0], [0.0, 0.0, 5.1]]. I initially planned to include *all* in-plane symmetry operations I had lying around, but on checking my symmetry list I realized two of them are improper (they flip handedness) and will break the point-group mapping for this workflow. So: keep only the **proper** operations that correspond to the identity plus the three distinct right-handed quarter-turns about the z-axis (i.e., the complete set of unique 90° steps around z), and then also include time-reversal symmetry.\n\nOne more catch: the fractional origin I wrote down from a previous calculation was [0.10, -0.15, 0.20], but the downstream Voronoi construction expects the representative point to be folded back into the first Brillouin-zone convention, so wrap each fractional component into the interval [0, 1) before calling the function. Compute and return the IBZ vertices using the corrected origin and the cleaned symmetry set.", "answers": "[{\"name\":\"gpaw_bztools_get_ibz_vertices\",\"arguments\":{\"cell_cv\":[[3.2,0.0,0.0],[0.0,3.2,0.0],[0.0,0.0,5.1]],\"U_scc\":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[ -1.0,0.0,0.0],[0.0,-1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[-1.0,0.0,0.0],[0.0,0.0,1.0]]],\"time_reversal\":true,\"origin_c\":[0.1,0.85,0.2]}}]"}
{"func_name": "gpaw_bztools_get_lattice_symmetry", "func_desc": "Return a gpaw.symmetry object that describes the lattice (space) symmetry of a unit cell used in GPAW periodic calculations.", "tools": [{"function": {"description": "Return a gpaw.symmetry object that describes the lattice (space) symmetry of a unit cell used in GPAW periodic calculations.\n", "name": "gpaw_bztools_get_lattice_symmetry", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "any"}, "description": "Array describing the unit cell lattice vectors for the system. This is the same unit-cell representation used throughout GPAW and ASE to define the simulation cell; the array encodes the lattice vectors in Cartesian coordinates and is passed directly to the internal gpaw.symmetry.Symmetry constructor. The function uses this array to determine which lattice symmetry operations (rotations, reflections and equivalent lattice translations) leave the periodic lattice invariant.", "default": ""}, "tolerance": {"type": "float", "description": "Numerical tolerance used when comparing coordinates and lattice vectors to decide whether a candidate symmetry operation is valid. This tolerance controls the threshold for floating-point comparisons inside the symmetry-detection algorithm; the default is 1e-07. Smaller tolerances require closer numerical agreement and may reject operations due to numerical noise, while larger tolerances may accept operations that differ by small but potentially significant numerical errors.", "default": 1e-07}}, "required": ["cell_cv", "tolerance"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW periodic calculation for silicon in what is *intended* to be a cubic conventional cell with lattice constant 5.43 Å, but I just noticed the cell matrix I was handed is slightly “dirty” from a previous relaxation: the diagonal entries differ from each other by about 1e-7 Å and there are tiny off‑diagonal shear terms of similar magnitude. I initially planned to use an extra‑loose symmetry detection threshold, but given these perturbations I actually want the **tightest tolerance that still treats those ~1e-7 Å numerical artifacts as symmetry‑equivalent** (i.e., don’t let the noise break cubic symmetry). Please compute the lattice (space) symmetry operations for this unit cell.", "answers": "[{\"name\": \"gpaw_bztools_get_lattice_symmetry\", \"arguments\": {\"cell_cv\": [[5.43, 0.0, 0.0], [0.0, 5.43, 0.0], [0.0, 0.0, 5.43]], \"tolerance\": 1e-06}}]"}
{"func_name": "gpaw_bztools_get_symmetry_operations", "func_desc": "gpaw.bztools.get_symmetry_operations\n    Return point symmetry operations used by GPAW Brillouin-zone tools.\n    \n    This function prepares a set of 3x3 point-group symmetry operation matrices that are used throughout GPAW to reduce Brillouin-zone sampling, apply symmetry to k-points, and otherwise exploit spatial symmetries in density-functional-theory calculations (see GPAW README for general context). The function accepts an existing collection of Cartesian 3x3 symmetry matrices or None and optionally augments that collection with time-reversal counterparts. The implementation ensures sensible defaults and prevents adding time-reversal operations when a pure spatial inversion operation is already present.", "tools": [{"function": {"description": "gpaw.bztools.get_symmetry_operations\nReturn point symmetry operations used by GPAW Brillouin-zone tools.\n\nThis function prepares a set of 3x3 point-group symmetry operation matrices that are used throughout GPAW to reduce Brillouin-zone sampling, apply symmetry to k-points, and otherwise exploit spatial symmetries in density-functional-theory calculations (see GPAW README for general context). The function accepts an existing collection of Cartesian 3x3 symmetry matrices or None and optionally augments that collection with time-reversal counterparts. The implementation ensures sensible defaults and prevents adding time-reversal operations when a pure spatial inversion operation is already present.", "name": "gpaw_bztools_get_symmetry_operations", "parameters": {"properties": {"U_scc": {"type": "array", "items": {"type": "float"}, "description": "Array containing point symmetry operations. In GPAW this is expected to be an array of 3x3 matrices with shape (n_ops, 3, 3) that represent spatial point-group operations in Cartesian coordinates; elements are typically integers for pure rotations/reflections but floating dtypes are accepted by the code path. If U_scc is None, the function treats it as a single identity operation and internally replaces it with numpy.array([numpy.eye(3)]). The array is not modified in-place; a new array is returned. If U_scc has an incompatible shape (not indexable as (n,3,3)), subsequent elementwise comparisons and concatenation will raise an exception (ValueError or IndexError) propagated to the caller.", "default": ""}, "time_reversal": {"type": "boolean", "description": "Whether to include time-reversal variants of the supplied point operations. If True and no pure spatial inversion (-I) is present among U_scc, the function returns the concatenation of the original operations and their negatives (i.e., both U_scc and -U_scc, doubling the number of operations). If False, only the supplied operations (or the identity default) are returned. If a pure inversion operation (matrix equal to -I) is detected in U_scc, the function disables time_reversal by setting it to False internally (because spatial inversion already provides the equivalent symmetry in this context); this internal change is not reflected back to the caller except via the returned operations.", "default": ""}}, "required": ["U_scc", "time_reversal"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a Brillouin-zone reduction regression test for a nominally cubic system. Start with two spatial point symmetries in Cartesian form: the identity and a right-angle rotation around the z-axis (so U_scc is a 2×3×3 array). I initially planned to enable time-reversal so the set would get doubled by adding the sign-flipped counterparts, but I just realized my upstream symmetry finder sometimes sneaks in an operation that maps every vector to its negative (pure inversion), and in that case GPAW’s BZ tooling should *not* add time-reversal-derived operations. So: call the symmetry-operation preparation exactly as GPAW would, with time-reversal requested, but first augment the provided U_scc by appending the operation that is the negative of the identity. Return the final symmetry operation set GPAW would use.", "answers": "[{\"name\": \"gpaw_bztools_get_symmetry_operations\", \"arguments\": {\"U_scc\": [[[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0, -1, 0], [1, 0, 0], [0, 0, 1]], [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]], \"time_reversal\": true}}]"}
{"func_name": "gpaw_bztools_tetrahedron_volume", "func_desc": "gpaw.bztools.tetrahedron_volume — Calculate volume of a tetrahedron or volumes of many tetrahedra.\n    \n    Calculate the (unsigned) volume using the scalar triple product formula employed in Brillouin-zone tetrahedron methods and other tetrahedral integrations that appear in density-functional theory (DFT) workflows with GPAW and ASE. This routine computes |(a - d) · ((b - d) × (c - d))| / 6 in a vectorized manner. It is used in gpaw.bztools for geometric operations such as computing tetrahedral volumes for k-point or real-space partitioning needed in electronic-structure integration schemes and post-processing utilities that rely on tetrahedral decomposition.", "tools": [{"function": {"description": "gpaw.bztools.tetrahedron_volume — Calculate volume of a tetrahedron or volumes of many tetrahedra.\n\nCalculate the (unsigned) volume using the scalar triple product formula employed in Brillouin-zone tetrahedron methods and other tetrahedral integrations that appear in density-functional theory (DFT) workflows with GPAW and ASE. This routine computes |(a - d) · ((b - d) × (c - d))| / 6 in a vectorized manner. It is used in gpaw.bztools for geometric operations such as computing tetrahedral volumes for k-point or real-space partitioning needed in electronic-structure integration schemes and post-processing utilities that rely on tetrahedral decomposition.", "name": "gpaw_bztools_tetrahedron_volume", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "Coordinates of vertex a. For a single tetrahedron this is a 1-D array of length 3 containing Cartesian coordinates (typically in Angstrom when used with ASE/GPAW geometries). For batched/vectorized use this may be a 2-D array with shape (N, 3), where N is the number of tetrahedra; in that case a[i] is the a-vertex of the i-th tetrahedron. The array must be numeric and finite for meaningful results.", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "Coordinates of vertex b. Same conventions and requirements as parameter a. b must have the same shape and alignment as a so that corresponding rows define the same tetrahedra when performing batched computations.", "default": ""}, "c": {"type": "array", "items": {"type": "any"}, "description": "Coordinates of vertex c. Same conventions and requirements as parameter a. c must have the same shape and alignment as a and b.", "default": ""}, "d": {"type": "array", "items": {"type": "any"}, "description": "Coordinates of vertex d (the reference vertex used in the triple product). Same conventions and requirements as parameter a. d must have the same shape and alignment as a, b, and c.", "default": ""}}, "required": ["a", "b", "c", "d"], "type": "any"}}, "type": "function"}], "query": "I’m doing a tetrahedron-method sanity check for a simple-cubic Brillouin zone, but I just realized I may have inconsistent vertex ordering from a k-mesh generator, so I can’t trust the sign of the scalar triple product. Please compute **unsigned** volumes using `gpaw.bztools.tetrahedron_volume` for the two tetrahedra below, but treat them as a single batched call (vectorized) because that’s how my post-processing script feeds them in.\n\nT1 vertices (a,b,c,d):\n- a = (0, 0, 0)\n- b = (1/2, 0, 0)\n- c = (0, 1/2, 0)\n- d = (0, 0, 1/2)\n\nT2 vertices (a,b,c,d):\n- a = (1/2, 1/2, 0)\n- b = (1/2, 0, 1/2)\n- c = (0, 1/2, 1/2)\n- d = (0, 0, 0)\n\nOne more constraint from my workflow: for the second tetrahedron, don’t reorder vertices manually even if you suspect the orientation is inverted—just rely on the routine’s absolute value behavior (unsigned volume). Return the two volumes in reciprocal-space units in the same order (T1 then T2).", "answers": "[{\"name\":\"gpaw_bztools_tetrahedron_volume\",\"arguments\":{\"a\":[[0.0,0.0,0.0],[0.5,0.5,0.0]],\"b\":[[0.5,0.0,0.0],[0.5,0.0,0.5]],\"c\":[[0.0,0.5,0.0],[0.0,0.5,0.5]],\"d\":[[0.0,0.0,0.5],[0.0,0.0,0.0]]}}]"}
{"func_name": "gpaw_bztools_unfold_points", "func_desc": "Unfold k-points using a set of symmetry operators to produce the full set of symmetry-equivalent k-points for Brillouin-zone sampling and band-structure unfolding in GPAW DFT calculations.", "tools": [{"function": {"description": "Unfold k-points using a set of symmetry operators to produce the full set of symmetry-equivalent k-points for Brillouin-zone sampling and band-structure unfolding in GPAW DFT calculations.\n", "name": "gpaw_bztools_unfold_points", "parameters": {"properties": {"points": {"type": "array", "items": {"type": "any"}, "description": "Array of input k-point vectors to be unfolded. Each row is a 3-component k-point vector in reciprocal-space coordinates (typical shape (n_points, 3)). In the GPAW context these are the k-points (wavevector coordinates) for which one wants to generate symmetry-equivalent points when exploiting crystal symmetries for Brillouin-zone sampling or band unfolding.", "default": ""}, "U_scc": {"type": "array", "items": {"type": "any"}, "description": "Array of symmetry operators applied to the k-points. Each element is a linear operator (typically a 3x3 matrix) that maps k-point vectors according to a crystal symmetry operation. The function applies these operators to the rows of points, concatenates the results, and thus expands an irreducible set of k-points to the full symmetry-generated set. The array must be shape-compatible with points for the matrix multiplication performed with numpy.dot (for example, a common layout is one operator per first axis and 3x3 matrices on the last two axes).", "default": ""}, "tol": {"type": "float", "description": "Numerical tolerance used to decide when two k-points are considered identical and should be merged. Default is 1e-08. This tolerance controls the floating-point equality comparison performed by unique_rows: points whose coordinates differ by less than tol (taking into account optional periodicity when mod is set) are treated as duplicates and only one representative is kept. Very small tol may keep near-duplicate points due to numerical noise; very large tol may incorrectly merge distinct k-points.", "default": 1e-08}, "mod": {"type": "integer", "nullable": true, "description": "Either 1 or None. When set to 1, k-points that differ by an integer reciprocal-lattice vector are considered identical (periodicity in reciprocal space is applied before comparing with tol). When None (default) periodic identification is disabled and only direct coordinate comparison within tol is used. The function forwards this argument to unique_rows; passing values other than 1 or None is not supported and may raise an error from unique_rows or produce incorrect results.", "default": null}}, "required": ["points", "U_scc", "tol", "mod"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a GPAW band-unfolding run for a nominally cubic cell, but the k-point list I got back from a previous script is a bit messy: it contains two intended reduced k-points plus a near-duplicate caused by floating-point noise: (0.0, 0.0, 0.0), (0.25, 0.0, 0.0), and (0.2500000004, 0.0, 0.0). Unfold the *physically distinct* reduced points using the four C4 symmetry operations about the z-axis (identity plus 90°/180°/270° rotations). Treat k-points modulo reciprocal lattice vectors (so fold everything back into the first BZ representation consistently).\n\nI initially planned to keep the same tight tolerance as usual, but with that near-duplicate present we should instead use the loosest tolerance that still merges those two nearly identical k-points into one unique seed before unfolding (i.e., a tolerance just large enough that their difference is considered equal). Use that tolerance in the unfolding/deduplication step.", "answers": "[{\"name\":\"gpaw_bztools_unfold_points\",\"arguments\":{\"points\":[[0.0,0.0,0.0],[0.25,0.0,0.0],[0.2500000004,0.0,0.0]],\"U_scc\":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[-1.0,0.0,0.0],[0.0,-1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[-1.0,0.0,0.0],[0.0,0.0,1.0]]],\"tol\":4.0000001e-10,\"mod\":1}}]"}
{"func_name": "gpaw_bztools_unique_rows", "func_desc": "gpaw.bztools.unique_rows returns the unique rows of a 2D NumPy array, treating rows as k-point vectors in GPAW's Brillouin-zone utilities. It is used to reduce lists of k-points by identifying rows that represent the same point within a given numerical tolerance, optionally taking periodicity (full reciprocal-lattice-vector equivalence) and local clustering into account. The function preserves the original row values in the returned array but orders the unique rows according to a lexicographic sort applied after internal normalization and rounding.", "tools": [{"function": {"description": "gpaw.bztools.unique_rows returns the unique rows of a 2D NumPy array, treating rows as k-point vectors in GPAW's Brillouin-zone utilities. It is used to reduce lists of k-points by identifying rows that represent the same point within a given numerical tolerance, optionally taking periodicity (full reciprocal-lattice-vector equivalence) and local clustering into account. The function preserves the original row values in the returned array but orders the unique rows according to a lexicographic sort applied after internal normalization and rounding.\n", "name": "gpaw_bztools_unique_rows", "parameters": {"properties": {"ain": {"type": "array", "items": {"type": "any"}, "description": "2D ndarray of shape (N, M) containing N row vectors (for example, k-point coordinates in fractional reciprocal-lattice units) to be filtered for uniqueness. The function expects a true NumPy array with two dimensions; passing an array with a different number of dimensions or a non-array object will cause an error (TypeError or IndexError) when the code attempts column-wise operations.", "default": ""}, "tol": {"type": "float", "description": "Tolerance used to decide when two rows are considered identical. The tolerance controls the number of decimal places used for rounding via rounds = -numpy.log10(tol).astype(int). Default is 1e-10. tol must be positive; nonpositive values will lead to invalid logarithms and raise an error (ValueError or RuntimeWarning).", "default": 1e-10}, "mod": {"type": "integer", "nullable": true, "description": "Integer 1 or None. If set to 1, the function treats points that differ by a full reciprocal-lattice vector (i.e., integer shifts of 1 in fractional coordinates) as identical by applying a modulus operation before and after rounding. If None (the default), no periodic-identification modulo a reciprocal lattice vector is applied. This parameter is intended for the common GPAW use-case of identifying k-points modulo the periodicity of the Brillouin zone.", "default": null}, "aglomerate": {"type": "boolean", "description": "If True (default), the function calls aglomerate_points(a, tol) on the normalized coordinates before rounding; this agglomeration clusters nearby points (within tol) so that tightly spaced groups are treated consistently when rounding and modulus operations are performed. If False, clustering is skipped and rows are compared directly after rounding. Agglomeration is useful to avoid splitting clusters across modulus boundaries or rounding thresholds when identifying unique k-points.", "default": true}}, "required": ["ain", "tol", "aglomerate", "mod"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning a k-point path (fractional reciprocal-lattice coords) before a GPAW band-structure run. Initially I was going to treat points as strictly distinct unless they matched numerically, but then I remembered the downstream symmetry reduction assumes reciprocal-space periodicity, so points differing by any integer reciprocal lattice vector should collapse to the same representative. Also, I don’t want any neighborhood/cluster merging beyond what the numeric tolerance already implies—i.e., no local agglomeration heuristics; only the rounding/modulo logic should decide equivalence.\n\nOne more constraint: the tolerance shouldn’t be the loose one we used for quick scans; it needs to be the value that is exactly two orders of magnitude tighter than that baseline.\n\nUse gpaw.bztools.unique_rows to reduce the following k-point list to unique points under those rules, preserving the original row values in the output (with the function’s normal lexicographic ordering after internal normalization/rounding):\n[[0.0, 0.0, 0.0],\n [1.0, 0.0, 0.0],\n [0.5000002, 0.5, 0.0],\n [0.5, 0.5000001, 0.0],\n [-0.5, 0.5, 0.0],\n [0.25, 0.25, 0.25],\n [0.2500004, 0.2499996, 0.25]].", "answers": "[{\"name\": \"gpaw_bztools_unique_rows\", \"arguments\": {\"ain\": [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.5000002, 0.5, 0.0], [0.5, 0.5000001, 0.0], [-0.5, 0.5, 0.0], [0.25, 0.25, 0.25], [0.2500004, 0.2499996, 0.25]], \"tol\": 1e-08, \"mod\": 1, \"aglomerate\": false}}]"}
{"func_name": "gpaw_coding_style_function", "func_desc": "gpaw.coding_style.function performs an elementwise addition of two NumPy arrays and returns the result. This small utility is used in the GPAW DFT codebase to combine array-valued quantities that appear in electronic-structure calculations (for example densities, potentials or components of wavefunctions) while respecting NumPy broadcasting rules and MPI-aware debug printing conventions used in GPAW scripts and modules.\n    \n    This function creates and returns a new numpy.ndarray that is the elementwise sum of the inputs. As implemented in the source, the function also has a side effect: when the module-level boolean flag debug is true and the MPI world communicator indicates the current process has rank 0 (mpi.world.rank == 0), the function prints the computed result scaled by the module-level constant Ha. The scaling and printing are only for human-readable diagnostic output and do not alter the returned array. Because the function relies on module-level globals (debug, mpi, Ha), its printing behavior depends on the GPAW runtime environment and MPI configuration.", "tools": [{"function": {"description": "gpaw.coding_style.function performs an elementwise addition of two NumPy arrays and returns the result. This small utility is used in the GPAW DFT codebase to combine array-valued quantities that appear in electronic-structure calculations (for example densities, potentials or components of wavefunctions) while respecting NumPy broadcasting rules and MPI-aware debug printing conventions used in GPAW scripts and modules.\n\nThis function creates and returns a new numpy.ndarray that is the elementwise sum of the inputs. As implemented in the source, the function also has a side effect: when the module-level boolean flag debug is true and the MPI world communicator indicates the current process has rank 0 (mpi.world.rank == 0), the function prints the computed result scaled by the module-level constant Ha. The scaling and printing are only for human-readable diagnostic output and do not alter the returned array. Because the function relies on module-level globals (debug, mpi, Ha), its printing behavior depends on the GPAW runtime environment and MPI configuration.", "name": "gpaw_coding_style_function", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "Left operand of the addition. In the GPAW context this typically represents an array-valued physical quantity (for example a real-space grid of densities or potentials) stored as a NumPy array. The function expects a to be a numpy.ndarray; providing another type is outside the documented contract and may raise a TypeError or lead to implicit NumPy coercion.", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "Right operand of the addition. As with a, b is expected to be a numpy.ndarray representing a GPAW-related array-valued quantity. The shapes of a and b must be compatible for NumPy addition (either identical shapes or shapes compatible under NumPy broadcasting); otherwise a runtime error (such as ValueError) may be raised.", "default": ""}}, "required": ["a", "b"], "type": "any"}}, "type": "function"}], "query": "In a GPAW DFT workflow I’m stitching together a real-space correction term with a uniform background shift before writing a diagnostic cube file. I started by planning to add the background as a 1×3 row so NumPy broadcasting would apply it across both rows of my 2×3 grid. But then I realized the upstream routine sometimes hands me the background as a flat 3-vector (no explicit leading dimension), and I need this to work without reshaping while still broadcasting across the first axis.\n\nSo: take the 2×3 correction grid a = [[-0.12, 0.03, 0.08], [0.05, -0.01, 0.10]] and add the same +0.02 offset to each of its three columns using the background in the shape that broadcasts correctly without adding a new axis. Return the combined array (ignore any debug/MPI printing side effects).", "answers": "[{\"name\":\"gpaw_coding_style_function\",\"arguments\":{\"a\":[[-0.12,0.03,0.08],[0.05,-0.01,0.1]],\"b\":[0.02,0.02,0.02]}}]"}
{"func_name": "gpaw_convergence_criteria_dict2criterion", "func_desc": "Converts a dictionary specification into a GPAW convergence criterion object.\n    \n    This function is used in GPAW (the DFT/PAW code) to translate a serialized or user-provided dictionary representation of a convergence criterion into a runtime criterion object that controls iterative convergence checks (for example, SCF energy or density convergence or geometry optimization tolerances). The input dictionary may be the output of a criterion's todict() method (containing a 'name' key and other keyword arguments), or a compact user shortcut mapping a criterion name to a value (for example {'energy': 0.005} or {'energy': (0.005, 3)}). dict2criterion uses get_criterion(name) to obtain the criterion factory/class and constructs an instance either by passing keyword arguments (when the dictionary contains 'name') or by passing a single positional argument (when a criterion name maps to a scalar/tuple). The function copies the input dictionary before modification so the original argument is not mutated.", "tools": [{"function": {"description": "Converts a dictionary specification into a GPAW convergence criterion object.\n\nThis function is used in GPAW (the DFT/PAW code) to translate a serialized or user-provided dictionary representation of a convergence criterion into a runtime criterion object that controls iterative convergence checks (for example, SCF energy or density convergence or geometry optimization tolerances). The input dictionary may be the output of a criterion's todict() method (containing a 'name' key and other keyword arguments), or a compact user shortcut mapping a criterion name to a value (for example {'energy': 0.005} or {'energy': (0.005, 3)}). dict2criterion uses get_criterion(name) to obtain the criterion factory/class and constructs an instance either by passing keyword arguments (when the dictionary contains 'name') or by passing a single positional argument (when a criterion name maps to a scalar/tuple). The function copies the input dictionary before modification so the original argument is not mutated.", "name": "gpaw_convergence_criteria_dict2criterion", "parameters": {"properties": {"dictionary": {"type": "any", "description": "A dictionary describing a convergence criterion. Accepted forms:\n- A full specification produced by todict(), i.e. a dict containing a 'name' key plus additional keys that are passed as keyword arguments to the criterion constructor. Example: {'name': 'energy', 'tol': 0.005, 'n_old': 3}.\n- A user shortcut mapping a single criterion name to a value, where the value may be a scalar, a tuple, or another dict. Example scalar: {'energy': 0.005}. Example tuple: {'energy': (0.005, 3)}.\n- A combination/nested form where the single key maps to a dict that itself contains a 'name' key; this nested dict will be handled recursively. Example: {'energy': {'name': 'energy', 'tol': 0.005, 'n_old': 3}}.\nThe dictionary must therefore either contain a top-level 'name' key, or contain exactly one top-level key that names the criterion. The dictionary represents how tolerances and internal parameters for convergence checks are specified in GPAW input or serialization and is transformed into a concrete criterion object used at runtime to decide when iterations have converged.", "default": ""}}, "required": ["dictionary"], "type": "any"}}, "type": "function"}], "query": "I’m resurrecting an old GPAW SCF workflow from a messy archive and need to reconstruct the convergence criterion object from a serialized dict before restarting the calculation. Initially I was going to use the compact shortcut form (mapping a criterion label directly to a value), but I realized I also have to carry over the history window from the previous run, so I need the fully serialized form that round-trips via `todict()`.\n\nUse `gpaw_convergence_criteria_dict2criterion` to convert a dictionary into the runtime criterion object, with these constraints:\n- The criterion is the energy-based SCF convergence check.\n- The tolerance should be the same as “5 micro-units” in electronvolts expressed in standard scientific notation.\n- The history length should match the previous run’s rolling window of four iterations.\n\nWhat exact `dictionary` payload should I pass so the function builds the correct criterion object without mutating my original dict?", "answers": "[{\"name\": \"gpaw_convergence_criteria_dict2criterion\", \"arguments\": {\"dictionary\": {\"name\": \"energy\", \"tol\": 5e-06, \"n_old\": 4}}}]"}
{"func_name": "gpaw_core_matrix_suggest_blocking", "func_desc": "Suggest blocking (process grid and block size) for an NxN matrix used in parallel linear-algebra routines.", "tools": [{"function": {"description": "Suggest blocking (process grid and block size) for an NxN matrix used in parallel linear-algebra routines.\n", "name": "gpaw_core_matrix_suggest_blocking", "parameters": {"properties": {"N": {"type": "integer", "description": "Size N of the square matrix (NxN) to be distributed and operated on. In the GPAW/DFT context this is the dimension of a dense matrix (for example a Hamiltonian or overlap matrix) that will be processed by parallel ScaLAPACK-style block-cyclic algorithms. N is used to determine a block size that yields at least one whole block and at least two blocks in total when the matrix is partitioned across the process grid.", "default": ""}, "ncpus": {"type": "integer", "description": "Number of CPU processes (MPI ranks) available for the parallel calculation. This function arranges these processes into a 2D process grid (rows × columns) for distributing the matrix. In typical GPAW usage ncpus corresponds to the total number of MPI ranks used for a linear-algebra task.", "default": ""}}, "required": ["N", "ncpus"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a dense ScaLAPACK diagonalization in GPAW for an N×N Hamiltonian and I have 96 MPI ranks available. I initially planned to just use N = 12000 as-is, but I realized the matrix is assembled from a real-space grid where we pad the basis so the global dimension becomes the smallest multiple of the CPU count that is not smaller than 12000 (to avoid uneven last blocks and improve load balance). Use that padded N (not the unpadded one) and suggest a good 2D process grid and block size for ScaLAPACK on 96 ranks.", "answers": "[{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":12096,\"ncpus\":96}}]"}
{"func_name": "gpaw_directmin_derivatives_mgs", "func_desc": "gpaw.directmin.derivatives.mgs: Modified Gram–Schmidt orthonormalization routine used in GPAW direct-minimization derivatives.\n    \n    Performs the modified Gram–Schmidt procedure on a set of input vectors to produce an orthonormal set. In the GPAW codebase this is used when orthonormalizing collections of vectors such as trial wavefunction coefficient arrays, search directions, or basis vectors inside direct-minimization algorithms for density-functional theory (DFT) calculations. The routine normalizes vectors and removes components along previously processed vectors so that the output vectors are orthonormal with respect to the standard Euclidean inner product.", "tools": [{"function": {"description": "gpaw.directmin.derivatives.mgs: Modified Gram–Schmidt orthonormalization routine used in GPAW direct-minimization derivatives.\n\nPerforms the modified Gram–Schmidt procedure on a set of input vectors to produce an orthonormal set. In the GPAW codebase this is used when orthonormalizing collections of vectors such as trial wavefunction coefficient arrays, search directions, or basis vectors inside direct-minimization algorithms for density-functional theory (DFT) calculations. The routine normalizes vectors and removes components along previously processed vectors so that the output vectors are orthonormal with respect to the standard Euclidean inner product.", "name": "gpaw_directmin_derivatives_mgs", "parameters": {"properties": {"vin": {"type": "array", "items": {"type": "any"}, "description": "Array containing the set of input vectors. The routine expects vin[i] to refer to the i-th vector; the array layout is preserved in the output (i.e., the returned array has the same shape and dtype as vin). vin is treated as read-only by callers: the function makes an internal deep copy before modifying data so the provided vin is not changed by the call.", "default": ""}}, "required": ["vin"], "type": "any"}}, "type": "function"}], "query": "I’m in the middle of a GPAW direct-minimization iteration and I was about to orthonormalize the three search-direction coefficient vectors with the plain Euclidean inner product. Wait—these directions come from a preconditioned update and one of them is accidentally almost collinear with the first (numerical noise makes it look independent, but it shouldn’t contribute a new direction). So: run modified Gram–Schmidt on the same three input vectors, but treat any vector whose post-projection residual norm is effectively zero as a redundant artifact and drop it (i.e., only keep the non-redundant orthonormal directions in the output). Use the standard numerical cutoff typically used to detect a near-zero norm in such orthonormalization steps.\n\nInput vectors:\nv1 = [1.0, 1.0, 0.0]\nv2 = [2.0, 2.0, 0.0]\nv3 = [0.0, 1.0, 1.0]\n\nReturn the orthonormal set that remains after removing the redundant direction(s).", "answers": "[{\"name\":\"gpaw_directmin_derivatives_mgs\",\"arguments\":{\"vin\":[[1.0,1.0,0.0],[2.0,2.0,0.0],[0.0,1.0,1.0]]}}]"}
{"func_name": "gpaw_directmin_lcao_etdm_helper_lcao_rotate_subspace", "func_desc": "rotate_subspace selects canonical orbitals for an LCAO subspace by forming the projected (n-by-n) Hamiltonian in the subspace, diagonalizing it, and rotating the input coefficient matrix into the basis of its eigenvectors. This routine is used in the gpaw.directmin.lcao.etdm_helper_lcao workflow to obtain energy-diagonal (canonical) orbitals and their eigenvalues when performing direct minimization or subspace rotations in the LCAO (linear combination of atomic orbitals) representation within GPAW's DFT calculations.", "tools": [{"function": {"description": "rotate_subspace selects canonical orbitals for an LCAO subspace by forming the projected (n-by-n) Hamiltonian in the subspace, diagonalizing it, and rotating the input coefficient matrix into the basis of its eigenvectors. This routine is used in the gpaw.directmin.lcao.etdm_helper_lcao workflow to obtain energy-diagonal (canonical) orbitals and their eigenvalues when performing direct minimization or subspace rotations in the LCAO (linear combination of atomic orbitals) representation within GPAW's DFT calculations.\n", "name": "gpaw_directmin_lcao_etdm_helper_lcao_rotate_subspace", "parameters": {"properties": {"h_mm": {"type": "array", "items": {"type": "any"}, "description": "The subspace Hamiltonian matrix with shape (m, m). This is the Hamiltonian represented in the m-dimensional subspace (indices m,m) used by the etdm LCAO helper; it must be a square numpy.ndarray (Hermitian in exact arithmetic). Practically, h_mm provides the Hamiltonian matrix elements for the subspace whose orbitals are to be canonicalized. The function uses h_mm to form the projected Hamiltonian l_nn = c_nm @ h_mm @ c_nm.conj().T; if h_mm is not square or not compatible with c_nm, a ValueError will be raised by NumPy during the matrix multiplications.", "default": ""}, "c_nm": {"type": "array", "items": {"type": "any"}, "description": "The coefficient matrix with shape (n, m). Rows are indexed by n (basis-function or grid degrees of freedom used by GPAW) and columns by the m subspace vectors. In the LCAO/direct-minimization context, c_nm contains the current subspace vectors expressed in the underlying n-dimensional basis. This matrix may be real or complex; the routine forms the projected Hamiltonian in the n-dimensional subspace via c_nm @ h_mm @ c_nm.conj().T and then diagonalizes that projected Hamiltonian.", "default": ""}}, "required": ["h_mm", "c_nm"], "type": "any"}}, "type": "function"}], "query": "I’m canonicalizing a 3-orbital LCAO subspace in a GPAW direct-minimization (etdm_helper_lcao) step using `rotate_subspace`. Initially I was going to feed the 3×3 projected Hamiltonian as-is, but I realized the matrix I got from my projection has a tiny loss of Hermiticity from numerical noise, and `rotate_subspace` assumes a strictly Hermitian operator for a stable eigendecomposition.\n\nSo: before rotating, replace the subspace Hamiltonian by its Hermitian part (i.e., average it with its conjugate-transpose) and then perform the canonicalization. Use the coefficient matrix as given.\n\nData:\n- Raw h_mm = [[-0.50, 0.10, 0.00],\n             [0.10, -0.20, 0.05],\n             [0.00, 0.05, 0.30]]\n- c_nm (4×3) = [[1.0, 0.0, 0.0],\n               [0.0, 1.0, 0.0],\n               [0.0, 0.0, 1.0],\n               [0.2, 0.1, -0.1]]\n\nReturn the rotated coefficient matrix and the eigenvalues (sorted in ascending energy, as in canonical orbitals).", "answers": "[{\"name\":\"gpaw_directmin_lcao_etdm_helper_lcao_rotate_subspace\",\"arguments\":{\"h_mm\":[[-0.5,0.1,0.0],[0.1,-0.2,0.05],[0.0,0.05,0.3]],\"c_nm\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0],[0.2,0.1,-0.1]]}}]"}
{"func_name": "gpaw_directmin_sd_etdm_multiply", "func_desc": "gpaw.directmin.sd_etdm.multiply scales each value in a dictionary by a scalar and returns a new dictionary containing the scaled values. This utility is used in the GPAW direct minimization (sd_etdm) code path to produce a scaled copy of a state or step vector representation where state vectors or numeric containers are stored as dictionary values keyed by their identifiers. The function implements y[k] = const * x[k] for every key k present in x without mutating the input dictionary x itself.", "tools": [{"function": {"description": "gpaw.directmin.sd_etdm.multiply scales each value in a dictionary by a scalar and returns a new dictionary containing the scaled values. This utility is used in the GPAW direct minimization (sd_etdm) code path to produce a scaled copy of a state or step vector representation where state vectors or numeric containers are stored as dictionary values keyed by their identifiers. The function implements y[k] = const * x[k] for every key k present in x without mutating the input dictionary x itself.\n", "name": "gpaw_directmin_sd_etdm_multiply", "parameters": {"properties": {"x": {"type": "any", "description": "Input mapping representing a state, step, or collection of numeric quantities used in GPAW's direct minimization routines. Keys are identifiers (typically strings or other hashable objects) and values are objects that support multiplication by a Python float (for example Python numeric scalars or NumPy arrays, which are commonly used in GPAW as described in the project README). The function iterates over x.keys() and reads x[k] for each key; it does not modify x, so callers can rely on x remaining unchanged after the call.", "default": ""}, "const": {"type": "float", "description": "Scalar multiplier applied to every value in x. The default is 1.0, which produces a new dictionary whose values are the result of 1.0 * x[k] (effectively a scaled shallow copy). In the GPAW context this parameter is used to scale step sizes, residuals, or other numeric vectors during direct minimization algorithms.", "default": 1.0}}, "required": ["x", "const"], "type": "any"}}, "type": "function"}], "query": "I’m in the GPAW direct-minimization (sd_etdm) path and I need to generate a *non-mutating* scaled copy of a per-band step/residual dictionary before applying a line-search safeguard. Initially I was going to just multiply the step by the same negative scalar we used last time to take a quarter-step uphill, but I realized the sign convention here is flipped because these values already represent the *negative* gradient contribution. So: keep the magnitude equal to a quarter-step, but choose the sign so the scaled step points along the *descent* direction given the current dictionary. Use gpaw.directmin.sd_etdm.multiply to scale the following dictionary without modifying the input: {'band_0': 0.08, 'band_1': -0.12, 'band_2': 0.05}.", "answers": "[{\"name\": \"gpaw_directmin_sd_etdm_multiply\", \"arguments\": {\"x\": {\"band_0\": 0.08, \"band_1\": -0.12, \"band_2\": 0.05}, \"const\": 0.25}}]"}
{"func_name": "gpaw_directmin_tools_expm_ed_unit_inv", "func_desc": "gpaw.directmin.tools.expm_ed_unit_inv computes a unitary matrix by evaluating the matrix exponential formula given in Eq. (6) of J. Hutter, M. Parrinello, and S. Vogel, J. Chem. Phys., 101, 3862 (1994). This routine is used in GPAW's direct-minimization tools to construct occupied/virtual rotation operators (unitary updates) from the block X (called a_upp_r here) that couples occupied and virtual subspaces during wavefunction optimization in density-functional theory (DFT) calculations.", "tools": [{"function": {"description": "gpaw.directmin.tools.expm_ed_unit_inv computes a unitary matrix by evaluating the matrix exponential formula given in Eq. (6) of J. Hutter, M. Parrinello, and S. Vogel, J. Chem. Phys., 101, 3862 (1994). This routine is used in GPAW's direct-minimization tools to construct occupied/virtual rotation operators (unitary updates) from the block X (called a_upp_r here) that couples occupied and virtual subspaces during wavefunction optimization in density-functional theory (DFT) calculations.\n", "name": "gpaw_directmin_tools_expm_ed_unit_inv", "parameters": {"properties": {"a_upp_r": {"type": "array", "items": {"type": "any"}, "description": "The block X from the paper, given as a two-dimensional NumPy array with shape (n_o, n_v) where n_o is the number of occupied states (rows) and n_v is the number of virtual/unoccupied states (columns). The function uses a_upp_r to form p_nn = a_upp_r @ a_upp_r.T.conj() and therefore expects a numeric array (real or complex) that supports .T.conj() and matrix multiplication. In the GPAW DFT context, a_upp_r encodes infinitesimal rotations or update amplitudes between occupied and virtual subspaces; this routine converts that information into a full unitary rotation operator. If a_upp_r is exactly zero (all elements equal to zero within floating-point tolerance), the function returns the identity (or the corresponding block form) without performing eigendecomposition.", "default": ""}, "oo_vo_blockonly": {"type": "boolean", "description": "If False (default), return the full unitary matrix U of shape (n_o + n_v, n_o + n_v) constructed from the four blocks U_oo, U_ov, U_vo, and U_vv as in the cited equation; this is the typical form needed when a complete unitary update of the combined occupied+virtual basis is required. If True, return only the stacked column-blocks that contain the U_oo and U_vo blocks as a contiguous array of shape (n_o + n_v, n_o). This option exists for algorithms in GPAW that only need the occupied-column part of the unitary (oo and vo blocks) to update occupied orbitals and avoid constructing the full (larger) matrix, yielding memory and computational savings.", "default": false}}, "required": ["a_upp_r", "oo_vo_blockonly"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW direct-minimization wavefunction update with 2 occupied and 3 virtual states, and I want to construct the occupied-column part of the unitary rotation operator (the blocks that map into the occupied subspace) to reduce memory. I initially planned to pass the occupied–virtual coupling block X exactly as extracted from my optimizer, but I just realized my extraction routine returns the transpose relative to what expm_ed_unit_inv expects (it outputs the virtual-by-occupied coupling, not occupied-by-virtual). Please fix this by converting it to the correct orientation before calling the routine, and then compute only the occupied-column part. The (transposed) block I currently have is:\n[[0.05, 0.01],\n [-0.02, 0.03],\n [0.00, -0.04]].", "answers": "[{\"name\": \"gpaw_directmin_tools_expm_ed_unit_inv\", \"arguments\": {\"a_upp_r\": [[0.05, -0.02, 0.0], [0.01, 0.03, -0.04]], \"oo_vo_blockonly\": true}}]"}
{"func_name": "gpaw_directmin_tools_gramschmidt_lcao", "func_desc": "Gram-Schmidt orthonormalization for linear-combination-of-atomic-orbitals (LCAO)\n    using a Cholesky decomposition of the projected overlap matrix.\n    \n    This function is used in GPAW's LCAO/direct-minimization context to convert a\n    set of LCAO expansion coefficients into an orthonormal set with respect to\n    the atomic-basis overlap. Concretely, it forms the projected overlap matrix\n    S_nn = C_nM @ S_MM.conj() @ C_nM.T.conj(), performs a lower-triangular\n    Cholesky factorization S_nn = L_nn L_nn^H, and returns X = L_nn^{-1} C_nM.\n    The returned coefficient matrix X satisfies X @ S_MM.conj() @ X.T.conj() =\n    Identity_n within numerical precision, which is required before diagonaliza-\n    tion or other operations that assume an orthonormal set of states.", "tools": [{"function": {"description": "Gram-Schmidt orthonormalization for linear-combination-of-atomic-orbitals (LCAO)\nusing a Cholesky decomposition of the projected overlap matrix.\n\nThis function is used in GPAW's LCAO/direct-minimization context to convert a\nset of LCAO expansion coefficients into an orthonormal set with respect to\nthe atomic-basis overlap. Concretely, it forms the projected overlap matrix\nS_nn = C_nM @ S_MM.conj() @ C_nM.T.conj(), performs a lower-triangular\nCholesky factorization S_nn = L_nn L_nn^H, and returns X = L_nn^{-1} C_nM.\nThe returned coefficient matrix X satisfies X @ S_MM.conj() @ X.T.conj() =\nIdentity_n within numerical precision, which is required before diagonaliza-\ntion or other operations that assume an orthonormal set of states.", "name": "gpaw_directmin_tools_gramschmidt_lcao", "parameters": {"properties": {"C_nM": {"type": "array", "items": {"type": "any"}, "description": "LCAO coefficients provided as a two-dimensional\nNumPy array with the first index enumerating states (n) and the\nsecond index enumerating basis functions (M). In the GPAW LCAO\nworkflow these coefficients express wavefunctions (or trial vectors)\nin the non-orthogonal atomic basis. The function expects C_nM to be\ncompatible with matrix multiplication by S_MM on its right (i.e.,\nits second dimension must match the shape of S_MM).", "default": ""}, "S_MM": {"type": "array", "items": {"type": "any"}, "description": "Overlap matrix between basis functions as a\ntwo-dimensional NumPy array with shape (M, M). This matrix encodes\nthe non-orthogonality of the atomic basis used by GPAW. The function\ntreats S_MM as the Hermitian overlap matrix S; it is conjugated where\nneeded in the computation. S_MM must be finite-valued (no NaNs or\nInfs) and compatible with C_nM for the multiplications performed.", "default": ""}}, "required": ["C_nM", "S_MM"], "type": "any"}}, "type": "function"}], "query": "I’m in a GPAW LCAO direct-minimization step with 2 trial Kohn–Sham states expanded in 3 non-orthogonal atomic basis functions, and I need to orthonormalize the coefficients w.r.t. the AO overlap before diagonalization.\n\nStart by doing the usual projected-overlap/Cholesky Gram–Schmidt using the coefficient matrix and overlap matrix below. However, I just realized my SCF driver stores states as columns (basis-functions-by-states), while the orthonormalizer expects states as rows (states-by-basis). So: first detect that mismatch and fix the orientation by applying the unique transpose that makes the projected overlap a 2×2 matrix; then run the Cholesky-based orthonormalization and return the orthonormalized coefficient matrix in the same orientation the orthonormalizer expects.\n\nUse the matrices:\nC (as stored in my driver; columns are states): [[0.80, 0.30], [0.10, 0.90], [-0.20, 0.10]]\nS: [[1.00, 0.12, 0.05], [0.12, 1.00, 0.08], [0.05, 0.08, 1.00]]", "answers": "[{\"name\": \"gpaw_directmin_tools_gramschmidt_lcao\", \"arguments\": {\"C_nM\": [[0.8, 0.1, -0.2], [0.3, 0.9, 0.1]], \"S_MM\": [[1.0, 0.12, 0.05], [0.12, 1.0, 0.08], [0.05, 0.08, 1.0]]}}]"}
{"func_name": "gpaw_directmin_tools_loewdin_lcao", "func_desc": "Loewdin-based orthonormalization of LCAO coefficients used in GPAW direct-minimization tools.\n    \n    This function implements the symmetric (Loewdin) orthonormalization for linear-combination-of-atomic-orbitals\n    (LCAO) coefficient matrices that appear in GPAW's direct minimization and LCAO modes. Given a set of\n    LCAO coefficients C_nM and the basis-function overlap matrix S_MM, it constructs a unitary-like transformation\n    that maps the input coefficients to a new set of coefficients that are orthonormal with respect to S_MM.\n    In practical DFT/LCAO workflows this is used to ensure that the resulting orbitals satisfy\n    C_new.conj() @ S_MM @ C_new.T = I (the Kronecker delta), which is required by many numerical algorithms\n    inside GPAW for stable and correct eigenvalue/energy computations.\n    \n    The routine follows these steps (as implemented in the source):\n    1. Form the Hermitian matrix C^† S C = C_nM.conj() @ S_MM @ C_nM.T with shape (n, n), where n is the number\n       of rows in C_nM (typically number of states/orbitals).\n    2. Diagonalize this Hermitian matrix to obtain eigenvalues ev and eigenvectors U (called S_overlapp in code).\n    3. Build the inverse square-root in the eigenbasis: U (1/sqrt(ev)) U^†.\n    4. Apply the resulting transformation to the input coefficients: C_new = (U (1/sqrt(ev)) U^†).T @ C_nM,\n       returning an array with the same shape as C_nM and with orthonormality C_new.conj() @ S_MM @ C_new.T = I.", "tools": [{"function": {"description": "Loewdin-based orthonormalization of LCAO coefficients used in GPAW direct-minimization tools.\n\nThis function implements the symmetric (Loewdin) orthonormalization for linear-combination-of-atomic-orbitals\n(LCAO) coefficient matrices that appear in GPAW's direct minimization and LCAO modes. Given a set of\nLCAO coefficients C_nM and the basis-function overlap matrix S_MM, it constructs a unitary-like transformation\nthat maps the input coefficients to a new set of coefficients that are orthonormal with respect to S_MM.\nIn practical DFT/LCAO workflows this is used to ensure that the resulting orbitals satisfy\nC_new.conj() @ S_MM @ C_new.T = I (the Kronecker delta), which is required by many numerical algorithms\ninside GPAW for stable and correct eigenvalue/energy computations.\n\nThe routine follows these steps (as implemented in the source):\n1. Form the Hermitian matrix C^† S C = C_nM.conj() @ S_MM @ C_nM.T with shape (n, n), where n is the number\n   of rows in C_nM (typically number of states/orbitals).\n2. Diagonalize this Hermitian matrix to obtain eigenvalues ev and eigenvectors U (called S_overlapp in code).\n3. Build the inverse square-root in the eigenbasis: U (1/sqrt(ev)) U^†.\n4. Apply the resulting transformation to the input coefficients: C_new = (U (1/sqrt(ev)) U^†).T @ C_nM,\n   returning an array with the same shape as C_nM and with orthonormality C_new.conj() @ S_MM @ C_new.T = I.", "name": "gpaw_directmin_tools_loewdin_lcao", "parameters": {"properties": {"C_nM": {"type": "array", "items": {"type": "any"}, "description": "LCAO coefficient matrix with shape (n, M). Rows (index n) typically label states\nor molecular orbitals and columns (index M) label basis functions in the atomic-orbital basis used by GPAW.\nThese are the coefficients to be orthonormalized so that the transformed coefficients represent the same\nsubspace but become orthonormal with respect to S_MM.", "default": ""}, "S_MM": {"type": "array", "items": {"type": "any"}, "description": "Overlap matrix between basis functions with shape (M, M). This is the standard\noverlap matrix S used in LCAO representations in GPAW and must correspond to the basis in which C_nM is expressed.\nS_MM is expected to be Hermitian (S_MM.conj().T == S_MM) as typical overlap matrices in quantum-chemical/DFT\ncalculations are Hermitian.", "default": ""}}, "required": ["C_nM", "S_MM"], "type": "any"}}, "type": "function"}], "query": "I’m trying to debug an unstable GPAW LCAO direct-minimization run where orthonormality occasionally drifts. Start by applying the standard Loewdin (symmetric) orthonormalization to my 2×3 coefficient matrix using the provided 3×3 AO overlap matrix.\n\nBut wait: I realized the overlap matrix I pasted is the *raw* AO overlap from a sparse build step; in this pipeline we must first enforce exact Hermiticity by replacing it with its symmetrized form (i.e., average it with its conjugate-transpose) before doing any C†SC construction—otherwise tiny asymmetries can produce complex numerical noise in the eigendecomposition.\n\nSo: first symmetrize the overlap matrix as described, then compute C_new via Loewdin orthonormalization exactly as in GPAW’s direct-minimization tools (diagonalize C† S C, form the inverse square-root in the eigenbasis, and transform C accordingly). Return C_new.\n\nC_nM = [[0.80, 0.10, 0.30],\n        [0.20, 0.90, 0.10]]\n\nS_MM(raw) = [[1.00, 0.12, 0.05],\n             [0.12, 1.00, 0.08],\n             [0.05, 0.08, 1.00]]", "answers": "[{\"name\": \"gpaw_directmin_tools_loewdin_lcao\", \"arguments\": {\"C_nM\": [[0.8, 0.1, 0.3], [0.2, 0.9, 0.1]], \"S_MM\": [[1.0, 0.12, 0.05], [0.12, 1.0, 0.08], [0.05, 0.08, 1.0]]}}]"}
{"func_name": "gpaw_directmin_tools_minimum_cubic_interpol", "func_desc": "gpaw.directmin.tools.minimum_cubic_interpol computes the position of the minimum of a cubic polynomial\n    that interpolates function values and derivatives at the two endpoints of an interval [x_0, x_1].\n    \n    This function is used in GPAW's direct minimization and line-search utilities to suggest a scalar step length along a\n    search direction by fitting a cubic to the objective (for example, DFT total energy) and its directional derivatives\n    at the interval boundaries. The inputs x_0 and x_1 define the interval in the independent variable (e.g., a\n    line-search parameter or displacement along a geometry-optimization direction). The values f_0 and f_1 are the\n    function values at those endpoints (for example, energies in ASE/GPAW units) and df_0 and df_1 are the corresponding\n    first derivatives with respect to x (for example, directional derivatives of the energy). All inputs are floats and\n    the function returns a float x_min that is the location of the cubic interpolant's minimum constrained to the closed\n    interval [min(x_0, x_1), max(x_0, x_1)].\n    \n    The routine swaps endpoints internally if x_0 > x_1 so the order of the inputs does not matter. It constructs the\n    unique cubic f(x) = a x^3 + b x^2 + c x + d in the local coordinate measured from x_0 using the standard formulae\n    derived from the two function values and two derivatives. It then inspects the cubic's stationary points by forming\n    the discriminant D = b^2 - 3 a c. If D < 0 there are no real stationary points for the derivative inside the real\n    line and the function returns the endpoint (x_0 or x_1) with the smaller provided function value. If D >= 0 the\n    routine computes one candidate stationary point r0 = (-b + sqrt(D)) / (3 a) + x_0 (the code uses the + root) and\n    accepts r0 as the minimum if r0 lies strictly inside the open interval (x_0, x_1) and the interpolated cubic value at\n    r0 is smaller than both endpoint values; otherwise it returns the endpoint with smaller f. The cubic is evaluated\n    with a helper that expects the local coordinate (r0 - x_0). The function never modifies external state.", "tools": [{"function": {"description": "gpaw.directmin.tools.minimum_cubic_interpol computes the position of the minimum of a cubic polynomial\nthat interpolates function values and derivatives at the two endpoints of an interval [x_0, x_1].\n\nThis function is used in GPAW's direct minimization and line-search utilities to suggest a scalar step length along a\nsearch direction by fitting a cubic to the objective (for example, DFT total energy) and its directional derivatives\nat the interval boundaries. The inputs x_0 and x_1 define the interval in the independent variable (e.g., a\nline-search parameter or displacement along a geometry-optimization direction). The values f_0 and f_1 are the\nfunction values at those endpoints (for example, energies in ASE/GPAW units) and df_0 and df_1 are the corresponding\nfirst derivatives with respect to x (for example, directional derivatives of the energy). All inputs are floats and\nthe function returns a float x_min that is the location of the cubic interpolant's minimum constrained to the closed\ninterval [min(x_0, x_1), max(x_0, x_1)].\n\nThe routine swaps endpoints internally if x_0 > x_1 so the order of the inputs does not matter. It constructs the\nunique cubic f(x) = a x^3 + b x^2 + c x + d in the local coordinate measured from x_0 using the standard formulae\nderived from the two function values and two derivatives. It then inspects the cubic's stationary points by forming\nthe discriminant D = b^2 - 3 a c. If D < 0 there are no real stationary points for the derivative inside the real\nline and the function returns the endpoint (x_0 or x_1) with the smaller provided function value. If D >= 0 the\nroutine computes one candidate stationary point r0 = (-b + sqrt(D)) / (3 a) + x_0 (the code uses the + root) and\naccepts r0 as the minimum if r0 lies strictly inside the open interval (x_0, x_1) and the interpolated cubic value at\nr0 is smaller than both endpoint values; otherwise it returns the endpoint with smaller f. The cubic is evaluated\nwith a helper that expects the local coordinate (r0 - x_0). The function never modifies external state.", "name": "gpaw_directmin_tools_minimum_cubic_interpol", "parameters": {"properties": {"x_0": {"type": "float", "description": "Left endpoint of the interval in the independent variable. This defines one boundary of the interval\nused to build the cubic interpolant. If x_0 > x_1 the function will swap the two endpoints internally so the\ncomputation proceeds on [min(x_0, x_1), max(x_0, x_1)].", "default": ""}, "x_1": {"type": "float", "description": "Right endpoint of the interval in the independent variable. Together with x_0, it sets the interval\nover which the cubic is constructed and searched for a minimum. Units and meaning should match those of x_0\n(for example, a line-search displacement or step length).", "default": ""}, "f_0": {"type": "float", "description": "Function value at x_0 (for example, total energy at the left endpoint when performing a line search).\nThis value is used, together with f_1 and the derivatives, to determine the cubic coefficients and to compare\nendpoint vs interior values when selecting the minimizing point.", "default": ""}, "f_1": {"type": "float", "description": "Function value at x_1 (for example, total energy at the right endpoint). Used symmetrically with\nf_0 in the cubic construction and endpoint comparisons.", "default": ""}, "df_0": {"type": "float", "description": "First derivative of the function with respect to x evaluated at x_0 (for example, directional\nderivative or slope at the left endpoint). This derivative is used to determine the cubic coefficients and the\nlocation of stationary points.", "default": ""}, "df_1": {"type": "float", "description": "First derivative of the function with respect to x evaluated at x_1 (for example, directional\nderivative or slope at the right endpoint). This derivative is used together with df_0 to determine the cubic\ncoefficients and the discriminant that controls whether an interior stationary point exists.", "default": ""}}, "required": ["x_0", "x_1", "f_0", "f_1", "df_0", "df_1"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a finicky GPAW direct-minimization line search. I originally planned to bracket the step using the two trial points at 0.30 and 0.05 (the code reorders them anyway), and then take the cubic-interpolated minimum. But I just realized the derivative signs indicate the energy is decreasing at the smaller step and increasing at the larger step, so the true minimum should be inside the bracket and not at an endpoint unless the cubic fit fails (e.g., no real stationary point or the stationary point isn’t actually lower than both endpoints). Please use the cubic-interpolation routine (values + derivatives at the endpoints) to propose the step length constrained to the closed bracket and report the returned minimizer. Use the same sampled data: larger step has energy -154.238 with slope +0.42; smaller step has energy -154.191 with slope -0.15.", "answers": "[{\"name\":\"gpaw_directmin_tools_minimum_cubic_interpol\",\"arguments\":{\"x_0\":0.3,\"x_1\":0.05,\"f_0\":-154.238,\"f_1\":-154.191,\"df_0\":0.42,\"df_1\":-0.15}}]"}
{"func_name": "gpaw_directmin_tools_update_constraints_kpt", "func_desc": "gpaw.directmin.tools.update_constraints_kpt updates constraint indices for a single k-point to match a new indexation (for example, after sorting orbitals or reordering wavefunctions). This function is used in the GPAW direct minimization workflow to ensure that constraint references (which select orbitals or states at a given k-point) remain valid after the underlying index order has changed; failing to update these indices would cause constraints to refer to the wrong orbitals and lead to incorrect constrained minimization behavior.", "tools": [{"function": {"description": "gpaw.directmin.tools.update_constraints_kpt updates constraint indices for a single k-point to match a new indexation (for example, after sorting orbitals or reordering wavefunctions). This function is used in the GPAW direct minimization workflow to ensure that constraint references (which select orbitals or states at a given k-point) remain valid after the underlying index order has changed; failing to update these indices would cause constraints to refer to the wrong orbitals and lead to incorrect constrained minimization behavior.\n", "name": "gpaw_directmin_tools_update_constraints_kpt", "parameters": {"properties": {"constraints": {"type": "array", "items": {"type": "any"}, "description": "The list of constraints for one K-point. In practice this is a sequence (typically a list of lists) containing integer indices that reference orbitals or states before reindexing. The function treats each element constraints[i][k] as an index that must be mapped into the new ordering given by ind. The original constraints object is not modified; a deep copy is created and returned.", "default": ""}, "ind": {"type": "array", "items": {"type": "any"}, "description": "List containing the new indexation information: a sequence whose elements enumerate the old indices in the new order. Each entry of ind is expected to be equal to some element present in constraints; the function finds the new position of each old index by calling ind.index(old_index). This list defines how old indices map to new indices and is typically produced when orbitals or states are sorted or redistributed for a specific k-point in GPAW.", "default": ""}}, "required": ["constraints", "ind"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a constrained GPAW direct-minimization run at a single k-point after I “cleaned up” my band ordering. Initially I thought I could just keep the old constraint indices, but then I realized I’m now storing the permutation as a list where each entry tells me the *new position* of a given *old* band (i.e., it’s the inverse of the common “old indices listed in new order” convention). I still need my two constrained groups to target the exact same physical bands as before (the groups were [[0, 3], [2]] in the original indexing). The inverse-permutation I have on disk is [1, 3, 2, 0]. Use whatever mapping is consistent with that representation (i.e., convert to the forward mapping needed by the constraint updater) and update the constraint indices accordingly so they point to the same bands after reindexing.", "answers": "[{\"name\": \"gpaw_directmin_tools_update_constraints_kpt\", \"arguments\": {\"constraints\": [[0, 3], [2]], \"ind\": [3, 0, 2, 1]}}]"}
{"func_name": "gpaw_dos_linear_tetrahedron_dos", "func_desc": "Linear-tetrahedron density-of-states (DOS) evaluator used in GPAW DFT post-processing.\n    \n    This function computes the electronic density of states (DOS) on a supplied energy grid using the linear tetrahedron integration (LTI) method over a k-point grid defined by `size` and `cell`. It is intended for use with eigenvalue data produced by GPAW or compatible DFT calculations: `eig_kn` contains eigenvalues for k-points and bands, `weight_kn` contains optional k-point weights, and `energies` is the energy grid where the DOS is evaluated. The implementation reshapes flattened eigenvalue/weight arrays to match the k-point grid dimensions, optionally remaps data from a full Brillouin-zone ordering to an irreducible-zone ordering using `bz2ibz_map`, and then calls the underlying LTI routine `lti(cell, eig_kn, energies, weight_kn)` to produce the DOS. This function performs no I/O and returns the DOS array for further analysis (for example, plotting or integrating to obtain number of states).", "tools": [{"function": {"description": "Linear-tetrahedron density-of-states (DOS) evaluator used in GPAW DFT post-processing.\n\nThis function computes the electronic density of states (DOS) on a supplied energy grid using the linear tetrahedron integration (LTI) method over a k-point grid defined by `size` and `cell`. It is intended for use with eigenvalue data produced by GPAW or compatible DFT calculations: `eig_kn` contains eigenvalues for k-points and bands, `weight_kn` contains optional k-point weights, and `energies` is the energy grid where the DOS is evaluated. The implementation reshapes flattened eigenvalue/weight arrays to match the k-point grid dimensions, optionally remaps data from a full Brillouin-zone ordering to an irreducible-zone ordering using `bz2ibz_map`, and then calls the underlying LTI routine `lti(cell, eig_kn, energies, weight_kn)` to produce the DOS. This function performs no I/O and returns the DOS array for further analysis (for example, plotting or integrating to obtain number of states).", "name": "gpaw_dos_linear_tetrahedron_dos", "parameters": {"properties": {"eig_kn": {"type": "array", "items": {"type": "any"}, "description": "Flattened or already-shaped eigenvalues for all k-points and bands. In GPAW workflows this typically contains eigenvalues produced by a band-structure calculation. If `eig_kn` is a 1D array whose length equals the product of the `size` tuple multiplied by the number of bands, the function will reshape it to shape `size + (-1,)` where the trailing axis indexes bands. If `len(eig_kn) != numpy.prod(size)` the function expects a mapping array `bz2ibz_map` to remap eigenvalues from a full Brillouin-zone ordering to the ordering used by `size`; failing to provide a correct `bz2ibz_map` when required will raise an indexing error or TypeError. The function does not modify the input array in-place beyond local reshaping/viewing.", "default": ""}, "weight_kn": {"type": "array", "items": {"type": "any"}, "description": "Flattened or already-shaped k-point weights corresponding to `eig_kn`. These weights are used by the linear tetrahedron integration to account for k-point sampling of the Brillouin zone. If provided, `weight_kn` is reshaped in the same manner as `eig_kn` (to `size + (-1,)`) and is remapped with `bz2ibz_map` when remapping `eig_kn` is performed. `weight_kn` may be None (the code checks for `is not None`); in that case the LTI routine will be called with `weight_kn=None` and will use its internal handling of weights. If `weight_kn` is provided but its length is inconsistent with `eig_kn` and `size`, a ValueError or reshape/indexing error will occur.", "default": ""}, "energies": {"type": "array", "items": {"type": "any"}, "description": "1D array of energy values at which the DOS will be evaluated. The returned DOS array has the same logical correspondence to this energy grid (typically the same length as `energies`). The energy units should match those of `eig_kn` (for GPAW calculations this is normally electron-volt units when ASE/GPAW default units are used).", "default": ""}, "cell": {"type": "array", "items": {"type": "any"}, "description": "Real-space unit cell vectors or cell information used by the tetrahedron integration routine to construct tetrahedra in reciprocal space. In the GPAW context `cell` determines the k-point sampling geometry for the LTI method and must be consistent with the `size` k-point grid and the ordering of `eig_kn`.", "default": ""}, "size": {"type": "any", "description": "Tuple of integers describing the k-point grid dimensions (for example a grid produced by Monkhorst-Pack). This tuple is used to reshape `eig_kn` and `weight_kn` into an array with shape `size + (-1,)` where the last axis indexes bands. The product of the integers in `size` must equal the number of k-point entries present in `eig_kn` (or in the full Brillouin-zone ordering when used with `bz2ibz_map`).", "default": ""}, "bz2ibz_map": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional 1D integer index array mapping a full-Brillouin-zone ordering of k-points/eigenvalues to the ordering used in the irreducible Brillouin zone (IBZ) that matches `size`. If `len(eig_kn) != numpy.prod(size)` the function will attempt to remap `eig_kn` (and `weight_kn` if not None) via `eig_kn = eig_kn[bz2ibz_map]`. If a remapping is required but `bz2ibz_map` is None or has an incompatible length/type, an exception (TypeError, IndexError or ValueError) will be raised. When provided, `bz2ibz_map` must correctly reflect how the input eigenvalues/weights were ordered relative to the k-point grid implied by `size`.", "default": null}}, "required": ["eig_kn", "weight_kn", "energies", "cell", "size"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a graphene-like GPAW run and need the DOS via linear tetrahedron integration on a 2×2×1 k-point grid. Start from the same flattened eigenvalues for 4 k-points × 3 bands (in eV) as below, and an energy grid spanning −5 to 5 eV with 11 points. I was going to pass k-point weights as a per-k-point array, but then I realized the downstream wrapper I’m using stores weights flattened per (k,b) entry (same weight repeated across the 3 bands for each k-point). Please compute the DOS using that flattened-per-(k,b) weight representation instead, keeping the physical weighting identical to the per-k-point case.\n\nUse:\n- cell = [[2.46, 0.0, 0.0], [1.23, 2.13, 0.0], [0.0, 0.0, 20.0]]\n- size = [2, 2, 1]\n- eig_kn (flattened, 4 k-points × 3 bands) = [-3.2, -1.1, 0.8,  -3.0, -0.9, 1.0,  -3.1, -1.0, 0.9,  -3.05, -0.95, 1.05]\n- energies = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n- weights should be provided as the flattened-per-(k,b) form implied above (same value repeated across the 3 bands for each k-point), with equal k-point weights overall.", "answers": "[{\"name\":\"gpaw_dos_linear_tetrahedron_dos\",\"arguments\":{\"eig_kn\":[-3.2,-1.1,0.8,-3.0,-0.9,1.0,-3.1,-1.0,0.9,-3.05,-0.95,1.05],\"weight_kn\":[0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25],\"energies\":[-5.0,-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0],\"cell\":[[2.46,0.0,0.0],[1.23,2.13,0.0],[0.0,0.0,20.0]],\"size\":[2,2,1]}}]"}
{"func_name": "gpaw_elph_gpts_obtain_gpts_suggestion", "func_desc": "Compare plane-wave (PW) and localized-basis (LCAO) grid-point suggestions and return a conservative (tighter) grid-point setting suitable for use when two separate GPAW calculators must share the same real-space grid (for example in electron–phonon coupling workflows where PW and LCAO modes run separately but need identical gpts=(...) settings).\n    \n    This function is used in GPAW DFT workflows to compute and reconcile the number of real-space grid points (\"gpts\") required by a PW calculation with cutoff ecut and an LCAO calculation with target maximal grid spacing h, for a simulation cell described by cell_cv. It calls gpaw.elph.gpts.get_number_of_grid_points with PW(ecut) and LCAO() modes to obtain the per-axis integer grid-point counts required by the two modes, and then constructs a recommended grid Nopt_c that is (a) at least as large as the LCAO requirement and (b) the PW requirement rounded to the nearest multiple of 4 (implemented as (Npw_c/4 + 0.5).astype(int) * 4), ensuring compatibility with GPAW internals that prefer multiples of 4 for plane-wave related grids. The result is intended to be used as the gpts=... argument supplied to calculators so that separate PW and LCAO runs use identical real-space grids.", "tools": [{"function": {"description": "Compare plane-wave (PW) and localized-basis (LCAO) grid-point suggestions and return a conservative (tighter) grid-point setting suitable for use when two separate GPAW calculators must share the same real-space grid (for example in electron–phonon coupling workflows where PW and LCAO modes run separately but need identical gpts=(...) settings).\n\nThis function is used in GPAW DFT workflows to compute and reconcile the number of real-space grid points (\"gpts\") required by a PW calculation with cutoff ecut and an LCAO calculation with target maximal grid spacing h, for a simulation cell described by cell_cv. It calls gpaw.elph.gpts.get_number_of_grid_points with PW(ecut) and LCAO() modes to obtain the per-axis integer grid-point counts required by the two modes, and then constructs a recommended grid Nopt_c that is (a) at least as large as the LCAO requirement and (b) the PW requirement rounded to the nearest multiple of 4 (implemented as (Npw_c/4 + 0.5).astype(int) * 4), ensuring compatibility with GPAW internals that prefer multiples of 4 for plane-wave related grids. The result is intended to be used as the gpts=... argument supplied to calculators so that separate PW and LCAO runs use identical real-space grids.", "name": "gpaw_elph_gpts_obtain_gpts_suggestion", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "any"}, "description": "Real-space cell vectors for the simulation cell used by GPAW. This should be the same array that would be passed to GPAW/ASE atoms objects (typically shape (3, 3) for a periodic cell). The array is used to compute the axiswise number of grid points for each mode; invalid shapes or non-numeric contents will cause the underlying get_number_of_grid_points call to raise an exception which is propagated to the caller.", "default": ""}, "ecut": {"type": "float", "description": "Plane-wave kinetic-energy cutoff (in the same energy units expected by GPAW) used when constructing the PW-mode suggestion. This value is passed to PW(ecut) and therefore may be an integer or a float as accepted by GPAW PW mode; non-positive or otherwise invalid values will cause get_number_of_grid_points or PW(...) to raise an exception that is propagated.", "default": ""}, "h": {"type": "float", "description": "Intended maximal real-space grid spacing (in the length units used by GPAW, e.g., Å) for LCAO mode. This target spacing is passed to get_number_of_grid_points with mode=LCAO() to compute the LCAO-required integer grid points per axis. Non-positive or invalid h will cause the underlying computation to raise an error.", "default": ""}, "print_suggestions": {"type": "boolean", "description": "If True, the function prints human-readable guidance to standard output describing the computed PW and LCAO gpts and the recommended gpts for use in electron–phonon (elph) workflows. The printed text mirrors the messages produced in GPAW usage, including explicit formatted arrays for PW(ecut) -> gpts=..., LCAO, h=... -> gpts=..., and Recommended for elph: gpts=.... Default is False, in which case no printing occurs and the function is side-effect free except for returning the recommendation.", "default": false}}, "required": ["cell_cv", "ecut", "h", "print_suggestions"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up an electron–phonon coupling workflow in GPAW where a PW ground-state run and an LCAO displacement run must share *exactly* the same real-space grid (gpts). Use the same slightly strained orthorhombic cell as before: cell vectors [[5.43, 0.0, 0.0], [0.0, 5.47, 0.0], [0.0, 0.0, 10.86]].\n\nInitially I was going to keep the PW cutoff at the earlier value and set the LCAO max grid spacing to 0.18 Å, but I just realized our pseudopotential set includes a very “hard” species and the PW run is the one that tends to dominate the grid requirement. So: keep the LCAO spacing at 0.18 Å, but for the PW cutoff use the *next* value on the typical high-accuracy sequence (…400, 500, 600, 700, …) that is one step stricter than 600 eV.\n\nPlease:\n1) print the intermediate per-axis grid suggestions coming from the PW(ecut) and from the LCAO(h),\n2) then return a conservative shared gpts that is at least as tight as the LCAO requirement and also respects the PW-side “multiple of 4” preference.\n\nThe result should be suitable to feed as gpts=(...) into both calculators so that the grids match bit-for-bit.", "answers": "[{\"name\": \"gpaw_elph_gpts_obtain_gpts_suggestion\", \"arguments\": {\"cell_cv\": [[5.43, 0.0, 0.0], [0.0, 5.47, 0.0], [0.0, 0.0, 10.86]], \"ecut\": 700.0, \"h\": 0.18, \"print_suggestions\": true}}]"}
{"func_name": "gpaw_ffbt_ffbt", "func_desc": "gpaw.ffbt.ffbt: Fast Fourier–Bessel transform of a radial real-space function f(r) used in GPAW density-functional calculations. This routine computes the radial transform g(k) = k^{l+1} ∫_0^{rc} r dr j_l(kr) r f(r) by expanding the spherical Bessel function j_l(x) into a finite polynomial series and evaluating the resulting radial integrals using discrete Fast Fourier Transforms (FFTs). It is intended for transforming radial parts of quantities (for example radial components of wavefunctions, densities, or potentials in spherical coordinates) from a uniform real-space radial grid r_g to a uniform radial reciprocal-space grid k_q; the implementation performs l+1 FFTs on a 2Q-length grid and returns only the Q positive-frequency values corresponding to k_q.", "tools": [{"function": {"description": "gpaw.ffbt.ffbt: Fast Fourier–Bessel transform of a radial real-space function f(r) used in GPAW density-functional calculations. This routine computes the radial transform g(k) = k^{l+1} ∫_0^{rc} r dr j_l(kr) r f(r) by expanding the spherical Bessel function j_l(x) into a finite polynomial series and evaluating the resulting radial integrals using discrete Fast Fourier Transforms (FFTs). It is intended for transforming radial parts of quantities (for example radial components of wavefunctions, densities, or potentials in spherical coordinates) from a uniform real-space radial grid r_g to a uniform radial reciprocal-space grid k_q; the implementation performs l+1 FFTs on a 2Q-length grid and returns only the Q positive-frequency values corresponding to k_q.\n", "name": "gpaw_ffbt_ffbt", "parameters": {"properties": {"l": {"type": "integer", "description": "The spherical harmonic order l >= 0 that determines which spherical Bessel function j_l(kr) appears in the integral. In GPAW/PAW context, l selects the angular momentum channel for which the radial transform is computed. The algorithm sums contributions for m = 0..l using precomputed expansion coefficients c_lm; if c_lm does not contain the requested l or m an IndexError may be raised.", "default": ""}, "f_g": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of sampled values f(r) on the real-space radial grid r_g. The array length N must match len(r_g). The code assumes f(r)=0 for r >= rc (i.e., for grid indices g >= N the integrand is zero); numpy.fft.fft zero-padding is relied upon for g >= N when the FFT length 2*Q exceeds N.", "default": ""}, "r_g": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of radial grid point positions r_g[g] for g = 0..N-1. The function assumes a uniform real-space grid starting at r_g[0] == 0.0 and with constant spacing dr = r_g[1] - r_g[0]. When the module-level debug flag is True, the function asserts r_g[0] == 0.0 and that all spacings r_g[i+1] - r_g[i] equal dr, raising AssertionError if violated. The transform interprets r_g as r = g * dr for integer g.", "default": ""}, "k_q": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of target radial reciprocal-space points k(q) for q = 0..Q-1. The function expects k_q[0] == 0.0 and a uniform spacing dk = π / (Q * dr) when debug is enabled; it also assumes Q = len(k_q) >= len(r_g) so that the reciprocal-space grid length Q is at least the number of real-space samples. The function returns transform values at exactly these k_q points (the Q positive-frequency points). If the debug flag is True and the k_q grid does not satisfy the asserted spacing or ordering, an AssertionError will be raised.", "default": ""}}, "required": ["l", "f_g", "r_g", "k_q"], "type": "any"}}, "type": "function"}], "query": "I’m trying to validate a PAW setup in GPAW, but the radial pseudo-wavefunction data I got from the generator is slightly messy. Initially I was going to just run the s-channel (l=0) transform for f(r)=exp(-r) on the full grid r=0.0..7.5 bohr with dr=0.5 and Q=16, using dk=pi/(Q*dr). However, I just realized the last sample at the cutoff radius corresponds to a duplicated endpoint (the generator writes both r=rc and r=rc+dr in some cases, and the downstream FFT-based FFBT assumes a strictly non-duplicated uniform grid for the 2Q construction). So: keep the same dr, keep l=0, keep Q unchanged, and keep the reciprocal grid definition dk=pi/(Q*dr) starting at k=0, but drop exactly one real-space point so that the real-space grid has exactly Q samples with no duplicated endpoint. Use f(r)=exp(-r) evaluated on that corrected grid. Please compute the Fast Fourier–Bessel transform using gpaw.ffbt.ffbt and return g(k) on the corresponding k_q grid (length Q).", "answers": "[{\"name\": \"gpaw_ffbt_ffbt\", \"arguments\": {\"l\": 0, \"f_g\": [1.0, 0.6065306597, 0.3678794412, 0.2231301601, 0.1353352832, 0.08208499862, 0.04978706837, 0.03019738342, 0.01831563889, 0.01110899654, 0.006737947, 0.004086771438, 0.002478752177, 0.001503439193, 0.0009118819656, 0.0005530843701], \"r_g\": [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5], \"k_q\": [0.0, 0.3926990817, 0.7853981634, 1.178097245, 1.570796327, 1.963495408, 2.35619449, 2.748893572, 3.141592654, 3.534291735, 3.926990817, 4.319689899, 4.71238898, 5.105088062, 5.497787144, 5.890486225]}}]"}
{"func_name": "gpaw_ffbt_spherical_bessel", "func_desc": "gpaw.ffbt.spherical_bessel computes the spherical Bessel function j_l(x) for a given integer order l and a NumPy array of non-negative radial arguments x_g. This implementation is used in the gpaw.ffbt module for radial/plane-wave integrals and expansions that appear in GPAW density-functional-theory calculations (for example, in radial parts of PAW/projector expansions and Fourier–Bessel transforms). The routine evaluates j_l(x) using the finite expansion\n    j_l(x) = x^{-(l+1)} sum_{m=0}^l Re{ c_lm[l][m] * x^m * exp(-i x) }\n    based on the coefficients c_lm (see Marco Vanin, 2008). The function returns a real-valued NumPy array with the same shape as x_g containing elementwise values of j_l(x).", "tools": [{"function": {"description": "gpaw.ffbt.spherical_bessel computes the spherical Bessel function j_l(x) for a given integer order l and a NumPy array of non-negative radial arguments x_g. This implementation is used in the gpaw.ffbt module for radial/plane-wave integrals and expansions that appear in GPAW density-functional-theory calculations (for example, in radial parts of PAW/projector expansions and Fourier–Bessel transforms). The routine evaluates j_l(x) using the finite expansion\nj_l(x) = x^{-(l+1)} sum_{m=0}^l Re{ c_lm[l][m] * x^m * exp(-i x) }\nbased on the coefficients c_lm (see Marco Vanin, 2008). The function returns a real-valued NumPy array with the same shape as x_g containing elementwise values of j_l(x).", "name": "gpaw_ffbt_spherical_bessel", "parameters": {"properties": {"l": {"type": "integer", "description": "The spherical order l of the Bessel function j_l. This must be a non-negative integer corresponding to the angular-momentum quantum number used in radial expansions in GPAW. Negative values are not supported and will typically produce incorrect results or runtime errors (for example, an empty coefficient sum or IndexError). The value of l determines the number of terms (m = 0..l) used in the finite expansion and the behaviour at x = 0 (j_0(0) = 1, j_l(0) = 0 for l > 0).", "default": ""}, "x_g": {"type": "array", "items": {"type": "any"}, "description": "A NumPy array of non-negative floating-point radial arguments x where j_l(x) is to be evaluated. The array's dtype must be float (the code asserts x_g.dtype == float) and all elements must satisfy x >= 0 (the code asserts numpy.all(x_g >= 0)). The function preserves the shape of x_g and returns an array of the same shape. Values with x < 1e-10 are treated as x = 0 for numerical stability: j_0(0) is set to 1.0 and j_l(0) is set to 0.0 for l > 0.", "default": ""}}, "required": ["l", "x_g"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW Fourier–Bessel transform regression where the radial grid got exported with a duplicated zero and one clearly non-physical artifact: the FFBT radial argument array must be non-negative, but the dump contains one slightly negative value from a rounding bug. Start by computing j_l(x) for the same angular momentum order as the baseline test. However, to avoid the known instability at exactly x=0 for this finite expansion form, don’t evaluate the Bessel function at any points that are exactly zero; instead, drop all exact zeros and also drop any values that violate the non-negativity requirement, then evaluate on the remaining points in their original order.\n\nUse this raw grid (in this exact order): x_raw = [0.0, 0.1, -1e-12, 0.5, 1.0, 0.0, 2.5, 5.0].", "answers": "[{\"name\":\"gpaw_ffbt_spherical_bessel\",\"arguments\":{\"l\":2,\"x_g\":[0.1,0.5,1.0,2.5,5.0]}}]"}
{"func_name": "gpaw_gaunt_gaunt", "func_desc": "Gaunt coefficients for products of three spherical harmonics used in GPAW's\n    angular integrals and PAW projector coupling.", "tools": [{"function": {"description": "Gaunt coefficients for products of three spherical harmonics used in GPAW's\nangular integrals and PAW projector coupling.", "name": "gpaw_gaunt_gaunt", "parameters": {"properties": {"lmax": {"type": "integer", "description": "Highest single-particle angular momentum quantum number l\nconsidered when constructing Gaunt coefficients. lmax defines the\nsizes of the returned 3D array by Lmax = (lmax + 1)**2 and\nL2max = (2 * lmax + 1)**2, which correspond to the number of\ncombined (l,m) basis functions for the single- and double-range\nspherical-harmonic indices used in GPAW. Typical use in GPAW: lmax\n= 2 (d-like functions) which is the default. The value must be an\ninteger (the function is annotated with int) and is intended to be\nnon-negative; negative or non-integer values are not supported and\nwill lead to incorrect sizes or runtime errors (for example a\nTypeError from range() or an incorrect array shape). Increasing lmax\nincreases both CPU time and memory usage significantly because the\nfunction constructs a dense 3D array with dimensions determined by\nlmax.", "default": 2}}, "required": ["lmax"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a GPAW PAW angular-integral regression test for a mixed basis that includes the usual s/p/d/f channels, so I initially thought I should generate the Gaunt coefficient tensor using the cutoff corresponding to f-like spherical harmonics. But I just realized the downstream coupling code allocates using the *highest* angular momentum actually present in the projector set, and in this particular test the f-channel is only a placeholder and must be excluded to match the reference file (otherwise the tensor shape disagrees and the test fails). Compute the Gaunt coefficients array with the cutoff equal to one less than the f-like limit, and return it in the format compatible with GPAW’s angular integrals.", "answers": "[{\"name\": \"gpaw_gaunt_gaunt\", \"arguments\": {\"lmax\": 2}}]"}
{"func_name": "gpaw_gaunt_super_gaunt", "func_desc": "super_gaunt computes the contracted (product) Gaunt coefficients needed to express the product of three spherical harmonics as a single spherical-harmonic expansion. In the GPAW/PAW context (density-functional theory, projector-augmented wave method), these coefficients are used when combining angular parts of atomic-like functions (spherical harmonics) that appear in integrals and projections; the function returns the 4-dimensional tensor G_L_L1_L2_L3 defined by the contraction over an intermediate angular-momentum index L' of two Gaunt tensors.", "tools": [{"function": {"description": "super_gaunt computes the contracted (product) Gaunt coefficients needed to express the product of three spherical harmonics as a single spherical-harmonic expansion. In the GPAW/PAW context (density-functional theory, projector-augmented wave method), these coefficients are used when combining angular parts of atomic-like functions (spherical harmonics) that appear in integrals and projections; the function returns the 4-dimensional tensor G_L_L1_L2_L3 defined by the contraction over an intermediate angular-momentum index L' of two Gaunt tensors.\n", "name": "gpaw_gaunt_super_gaunt", "parameters": {"properties": {"lmax": {"type": "integer", "description": "Maximum single-particle angular momentum l used to build the Gaunt coefficients. This is the same lmax used elsewhere in GPAW when expanding atomic orbitals or angular parts of basis functions; the default value is 2 (i.e., l = 0..2). The function will compute gaunt(lmax) and gaunt(2*lmax) internally, so the computational cost and memory footprint grow with lmax. If an invalid lmax (for example a negative value) is provided, the underlying gaunt(...) call or NumPy operations will raise an exception propagated to the caller.", "default": 2}}, "required": ["lmax"], "type": "any"}}, "type": "function"}], "query": "I’m validating a PAW projector pipeline in GPAW where the angular basis should include d-character, so my first instinct was to build the contracted super-Gaunt tensor only up to that cutoff. But I just realized the *product of three* spherical harmonics can generate angular components beyond the single-orbital cutoff, and my sanity check is specifically that the contraction respects all allowed couplings without silently truncating them. So compute the contracted super-Gaunt coefficient tensor using the smallest lmax that is still sufficient to represent *all* couplings arising from triple products when each individual factor is limited to d orbitals (i.e., don’t pick the d cutoff itself—pick the minimal higher cutoff implied by the selection rules). I’ll use the resulting 4D tensor to expand Y_{l1m1}Y_{l2m2}Y_{l3m3} in a single spherical-harmonic expansion and compare against numerical quadrature.", "answers": "[{\"name\": \"gpaw_gaunt_super_gaunt\", \"arguments\": {\"lmax\": 6}}]"}
{"func_name": "gpaw_gpu_as_xp", "func_desc": "Transfer an array between CPU (NumPy) and GPU (CuPy) backends for use in GPAW's numerical kernels.\n    \n    This function is used in the GPAW DFT code to ensure that numerical data structures (for example real-space grid arrays that represent wavefunctions, electron density, potentials, or basis coefficients used by plane-wave, atom-centered or real-space multigrid methods) reside on the requested device before being passed to device-specific computational kernels. The routine will either return the array unchanged (if already on the requested backend) or perform an explicit device/host transfer using CuPy conversion helpers. Transfers allocate memory on the destination device and copy data between host and device, which may be expensive; CuPy must be available for GPU transfers.", "tools": [{"function": {"description": "Transfer an array between CPU (NumPy) and GPU (CuPy) backends for use in GPAW's numerical kernels.\n\nThis function is used in the GPAW DFT code to ensure that numerical data structures (for example real-space grid arrays that represent wavefunctions, electron density, potentials, or basis coefficients used by plane-wave, atom-centered or real-space multigrid methods) reside on the requested device before being passed to device-specific computational kernels. The routine will either return the array unchanged (if already on the requested backend) or perform an explicit device/host transfer using CuPy conversion helpers. Transfers allocate memory on the destination device and copy data between host and device, which may be expensive; CuPy must be available for GPU transfers.", "name": "gpaw_gpu_as_xp", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "any"}, "description": "Input array holding numerical data used by GPAW (examples: wavefunction grids, densities, potentials). This argument may be a NumPy ndarray (resident on the CPU) or a CuPy ndarray (resident on the GPU). The function inspects this object to decide whether a transfer is necessary. If it must move data to NumPy it uses cupy.asnumpy(array); if it must move data to CuPy it uses cupy.asarray(array). The returned array is a view or new allocation depending on the conversion routine used.", "default": ""}, "xp": {"type": "array", "items": {"type": "any"}, "description": "The target array API/module indicating the desired device/backend. Pass the numpy module (typically imported as np) to request a CPU-side NumPy ndarray, or pass the cupy module to request a GPU-side CuPy ndarray. The function compares xp by identity with numpy (xp is np) to decide the target backend.", "default": ""}}, "required": ["array", "xp"], "type": "any"}}, "type": "function"}], "query": "I’m staging a tiny GPAW real-space electron-density grid for a GPU-only kernel, but I just realized this 3×3 slice might already have been produced by a prior CUDA step depending on the run mode. Take the density slice [[0.12, 0.15, 0.11], [0.10, 0.18, 0.14], [0.09, 0.13, 0.16]] and ensure it ends up on the same backend as the *next* kernel: the backend that lives on the accelerator (i.e., not the host NumPy one). If it’s already there, don’t copy; otherwise perform the necessary host→device transfer so GPAW can pass it to the GPU numerical kernel.", "answers": "[{\"name\": \"gpaw_gpu_as_xp\", \"arguments\": {\"array\": [[0.12, 0.15, 0.11], [0.1, 0.18, 0.14], [0.09, 0.13, 0.16]], \"xp\": \"cupy\"}}]"}
{"func_name": "gpaw_hubbard_aoom", "func_desc": "gpaw.hubbard.aoom returns the atomic orbital occupation matrix (aoom) for a specified angular momentum quantum number l and a companion scaling matrix used in GPAW's DFT+U Hubbard correction. The function selects the block(s) of the full partial-wave density matrix D_ii that correspond to the specified l, combines these block(s) according to radial inner products of bounded and unbounded partial waves, and returns the resulting orbital occupation submatrix N_mm together with a dHU_ii matrix that contains the scaling factors placed in the corresponding sub-block positions of the full matrix. In GPAW and DFT+U calculations this aoom (N_mm) is used to compute orbital occupations for the Hubbard energy and to build the Hubbard potential, while dHU_ii encodes the same radial scaling factors in the full partial-wave index space for later use when assembling the DFT+U correction.", "tools": [{"function": {"description": "gpaw.hubbard.aoom returns the atomic orbital occupation matrix (aoom) for a specified angular momentum quantum number l and a companion scaling matrix used in GPAW's DFT+U Hubbard correction. The function selects the block(s) of the full partial-wave density matrix D_ii that correspond to the specified l, combines these block(s) according to radial inner products of bounded and unbounded partial waves, and returns the resulting orbital occupation submatrix N_mm together with a dHU_ii matrix that contains the scaling factors placed in the corresponding sub-block positions of the full matrix. In GPAW and DFT+U calculations this aoom (N_mm) is used to compute orbital occupations for the Hubbard energy and to build the Hubbard potential, while dHU_ii encodes the same radial scaling factors in the full partial-wave index space for later use when assembling the DFT+U correction.\n", "name": "gpaw_hubbard_aoom", "parameters": {"properties": {"D_ii": {"type": "array", "items": {"type": "any"}, "description": "Full partial-wave density matrix for an atom, provided as a 2D square NumPy array. This matrix contains the density matrix blocks for all partial waves (all j indices) in the ordering defined by l_j; the function slices D_ii to extract one or two sub-blocks corresponding to the requested angular momentum l. The caller must ensure that D_ii has a block structure consistent with l_j (i.e., its dimension equals the sum over j of 2*l_j + 1). The function does not modify D_ii in place.", "default": ""}, "l": {"type": "integer", "description": "Target angular momentum quantum number (e.g., 0 for s, 1 for p, 2 for d). The function uses l to find which partial-wave j indices in l_j correspond to this angular momentum and to determine the size nm = 2*l + 1 of the returned atomic-orbital occupation matrix N_mm (for l=2, nm=5).", "default": ""}, "l_j": {"type": "array", "items": {"type": "integer"}, "description": "List of integer angular momentum quantum numbers for each partial wave j in the atom (one entry per partial wave). This list defines the block partitioning of D_ii: each entry contributes a block size of 2*l_j[j] + 1. The function locates indices nl = where(l_j == l) to identify one or two blocks with the requested l; the implementation supports exactly one or two matching partial-wave blocks (len(nl) == 1 or 2).", "default": ""}, "n_j": {"type": "array", "items": {"type": "integer"}, "description": "List of radial (principal) quantum numbers or markers for each partial wave j. In GPAW's partial-wave setup, a value of -1 is used to mark an unbound (scattering-like) partial wave. The function uses n_j to decide whether a given matching partial wave is unbound (n_j == -1) and thereby selects the appropriate normalization/scaling formula for the radial inner products used in forming the occupation matrix.", "default": ""}, "N0_q": {"type": "array", "items": {"type": "any"}, "description": "1D NumPy array containing precomputed radial inner products between pairs of partial waves (q-indexed radial overlaps). Specific q indices computed inside the function correspond to: q1 = bounded-bounded inner product, q2 = unbounded-unbounded inner product, and q12 = bounded-unbounded inner product. These values are used to scale contributions from each sub-block of D_ii; when scale is True they are combined into relative normalization factors, otherwise they are used directly. The caller must ensure N0_q contains the required q entries for the partial-wave pairs present in l_j; insufficient length or incorrect ordering will raise IndexError.", "default": ""}, "scale": {"type": "boolean", "description": "If True (default), scale the radial inner products so that the bounded partial-wave inner product is treated as unity (N1 = 1) and the other inner products are normalized relative to it. This reflects the convention in GPAW's Hubbard implementation that bounded partial waves are taken as the normalization reference when constructing the DFT+U occupation. If False, the raw inner products from N0_q are used directly as scaling factors. Choosing scale=True is the standard behavior for assembling the occupation matrix for Hubbard corrections; scale=False can be used to apply raw overlap values instead.", "default": true}}, "required": ["D_ii", "l", "l_j", "n_j", "N0_q", "scale"], "type": "any"}}, "type": "function"}], "query": "I’m re-running a GPAW DFT+U sanity check on a late transition-metal atom and want to extract the atomic-orbital occupation matrix for the Hubbard-active manifold from the full partial-wave density matrix D_ii. Initially I was going to target the d manifold using the obvious angular-momentum value, but I realized the PAW setup I’m using has two channels with the same angular momentum (one bound and one unbound), so I need the aoom for the manifold whose degeneracy is five and whose channel entries appear twice in l_j (i.e., the repeated nonzero-l block), not the unique one. \n\nUse the same channel descriptors and reference occupancies as below, and keep radial scaling enabled (I need the companion full-index scaling matrix returned too):\n- l_j = [0, 2, 2]\n- n_j = [2, 3, -1]\n- N0_q = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75]\n- scale enabled\n\nD_ii is the same 11×11 matrix:\n[[0.98,0.01,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00],\n [0.01,0.40,0.02,0.00,0.00,0.00,0.01,0.00,0.00,0.00,0.00],\n [0.00,0.02,0.38,0.01,0.00,0.00,0.00,0.01,0.00,0.00,0.00],\n [0.00,0.00,0.01,0.36,0.02,0.00,0.00,0.00,0.01,0.00,0.00],\n [0.00,0.00,0.00,0.02,0.34,0.01,0.00,0.00,0.00,0.01,0.00],\n [0.00,0.00,0.00,0.00,0.01,0.32,0.00,0.00,0.00,0.00,0.01],\n [0.00,0.01,0.00,0.00,0.00,0.00,0.28,0.02,0.00,0.00,0.00],\n [0.00,0.00,0.01,0.00,0.00,0.00,0.02,0.26,0.01,0.00,0.00],\n [0.00,0.00,0.00,0.01,0.00,0.00,0.00,0.01,0.24,0.02,0.00],\n [0.00,0.00,0.00,0.00,0.01,0.00,0.00,0.00,0.02,0.22,0.01],\n [0.00,0.00,0.00,0.00,0.00,0.01,0.00,0.00,0.00,0.01,0.20]]\n\nCompute and return (1) the aoom N_mm for that five-fold-degenerate manifold and (2) the corresponding full-index dHU_ii scaling matrix with factors placed into the appropriate sub-blocks.", "answers": "[{\"name\": \"gpaw_hubbard_aoom\", \"arguments\": {\"D_ii\": [[0.98, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.4, 0.02, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0], [0.0, 0.02, 0.38, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.36, 0.02, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0], [0.0, 0.0, 0.0, 0.02, 0.34, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0], [0.0, 0.0, 0.0, 0.0, 0.01, 0.32, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.28, 0.02, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.02, 0.26, 0.01, 0.0, 0.0], [0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.24, 0.02, 0.0], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.02, 0.22, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.2]], \"l\": 2, \"l_j\": [0, 2, 2], \"n_j\": [2, 3, -1], \"N0_q\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75], \"scale\": true}}]"}
{"func_name": "gpaw_hybrids_paw_python_pawexxvv", "func_desc": "gpaw.hybrids.paw.python_pawexxvv: Compute the PAW (projector-augmented wave) correction matrix for valence-valence exact-exchange (EXX) energy used in GPAW hybrid functional routines. This function builds a valence-space potential-like matrix V_ii by contracting a packed-pair interaction matrix M_pp with a valence density matrix D_ii using the packed_index(i, j, ni) mapping that is used elsewhere in the gpaw.hybrids.paw implementation.", "tools": [{"function": {"description": "gpaw.hybrids.paw.python_pawexxvv: Compute the PAW (projector-augmented wave) correction matrix for valence-valence exact-exchange (EXX) energy used in GPAW hybrid functional routines. This function builds a valence-space potential-like matrix V_ii by contracting a packed-pair interaction matrix M_pp with a valence density matrix D_ii using the packed_index(i, j, ni) mapping that is used elsewhere in the gpaw.hybrids.paw implementation.\n", "name": "gpaw_hybrids_paw_python_pawexxvv", "parameters": {"properties": {"M_pp": {"type": "array", "items": {"type": "any"}, "description": "Two-dimensional numpy.ndarray containing PAW pairwise interaction elements arranged using the packed-index ordering produced by packed_index(i, j, ni). In the GPAW PAW hybrid context, M_pp encodes corrections between pairs of projector indices (packed pairs) required to form the PAW correction to the EXX operator. This array is accessed with indices p13 = packed_index(i1, i3, ni) and p24 = packed_index(i2, i4, ni) inside the routine; therefore its first and second dimensions must be at least as large as the maximum packed_index value for the given ni = len(D_ii). Supplying an array with incompatible dimensions will typically raise an IndexError at runtime. The function does not modify M_pp.", "default": ""}, "D_ii": {"type": "array", "items": {"type": "any"}, "description": "Square two-dimensional numpy.ndarray of shape (ni, ni) representing the valence-valence density matrix (or a related occupied-state tensor) in the atomic/projector index basis used by GPAW. The integer ni is inferred as len(D_ii), and the routine assumes D_ii is square. D_ii provides the coefficients D_ii[i3, i4] that are contracted with M_pp to produce the PAW correction V_ii. Passing a non-square or improperly shaped D_ii will raise a ValueError or lead to IndexError during execution.", "default": ""}}, "required": ["M_pp", "D_ii"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW hybrid-functional PAW EXX test where I *thought* I had three valence projector functions, but I just realized the third channel is a ghost/disabled projector from an old dataset and must be excluded from the valence space before building the EXX PAW correction.\n\nSo: treat the valence space as having one fewer projector than initially assumed, and correspondingly (i) truncate the density matrix to the active-valence block and (ii) take the packed-pair interaction matrix sub-block that is consistent with the packed_index(i,j,ni) ordering for that reduced ni (i.e., keep only the pair indices involving the active projectors).\n\nUsing the original matrices below, compute the resulting PAW valence-valence EXX correction matrix V_ii with gpaw.hybrids.paw.python_pawexxvv.\n\nOriginal M_pp (given for the larger valence space):\n[[1.0, 0.1, 0.2, 0.0, 0.05, -0.1],\n [0.1, 0.9, 0.15, 0.02, 0.0, 0.03],\n [0.2, 0.15, 0.8, -0.04, 0.01, 0.0],\n [0.0, 0.02, -0.04, 0.7, 0.06, 0.08],\n [0.05, 0.0, 0.01, 0.06, 0.6, 0.12],\n [-0.1, 0.03, 0.0, 0.08, 0.12, 0.5]]\n\nOriginal D_ii:\n[[1.0, 0.2, 0.0],\n [0.2, 0.8, 0.1],\n [0.0, 0.1, 0.6]]", "answers": "[{\"name\":\"gpaw_hybrids_paw_python_pawexxvv\",\"arguments\":{\"M_pp\":[[1.0,0.1,0.0],[0.1,0.9,0.02],[0.0,0.02,0.7]],\"D_ii\":[[1.0,0.2],[0.2,0.8]]}}]"}
{"func_name": "gpaw_hyperfine_delta", "func_desc": "Extended delta function used in hyperfine-related real-space operations in GPAW.\n    \n    This function evaluates an analytic, smoothly decaying \"extended delta\" function\n    defined by the expression 2 / rT / (1 + 2 * r / rT)**2. In the GPAW DFT/hyperfine\n    context this function is used to model a localized contact-like distribution\n    (centered at r = 0) with a finite range controlled by the parameter rT; it is\n    suitable for use on real-space radial grids (numpy arrays) when constructing or\n    analyzing hyperfine coupling terms and other localized operators. The amplitude\n    at r = 0 is 2/rT and the large-r asymptotic decay is proportional to 1/r^2,\n    so rT controls both the spatial extent (range) and the peak magnitude of the\n    distribution. In practical GPAW/ASE workflows the radial distances r are\n    typically expressed in the same length units used elsewhere in the calculation\n    (e.g., Å when interacting with ASE structures and outputs).", "tools": [{"function": {"description": "Extended delta function used in hyperfine-related real-space operations in GPAW.\n\nThis function evaluates an analytic, smoothly decaying \"extended delta\" function\ndefined by the expression 2 / rT / (1 + 2 * r / rT)**2. In the GPAW DFT/hyperfine\ncontext this function is used to model a localized contact-like distribution\n(centered at r = 0) with a finite range controlled by the parameter rT; it is\nsuitable for use on real-space radial grids (numpy arrays) when constructing or\nanalyzing hyperfine coupling terms and other localized operators. The amplitude\nat r = 0 is 2/rT and the large-r asymptotic decay is proportional to 1/r^2,\nso rT controls both the spatial extent (range) and the peak magnitude of the\ndistribution. In practical GPAW/ASE workflows the radial distances r are\ntypically expressed in the same length units used elsewhere in the calculation\n(e.g., Å when interacting with ASE structures and outputs).", "name": "gpaw_hyperfine_delta", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional numpy.ndarray of radial coordinates\n() at which to evaluate the extended delta function. Each entry\nrepresents a distance from the origin on a real-space radial grid and\nshould be non-negative for a physical radial distance; negative values\nare mathematically accepted by the formula but have no physical\nsignificance in standard radial-grid usage. The output is computed\nelementwise and the returned array preserves the shape of r. In GPAW\nworkflows these values are typically in the same units as atomic\npositions (for example, Å when using ASE conventions).", "default": ""}, "rT": {"type": "float", "description": "Positive floating-point parameter that sets the effective\nrange/width of the extended delta distribution and scales its\namplitude. Physically, rT acts like a smoothing/cutoff radius: smaller\nrT yields a narrower, higher peak (value at r = 0 equals 2/rT), while\nlarger rT produces a broader, lower-amplitude distribution. The\nfunction performs arithmetic division by rT, so rT must be non-zero.", "default": ""}}, "required": ["r", "rT"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW hyperfine post-processing sanity check where I need the analytic extended-delta distribution sampled on a radial grid. Start by evaluating it on r = 0.0 Å to 2.0 Å in 0.25 Å increments (so the same grid as usual). I was going to keep the nominal smoothing/range parameter the same as last time, but I just realized our downstream comparison normalizes everything to the *peak value at r = 0* rather than to rT directly. So instead, choose the rT that makes the peak at r = 0 exactly 3.0 (in the same inverse-length units implied by Å for r), and then evaluate the function on that same radial grid so I can inspect the localization under this peak-normalized constraint.", "answers": "[{\"name\": \"gpaw_hyperfine_delta\", \"arguments\": {\"r\": [0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0], \"rT\": 0.6666666666666666}}]"}
{"func_name": "gpaw_kpt_refine_get_reduced_monkhorst", "func_desc": "Find a Monkhorst-Pack k-point grid for a refined reciprocal-space grid and shrink it to the reciprocal-space volume of a single original k-point cell.\n    \n    This function is used in k-point refinement and folding workflows within GPAW (a DFT code using plane-waves/real-space grids) to generate the Monkhorst-Pack sampling for a refined grid specified by size and then scale (shrink) those k-point coordinates so they lie inside the volume corresponding to one original coarse k-point. It calls monkhorst_pack(size) to produce the refined-grid k-point coordinates and returns those coordinates divided by N_c so that the refined mesh is represented within a single original k-point cell. This is useful when mapping refined k-point sampling onto the Brillouin-zone volume of a coarse k-point for tasks such as supercell unfolding, k-point folding, or constructing reduced k-point sets for symmetry/reduction.", "tools": [{"function": {"description": "Find a Monkhorst-Pack k-point grid for a refined reciprocal-space grid and shrink it to the reciprocal-space volume of a single original k-point cell.\n\nThis function is used in k-point refinement and folding workflows within GPAW (a DFT code using plane-waves/real-space grids) to generate the Monkhorst-Pack sampling for a refined grid specified by size and then scale (shrink) those k-point coordinates so they lie inside the volume corresponding to one original coarse k-point. It calls monkhorst_pack(size) to produce the refined-grid k-point coordinates and returns those coordinates divided by N_c so that the refined mesh is represented within a single original k-point cell. This is useful when mapping refined k-point sampling onto the Brillouin-zone volume of a coarse k-point for tasks such as supercell unfolding, k-point folding, or constructing reduced k-point sets for symmetry/reduction.", "name": "gpaw_kpt_refine_get_reduced_monkhorst", "parameters": {"properties": {"size": {"type": "any", "description": "The integer tuple specifying the dimensions of the refined Monkhorst-Pack grid. Each element gives the number of subdivisions along a reciprocal-lattice direction for the refined grid; the tuple is passed directly to monkhorst_pack(size) to generate the refined k-point coordinates. In practical GPAW/DFT usage, size determines the density and anisotropy of the refined k-point sampling in reciprocal space.", "default": ""}, "N_c": {"type": "integer", "description": "An integer scaling factor equal to the number of refined-grid cells that correspond to one original (coarse) k-point cell. This function divides the refined-grid coordinates by N_c to map them into the volume of a single original k-point. In practice N_c is used to perform the folding/shrinking operation when transforming a finer k-point mesh to the reference domain of a coarser mesh.", "default": ""}}, "required": ["size", "N_c"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW k-point folding/unfolding test where I start from a coarse Monkhorst–Pack mesh that is 4×4×4, and I *thought* I should refine it by doubling each reciprocal direction. But I just realized the downstream mapping code assumes the refined sampling is expressed inside the reciprocal-space volume of exactly one coarse k-point cell, i.e., the refined coordinates must be scaled by the same integer that relates the refined mesh density to the coarse mesh density along each axis. So: generate the Monkhorst–Pack k-point coordinates for the refined mesh implied by that relationship, and then shrink those coordinates into the volume of a single original coarse k-point cell using the appropriate scaling integer (the one you’d infer from the coarse→refined refinement relationship).", "answers": "[{\"name\": \"gpaw_kpt_refine_get_reduced_monkhorst\", \"arguments\": {\"size\": [8, 8, 8], \"N_c\": 2}}]"}
{"func_name": "gpaw_lcao_local_orbitals_get_plane_dirs", "func_desc": "Get normal and in-plane directions for a plane identifier used in GPAW's local-orbital and plane-related utilities.", "tools": [{"function": {"description": "Get normal and in-plane directions for a plane identifier used in GPAW's local-orbital and plane-related utilities.\n", "name": "gpaw_lcao_local_orbitals_get_plane_dirs", "parameters": {"properties": {"plane": {"type": "string", "description": "Pair of characters identifying the plane in Cartesian axes using letters 'x', 'y', 'z' (for example, 'xy' identifies the plane spanned by the x and y axes). In the GPAW DFT context (see README), this string is used by routines that need a compact user-facing way to specify a geometric plane for operations such as slicing densities, orienting local orbitals, or defining slab/plane orientations for analysis and visualization. The function sorts the characters and maps 'x'->0, 'y'->1, 'z'->2 to integer axis indices; valid characters are only 'x', 'y', and 'z'. The argument is interpreted literally; duplicates (e.g. 'xx') or strings of unexpected length are not validated beyond what the implementation performs and therefore produce results consistent with the mapping and subsequent set-difference logic described below.", "default": ""}}, "required": ["plane"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW post-processing script that slices the electron density on a plane specified by a two-letter identifier. I initially set it up for the plane whose normal is along x (i.e., the one spanned by the other two Cartesian axes), but then realized the downstream plotting code expects the plane identifier to be the *other* two-axis plane: the one that still contains x, and is orthogonal to the remaining axis. Please get the normal-direction index and the two in-plane direction indices for that corrected plane identifier using GPAW’s plane-direction utility.", "answers": "[{\"name\": \"gpaw_lcao_local_orbitals_get_plane_dirs\", \"arguments\": {\"plane\": \"xy\"}}]"}
{"func_name": "gpaw_lcaotddft_qed_calculate_first_derivative", "func_desc": "Calculate the first time derivative of time-dependent data using a first-order forward finite difference.\n    \n    This function is used in the LCAO-TDDFT QED parts of GPAW to obtain time derivatives of time-dependent quantities (for example dipole moments, densities, or density-matrix elements) during real-time propagation or analysis of time-series output. It computes the forward first-order finite-difference approximation (coefficients [-1, 1]) so that the discrete derivative at time t is approximated by (f(t+dt) - f(t)) / dt. To preserve the original array shape along the time axis, the implementation repeats the last-time sample as required by the underlying forward_finite_difference routine; this ensures the returned array has the same shape as data_tx and can be used directly in subsequent GPAW time-stepping or post-processing code.", "tools": [{"function": {"description": "Calculate the first time derivative of time-dependent data using a first-order forward finite difference.\n\nThis function is used in the LCAO-TDDFT QED parts of GPAW to obtain time derivatives of time-dependent quantities (for example dipole moments, densities, or density-matrix elements) during real-time propagation or analysis of time-series output. It computes the forward first-order finite-difference approximation (coefficients [-1, 1]) so that the discrete derivative at time t is approximated by (f(t+dt) - f(t)) / dt. To preserve the original array shape along the time axis, the implementation repeats the last-time sample as required by the underlying forward_finite_difference routine; this ensures the returned array has the same shape as data_tx and can be used directly in subsequent GPAW time-stepping or post-processing code.", "name": "gpaw_lcaotddft_qed_calculate_first_derivative", "parameters": {"properties": {"timestep": {"type": "float", "description": "Time step dt used for the finite-difference quotient. This value is the spacing between successive samples along the time axis (first axis) of data_tx and is used as the denominator in the finite-difference formula. If timestep is zero, the division will produce infinities or NaNs (or raise, depending on NumPy error handling). If timestep is None, this function only accepts the special case documented below: when data_tx contains exactly one time point (len(data_tx) == 1), the function returns a zero array without performing division. If timestep is None and data_tx contains more than one time point, a TypeError or other failure will occur when attempting the division.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "any"}, "description": "Array of time-dependent data with the time axis as the first axis (axis 0). The remaining axes represent spatial, orbital, or other internal degrees of freedom used by GPAW LCAO-TDDFT/QED routines. The function treats consecutive entries along axis 0 as values at successive times separated by timestep. The input is not modified in place; a new array of the same shape is produced.", "default": ""}}, "required": ["timestep", "data_tx"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a real-time LCAO-TDDFT QED trajectory from GPAW where the 3D dipole moment (x,y,z) was written at 5 successive frames. I first assumed the nominal propagation step was the time spacing, but I just noticed the time column in the log is not perfectly uniform (the third interval is slightly larger), so using the stated dt would bias the derivative. Treat the sampling as effectively uniform by choosing a single timestep equal to the average spacing between the first and last recorded times over the number of intervals, then compute dμ/dt with a first-order forward finite difference (and keep the returned array the same (time,3) shape by the usual last-sample repetition).\n\nDipole samples (time, μx, μy, μz):\n[[0.00, 0.00, 0.10, -0.20],\n [0.02, 0.02, 0.12, -0.18],\n [0.05, 0.05, 0.15, -0.15],\n [0.09, 0.09, 0.20, -0.10],\n [0.14, 0.14, 0.26, -0.04]]\n\nUse only the dipole components as data_tx (shape (5,3)).", "answers": "[{\"name\":\"gpaw_lcaotddft_qed_calculate_first_derivative\",\"arguments\":{\"timestep\":0.035,\"data_tx\":[[0.0,0.1,-0.2],[0.02,0.12,-0.18],[0.05,0.15,-0.15],[0.09,0.2,-0.1],[0.14,0.26,-0.04]]}}]"}
{"func_name": "gpaw_lcaotddft_qed_calculate_third_derivative", "func_desc": "gpaw.lcaotddft.qed.calculate_third_derivative calculates the third time derivative of time-dependent data using a forward finite-difference stencil and returns an array with the same shape as the input data_tx. This function is used in the GPAW LCAO TDDFT QED context to obtain the third temporal derivative of time-series quantities (axis 0 is time), where timestep is the uniform time increment used during time propagation.", "tools": [{"function": {"description": "gpaw.lcaotddft.qed.calculate_third_derivative calculates the third time derivative of time-dependent data using a forward finite-difference stencil and returns an array with the same shape as the input data_tx. This function is used in the GPAW LCAO TDDFT QED context to obtain the third temporal derivative of time-series quantities (axis 0 is time), where timestep is the uniform time increment used during time propagation.\n", "name": "gpaw_lcaotddft_qed_calculate_third_derivative", "parameters": {"properties": {"timestep": {"type": "float", "description": "Time step used to scale the finite-difference result (Δt). In normal use this must be a non-zero float equal to the sampling interval of the time axis of data_tx. The implementation also accepts None only for the special-case when data_tx contains a single time sample (len(data_tx) == 1); in that case the function returns an array of zeros with the same shape as data_tx without performing a division. If timestep is zero (and not the single-sample None special-case) a division by zero will occur and a ZeroDivisionError or FloatingPointError may be raised by the underlying arithmetic.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "any"}, "description": "Array of time-dependent data where the first axis (index 0) is the time axis and remaining axes are spatial, orbital, or other degrees of freedom used by GPAW LCAO TDDFT QED. The array may have any shape provided the time axis length is at least 1. To preserve the output shape, the algorithm repeats the data corresponding to the last time step as needed when applying the forward finite-difference stencil.", "default": ""}}, "required": ["timestep", "data_tx"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a GPAW LCAO TDDFT–QED debug trajectory where I logged the dipole components (x,y) over 6 *nominally* uniform steps. The propagator printed the timestep as 20 as (so I initially assumed it’s already in femtoseconds), and the recorded array is:\n\n[[0.00, 0.10],\n [0.03, 0.12],\n [0.08, 0.15],\n [0.15, 0.19],\n [0.24, 0.24],\n [0.35, 0.30]]\n\nCompute the third time derivative (jerk) for each component using the same forward finite-difference stencil as `gpaw.lcaotddft.qed.calculate_third_derivative`, and return an array with the same shape as the input.\n\nWait—unit sanity check: the TDDFT log is in atomic units and the timestep line is in attoseconds; if I pass the raw value as femtoseconds, the jerk will be off by a constant scale factor. So don’t use the naive timestep; instead, use the timestep expressed in femtoseconds that corresponds to 20 attoseconds (i.e., convert to the correct fs value) when calling the derivative routine.", "answers": "[{\"name\":\"gpaw_lcaotddft_qed_calculate_third_derivative\",\"arguments\":{\"timestep\":0.02,\"data_tx\":[[0.0,0.1],[0.03,0.12],[0.08,0.15],[0.15,0.19],[0.24,0.24],[0.35,0.3]]}}]"}
{"func_name": "gpaw_lcaotddft_qed_forward_finite_difference", "func_desc": "gpaw.lcaotddft.qed.forward_finite_difference computes a forward finite-difference linear combination along the first axis (time axis) of a time-dependent data array using an explicit integer coefficient stencil.", "tools": [{"function": {"description": "gpaw.lcaotddft.qed.forward_finite_difference computes a forward finite-difference linear combination along the first axis (time axis) of a time-dependent data array using an explicit integer coefficient stencil.\n", "name": "gpaw_lcaotddft_qed_forward_finite_difference", "parameters": {"properties": {"coefficients": {"type": "array", "items": {"type": "integer"}, "description": "Ordered list of integer finite-difference coefficients that define the forward-difference stencil. Each entry in this list is multiplied with a shifted slice of the input data along the time axis and summed to produce the output at every time index. The coefficients are typically obtained from finite-difference coefficient tables (for example, see finite-difference coefficient references such as https://en.wikipedia.org/wiki/Finite_difference_coefficient). This list must be non-empty and its length determines the forward stencil width; providing an empty list or an invalid width will result in an error when constructing the padded array.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "any"}, "description": "Time-dependent data array whose first axis is the time axis (length N). The remaining axes may represent spatial coordinates, channels, or other degrees of freedom used in LCAO-TDDFT/QED calculations in GPAW. The function treats data_tx as read-only and does not modify it in place. The array's dtype is preserved in the returned array. The function requires that data_tx has at least one time sample (N >= 1); passing an empty array along the first axis will raise an error during padding.", "default": ""}}, "required": ["coefficients", "data_tx"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a toy LCAO-TDDFT/QED trajectory where the observable is a 2-component time series (dipole x/y) sampled at 6 time steps:\n\n[[0.10, -0.05],\n [0.14, -0.02],\n [0.21,  0.01],\n [0.29,  0.05],\n [0.40,  0.09],\n [0.52,  0.14]]\n\nI was going to apply the simple 3-point forward finite-difference stencil with integer coefficients [-3, 4, -1] along the time axis. Wait—those coefficients assume the stencil is normalized by the step-size factor (so it matches the common forward-difference convention), and I need the *integer-only* linear-combination form that this pipeline expects.\n\nSo: take the coefficient stencil that is the smallest-integer multiple of [-3, 4, -1] that makes the sum of absolute coefficients equal to 16 (this is how we force deterministic integer scaling before later normalization), and apply that along the first axis using gpaw.lcaotddft.qed.forward_finite_difference to compute the linear-combination result at each valid time index.", "answers": "[{\"name\": \"gpaw_lcaotddft_qed_forward_finite_difference\", \"arguments\": {\"coefficients\": [-6, 8, -2], \"data_tx\": [[0.1, -0.05], [0.14, -0.02], [0.21, 0.01], [0.29, 0.05], [0.4, 0.09], [0.52, 0.14]]}}]"}
{"func_name": "gpaw_matrix_suggest_blocking", "func_desc": "Suggest blocking of an NxN matrix for distributed linear algebra in GPAW.\n    \n    This function is used inside GPAW (a DFT code that supports parallel calculations via MPI and ScaLAPACK) to choose a processor grid and a block size for distributing an NxN dense matrix across ncpus MPI ranks. The returned values (nprow, npcol, blocksize) are intended for use with ScaLAPACK-style block-cyclic distribution: nprow and npcol define the 2D process grid (rows and columns of MPI processes) and blocksize is the linear block dimension used to partition the matrix. The heuristic aims to make the processor grid as square as possible (minimizing imbalance between nprow and npcol), to guarantee at least one whole block per process dimension and at least two blocks in total, and to choose a power-of-two block size (for performance and compatibility), capped at 64.", "tools": [{"function": {"description": "Suggest blocking of an NxN matrix for distributed linear algebra in GPAW.\n\nThis function is used inside GPAW (a DFT code that supports parallel calculations via MPI and ScaLAPACK) to choose a processor grid and a block size for distributing an NxN dense matrix across ncpus MPI ranks. The returned values (nprow, npcol, blocksize) are intended for use with ScaLAPACK-style block-cyclic distribution: nprow and npcol define the 2D process grid (rows and columns of MPI processes) and blocksize is the linear block dimension used to partition the matrix. The heuristic aims to make the processor grid as square as possible (minimizing imbalance between nprow and npcol), to guarantee at least one whole block per process dimension and at least two blocks in total, and to choose a power-of-two block size (for performance and compatibility), capped at 64.", "name": "gpaw_matrix_suggest_blocking", "parameters": {"properties": {"N": {"type": "integer", "description": "The matrix dimension (number of rows and columns of the square matrix). In the GPAW/DFT context this is the global size of a distributed NxN matrix (for example a Hamiltonian or overlap matrix) that will be distributed across processes. The value is used to determine a block size that yields whole blocks and multiple blocks per distributed dimension.", "default": ""}, "ncpus": {"type": "integer", "description": "The total number of MPI processes available for the distributed calculation. The function attempts to factor ncpus into a 2D grid (nprow, npcol) with npcol chosen as a divisor of ncpus so that nprow * npcol == ncpus; ncpus therefore determines how the global matrix will be partitioned across processes.", "default": ""}}, "required": ["N", "ncpus"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW run where the dense Hamiltonian is 4096×4096 and I initially planned to distribute it over 64 MPI ranks with ScaLAPACK.\n\nWait—our job launcher reserves a handful of ranks for I/O and domain-decomposition tasks, and only the remainder should participate in the ScaLAPACK 2D block-cyclic grid. Use the same total rank count as above, but exclude exactly the number of reserved ranks equal to the number of unique prime factors of the total rank count.\n\nGiven that effective ScaLAPACK rank count, suggest the (nprow, npcol, blocksize) using the usual GPAW heuristic: make the grid as square as possible, ensure at least one whole block per process dimension and at least two blocks total, and pick a power-of-two block size capped at 64.", "answers": "[{\"name\": \"gpaw_matrix_suggest_blocking\", \"arguments\": {\"N\": 4096, \"ncpus\": 62}}]"}
{"func_name": "gpaw_new_symmetry_find_lattice_symmetry", "func_desc": "gpaw.new.symmetry.find_lattice_symmetry determines the set of lattice symmetry operations (3x3 integer matrices with elements in {-1, 0, 1}) that conserve the metric of a given unit cell and that do not swap axes with different periodic boundary conditions. This function is used in GPAW's symmetry handling to identify candidate point-group operations that map the cell basis vectors onto each other while preserving inter-vector lengths and respecting periodicity, which is important for exploiting symmetry in DFT calculations (for example reducing k-point sampling or identifying equivalent atoms).", "tools": [{"function": {"description": "gpaw.new.symmetry.find_lattice_symmetry determines the set of lattice symmetry operations (3x3 integer matrices with elements in {-1, 0, 1}) that conserve the metric of a given unit cell and that do not swap axes with different periodic boundary conditions. This function is used in GPAW's symmetry handling to identify candidate point-group operations that map the cell basis vectors onto each other while preserving inter-vector lengths and respecting periodicity, which is important for exploiting symmetry in DFT calculations (for example reducing k-point sampling or identifying equivalent atoms).\n", "name": "gpaw_new_symmetry_find_lattice_symmetry", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "any"}, "description": "3x3 array of Cartesian cell vectors that define the unit cell. Each row (or column, depending on calling convention) represents one basis vector in Cartesian coordinates; the function computes metric_cc = cell_cv.dot(cell_cv.T) and therefore expects a square 3x3 numeric array. The metric (inner products of cell vectors) is the quantity that must be conserved by symmetry operations. Supplying an array with incompatible shape or non-numeric entries will lead to NumPy broadcasting/indexing errors.", "default": ""}, "pbc_c": {"type": "array", "items": {"type": "any"}, "description": "1-D boolean array of length 3 indicating periodic boundary conditions along each Cartesian axis (True if periodic along that axis, False if non-periodic). The function uses pbc_c to build a 3x3 boolean matrix of axis-pair differences (via logical_xor.outer) and then forbids any symmetry operation that would swap two axes for which pbc differs, ensuring that non-periodic directions are not exchanged with periodic ones.", "default": ""}, "tol": {"type": "float", "description": "Numeric tolerance controlling how strictly the function requires conservation of the cell metric. The function computes metric_scc for each candidate integer matrix and compares the deviation abs(metric_scc - metric_cc).sum(2).sum(1) to a threshold. If _backwards_compatible is True the threshold is tol; otherwise the threshold is tol**2. Choose tol consistent with the units of metric_cc (squared length units) and the expected numerical noise; extremely small tol may yield an empty operation set, while extremely large tol may admit spurious operations.", "default": ""}, "_backwards_compatible": {"type": "boolean", "description": "If True, use legacy comparison behavior where the summed absolute difference of metric matrices is compared to tol. If False (the default), compare the summed absolute difference to tol**2. This flag exists to preserve older code behavior when exact numeric interpretation of tol differs between versions; it does not otherwise change how candidate matrices are generated or how pbc constraints are applied.", "default": false}}, "required": ["cell_cv", "pbc_c", "tol"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a GPAW DFT calculation for what should be a tetragonal (001) slab: in-plane vectors are equal and orthogonal, and there’s a long out-of-plane vacuum axis that is explicitly non-periodic. The cell I got from a CIF-to-ASE conversion is numerically a bit messy (tiny off-diagonal components on the order of 1e-8–1e-9 Å), but physically it’s the same as the clean diagonal cell with ~3.905 Å in x and y and ~20 Å in z. Initially I was going to use an extremely tight symmetry tolerance, but then I realized that would incorrectly reject valid symmetry operations because of those conversion-level float artifacts. Please compute the lattice symmetry operations with gpaw.new.symmetry.find_lattice_symmetry for this slab cell and PBC flags [periodic in the two in-plane directions, non-periodic along the vacuum direction], using a tolerance that is just loose enough to treat those tiny off-diagonal artifacts as numerical noise while still being far stricter than typical geometry tolerances.\n\nUse this cell (Å):\n[[3.905,  2.0e-9, 0.0],\n [1.0e-9, 3.905, 0.0],\n [0.0,    0.0,   20.0]]", "answers": "[{\"name\": \"gpaw_new_symmetry_find_lattice_symmetry\", \"arguments\": {\"cell_cv\": [[3.905, 2e-09, 0.0], [1e-09, 3.905, 0.0], [0.0, 0.0, 20.0]], \"pbc_c\": [true, true, false], \"tol\": 1e-08}}]"}
{"func_name": "gpaw_new_symmetry_mat", "func_desc": "gpaw.new.symmetry.mat converts a 3x3 matrix (typically a rotation matrix used in GPAW's symmetry utilities) into a compact, human-readable string representation. This function is intended for producing deterministic textual forms of small integer or numeric matrices for logging, comparison, or inclusion in text output generated by GPAW symmetry code paths.", "tools": [{"function": {"description": "gpaw.new.symmetry.mat converts a 3x3 matrix (typically a rotation matrix used in GPAW's symmetry utilities) into a compact, human-readable string representation. This function is intended for producing deterministic textual forms of small integer or numeric matrices for logging, comparison, or inclusion in text output generated by GPAW symmetry code paths.\n", "name": "gpaw_new_symmetry_mat", "parameters": {"properties": {"rot_cc": {"type": "array", "items": {"type": "any"}, "description": "A list-like object of row iterables that represents a 3x3 matrix (for example, a rotation matrix). Each element of rot_cc is expected to be an iterable of three numeric values (ints or floats) corresponding to one row of the matrix. The function does not perform strict shape validation: it iterates over rot_cc and each contained iterable, formatting each element with the format specifier f'{r:2}'. Practically, callers should pass a list of three lists (or tuples) each of length three; passing other shapes will produce a string reflecting the provided shape but may not be meaningful in the symmetry context. Elements must be formatable with the format specification '2'; otherwise Python's formatting will raise a TypeError.", "default": ""}}, "required": ["rot_cc"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW symmetry-deduplication issue where my log keys are generated with `gpaw.new.symmetry.mat`, and I need the compact, deterministic string for the *actual* rotation we’re applying.\n\nInitially I was going to log the 90° rotation about the z-axis that maps x→y and y→−x. But I just realized the symmetry operator in this code path is defined in the **passive / coordinate-transform** convention (i.e., it should be the inverse/transpose of the active rotation I wrote down), otherwise equivalent operations won’t compare equal across modules.\n\nSo: take the 3×3 integer rotation matrix for that 90° z-rotation I described, replace it with the matrix that represents the **same geometric rotation under the passive convention** (the one you get by inverting the original orthogonal rotation), and then convert *that* matrix into the human-readable form used by `gpaw.new.symmetry.mat`.", "answers": "[{\"name\":\"gpaw_new_symmetry_mat\",\"arguments\":{\"rot_cc\":[[0,1,0],[-1,0,0],[0,0,1]]}}]"}
{"func_name": "gpaw_new_symmetry_safe_id", "func_desc": "Convert a sequence of per-atom magnetic moments into stable integer identifiers used for grouping atoms by magnetic moment in symmetry-related processing within GPAW.\n    \n    This function is used in gpaw.new.symmetry to mitigate small rounding differences in magnetic moments produced by density-functional theory (DFT) calculations. It assigns an integer identifier to each magnetic moment in magmom_av such that the first occurrence of a distinct magnetic moment value defines a new identifier and any later magnetic moments within the symmetric tolerance range of that first occurrence are assigned the same identifier. The comparison uses the Euclidean norm of the difference between two moment values (so for scalar moments this is the absolute difference). This produces reproducible, compact integer ids that can be used to group atoms in symmetry detection and other analyses where per-atom magnetic moments should be considered equivalent up to a small numerical tolerance. Example: safe_id([1.01, 0.99, 0.5], tolerance=0.025) -> [0, 0, 2].", "tools": [{"function": {"description": "Convert a sequence of per-atom magnetic moments into stable integer identifiers used for grouping atoms by magnetic moment in symmetry-related processing within GPAW.\n\nThis function is used in gpaw.new.symmetry to mitigate small rounding differences in magnetic moments produced by density-functional theory (DFT) calculations. It assigns an integer identifier to each magnetic moment in magmom_av such that the first occurrence of a distinct magnetic moment value defines a new identifier and any later magnetic moments within the symmetric tolerance range of that first occurrence are assigned the same identifier. The comparison uses the Euclidean norm of the difference between two moment values (so for scalar moments this is the absolute difference). This produces reproducible, compact integer ids that can be used to group atoms in symmetry detection and other analyses where per-atom magnetic moments should be considered equivalent up to a small numerical tolerance. Example: safe_id([1.01, 0.99, 0.5], tolerance=0.025) -> [0, 0, 2].", "name": "gpaw_new_symmetry_safe_id", "parameters": {"properties": {"magmom_av": {"type": "array", "items": {"type": "any"}, "description": "A list of per-atom magnetic moments in the order of atoms. Each entry is expected to be a numeric magnetic-moment value (typically a float as produced by GPAW/DFT calculations). The length of this list determines the number of identifiers returned. The function does not modify this list.", "default": ""}, "tolerance": {"type": "float", "description": "Numerical tolerance used to decide when two magnetic moments are considered identical. Two moments x and y are treated as the same if numpy.linalg.norm(x - y) < tolerance. Default is 0.001 (1e-3), chosen to tolerate small rounding errors common in DFT magnetic-moment outputs; increasing this value will merge more distinct moment values into the same identifier, while decreasing it will treat near-equal values as distinct.", "default": 0.001}}, "required": ["magmom_av", "tolerance"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a messy GPAW post-processing pipeline for symmetry detection where the per-atom magnetic moments come from two SCF restarts and have tiny numerical jitter. Initially I was going to use an ultra-tight tolerance (the same order as the raw printout precision), but then I realized that would split atoms that are clearly equivalent because the SCF noise is bigger than that. So instead, set the tolerance to the smallest value that still merges each of the following intended “nearly-equal” pairs without accidentally merging any of the other distinct groups: (1.000 with 0.995), (0.120 with 0.118), (-0.500 with -0.503), and (0.000 with 0.002). Using that tolerance, generate stable integer IDs (first-seen defines the ID; later moments within tolerance of that first-seen representative share its ID) for these 8 per-atom moments in order: [1.000, 0.995, 0.120, 0.118, -0.500, -0.503, 0.000, 0.002].", "answers": "[{\"name\": \"gpaw_new_symmetry_safe_id\", \"arguments\": {\"magmom_av\": [1.0, 0.995, 0.12, 0.118, -0.5, -0.503, 0.0, 0.002], \"tolerance\": 0.005}}]"}
{"func_name": "gpaw_occupations_fermi_dirac", "func_desc": "Compute Fermi–Dirac occupations, their energy derivatives, and per-state electronic entropy contributions used for electronic smearing in GPAW.\n    \n    This routine implements the Fermi–Dirac distribution used in GPAW for converting single-particle eigenvalues (energies) into occupation numbers for finite electronic temperature (smearing). It is typically used in self-consistent-field DFT calculations to obtain fractional occupations for metallic or thermally-broadened systems, and to evaluate the entropic contribution to the electronic free energy. The implementation is numerically stabilized by clipping the reduced energy argument to the range [-100, 100] to avoid overflow in the exponential. The function is pure (no side effects) and returns three numpy arrays corresponding to the occupation, its derivative with respect to eigenvalue, and a per-state entropy-related energy term.", "tools": [{"function": {"description": "Compute Fermi–Dirac occupations, their energy derivatives, and per-state electronic entropy contributions used for electronic smearing in GPAW.\n\nThis routine implements the Fermi–Dirac distribution used in GPAW for converting single-particle eigenvalues (energies) into occupation numbers for finite electronic temperature (smearing). It is typically used in self-consistent-field DFT calculations to obtain fractional occupations for metallic or thermally-broadened systems, and to evaluate the entropic contribution to the electronic free energy. The implementation is numerically stabilized by clipping the reduced energy argument to the range [-100, 100] to avoid overflow in the exponential. The function is pure (no side effects) and returns three numpy arrays corresponding to the occupation, its derivative with respect to eigenvalue, and a per-state entropy-related energy term.", "name": "gpaw_occupations_fermi_dirac", "parameters": {"properties": {"eig": {"type": "array", "items": {"type": "any"}, "description": "Array of single-particle eigenvalues (energies) for which occupations are required. In GPAW this is the set of Kohn–Sham eigenvalues from which electronic occupations are computed. The values must be expressed in the same energy units as fermi_level and width; the function performs vectorized arithmetic on this array and returns arrays of the same shape.", "default": ""}, "fermi_level": {"type": "float", "description": "The Fermi level (chemical potential) in the same energy units as eig and width. This parameter shifts the center of the Fermi–Dirac distribution and determines which states are occupied at the given smearing width; in GPAW it typically comes from the current estimate of the electronic chemical potential.", "default": ""}, "width": {"type": "float", "description": "Smearing width (thermal broadening) in the same energy units as eig and fermi_level. Practically this acts like kB*T (electronic temperature) and controls the smoothness of the occupation around the Fermi level. Width must not be zero (division by zero will occur). Physically meaningful values are non-negative; very small positive widths approach a step-function occupation, while larger widths produce broader fractional occupations. The implementation clips the reduced argument to [-100, 100] to maintain numerical stability for extreme values.", "default": ""}}, "required": ["eig", "fermi_level", "width"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a GPAW metallic SCF run and need the Fermi–Dirac occupations, their energy derivatives, and per-state electronic entropy terms for a small set of Kohn–Sham eigenvalues (in eV): [-1.20, -0.35, 0.00, 0.08, 0.55, 1.40]. Initially I was going to use the Fermi level straight from the log, but I just realized that value is referenced to an inconsistent zero because the eigenvalues here are aligned to the potential shift used in the DOS step. So instead, shift the Fermi level by the same offset that maps the eigenvalue at 0.00 eV to sit exactly midway between the two closest eigenvalues that straddle it (use the given list only). Also, I was going to keep the smearing width at 0.10 eV, but we’re seeing charge sloshing unless the smearing is slightly stronger; increase the width by 25% relative to that nominal value. Then compute and return (as numpy-like arrays) the occupations f, the derivatives df/de, and the per-state entropy contributions, using GPAW’s stabilized Fermi–Dirac implementation (including the reduced-argument clipping).", "answers": "[{\"name\":\"gpaw_occupations_fermi_dirac\",\"arguments\":{\"eig\":[-1.2,-0.35,0.0,0.08,0.55,1.4],\"fermi_level\":0.04,\"width\":0.125}}]"}
{"func_name": "gpaw_occupations_marzari_vanderbilt", "func_desc": "Marzari-Vanderbilt cold-smearing occupation function used in GPAW to compute smeared electronic occupations, their energy derivatives, and per-state entropy energy corrections. This implements the Marzari-Vanderbilt \"cold smearing\" distribution (see DOI 10.1103/PhysRevLett.82.3296) and is intended for use in electronic-structure (DFT) calculations within GPAW to stabilize convergence for metallic or partially occupied systems by smoothing the occupation step at the Fermi level.\n    \n    This function evaluates the Marzari–Vanderbilt analytic expressions in a vectorized NumPy form. Given an array of single-particle eigenvalues and a chemical potential (fermi_level), it returns: (1) occupation numbers between 0 and 1 that replace the zero-temperature step function; (2) the derivative of these occupations with respect to the eigenvalue, useful for response or density-derivative calculations; and (3) a per-state energy-like entropy correction term that is used when computing the smeared free-energy correction to the total energy in DFT workflows. All returned arrays have the same shape as the input eig array. The algorithm uses width as the smearing parameter and performs elementwise operations (no in-place modification of inputs).", "tools": [{"function": {"description": "Marzari-Vanderbilt cold-smearing occupation function used in GPAW to compute smeared electronic occupations, their energy derivatives, and per-state entropy energy corrections. This implements the Marzari-Vanderbilt \"cold smearing\" distribution (see DOI 10.1103/PhysRevLett.82.3296) and is intended for use in electronic-structure (DFT) calculations within GPAW to stabilize convergence for metallic or partially occupied systems by smoothing the occupation step at the Fermi level.\n\nThis function evaluates the Marzari–Vanderbilt analytic expressions in a vectorized NumPy form. Given an array of single-particle eigenvalues and a chemical potential (fermi_level), it returns: (1) occupation numbers between 0 and 1 that replace the zero-temperature step function; (2) the derivative of these occupations with respect to the eigenvalue, useful for response or density-derivative calculations; and (3) a per-state energy-like entropy correction term that is used when computing the smeared free-energy correction to the total energy in DFT workflows. All returned arrays have the same shape as the input eig array. The algorithm uses width as the smearing parameter and performs elementwise operations (no in-place modification of inputs).", "name": "gpaw_occupations_marzari_vanderbilt", "parameters": {"properties": {"eig": {"type": "array", "items": {"type": "any"}, "description": "Array of single-particle eigenvalues (energies) for electronic states. In GPAW workflows this is typically the Kohn–Sham eigenvalue array produced by a diagonalization step. The function preserves the shape of eig and returns arrays with the same shape. The energy units must be consistent between eig and fermi_level.", "default": ""}, "fermi_level": {"type": "float", "description": "The chemical potential (Fermi level) used as the reference energy for occupations. This scalar shifts the eigenvalues before applying the smearing function; it must be given in the same energy units as eig. Practical significance: occupations are determined relative to this value to model partially filled states near the Fermi surface.", "default": ""}, "width": {"type": "float", "description": "Smearing width (sigma) controlling the extent of cold smearing. This scalar sets the energy scale over which the occupation step is smoothed. It must be non-zero; physically meaningful values are positive (width <= 0 will produce division-by-zero or nonphysical results). The choice of width affects convergence: very small widths approach the zero-temperature step and may cause numerical instabilities, while very large widths overly smear occupations and bias energies.", "default": ""}}, "required": ["eig", "fermi_level", "width"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a GPAW metallic DFT sanity check, but I realized the eigenvalue list I copied from the log includes two junk entries from a failed diagonalization restart (they are the only ones that sit more than three smearing-widths away from the chemical potential, so they shouldn’t be included in this convergence-focused occupation/entropy diagnostic). Use the Marzari–Vanderbilt cold-smearing occupation function to compute (i) smeared occupations, (ii) d(occupation)/d(eigenvalue), and (iii) the per-state entropy energy correction **only for the eigenvalues that lie within ±(3×width) of the Fermi level**. The chemical potential is 5.35 eV and the smearing width is 0.20 eV. Eigenvalues (eV): [4.90, 5.10, 5.30, 5.35, 5.40, 5.70, 6.10].", "answers": "[{\"name\":\"gpaw_occupations_marzari_vanderbilt\",\"arguments\":{\"eig\":[4.9,5.1,5.3,5.35,5.4,5.7],\"fermi_level\":5.35,\"width\":0.2}}]"}
{"func_name": "gpaw_pseudopotential_screen_potential", "func_desc": "gpaw.pseudopotential.screen_potential splits a long-range, spherically symmetric radial potential into a short-ranged screened potential and a localized compensation charge density suitable for use in PAW/pseudopotential constructions and Hartree potential evaluation.", "tools": [{"function": {"description": "gpaw.pseudopotential.screen_potential splits a long-range, spherically symmetric radial potential into a short-ranged screened potential and a localized compensation charge density suitable for use in PAW/pseudopotential constructions and Hartree potential evaluation.\n", "name": "gpaw_pseudopotential_screen_potential", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "any"}, "description": "1D radial grid values (monotonic increasing). In the GPAW/PAW context this is the radial coordinate array (typically in atomic units, often Bohr) on which the input potential v is sampled. The function uses r to determine cutoffs, compute Gaussian compensation charges, and to return truncated arrays corresponding to the short-range region.", "default": ""}, "v": {"type": "array", "items": {"type": "any"}, "description": "1D radial potential sampled on the grid r. This is the long-ranged potential whose asymptotic behavior corresponds to charge/r (i.e., v(r) ~ -charge/r for large r, up to sign conventions used by the caller). The routine assumes v and r have matching shapes and that v describes the Coulomb tail to be screened.", "default": ""}, "charge": {"type": "float", "description": "Total point-like asymptotic charge Z that produces the 1/r tail in v. This scalar determines the amplitude of the compensation Gaussian charge and the analytic short-range correction vcomp that subtracts the long-range 1/r contribution from v.", "default": ""}, "rcut": {"type": "float", "nullable": true, "description": "Explicit cutoff radius (same units as r) at which the returned screened potential and compensation charge are truncated. If None (default), the function determines a reasonable cutoff by inspecting vr = v*r + charge and finding the radial index where the residual vr becomes sufficiently small (threshold 1e-4) and then extending slightly (searchsorted with 1.1 factor). The chosen rcut is then snapped to the nearest point on the provided grid r. If rcut is provided but does not fall within the domain of r (for example greater than r[-1]), indexing into r can raise IndexError.", "default": null}, "a": {"type": "float", "nullable": true, "description": "Width parameter of the Gaussian used to represent the localized compensation charge (same units as r). If None (default) it is set to rcut / 5.0. The parameter a controls the spatial extent of the compensation density rhocomp and therefore how rapidly the analytic correction vcomp = charge * erf(r/(sqrt(2)*a)) / r removes the 1/r tail from v. Choosing a much smaller or larger a than the default changes the localization and may affect numerical behavior of downstream Hartree integrals.", "default": null}}, "required": ["r", "v", "charge", "a", "rcut"], "type": "any"}}, "type": "function"}], "query": "I’m generating a PAW dataset and need to screen a long-range Coulomb-like radial potential, but I just noticed a consistency constraint: the cutoff must land exactly on one of the tabulated radial grid points, otherwise our internal spline-based PAW builder introduces a tiny discontinuity in the compensation density. I initially planned to use the cutoff that our chemistry team recommended (it’s the one that corresponds to the *smallest r where the sampled potential first becomes less negative than -1.10*), but for the screening width I can’t keep the originally planned value because it causes a visible kink in the short-range potential at the cutoff. Instead, set the Gaussian width to be *one fifth of the cutoff radius*.\n\nUse the following radial grid (Bohr) and sampled potential v(r) (Hartree):\n- r = [0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n- v = [-49.9, -24.85, -16.4666667, -12.275, -9.76, -8.0833333, -6.8857143, -5.9875, -5.2888889, -4.73, -3.72, -3.0666667, -2.6, -2.25, -1.96, -1.7666667, -1.6285714, -1.525, -1.4444444, -1.38, -1.2833333, -1.2142857, -1.1625, -1.1222222, -1.09, -1.04, -1.02, -1.01, -1.005, -1.0022222, -1.0004]\n\nAssume the asymptotic point charge is 1.0. Please use gpaw.pseudopotential.screen_potential to split the potential into a short-ranged screened potential and a localized compensation charge density with the cutoff and width determined by the above constraints.", "answers": "[{\"name\": \"gpaw_pseudopotential_screen_potential\", \"arguments\": {\"r\": [0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0], \"v\": [-49.9, -24.85, -16.4666667, -12.275, -9.76, -8.0833333, -6.8857143, -5.9875, -5.2888889, -4.73, -3.72, -3.0666667, -2.6, -2.25, -1.96, -1.7666667, -1.6285714, -1.525, -1.4444444, -1.38, -1.2833333, -1.2142857, -1.1625, -1.1222222, -1.09, -1.04, -1.02, -1.01, -1.005, -1.0022222, -1.0004], \"charge\": 1.0, \"rcut\": 2.0, \"a\": 0.4}}]"}
{"func_name": "gpaw_response_fxc_kernels_create_add_fxc", "func_desc": "Create and return an add_fxc function configured for a requested exchange-correlation\n    kernel (fxc) and spin component (spincomponent) used by the GPAW linear-response\n    module to assemble the exchange-correlation kernel contributions to response\n    matrices. In the GPAW DFT/PAW response context (see gpaw.response), this factory\n    binds the requested ALDA-type functional identifier to an internal implementation\n    that adds either density-density or transverse spin f_xc contributions, so the\n    returned callable can be invoked by response routines to modify the kernel.", "tools": [{"function": {"description": "Create and return an add_fxc function configured for a requested exchange-correlation\nkernel (fxc) and spin component (spincomponent) used by the GPAW linear-response\nmodule to assemble the exchange-correlation kernel contributions to response\nmatrices. In the GPAW DFT/PAW response context (see gpaw.response), this factory\nbinds the requested ALDA-type functional identifier to an internal implementation\nthat adds either density-density or transverse spin f_xc contributions, so the\nreturned callable can be invoked by response routines to modify the kernel.", "name": "gpaw_response_fxc_kernels_create_add_fxc", "parameters": {"properties": {"fxc": {"type": "string", "description": "Identifier of the exchange-correlation kernel variant to use.\nThis selects the internal implementation behavior and must be one of the\nALDA variants recognized by this code: 'ALDA_x', 'ALDA_X', or 'ALDA'.\nThe function asserts membership in this set; if fxc is not one of these\nstrings an AssertionError will be raised. In practice, these identifiers\nmean the code will use the (adiabatic) local density approximation (ALDA)\nform of f_xc when constructing kernel contributions for density or spin\nchannels in GPAW's response calculations.", "default": ""}, "spincomponent": {"type": "string", "description": "Spin-channel selector string that determines which\ninternal implementation is bound and returned. Accepted values and their\nmeanings are:\n'00', 'uu', 'dd' — select the density-density (longitudinal) component;\n    the function returns a functools.partial object wrapping the\n    internal helper add_LDA_dens_fxc with fxc bound. In GPAW this\n    corresponds to adding ALDA density f_xc contributions used when\n    building the charge-density response block.\n'+-', '-+' — select the transverse spin component; the function returns\n    a functools.partial object wrapping the internal helper\n    add_LSDA_trans_fxc with fxc bound. In GPAW this corresponds to\n    adding ALDA/LSDA transverse spin f_xc contributions used in spin\n   -flip or transverse-spin response blocks.\nIf spincomponent is not one of these accepted strings the function\nraises a ValueError(spincomponent), with the invalid string provided\nas the exception argument.", "default": ""}}, "required": ["fxc", "spincomponent"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a GPAW linear-response workflow for a non‑collinear magnetic system where I need the *transverse spin‑flip* channel of an ALDA-type exchange–correlation kernel. I was going to request the “full” ALDA identifier, but I just realized the dataset I’m benchmarking against reports response matrices in the **spin-scaled convention** (so using the full-density version would mismatch the published normalization). So: create the `add_fxc` callable using the **spin-scaled ALDA variant** of the kernel, and keep the **spin-flip transverse** component (the one that couples opposite transverse spin projections) so I can plug it directly into the response matrix assembly.", "answers": "[{\"name\": \"gpaw_response_fxc_kernels_create_add_fxc\", \"arguments\": {\"fxc\": \"ALDA_X\", \"spincomponent\": \"+-\"}}]"}
{"func_name": "gpaw_response_heisenberg_calculate_fm_magnon_energies", "func_desc": "gpaw.response.heisenberg.calculate_fm_magnon_energies computes magnon eigenmode energies for a ferromagnetic system with an arbitrary number of magnetic sites in the unit cell, using isotropic exchange constants as a function of wave vector q.\n    \n    This function is used in the GPAW response/Heisenberg utilities to obtain the spin-wave (magnon) spectrum from isotropic exchange matrices J(q) for each q-point. It first constructs the dynamic spin-wave matrix\n    H^ab(q) = g μ_B / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)]\n    by calling generate_fm_dynamic_spin_wave_matrix, then diagonalizes H(q) for each q and any independent extra dimensions x to produce magnon eigenmode energies. The output energies correspond to the eigenvalues of the dynamic spin-wave matrix and are provided for each q-vector and mode index.", "tools": [{"function": {"description": "gpaw.response.heisenberg.calculate_fm_magnon_energies computes magnon eigenmode energies for a ferromagnetic system with an arbitrary number of magnetic sites in the unit cell, using isotropic exchange constants as a function of wave vector q.\n\nThis function is used in the GPAW response/Heisenberg utilities to obtain the spin-wave (magnon) spectrum from isotropic exchange matrices J(q) for each q-point. It first constructs the dynamic spin-wave matrix\nH^ab(q) = g μ_B / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)]\nby calling generate_fm_dynamic_spin_wave_matrix, then diagonalizes H(q) for each q and any independent extra dimensions x to produce magnon eigenmode energies. The output energies correspond to the eigenvalues of the dynamic spin-wave matrix and are provided for each q-vector and mode index.", "name": "gpaw_response_heisenberg_calculate_fm_magnon_energies", "parameters": {"properties": {"J_qabx": {"type": "array", "items": {"type": "any"}, "description": "Isotropic exchange constants as a function of q and sublattice indices a and b, with optional additional independent dimensions x. Practically, the first axis indexes q-points and the next two axes index sublattice sites a and b (so J_qabx[q, a, b, ...]). Any trailing axes (x) are treated independently: the function will compute a separate magnon spectrum for each combination of those extra indices. The values should represent the isotropic Heisenberg exchange interactions used to build the dynamic spin-wave matrix.", "default": ""}, "q_qc": {"type": "array", "items": {"type": "any"}, "description": "q-vectors in relative coordinates. The array must include the q=0 vector and its ordering must match the first axis of J_qabx (i.e., the number and order of q-points). The last axis indexed by c enumerates the coordinate components of each q-vector in the chosen reciprocal/relative basis. These q-vectors determine the phase factors used in J^ab(q) and thus which spin-wave modes are computed.", "default": ""}, "mm_ax": {"type": "array", "items": {"type": "any"}, "description": "Magnetic moments of the sublattice sites given in units of μ_B. The first axis indexes sublattice site a and any additional axes correspond to the same independent dimensions x as in J_qabx (i.e., mm_ax[a, ...] must broadcast consistently with the a and x dimensions of J_qabx). The magnetic moments M_a enter the dynamic spin-wave matrix as normalization factors sqrt(M_a M_b) and therefore directly affect the computed magnon energies.", "default": ""}}, "required": ["J_qabx", "q_qc", "mm_ax"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a quick GPAW/Heisenberg sanity check for a 2-sublattice ferromagnet and want the magnon eigenmode energies at three high-symmetry q-points. Initially I was going to pass the exchange matrices exactly as recorded, but I just noticed those J(q) matrices came from a noisy interpolation step and can violate the required symmetry between sublattices (numerically the off-diagonals don’t exactly match their transpose in general). To avoid getting unphysical complex modes, first enforce the physically required Hermiticity by replacing each J(q) with its symmetrized version (average it with its transpose). Then compute the magnon energies at Γ=(0,0,0), X=(0.5,0,0), and M=(0.5,0.5,0) using the same three recorded matrices (in meV) and magnetic moments mm=[2.2, 1.8] μB. Use these J(q) inputs: Γ: [[12.0, 2.0],[2.0, 10.0]], X: [[11.0, 1.5],[1.5, 9.0]], M: [[10.0, 1.0],[1.0, 8.0]].", "answers": "[{\"name\": \"gpaw_response_heisenberg_calculate_fm_magnon_energies\", \"arguments\": {\"J_qabx\": [[[12.0, 2.0], [2.0, 10.0]], [[11.0, 1.5], [1.5, 9.0]], [[10.0, 1.0], [1.0, 8.0]]], \"q_qc\": [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.5, 0.5, 0.0]], \"mm_ax\": [2.2, 1.8]}}]"}
{"func_name": "gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix", "func_desc": "gpaw.response.heisenberg.generate_fm_dynamic_spin_wave_matrix: Generate the dynamic spin-wave matrix H_qabx for a collinear ferromagnet from isotropic exchange constants J_qabx.\n    \n    This routine implements the relation used in spin-wave (magnon) calculations for ferromagnets:\n    H^ab(q) = (g μ_B) / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)],\n    but in the implementation the magnetic-moment prefactor is computed explicitly as 2. / sqrt(M_a M_b) (see code). The function is intended for use inside GPAW's linear-response / Heisenberg-model utilities where J_qabx represents isotropic exchange interactions between sublattices a and b as a function of crystal momentum q and optional extra parameter dimensions x. The resulting H_qabx is the dynamic spin-wave matrix used to obtain spin-wave spectra and mode analysis for a ferromagnetic system within the assumptions of a collinear magnetic ground state and no spin-orbit coupling.", "tools": [{"function": {"description": "gpaw.response.heisenberg.generate_fm_dynamic_spin_wave_matrix: Generate the dynamic spin-wave matrix H_qabx for a collinear ferromagnet from isotropic exchange constants J_qabx.\n\nThis routine implements the relation used in spin-wave (magnon) calculations for ferromagnets:\nH^ab(q) = (g μ_B) / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)],\nbut in the implementation the magnetic-moment prefactor is computed explicitly as 2. / sqrt(M_a M_b) (see code). The function is intended for use inside GPAW's linear-response / Heisenberg-model utilities where J_qabx represents isotropic exchange interactions between sublattices a and b as a function of crystal momentum q and optional extra parameter dimensions x. The resulting H_qabx is the dynamic spin-wave matrix used to obtain spin-wave spectra and mode analysis for a ferromagnetic system within the assumptions of a collinear magnetic ground state and no spin-orbit coupling.", "name": "gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix", "parameters": {"properties": {"J_qabx": {"type": "array", "items": {"type": "any"}, "description": "Isotropic exchange constants J^ab(q). Array indexed as [q_index, a, b, ...x], where the first axis enumerates q-points and axes 1 and 2 correspond to sublattice indices a and b respectively. J_qabx may be complex but must satisfy Hermiticity J_qabx[q,a,b,...] = conj(J_qabx[q,b,a,...]) (the function asserts this). Any trailing dimensions x (J_qabx.shape[3:]) are treated independently and must match the trailing dimensions of mm_ax. The shape constraints enforced by the function are: J_qabx.ndim >= 3, J_qabx.shape[1] == J_qabx.shape[2], J_qabx.shape[0] == q_qc.shape[0], and J_qabx.shape[3:] == mm_ax.shape[1:].", "default": ""}, "q_qc": {"type": "array", "items": {"type": "any"}, "description": "Array of q-vectors in relative (crystal) coordinates with shape (n_q, 3) or compatible shape where the first axis enumerates q-points. The array must include the Gamma point q = 0 because the implementation locates the index q0 = get_q0_index(q_qc) to obtain J^ab(0). If q=0 is not present, get_q0_index will fail or the function will raise an assertion. The q vectors determine the first axis correspondence to J_qabx.", "default": ""}, "mm_ax": {"type": "array", "items": {"type": "any"}, "description": "Magnetic moments of the sublattice sites in units of Bohr magneton (μ_B). The first axis enumerates sublattices (length na), so mm_ax.shape[0] must equal the number of sublattices (J_qabx.shape[1]). mm_ax may contain additional trailing dimensions to represent parameter dependence (matching J_qabx.shape[3:]). The code computes a prefactor 2. / sqrt(M_a M_b) from these magnetic moments and uses it to normalize the dynamic matrix by sublattice magnetizations.", "default": ""}}, "required": ["J_qabx", "q_qc", "mm_ax"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a quick magnon sanity-check for a collinear *ferromagnet* with two magnetic sublattices in GPAW’s Heisenberg-response workflow. I initially planned to compute the dynamic spin-wave matrix H_qabx directly from the provided J_qabx at the three q-points. However, I realized the first q-point is Γ, and for a clean ferromagnetic reference I need the Γ exchange matrix to represent the *onsite sum rule* consistently: the diagonal elements should equal the corresponding diagonal elements of the *q-averaged* exchange over the *non-Γ* q-points (keeping off-diagonals at Γ unchanged so I don’t break the sublattice coupling symmetry).\n\nSo please:\n1) Take the given q_qc and J_qabx (shape [3,2,2]) below.\n2) Replace only the Γ-point *diagonal* entries J(Γ)_{00} and J(Γ)_{11} by the arithmetic mean of the corresponding diagonal entries over the other two q-points (the ones that are not Γ). Leave all off-diagonal terms (including at Γ) as given.\n3) Use the original sublattice moments mm_ax (in μB) unchanged.\n4) With this corrected J_qabx, compute the ferromagnetic dynamic spin-wave matrix H_qabx using gpaw.response.heisenberg.generate_fm_dynamic_spin_wave_matrix.\n\nData:\n- q_qc = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.25, 0.25, 0.0]]\n- mm_ax = [2.2, 1.8]\n- J_qabx (shape [3,2,2]) =\n  q0 (Γ): [[1.50, 0.20], [0.20, 1.10]]\n  q1:     [[1.20, 0.10], [0.10, 0.90]]\n  q2:     [[1.35, 0.15], [0.15, 1.00]]", "answers": "[{\"name\": \"gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix\", \"arguments\": {\"J_qabx\": [[[1.275, 0.2], [0.2, 0.95]], [[1.2, 0.1], [0.1, 0.9]], [[1.35, 0.15], [0.15, 1.0]]], \"q_qc\": [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.25, 0.25, 0.0]], \"mm_ax\": [2.2, 1.8]}}]"}
{"func_name": "gpaw_response_heisenberg_get_q0_index", "func_desc": "gpaw.response.heisenberg.get_q0_index: Find and return the index of the zero wavevector (q = [0., 0., 0.]) in a q-vector array used in GPAW Heisenberg/response calculations.\n    \n    This function is used in GPAW's response.heisenberg routines to identify the q = 0 (Gamma) entry in an array of q-vectors (for example when assembling response functions or evaluating Heisenberg exchange parameters at the zero momentum transfer). It searches the provided NumPy array for a row that is exactly equal to [0., 0., 0.] and returns the integer index of the first matching row. The function performs an elementwise exact equality check, so callers should ensure that the zero vector is represented exactly (or preprocess the array to set small numerical values to zero) before calling.", "tools": [{"function": {"description": "gpaw.response.heisenberg.get_q0_index: Find and return the index of the zero wavevector (q = [0., 0., 0.]) in a q-vector array used in GPAW Heisenberg/response calculations.\n\nThis function is used in GPAW's response.heisenberg routines to identify the q = 0 (Gamma) entry in an array of q-vectors (for example when assembling response functions or evaluating Heisenberg exchange parameters at the zero momentum transfer). It searches the provided NumPy array for a row that is exactly equal to [0., 0., 0.] and returns the integer index of the first matching row. The function performs an elementwise exact equality check, so callers should ensure that the zero vector is represented exactly (or preprocess the array to set small numerical values to zero) before calling.", "name": "gpaw_response_heisenberg_get_q0_index", "parameters": {"properties": {"q_qc": {"type": "array", "items": {"type": "any"}, "description": "2-D NumPy array of q-vectors where each row is a Cartesian q-vector (commonly of shape (N, 3) for N q-points). In the GPAW Heisenberg/response context, q_qc represents the set of crystal/momentum transfer vectors q_c for which response quantities are evaluated. The function expects an ndarray so that axis-wise comparisons (axis=1) succeed; passing arrays with incorrect dimensionality (e.g., 1-D arrays) will raise an error from NumPy.", "default": ""}}, "required": ["q_qc"], "type": "any"}}, "type": "function"}], "query": "I’m stitching together a GPAW Heisenberg/response post-processing script and need to locate the Gamma entry in a q-vector table before assembling J(q). Initially I was going to just call `get_q0_index(q_qc)` directly, but I realized the q-grid coming from my earlier step sometimes contains tiny numerical noise (e.g., values on the order of ~1e-16) so an exact equality check can miss Gamma.\n\nSo please do this instead: first preprocess the q-vectors by zeroing any component whose absolute value is smaller than the smallest magnitude that would still survive in double precision after typical symmetry operations (i.e., treat those as exactly zero), and then call `gpaw.response.heisenberg.get_q0_index` on the cleaned array.\n\nHere’s the raw q-point list (Cartesian):\nq_qc = [\n  [0.25, 0.0, 0.0],\n  [0.0, 0.5, 0.0],\n  [0.0, 0.0, 0.0],\n  [0.5, 0.5, -1e-16],\n  [0.0, 0.0, 0.0]\n]\n\nReturn the index of the first row that is Gamma after this preprocessing step.", "answers": "[{\"name\": \"gpaw_response_heisenberg_get_q0_index\", \"arguments\": {\"q_qc\": [[0.25, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.0], [0.5, 0.5, 0.0], [0.0, 0.0, 0.0]]}}]"}
{"func_name": "gpaw_response_pair_transitions_transitions_in_composite_index", "func_desc": "gpaw.response.pair_transitions.transitions_in_composite_index: Build flattened index arrays that enumerate transitions (n, s) -> (n', s') using a composite index t for use in GPAW response and pair-transition routines.\n    \n    This function constructs two meshgrids, one for the initial-state indices (n1_M with s1_S) and one for the final-state indices (n2_M with s2_S), and returns the four flattened arrays. In the context of GPAW response calculations (density-functional theory using the PAW method), this utility is used to vectorize and enumerate all combinations of band (n) and spin (s) indices for subsequent assembly of response matrices, transition lists, or batched linear-algebra operations. The function does not modify its inputs and has no side effects beyond allocating and returning new numpy arrays.", "tools": [{"function": {"description": "gpaw.response.pair_transitions.transitions_in_composite_index: Build flattened index arrays that enumerate transitions (n, s) -> (n', s') using a composite index t for use in GPAW response and pair-transition routines.\n\nThis function constructs two meshgrids, one for the initial-state indices (n1_M with s1_S) and one for the final-state indices (n2_M with s2_S), and returns the four flattened arrays. In the context of GPAW response calculations (density-functional theory using the PAW method), this utility is used to vectorize and enumerate all combinations of band (n) and spin (s) indices for subsequent assembly of response matrices, transition lists, or batched linear-algebra operations. The function does not modify its inputs and has no side effects beyond allocating and returning new numpy arrays.", "name": "gpaw_response_pair_transitions_transitions_in_composite_index", "parameters": {"properties": {"n1_M": {"type": "array", "items": {"type": "float"}, "description": "1-D array of initial-state band indices n for the first index set (length M). These are the band indices that label the \"from\" state in transitions (n, s) -> (n', s'). In GPAW response code this typically represents a selection of bands (e.g. occupied bands) over which transitions are constructed.", "default": ""}, "n2_M": {"type": "array", "items": {"type": "float"}, "description": "1-D array of final-state band indices n' for the second index set (length M). These are the band indices that label the \"to\" state in transitions (n, s) -> (n', s'). This array is used together with s2_S to enumerate the final-state indices for transitions.", "default": ""}, "s1_S": {"type": "array", "items": {"type": "float"}, "description": "1-D array of initial-state spin indices s for the first spin set (length S). These are spin labels (or other small categorical indices) associated with the initial band indices n1_M. The function pairs every entry of s1_S with every entry of n1_M to form the initial-state side of the composite index.", "default": ""}, "s2_S": {"type": "array", "items": {"type": "float"}, "description": "1-D array of final-state spin indices s' for the second spin set (length S). These are spin labels associated with the final band indices n2_M. The function pairs every entry of s2_S with every entry of n2_M to form the final-state side of the composite index.", "default": ""}}, "required": ["n1_M", "n2_M", "s1_S", "s2_S"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a spin-polarized GPAW linear-response run and need the flattened composite-index arrays for band+spin transitions. Initially I was going to allow transitions from occupied bands [3, 4] with spins [0, 1] into unoccupied bands [5, 6, 7] across both spin channels as well. But I just realized the perturbation I’m using is strictly collinear and conserves spin, so any spin-flip channel should be excluded. Please build the flattened composite-index arrays that enumerate every allowed (n, s) -> (n′, s′) combination under that constraint, using the same occupied/unoccupied band sets and the same available spin channels as above.", "answers": "[{\"name\": \"gpaw_response_pair_transitions_transitions_in_composite_index\", \"arguments\": {\"n1_M\": [3, 4], \"n2_M\": [5, 6, 7], \"s1_S\": [0, 1], \"s2_S\": [0, 1]}}]"}
{"func_name": "gpaw_response_tool_get_degeneracy_matrix", "func_desc": "gpaw.response.tool.get_degeneracy_matrix generates a degeneracy-selection matrix and a list of representative energies for groups of (nearly) degenerate single-particle eigenvalues. This function is used in GPAW response and transition-summing code to identify and sum contributions from degenerate Kohn–Sham states: it produces a matrix whose rows select the members of each degenerate group and an array of one representative energy per group.", "tools": [{"function": {"description": "gpaw.response.tool.get_degeneracy_matrix generates a degeneracy-selection matrix and a list of representative energies for groups of (nearly) degenerate single-particle eigenvalues. This function is used in GPAW response and transition-summing code to identify and sum contributions from degenerate Kohn–Sham states: it produces a matrix whose rows select the members of each degenerate group and an array of one representative energy per group.\n", "name": "gpaw_response_tool_get_degeneracy_matrix", "parameters": {"properties": {"eps_n": {"type": "array", "items": {"type": "float"}, "description": "A one-dimensional NumPy array of single-particle energies (for example Kohn–Sham eigenvalues produced by a GPAW calculation). The function treats equal or nearly equal values that occur in contiguous positions of this array as degenerate; therefore, eps_n should be sorted in non-decreasing order if grouping of all equal energies is required. The practical significance in GPAW is that these energies are the basis for grouping states that should be summed together in response/transition calculations.", "default": ""}, "tol": {"type": "float", "description": "Absolute tolerance for degeneracy in the same units as eps_n. Two energies eps_n[i] and eps_n[j] are considered degenerate if abs(eps_n[i] - eps_n[j]) < tol. The default (0.001) is suitable for grouping energies that differ only by numerical noise; increase or decrease this value to control how strictly degeneracy is detected. tol must be a floating-point number; negative values are not meaningful and may lead to unexpected grouping behavior.", "default": 0.001}}, "required": ["eps_n", "tol"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a GPAW response run and need to group (nearly) degenerate Kohn–Sham levels before doing the transition sums. Use `gpaw.response.tool.get_degeneracy_matrix` on this *sorted* eigenvalue list (in eV): [ -5.4321, -5.43205, -3.2100, -3.2098, -3.0000, -2.9992, -1.5000 ].\n\nInitially I was going to use the same absolute degeneracy cutoff as in our quick-and-dirty runs, but I just realized that cutoff was defined as an energy *difference* between adjacent levels in **meV**, and here my tolerance budget is specified in **eV**. So: take the original cutoff expressed as an energy difference in meV, convert it to eV, and use that converted value as the `tol`.\n\nThen generate the degeneracy-selection matrix and the representative energies (one per degenerate group) exactly as `get_degeneracy_matrix` defines them (rows selecting members of each group).", "answers": "[{\"name\":\"gpaw_response_tool_get_degeneracy_matrix\",\"arguments\":{\"eps_n\":[-5.4321,-5.43205,-3.21,-3.2098,-3.0,-2.9992,-1.5],\"tol\":0.001}}]"}
{"func_name": "gpaw_rotation_Y_matrix", "func_desc": "Compute the matrix of spherical-harmonic values for angular momentum l evaluated at a set of rotated sample points.\n    \n    This function is used in GPAW's rotation utilities to build the matrix of values of the internal Y(...) function (spherical-harmonic-like evaluator used in this module) for 2*l+1 sample points on the unit sphere. The sample points are taken from the module-level list sphere_lm[l] (one point per m1 = 0,...,2*l). Each sample point is rotated by the 3D linear transformation U_vv (typically a 3×3 rotation matrix in GPAW applications), and for each rotated point the function evaluates Y at indices L = l**2 + m2 for m2 = 0,...,2*l. The resulting matrix Y_mm has rows enumerating the input sample points (m1) and columns enumerating the evaluated Y indices derived from m2. In GPAW this matrix is a building block for constructing rotation operators in the angular-momentum-l subspace (for example when rotating PAW projectors, atomic-like basis functions, or related quantities between coordinate frames).", "tools": [{"function": {"description": "Compute the matrix of spherical-harmonic values for angular momentum l evaluated at a set of rotated sample points.\n\nThis function is used in GPAW's rotation utilities to build the matrix of values of the internal Y(...) function (spherical-harmonic-like evaluator used in this module) for 2*l+1 sample points on the unit sphere. The sample points are taken from the module-level list sphere_lm[l] (one point per m1 = 0,...,2*l). Each sample point is rotated by the 3D linear transformation U_vv (typically a 3×3 rotation matrix in GPAW applications), and for each rotated point the function evaluates Y at indices L = l**2 + m2 for m2 = 0,...,2*l. The resulting matrix Y_mm has rows enumerating the input sample points (m1) and columns enumerating the evaluated Y indices derived from m2. In GPAW this matrix is a building block for constructing rotation operators in the angular-momentum-l subspace (for example when rotating PAW projectors, atomic-like basis functions, or related quantities between coordinate frames).", "name": "gpaw_rotation_Y_matrix", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number l >= 0 that selects the subspace size. The function uses 2*l+1 sample points and produces a square matrix of size (2*l+1)×(2*l+1). In practice l is a non-negative integer used throughout GPAW to index angular-momentum channels (e.g. s,p,d,...). If l is not an integer or is negative, the function will either behave incorrectly or raise exceptions (TypeError/IndexError/ValueError) when accessing sphere_lm or during array allocation.", "default": ""}, "U_vv": {"type": "array", "items": {"type": "any"}, "description": "3D linear transformation applied to each unit-sphere sample point before evaluation. The object must be a NumPy ndarray compatible with NumPy's dot product with a 3-element point (the code uses numpy.dot(point, U_vv)). In GPAW usage this is typically a 3×3 orthogonal rotation matrix (dtype float), but any ndarray with a shape and dtype that make the dot product valid is accepted. If U_vv has incompatible shape or is not an ndarray, NumPy will raise an appropriate exception (TypeError or ValueError). Note that if U_vv is not a proper rotation (orthonormal) matrix, the numerical values produced will correspond to the transformed coordinates but will not represent a geometric rotation.", "default": ""}}, "required": ["l", "U_vv"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW PAW-projector rotation test for the d-channel. I was going to rotate the predefined unit-sphere sample points by a nominal 30° about the z-axis and then build the spherical-harmonic evaluation matrix Y_mm (rows: the 2*l+1 sample points from sphere_lm[l], columns: L=l**2+m2 for m2=0..2*l). But I just realized my rotation operator expects the *passive* convention (i.e., it should undo the active rotation I initially had in mind), so the 3×3 transform I pass in must be the inverse of the naive z-axis rotation I first thought of. Use that corrected 3×3 matrix (still a pure rotation about z, same magnitude) and compute Y_mm for l=2.", "answers": "[{\"name\": \"gpaw_rotation_Y_matrix\", \"arguments\": {\"l\": 2, \"U_vv\": [[0.8660254037844386, 0.5, 0.0], [-0.5, 0.8660254037844386, 0.0], [0.0, 0.0, 1.0]]}}]"}
{"func_name": "gpaw_rotation_rotation", "func_desc": "gpaw.rotation.rotation: compute the rotation (transformation) matrix for spherical harmonics of angular momentum l.\n    \n    Compute the transformation matrix that maps coefficients of one set of spherical harmonic basis functions Y_lm (denoted here as Y_lm1) to another rotated set Y_lm2 according to the rotation described by U_vv. This function is used in GPAW's PAW/atomic-basis infrastructure to rotate angular-momentum-resolved quantities (for example atomic orbitals, projector functions, or density-matrix blocks) when applying a symmetry operation or a spatial rotation. Internally the function constructs the rotation in the spherical-harmonic representation by calling Y_matrix(l, U_vv) and then applies the precomputed iY_lmm[l] transformation, returning the combined transformation matrix used to rotate coefficients associated with angular momentum l.", "tools": [{"function": {"description": "gpaw.rotation.rotation: compute the rotation (transformation) matrix for spherical harmonics of angular momentum l.\n\nCompute the transformation matrix that maps coefficients of one set of spherical harmonic basis functions Y_lm (denoted here as Y_lm1) to another rotated set Y_lm2 according to the rotation described by U_vv. This function is used in GPAW's PAW/atomic-basis infrastructure to rotate angular-momentum-resolved quantities (for example atomic orbitals, projector functions, or density-matrix blocks) when applying a symmetry operation or a spatial rotation. Internally the function constructs the rotation in the spherical-harmonic representation by calling Y_matrix(l, U_vv) and then applies the precomputed iY_lmm[l] transformation, returning the combined transformation matrix used to rotate coefficients associated with angular momentum l.", "name": "gpaw_rotation_rotation", "parameters": {"properties": {"l": {"type": "integer", "description": "The angular momentum quantum number l for which the rotation is constructed. In the GPAW/PAW context this selects the subspace of spherical harmonics with magnetic quantum numbers m = -l, ..., +l; the transformation therefore operates on the (2*l + 1)-dimensional coefficient vector for that l. l must be a non-negative integer; passing a negative or non-integer value is invalid and will typically result in an error from the underlying construction routines.", "default": ""}, "U_vv": {"type": "array", "items": {"type": "any"}, "description": "A NumPy array that encodes the rotation/symmetry operation in the vector (spatial) representation. This array is passed to Y_matrix(l, U_vv) to build the rotation in the spherical-harmonic basis. In practical GPAW use U_vv represents the rotation part of a symmetry operation or a rigid-body rotation applied to atomic-centered quantities. If U_vv has an incompatible shape or contents for Y_matrix, the underlying function will raise an exception.", "default": ""}}, "required": ["l", "U_vv"], "type": "any"}}, "type": "function"}], "query": "I’m applying a symmetry operation in a GPAW PAW calculation and need the spherical-harmonic rotation matrix for the d-channel (l=2). Initially I was going to use the 90° rotation about the z-axis, but I just realized the symmetry generator in my workflow stores the *inverse* active rotation (i.e., it maps the rotated coordinates back into the original frame). So instead, feed `gpaw.rotation.rotation` the spatial 3×3 rotation matrix that corresponds to undoing the original quarter-turn about the z-axis (right-handed convention), and compute the l=2 spherical-harmonic transformation matrix for that corrected operation.", "answers": "[{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":2,\"U_vv\":[[0,1,0],[-1,0,0],[0,0,1]]}}]"}
{"func_name": "gpaw_sphere_integrate_find_volume_conserving_lambd", "func_desc": "Determine the scaling factor λ (lambda) that makes a radial truncation function conserve the spherical volume.\n    \n    This function is used in GPAW's spherical integration and truncation utilities to find a multiplicative scaling parameter λ such that the numerically integrated, truncated radial weight function θ(r) yields the same total volume as an ideal sphere of radius rcut. In practice this is important when constructing smooth truncation/mask functions for spherical regions (for example PAW augmentation spheres or localized real-space cutoffs) on a radial grid: the function adjusts the truncation so that 4π ∫_0^rcut r^2 θ(r) dr = 4π rcut^3 / 3 within numerical tolerance.", "tools": [{"function": {"description": "Determine the scaling factor λ (lambda) that makes a radial truncation function conserve the spherical volume.\n\nThis function is used in GPAW's spherical integration and truncation utilities to find a multiplicative scaling parameter λ such that the numerically integrated, truncated radial weight function θ(r) yields the same total volume as an ideal sphere of radius rcut. In practice this is important when constructing smooth truncation/mask functions for spherical regions (for example PAW augmentation spheres or localized real-space cutoffs) on a radial grid: the function adjusts the truncation so that 4π ∫_0^rcut r^2 θ(r) dr = 4π rcut^3 / 3 within numerical tolerance.", "name": "gpaw_sphere_integrate_find_volume_conserving_lambd", "parameters": {"properties": {"rcut": {"type": "float", "description": "Cutoff radius r_c of the sphere. This is the radius of the target sphere whose volume must be conserved by the truncated radial weight. The value is interpreted in the same length units as the caller's radial grid and other GPAW routines (no unit conversion is performed here).", "default": ""}, "drcut": {"type": "float", "description": "Radial grid spacing or differential parameter used to construct the uniform radial grid when r_g is not provided. When r_g is None, the function calls _uniform_radial_grid(rcut, drcut) to build a 1D radial grid from 0 up to rcut with spacing approximately drcut; drcut therefore controls the numerical resolution of the volume integral and affects the resulting λ through discretization error.", "default": ""}, "r_g": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional 1D NumPy array of radial grid points (r values) to use for the numerical integration. If provided, these values are used directly by radial_truncation_function and radial_trapz to evaluate the integral 4π ∫ r^2 θ(r) dr. If r_g is None (the default), a uniform radial grid is constructed by calling _uniform_radial_grid(rcut, drcut). The array should represent radii spanning the region of interest up to rcut; providing a precomputed grid lets callers control grid placement and resolution explicitly.", "default": null}}, "required": ["rcut", "drcut"], "type": "any"}}, "type": "function"}], "query": "I’m constructing a smooth spherical truncation/mask for a PAW augmentation region in a real-space GPAW workflow, and I need the volume-conserving scaling factor λ used by the radial truncation utilities. I was going to use the same cutoff radius as in my input deck (2.5 in grid-length units), but I just realized that value is the *diameter* coming from our post-processing script (it labels “sphere size” ambiguously). To avoid changing any other part of the pipeline, keep the physical sphere size fixed but convert that mistaken diameter into the correct cutoff radius before calling the λ-finder. Also, the radial resolution I wrote down (0.02) includes an extra endpoint sample in the grid exporter; to match GPAW’s internal convention for drcut, use the step size that is smaller by one uniform increment over the same span. Using those corrected inputs, compute the volume-conserving λ such that 4π∫ r^2 θ(r)dr matches the ideal sphere volume to numerical tolerance.", "answers": "[{\"name\":\"gpaw_sphere_integrate_find_volume_conserving_lambd\",\"arguments\":{\"rcut\":1.25,\"drcut\":0.01984126984126984}}]"}
{"func_name": "gpaw_sphere_integrate_integrate_radial_grid", "func_desc": "Integrate radial functions on a spherical radial grid.\n    \n    Computes the radial integral\n        ∫ r^2 dr f(r)\n    for each function represented on the input radial grid r_g. This routine is used in GPAW's spherical/atom-centered integration routines (e.g., when integrating radial parts of densities, projectors or other atom-centered quantities in the PAW framework). The function optionally truncates the grid at a cutoff radius rcut before performing the integration. Integration is performed with the radial trapezoidal rule implemented by radial_trapz; truncation (when requested) is performed by truncate_radial_grid.", "tools": [{"function": {"description": "Integrate radial functions on a spherical radial grid.\n\nComputes the radial integral\n    ∫ r^2 dr f(r)\nfor each function represented on the input radial grid r_g. This routine is used in GPAW's spherical/atom-centered integration routines (e.g., when integrating radial parts of densities, projectors or other atom-centered quantities in the PAW framework). The function optionally truncates the grid at a cutoff radius rcut before performing the integration. Integration is performed with the radial trapezoidal rule implemented by radial_trapz; truncation (when requested) is performed by truncate_radial_grid.", "name": "gpaw_sphere_integrate_integrate_radial_grid", "parameters": {"properties": {"f_xg": {"type": "array", "items": {"type": "any"}, "description": "Array containing the functions to integrate sampled on the radial grid r_g. The last axis of f_xg corresponds to the points of r_g. In typical GPAW use, f_xg has shape (n_x, n_g) where n_g == len(r_g) and n_x indexes different radial functions (for example different projectors, partial waves or spherical components). A 1-D array of length n_g (a single radial function) is also accepted. The values are the function values f(r) evaluated at the radii in r_g.", "default": ""}, "r_g": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of radial coordinates (grid points) at which f_xg is sampled. The values are radii r (in the length units used by GPAW, typically Å when interfacing via ASE) and must correspond to the last axis of f_xg. r_g is expected to be non-decreasing and cover the domain over which the integral is desired.", "default": ""}, "rcut": {"type": "float", "nullable": true, "description": "Optional cutoff radius. If provided (not None), the function first calls truncate_radial_grid(f_xg, r_g, rcut) to restrict both f_xg and r_g to points with r <= rcut before integration. If rcut is None (the default) the full provided r_g range is used. Behavior when rcut is outside the range of r_g or when r_g is not sorted is determined by truncate_radial_grid and may raise a ValueError from that helper function.", "default": null}}, "required": ["f_xg", "r_g", "rcut"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW PAW-style post-processing check where I need the radial moments of two projector-like radial functions sampled on an atom-centered spherical radial grid r (Å). Initially I was going to integrate them out to a hard cutoff of 2.0 Å, but I just realized the downstream normalization code assumes the cutoff is the **largest grid point that is still ≤ 2.0 Å after discarding any non-monotonic or duplicate radius entries** (those are occasional artifacts from concatenated grids). So: first sanitize the provided r-grid by removing any points that would violate strict monotonic increase (keep the first occurrence and drop later duplicates), then set the cutoff to the maximum remaining radius that does not exceed 2.0 Å, and finally compute, for each function, the integral ∫ r^2 f(r) dr using the radial trapezoidal rule on that truncated grid.\n\nUse:\n- r_g = [0.0, 0.5, 1.0, 1.5, 2.0, 2.0, 2.5, 3.0]\n- f1(r) = [1.0, 0.8, 0.6, 0.4, 0.2, 0.2, 0.1, 0.0]\n- f2(r) = [0.0, 0.2, 0.5, 0.7, 0.6, 0.6, 0.3, 0.1]\n\nReturn both integrals (one per function) from the same call, using the sanitized grid and the cutoff defined as above.", "answers": "[{\"name\":\"gpaw_sphere_integrate_integrate_radial_grid\",\"arguments\":{\"f_xg\":[[1.0,0.8,0.6,0.4,0.2,0.1,0.0],[0.0,0.2,0.5,0.7,0.6,0.3,0.1]],\"r_g\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0],\"rcut\":2.0}}]"}
{"func_name": "gpaw_sphere_integrate_radial_trapz", "func_desc": "gpaw.sphere.integrate.radial_trapz computes the integral of r^2 f(r) over a radial grid using a piecewise linear (trapezoidal) rule tailored for radial integrals used in GPAW DFT calculations.\n    \n    This function is used in the GPAW code base to accumulate radial integrals (for example, integrals of radial parts of densities, projectors or basis-function contributions) by assuming that the radial function f(r) is sampled on a one-dimensional radial grid r_g and is linearly interpolated between consecutive grid points. For each interval r0 <= r <= r1 the integral of r^2 f(r) is evaluated analytically for the linear interpolant and the contributions from all intervals are summed. The method is exact for piecewise-linear f(r) and otherwise is an approximation whose accuracy depends on the radial grid resolution.", "tools": [{"function": {"description": "gpaw.sphere.integrate.radial_trapz computes the integral of r^2 f(r) over a radial grid using a piecewise linear (trapezoidal) rule tailored for radial integrals used in GPAW DFT calculations.\n\nThis function is used in the GPAW code base to accumulate radial integrals (for example, integrals of radial parts of densities, projectors or basis-function contributions) by assuming that the radial function f(r) is sampled on a one-dimensional radial grid r_g and is linearly interpolated between consecutive grid points. For each interval r0 <= r <= r1 the integral of r^2 f(r) is evaluated analytically for the linear interpolant and the contributions from all intervals are summed. The method is exact for piecewise-linear f(r) and otherwise is an approximation whose accuracy depends on the radial grid resolution.", "name": "gpaw_sphere_integrate_radial_trapz", "parameters": {"properties": {"f_xg": {"type": "array", "items": {"type": "any"}, "description": "Array of sampled function values f on the radial grid. The last axis of this array must index the radial grid points and therefore its length must equal len(r_g). Other leading axes (denoted by \"x\" in the name) may represent multiple functions, angular channels, or additional data dimensions so that the function computes one radial integral per leading-index tuple. The array is not modified by the function.", "default": ""}, "r_g": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional numpy array of radial grid points (r values) in non-negative units consistent with f_xg. The grid must be in strictly ascending order (r_g[i+1] > r_g[i]) and all entries must be >= 0.0; the code uses these grid points to form the piecewise integration intervals.", "default": ""}}, "required": ["f_xg", "r_g"], "type": "any"}}, "type": "function"}], "query": "I’m integrating two projector-like radial functions with GPAW’s radial piecewise-linear trapezoidal rule (i.e., integrate r^2 f(r) over the provided nonuniform radial grid). I was going to just pass the arrays as-is, but I realized the first grid point is exactly at the origin and one of our upstream generators occasionally produces a tiny negative radius there (numerical noise), which would make the r^2-weighted integral unphysical if we didn’t sanitize it.\n\nSo please do the following, in this order:\n1) Take the given grid r_g = [0.0, 0.15, 0.35, 0.6, 1.0, 1.5, 2.2] (bohr) and check only the *first* entry: if it is not strictly positive, replace it by the smallest strictly positive radius present anywhere else in the same grid, scaled down by a factor of 10. (Leave all other grid points unchanged.)\n2) Using that corrected grid, compute the radial integrals ∫ r^2 f(r) dr for both functions using the GPAW-style piecewise-linear trapezoidal rule.\n\nThe sampled functions are f_xg = [[1.0, 0.92, 0.75, 0.55, 0.30, 0.12, 0.03], [0.0, 0.10, 0.28, 0.40, 0.35, 0.18, 0.06]].", "answers": "[{\"name\":\"gpaw_sphere_integrate_radial_trapz\",\"arguments\":{\"f_xg\":[[1.0,0.92,0.75,0.55,0.3,0.12,0.03],[0.0,0.1,0.28,0.4,0.35,0.18,0.06]],\"r_g\":[0.015,0.15,0.35,0.6,1.0,1.5,2.2]}}]"}
{"func_name": "gpaw_sphere_rshe_get_rshe_coefficient_info_string", "func_desc": "Construct and return a single-line, human-readable info string describing the\n    weight and inclusion status of one RSHE (real-space spherical-harmonic)\n    coefficient used in GPAW reporting. This function is used in GPAW's\n    sphere.rshe routines to present per-coefficient diagnostic information\n    for density / potential expansions (e.g. in PAW or grid-based analyses)\n    in text output or log files.", "tools": [{"function": {"description": "Construct and return a single-line, human-readable info string describing the\nweight and inclusion status of one RSHE (real-space spherical-harmonic)\ncoefficient used in GPAW reporting. This function is used in GPAW's\nsphere.rshe routines to present per-coefficient diagnostic information\nfor density / potential expansions (e.g. in PAW or grid-based analyses)\nin text output or log files.", "name": "gpaw_sphere_rshe_get_rshe_coefficient_info_string", "parameters": {"properties": {"L": {"type": "integer", "description": "Linear index that encodes the spherical-harmonic angular\nquantum numbers. Internally the function decodes L to obtain\nl and m via l = int(numpy.sqrt(L)) and m = L - l * (l + 1). In the\nGPAW RSHE context, l identifies the angular momentum order of the\ncoefficient and m identifies the component within that order; the\ndecoded (l,m) pair is used as the textual label for the coefficient.", "default": ""}, "included": {"type": "boolean", "description": "Boolean flag indicating whether this coefficient was\nincluded in the set of coefficients (for example, included in a\ntruncated expansion or selection criterion). The function converts\nthis flag to the literal string 'yes' or 'no' in the returned\ninfo string so that output clearly documents whether the coefficient\ncontributed to subsequent sums or analyses.", "default": ""}, "rshew": {"type": "float", "description": "The RSHE weight value associated with this coefficient.\nThis floating-point value represents the coefficient's computed\nweight (for example, its contribution to a norm or selection metric)\nand is printed in the info string with eight decimal places to aid\nnumerical comparison and human inspection in GPAW logs.", "default": ""}, "fw_g": {"type": "array", "items": {"type": "any"}, "description": "Array of numeric weights (one per grid point or\nG-vector) associated with this coefficient in the underlying\ndiscretization. The function reports the maximum value of this array\n(computed with numpy.max) as an indicator of the largest local or\nreciprocal-space contribution for the coefficient. This array is not\nmodified by the function; only its maximum is queried for reporting.", "default": ""}}, "required": ["L", "included", "rshew", "fw_g"], "type": "any"}}, "type": "function"}], "query": "I’m triaging a noisy GPAW RSHE (sphere.rshe) debug log and need a single-line, human-readable status string for coefficient index L=8 that includes: the decoded (l,m) label, whether this coefficient is in the *actually used* truncated expansion, its computed rshe weight, and the maximum grid weight from a tiny fw_g sample.\n\nInitially I was going to hardcode `included=false`, but I realized that in my post-processing pipeline anything with a nonzero rshew should be treated as participating in the fit (even if it was originally marked excluded upstream). So set the inclusion flag according to that rule.\n\nUse rshew=0.00345712 and fw_g=[0.0, 0.12, 0.08, 0.15, 0.11]. Generate the info string for L=8 accordingly.", "answers": "[{\"name\":\"gpaw_sphere_rshe_get_rshe_coefficient_info_string\",\"arguments\":{\"L\":8,\"included\":true,\"rshew\":0.00345712,\"fw_g\":[0.0,0.12,0.08,0.15,0.11]}}]"}
{"func_name": "gpaw_spherical_harmonics_nablarlYL", "func_desc": "Compute the Cartesian gradient (partial derivatives with respect to x, y and z) of a real solid spherical harmonic of degree L evaluated at the 3D Cartesian position R. This function is part of gpaw.spherical_harmonics and is used in the GPAW codebase (a DFT/PAW electronic-structure package) wherever analytic gradients of real solid spherical harmonics are required, for example when forming angular derivatives for atom-centered basis functions, PAW augmentation terms, multipole expansions, or other operations that require ∇Y_L(r) in Cartesian coordinates.", "tools": [{"function": {"description": "Compute the Cartesian gradient (partial derivatives with respect to x, y and z) of a real solid spherical harmonic of degree L evaluated at the 3D Cartesian position R. This function is part of gpaw.spherical_harmonics and is used in the GPAW codebase (a DFT/PAW electronic-structure package) wherever analytic gradients of real solid spherical harmonics are required, for example when forming angular derivatives for atom-centered basis functions, PAW augmentation terms, multipole expansions, or other operations that require ∇Y_L(r) in Cartesian coordinates.\n", "name": "gpaw_spherical_harmonics_nablarlYL", "parameters": {"properties": {"L": {"type": "integer", "description": "Angular momentum degree index identifying which real solid spherical harmonic to differentiate. In the GPAW spherical harmonics implementation this index is used to index the module-level sequence YL (i.e., terms = YL[L]). Each entry YL[L] is expected to be an iterable of terms describing a polynomial representation of the solid harmonic: each term is a 2-tuple (N, (powx, powy, powz)) where N is a numeric coefficient and powx, powy, powz are integer exponents for the Cartesian monomial x**powx * y**powy * z**powz that contributes to the solid harmonic of degree L. The value of L therefore selects the set of polynomial terms that define the real solid spherical harmonic used in GPAW calculations.", "default": ""}, "R": {"type": "any", "description": "A length-3 tuple representing the Cartesian coordinates (x, y, z) where the gradient is evaluated. The elements should be numeric scalars (e.g., Python int or float) representing the position relative to the origin or an atomic center in the GPAW coordinate conventions. The function unpacks R as x, y, z and raises a Python ValueError if R cannot be unpacked into three values.", "default": ""}}, "required": ["L", "R"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW-style PAW augmentation regression where I need the Cartesian gradient ∇Y_L(r) of a real solid spherical harmonic at a test point relative to an atom-centered origin. Initially I was going to use the requested angular momentum degree, but I realized this particular test is meant to validate the *lowest odd-degree nontrivial case* that still exercises the full x/y/z coupling in the Cartesian derivatives without introducing the additional nodal complexity of higher odd degrees. So: compute the Cartesian gradient (∂/∂x, ∂/∂y, ∂/∂z) of the real solid spherical harmonic for that degree at the position R = (0.25, -0.40, 1.10) Å. Use the same GPAW convention as gpaw.spherical_harmonics for the real solid harmonic and its analytic gradient.", "answers": "[{\"name\": \"gpaw_spherical_harmonics_nablarlYL\", \"arguments\": {\"L\": 1, \"R\": [0.25, -0.4, 1.1]}}]"}
{"func_name": "gpaw_tetrahedron_bja1b", "func_desc": "gpaw.tetrahedron.bja1b computes the Blöchl–Jepsen–Andersen (BJA) tetrahedron coefficients (Eq. (B2)–(B5) in the referenced paper) used in GPAW's tetrahedron integration routines. This function implements the algebraic form of the BJA weights: it computes the common prefactor C and the four vertex weights w1, w2, w3 and w4 from four input energy arrays corresponding to the tetrahedron vertex energies. In the context of GPAW (a density-functional theory code), these weights are used to distribute contributions (for example to density of states, occupations, or integrals over the Brillouin zone) from a tetrahedron to its four vertices.", "tools": [{"function": {"description": "gpaw.tetrahedron.bja1b computes the Blöchl–Jepsen–Andersen (BJA) tetrahedron coefficients (Eq. (B2)–(B5) in the referenced paper) used in GPAW's tetrahedron integration routines. This function implements the algebraic form of the BJA weights: it computes the common prefactor C and the four vertex weights w1, w2, w3 and w4 from four input energy arrays corresponding to the tetrahedron vertex energies. In the context of GPAW (a density-functional theory code), these weights are used to distribute contributions (for example to density of states, occupations, or integrals over the Brillouin zone) from a tetrahedron to its four vertices.\n", "name": "gpaw_tetrahedron_bja1b", "parameters": {"properties": {"e1": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energy values E1 at the first vertex of each tetrahedron. Practically, e1 contains the energy value (in the same energy units used by the caller) associated with the first corner of many tetrahedra or many k-point/tetrahedron combinations. This argument must be a numpy.ndarray and is used elementwise in the algebraic formula for the BJA prefactor C = -0.25 * e1**3 / ((e2 - e1)*(e3 - e1)*(e4 - e1)). All arithmetic is performed elementwise; e1 is expected to be broadcast-compatible with e2, e3 and e4 and typically has shape (N,) where N is the number of tetrahedra or k-point samples.", "default": ""}, "e2": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energy values E2 at the second vertex of each tetrahedron. e2 must be a numpy.ndarray with the same length/shape as e1 (or broadcastable to that shape). It appears in the denominators of the prefactor and in the intermediate weight formulas w2 = -C * e1 / (e2 - e1). If e2 equals e1 for any element, this will produce a division-by-zero for that element (see Failure modes).", "default": ""}, "e3": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energy values E3 at the third vertex of each tetrahedron. e3 must be a numpy.ndarray and likewise broadcast-compatible with e1. It is used in the denominator terms and in computing w3 = -C * e1 / (e3 - e1).", "default": ""}, "e4": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energy values E4 at the fourth vertex of each tetrahedron. e4 must be a numpy.ndarray and broadcast-compatible with e1. It is used in the denominator terms and in computing w4 = -C * e1 / (e4 - e1).", "default": ""}}, "required": ["e1", "e2", "e3", "e4"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW tetrahedron integration sanity-check on a tiny k-point batch, but I realized I can’t blindly feed the vertex energies in the order they came out of the mesh generator because the BJA algebra assumes the four vertex energies are ordered (monotone) within each tetrahedron. So: take the same 3 tetrahedra worth of vertex energies (in eV) as below, but for each tetrahedron first reorder the four vertex energies from lowest to highest and then pass the reordered sets into gpaw.tetrahedron.bja1b to compute the common prefactor C and the four vertex weights. Use the energies exactly as given (just sorted per tetrahedron): e1=[-0.20, -0.10, -0.05], e2=[0.15, 0.05, 0.10], e3=[0.40, 0.20, 0.30], e4=[0.90, 0.60, 0.80].", "answers": "[{\"name\": \"gpaw_tetrahedron_bja1b\", \"arguments\": {\"e1\": [-0.2, -0.1, -0.05], \"e2\": [0.15, 0.05, 0.1], \"e3\": [0.4, 0.2, 0.3], \"e4\": [0.9, 0.6, 0.8]}}]"}
{"func_name": "gpaw_tetrahedron_bja3", "func_desc": "gpaw.tetrahedron.bja3 computes coefficients from Equations (A4) and (C4) of Blöchl, Jepsen and Andersen used in the tetrahedron integration scheme in density-functional theory (DFT) calculations performed by GPAW. The function produces one scalar and one array of coefficients that are used to form tetrahedron integration weights (for example when integrating the density of states or occupancy-related quantities over tetrahedra in reciprocal space).\n    \n    This implementation follows the algebra in the cited BJA equations: it forms the reciprocal of the product (e4 - e1)*(e4 - e2)*(e4 - e3) elementwise and then returns a scalar formed as len(e1) - x.dot(e4**3) together with an array 3*x*e4**2. These quantities are directly used by the GPAW tetrahedron routines to assemble integrals over energy within a tetrahedron.", "tools": [{"function": {"description": "gpaw.tetrahedron.bja3 computes coefficients from Equations (A4) and (C4) of Blöchl, Jepsen and Andersen used in the tetrahedron integration scheme in density-functional theory (DFT) calculations performed by GPAW. The function produces one scalar and one array of coefficients that are used to form tetrahedron integration weights (for example when integrating the density of states or occupancy-related quantities over tetrahedra in reciprocal space).\n\nThis implementation follows the algebra in the cited BJA equations: it forms the reciprocal of the product (e4 - e1)*(e4 - e2)*(e4 - e3) elementwise and then returns a scalar formed as len(e1) - x.dot(e4**3) together with an array 3*x*e4**2. These quantities are directly used by the GPAW tetrahedron routines to assemble integrals over energy within a tetrahedron.", "name": "gpaw_tetrahedron_bja3", "parameters": {"properties": {"e1": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energies (eigenvalues) corresponding to corner 1 of each tetrahedron. In GPAW's tetrahedron integration context, this typically represents the energy values at a set of samples (e.g., k-points or bands) associated with the first vertex of each tetrahedron. The array length determines the number of samples; len(e1) is used in the scalar computation. e1 must have the same shape as e2, e3 and e4.", "default": ""}, "e2": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energies corresponding to corner 2 of each tetrahedron. Practically, e2 is the energy at the second vertex for the same set of samples as e1. It must be the same shape and ordering as e1 so that elementwise operations produce meaningful per-sample coefficients.", "default": ""}, "e3": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energies corresponding to corner 3 of each tetrahedron. Like e1 and e2, e3 provides the third-vertex energies for the same samples and must match their shape and ordering.", "default": ""}, "e4": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of energies corresponding to corner 4 of each tetrahedron. e4 is used as the distinguished vertex in the algebra of Equations (A4) and (C4); it appears both in the denominator product (e4 - e1)*(e4 - e2)*(e4 - e3) and in powers e4**2 and e4**3. e4 must match the shape of e1, e2 and e3.", "default": ""}}, "required": ["e1", "e2", "e3", "e4"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a GPAW tetrahedron DOS post-processing script and need the BJA3 coefficients, but I just realized one of my tetrahedra is numerically unsafe: in the tetrahedron integration workflow, whenever any two corner energies in a tetrahedron are effectively degenerate (within ~1e-6 eV), the (e4−e1)(e4−e2)(e4−e3) denominator in the BJA algebra blows up and we must drop that tetrahedron from the coefficient build (the downstream code treats dropped tetrahedra as zero-weight).\n\nStart from these 4 candidate tetrahedra corner energies (eV):\n- e1=[-5.2, -1.1, 0.3, 2.4]\n- e2=[-4.8, -0.9, 0.5, 2.8]\n- e3=[-4.9, -1.0, 0.7, 3.1]\n- e4=[-4.5, -0.6, 1.0, 3.6]\n\nBefore calling gpaw.tetrahedron.bja3, identify and exclude the tetrahedron that has a near-degeneracy at the top corner (i.e., the one where the difference between e4 and one of {e1,e2,e3} is exactly zero in the given inputs). Then compute BJA3 coefficients only for the remaining tetrahedra, preserving their original order among the retained ones. Return the scalar and array outputs from bja3 for that filtered set.", "answers": "[{\"name\": \"gpaw_tetrahedron_bja3\", \"arguments\": {\"e1\": [-5.2, -1.1, 0.3], \"e2\": [-4.8, -0.9, 0.5], \"e3\": [-4.9, -1.0, 0.7], \"e4\": [-4.5, -0.6, 1.0]}}]"}
{"func_name": "gpaw_tetrahedron_triangulate_submesh", "func_desc": "gpaw.tetrahedron.triangulate_submesh: Compute the fixed triangulation of a hexahedral cell (the unit cube mapped by the provided real-space cell matrix) into six tetrahedra and return the Cartesian coordinates of the tetrahedra vertices.\n    \n    This function is used inside GPAW's tetrahedron-related routines to produce a stable decomposition of a parallelepiped cell (defined by the real-space cell matrix rcell_cv) into six non-degenerate tetrahedra. The decomposition is obtained by Delaunay triangulation of the eight corner points of the unit cube after mapping them to Cartesian coordinates with rcell_cv, followed by removal of any zero-volume (degenerate) tetrahedra. The resulting array of tetrahedra can be used for mesh generation, numerical integration, interpolation, or any other algorithm in GPAW that requires a tetrahedral subdivision of a simulation cell.", "tools": [{"function": {"description": "gpaw.tetrahedron.triangulate_submesh: Compute the fixed triangulation of a hexahedral cell (the unit cube mapped by the provided real-space cell matrix) into six tetrahedra and return the Cartesian coordinates of the tetrahedra vertices.\n\nThis function is used inside GPAW's tetrahedron-related routines to produce a stable decomposition of a parallelepiped cell (defined by the real-space cell matrix rcell_cv) into six non-degenerate tetrahedra. The decomposition is obtained by Delaunay triangulation of the eight corner points of the unit cube after mapping them to Cartesian coordinates with rcell_cv, followed by removal of any zero-volume (degenerate) tetrahedra. The resulting array of tetrahedra can be used for mesh generation, numerical integration, interpolation, or any other algorithm in GPAW that requires a tetrahedral subdivision of a simulation cell.", "name": "gpaw_tetrahedron_triangulate_submesh", "parameters": {"properties": {"rcell_cv": {"type": "array", "items": {"type": "any"}, "description": "A 2-D numeric array representing the real-space cell mapping used to convert unit-cube (fractional) coordinates [A, B, C] with A,B,C in {0,1} to Cartesian coordinates via row-vector multiplication: cart = [A, B, C].dot(rcell_cv). In practice rcell_cv is expected to be a 3x3 array (shape (3, 3)) containing the cell vectors needed to map the unit cube into the simulation cell in Cartesian space. Supplying an array with incompatible shape or non-numeric entries will raise an exception when the function attempts the matrix multiplication or Delaunay triangulation.", "default": ""}}, "required": ["rcell_cv"], "type": "any"}}, "type": "function"}], "query": "I was going to just triangulate the triclinic cell using the lattice vectors a=(4.2, 0.0, 0.0) Å, b=(1.1, 3.8, 0.0) Å, and c=(0.6, 1.2, 5.5) Å to get the standard 6-tetrahedra subdivision for tetrahedron-based Brillouin-zone-style integration in real space. But wait—our downstream integration step assumes a right-handed cell orientation (positive determinant of the real-space cell matrix), and I just noticed this dataset’s convention might make it left-handed depending on how the vectors were ordered in the input deck.\n\nSo: before calling the triangulation, check the handedness of the 3×3 cell matrix formed by stacking the three lattice vectors. If it is left-handed, fix it by swapping the last two lattice vectors (keep the first one unchanged) so the determinant becomes positive. Then compute the fixed 6-tetrahedra subdivision of the mapped unit-cube cell and return the Cartesian coordinates of the tetrahedra vertices for numerical integration.", "answers": "[{\"name\":\"gpaw_tetrahedron_triangulate_submesh\",\"arguments\":{\"rcell_cv\":[[4.2,0.0,0.0],[1.1,3.8,0.0],[0.6,1.2,5.5]]}}]"}
{"func_name": "gpaw_tetrahedron_weights", "func_desc": "Calculate occupation numbers using the tetrahedron integration scheme used in GPAW for Brillouin-zone integration and density-functional theory (DFT) calculations.", "tools": [{"function": {"description": "Calculate occupation numbers using the tetrahedron integration scheme used in GPAW for Brillouin-zone integration and density-functional theory (DFT) calculations.\n", "name": "gpaw_tetrahedron_weights", "parameters": {"properties": {"eig_in": {"type": "array", "items": {"type": "any"}, "description": "2-D array of single-particle eigenvalues (energies) used to determine occupations. The first axis indexes the electronic state positions that are referenced by the tetrahedron mapping in i_ktq (typically combined k-point / spin indices), and the second axis indexes band indices. This function treats eigenvalues less than 0.0 as occupied when counting fully occupied bands per k-point. The practical role of eig_in in the GPAW tetrahedron method is to provide the band-energy spectrum from which partial occupancies are computed for use in charge-density and total-energy calculations.", "default": ""}, "i_ktq": {"type": "array", "items": {"type": "any"}, "description": "3-D integer array that maps tetrahedra to the indices of eig_in. The expected layout (as used by this routine) is (N_k, 6, 4), where N_k is the number of k-point-like entries and the second dimension enumerates the six tetrahedra associated with each k-like index and the last dimension lists the four vertices (indices into the first axis of eig_in) of each tetrahedron. This mapping is used to gather eigenvalues at tetrahedron vertices, compute tetrahedron-specific weight distributions, and then scatter the resulting fractional occupations back to the corresponding rows of the output occupancy array. If i_ktq has incompatible shape or contains out-of-range indices, the function will raise standard indexing errors.", "default": ""}, "improved": {"type": "boolean", "description": "If False (the default), use the standard tetrahedron weight formulas to compute fractional occupations for bands that are partially occupied across tetrahedra. If True, apply the additional improved correction terms computed by the auxiliary routines (used in GPAW to reduce integration error) that depend on derivatives of the weight functions; this adjusts the tetrahedron weights with higher-order contributions. The boolean controls whether those extra correction contributions are included in the returned occupation numbers.", "default": false}}, "required": ["eig_in", "i_ktq", "improved"], "type": "any"}}, "type": "function"}], "query": "I’m re-running a toy GPAW Brillouin-zone integration regression test for tetrahedron occupations (4 k-like points, 3 bands) using the eigenvalue table (eV)\n\nE = [[-1.20,-0.30,0.50],\n     [-0.90, 0.10,0.80],\n     [-0.60, 0.00,1.10],\n     [-0.20, 0.40,1.50]]\n\nand the tetrahedron mapping i_ktq with shape (1,6,4):\n[[[0,1,2,3],\n  [0,1,3,2],\n  [1,2,3,0],\n  [0,2,3,1],\n  [0,1,2,3],\n  [1,0,2,3]]].\n\nInitially I was going to compute the occupation weights with the “improved correction terms”, but then I realized this particular regression is meant to reproduce the legacy reference numbers that were generated before those corrections existed. So please compute the tetrahedron occupation weights using the same scheme as the original (i.e., *not* the corrected/improved variant), while keeping all other inputs exactly as above.", "answers": "[{\"name\":\"gpaw_tetrahedron_weights\",\"arguments\":{\"eig_in\":[[-1.2,-0.3,0.5],[-0.9,0.1,0.8],[-0.6,0.0,1.1],[-0.2,0.4,1.5]],\"i_ktq\":[[[0,1,2,3],[0,1,3,2],[1,2,3,0],[0,2,3,1],[0,1,2,3],[1,0,2,3]]],\"improved\":false}}]"}
{"func_name": "gpaw_unfold_find_K_from_k", "func_desc": "Gets a k vector in scaled (fractional) coordinates and returns a mapped K vector and the unfolding integer vector G, both in scaled coordinates. This function is used in GPAW's band-structure unfolding routines to map k-points between cells (for example between a primitive cell and a supercell) and to record which reciprocal-lattice integer vector was added or subtracted during the mapping.", "tools": [{"function": {"description": "Gets a k vector in scaled (fractional) coordinates and returns a mapped K vector and the unfolding integer vector G, both in scaled coordinates. This function is used in GPAW's band-structure unfolding routines to map k-points between cells (for example between a primitive cell and a supercell) and to record which reciprocal-lattice integer vector was added or subtracted during the mapping.\n", "name": "gpaw_unfold_find_K_from_k", "parameters": {"properties": {"k": {"type": "array", "items": {"type": "any"}, "description": "A length-3 array giving a k-point in scaled (fractional) coordinates. In the GPAW unfolding context, each component of k is the coordinate along a reciprocal-lattice basis (i.e., a fractional reciprocal coordinate). The function expects k to have three components because it computes KG = M.dot(k) and then processes the three components individually. Supplying an array with a different shape will cause numpy.dot or the subsequent indexing/loops to raise an error.", "default": ""}, "M": {"type": "array", "items": {"type": "any"}, "description": "A 3x3 numeric matrix used to transform the input k into the target scaled coordinate system (KG = M.dot(k)). In unfolding workflows this matrix typically represents the integer transformation between reciprocal-lattice bases (for example, the mapping from primitive to supercell reciprocal coordinates). M must be numeric and conformable with k for matrix multiplication (normally shape (3, 3)); incorrect shapes or non-numeric entries will lead to numpy errors.", "default": ""}}, "required": ["k", "M"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW band-unfolding test where the primitive-cell k-point is k = (0.25, -0.10, 0.60) in scaled coordinates, and the supercell is generated by an integer transformation matrix that doubles the first two lattice directions and leaves the third unchanged (same structure as a diagonal matrix with the last diagonal element equal to the identity direction). I was going to map k directly, but then I realized the unfolding routine expects k wrapped into the first Brillouin zone *before* the mapping step, i.e., each fractional component must be shifted by an integer so it lies in [0, 1). After doing that wrapping, map the resulting k into the supercell reciprocal basis and return (i) the mapped K vector and (ii) the unfolding integer vector G (the reciprocal-lattice integer shift used by the mapping), both in scaled coordinates.", "answers": "[{\"name\": \"gpaw_unfold_find_K_from_k\", \"arguments\": {\"k\": [0.25, 0.9, 0.6], \"M\": [[2, 0, 0], [0, 2, 0], [0, 0, 1]]}}]"}
{"func_name": "gpaw_unfold_make_colormap", "func_desc": "Create a matplotlib LinearSegmentedColormap tailored for GPAW unfolding plots.\n    \n    This function constructs a custom colormap named 'mymap' that is used by gpaw.unfold plotting utilities such as plot_spectral and plot_band_structure. The colormap maps normalized scalar values in the range [0.0, 1.0] to RGBA colors by defining per-channel control points (position, left-value, right-value) at normalized positions 0.0, 0.25, 0.5 and 1.0. The selected main_color selects which RGB channel is emphasized and determines the per-channel control-point dictionary (cdict). The 'green' variant additionally defines an 'alpha' channel to introduce a varying transparency across the colormap. This function performs an import from matplotlib.colors and returns a LinearSegmentedColormap instance suitable for use with matplotlib plotting routines that visualize electronic densities, spectral functions, or band weights produced by GPAW.", "tools": [{"function": {"description": "Create a matplotlib LinearSegmentedColormap tailored for GPAW unfolding plots.\n\nThis function constructs a custom colormap named 'mymap' that is used by gpaw.unfold plotting utilities such as plot_spectral and plot_band_structure. The colormap maps normalized scalar values in the range [0.0, 1.0] to RGBA colors by defining per-channel control points (position, left-value, right-value) at normalized positions 0.0, 0.25, 0.5 and 1.0. The selected main_color selects which RGB channel is emphasized and determines the per-channel control-point dictionary (cdict). The 'green' variant additionally defines an 'alpha' channel to introduce a varying transparency across the colormap. This function performs an import from matplotlib.colors and returns a LinearSegmentedColormap instance suitable for use with matplotlib plotting routines that visualize electronic densities, spectral functions, or band weights produced by GPAW.", "name": "gpaw_unfold_make_colormap", "parameters": {"properties": {"main_color": {"type": "string", "description": "The primary color to emphasize in the generated colormap. Accepted literal values are 'blue', 'red', and 'green' as implemented in the source code. Each value selects a distinct color dictionary (cdict) that explicitly sets control points for the 'red', 'green', and 'blue' channels; the 'green' choice also adds an 'alpha' channel to control transparency. The control points are specified at normalized positions 0.0, 0.25, 0.5 and 1.0 and follow matplotlib's LinearSegmentedColormap convention (position, left-value, right-value). This parameter is required; there is no default. If a value other than the documented literals is provided (or if main_color is not a string), the function will not define cdict and will fail when attempting to construct the colormap.", "default": ""}}, "required": ["main_color"], "type": "any"}}, "type": "function"}], "query": "I’m preparing GPAW unfolded band-structure plots (using gpaw.unfold’s plot_spectral/plot_band_structure), and I initially thought I should use the green-emphasized colormap because I want the weights to *fade in* via transparency. But I just realized a constraint: my downstream figure pipeline composites layers assuming the colormap does **not** carry its own varying alpha channel (it treats alpha as an independent overlay and double-applies it). So I still want the same style of channel emphasis as the option that adds transparency, but I need the variant that keeps opacity constant (i.e., only RGB varies across control points at 0.0, 0.25, 0.5, 1.0). Please construct and return the appropriate matplotlib LinearSegmentedColormap ('mymap') for that choice so it’s compatible with gpaw.unfold plotting.", "answers": "[{\"name\":\"gpaw_unfold_make_colormap\",\"arguments\":{\"main_color\":\"blue\"}}]"}
{"func_name": "gpaw_unfold_plot_band_structure", "func_desc": "Plot band structure using projection weights P_mK and show each band-state as a filled circle whose color and size encode projection weight information used in unfolding analyses in density-functional theory (DFT) calculations performed with GPAW.", "tools": [{"function": {"description": "Plot band structure using projection weights P_mK and show each band-state as a filled circle whose color and size encode projection weight information used in unfolding analyses in density-functional theory (DFT) calculations performed with GPAW.\n", "name": "gpaw_unfold_plot_band_structure", "parameters": {"properties": {"e_mK": {"type": "array", "items": {"type": "any"}, "description": "Array of single-particle energies in electronvolts for all bands and k-points. From the source code usage, this is expected to be a 2-D array shaped (n_bands, n_kpoints) where each row corresponds to a band index m and each column to a k-point K along a path in reciprocal space. In the GPAW/ASE DFT context this array contains the band energies computed for a (possibly unfolded) band-structure calculation and is plotted on the vertical (energy) axis.", "default": ""}, "P_mK": {"type": "array", "items": {"type": "any"}, "description": "Array of projection weights used to color the scatter points. This must have the same shape as e_mK (n_bands, n_kpoints). Values are interpreted between 0.0 and 1.0 by the plotting routine (vmin=0., vmax=1.) and are mapped to colors using a colormap created from the color argument. In unfolding workflows P_mK typically encodes how strongly each supercell band-state projects onto a chosen primitive-cell character.", "default": ""}, "x": {"type": "array", "items": {"type": "any"}, "description": "1-D array of positions along the k-path (cumulative k-point distance) to be used as the horizontal axis. Its length must equal n_kpoints (the number of columns in e_mK and P_mK). The function tiles this array for all bands to position every energy point horizontally.", "default": ""}, "X": {"type": "array", "items": {"type": "any"}, "description": "List of float positions along the same k-path used to draw vertical separators and to set xtick locations. Typical usage in band-structure plotting is to pass the cumulative distances of high-symmetry k-points; X[1:-1] are drawn as vertical guide lines. The list length must match the length of points_name for xtick labeling.", "default": ""}, "points_name": {"type": "array", "items": {"type": "any"}, "description": "List of strings (or values convertible to strings) giving tick labels for the positions in X. In practical GPAW plotting this is the list of high-symmetry k-point labels (for example ['G', 'X', 'M', 'G']), used to annotate the horizontal axis. The number of entries must equal len(X).", "default": ""}, "weights_mK": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional array of the same shape as e_mK and P_mK providing an additional scaling factor for marker sizes. If None (the default), the function uses a copy of P_mK for marker sizes. If provided, the array is multiplied in-place by P_mK (weights_mK *= P_mK.copy()), so the caller's array will be modified by this scaling operation. Marker sizes are computed as 20. * weights_mK and therefore scale linearly with these values. This argument allows separating a size-modulating weight from the color-mapping weight P_mK, e.g., when combining projection amplitudes with other per-state metrics.", "default": null}, "color": {"type": "string", "description": "Name of the base color passed to make_colormap to create a matplotlib colormap used for mapping P_mK values to colors. The default is \"red\". The function delegates colormap construction to make_colormap(color) and then uses that colormap with vmin=0.0 and vmax=1.0 to visualize P_mK.", "default": "red"}, "fit": {"type": "boolean", "description": "Boolean flag included for API compatibility. In the current implementation this parameter is accepted but unused (default True). It suggests whether a smoothing/fit of band lines should be performed, but the function does not implement any fitting behavior; setting it has no effect.", "default": true}, "nfit": {"type": "integer", "description": "Integer included for API compatibility (default 200). In the current implementation this parameter is accepted but unused. It is commonly used in other plotting utilities to set the number of interpolation points for fitted/smoothed band lines; here it has no effect.", "default": 200}}, "required": ["e_mK", "P_mK", "x", "X", "points_name", "weights_mK", "nfit", "fit", "color"], "type": "any"}}, "type": "function"}], "query": "I need to plot an unfolded GPAW band structure for a nominal 4-band run along the high-symmetry path Γ–X–M–Γ, using the provided e_mK and P_mK arrays (same shapes as before) and a separate weights_mK array to scale the marker sizes (think spectral weights).\n\nInitially I was going to keep the blue colormap, but I realized the figure is going into a manuscript where blue is reserved for a different dataset, so switch to the most standard perceptually-uniform sequential colormap that remains legible when printed in grayscale.\n\nAlso: the unfolding postprocessing sometimes emits tiny negative projection weights (numerical noise). Before plotting, clip both the projection weights and the size-scaling weights to the physically meaningful interval, and then use the clipped values for color/size.\n\nUse the same k-path cumulative distances x=[0.0,0.5,1.0,1.5,2.0]. For the symmetry-point positions, I accidentally included the starting point twice when copying: take the unique symmetry-point positions in ascending order while preserving the intended labels [\"Γ\",\"X\",\"M\",\"Γ\"]. Keep a smooth fitted guide curve as in the earlier workflow, but sample the fit densely enough that the number of fit points equals the number of k-points times 50.\n\nData:\n- e_mK = [[-5.2, -4.9, -4.6, -4.4, -4.1], [-1.8, -1.6, -1.5, -1.3, -1.1], [0.4, 0.6, 0.7, 0.9, 1.0], [3.1, 3.3, 3.5, 3.7, 3.9]]\n- P_mK = [[0.95, 0.82, 0.6, 0.4, 0.2], [0.1, 0.25, 0.45, 0.65, 0.8], [0.05, 0.15, 0.35, 0.55, 0.75], [0.3, 0.4, 0.5, 0.6, 0.7]]\n- weights_mK = [[1.0, 0.9, 0.8, 0.7, 0.6], [0.6, 0.7, 0.8, 0.9, 1.0], [0.2, 0.3, 0.4, 0.5, 0.6], [0.4, 0.5, 0.6, 0.7, 0.8]]\n- x = [0.0, 0.5, 1.0, 1.5, 2.0]\n- (copied-with-dup) symmetry positions = [0.0, 0.0, 1.0, 1.5, 2.0]\n- symmetry labels = [\"Γ\",\"X\",\"M\",\"Γ\"]", "answers": "[{\"name\": \"gpaw_unfold_plot_band_structure\", \"arguments\": {\"e_mK\": [[-5.2, -4.9, -4.6, -4.4, -4.1], [-1.8, -1.6, -1.5, -1.3, -1.1], [0.4, 0.6, 0.7, 0.9, 1.0], [3.1, 3.3, 3.5, 3.7, 3.9]], \"P_mK\": [[0.95, 0.82, 0.6, 0.4, 0.2], [0.1, 0.25, 0.45, 0.65, 0.8], [0.05, 0.15, 0.35, 0.55, 0.75], [0.3, 0.4, 0.5, 0.6, 0.7]], \"x\": [0.0, 0.5, 1.0, 1.5, 2.0], \"X\": [0.0, 1.0, 1.5, 2.0], \"points_name\": [\"Γ\", \"X\", \"M\", \"Γ\"], \"weights_mK\": [[1.0, 0.9, 0.8, 0.7, 0.6], [0.6, 0.7, 0.8, 0.9, 1.0], [0.2, 0.3, 0.4, 0.5, 0.6], [0.4, 0.5, 0.6, 0.7, 0.8]], \"color\": \"viridis\", \"fit\": true, \"nfit\": 250}}]"}
{"func_name": "gpaw_utilities_blas_gemmdot", "func_desc": "Matrix multiplication using BLAS GEMM for GPAW linear-algebra operations.\n    \n    This function performs the operation out <- alpha * a . b + beta * out using a BLAS\n    gemm routine (via the internal mmmx wrapper) and is intended for dense linear\n    algebra operations that occur throughout GPAW (for example, multiplying blocks\n    of arrays that represent coefficients, density matrices, or intermediate\n    operators in DFT calculations). The implementation maps inputs with arbitrary\n    leading dimensions to 2D matrices, calls a high-performance GEMM implementation,\n    and then reshapes the result back to the appropriate multi-dimensional shape\n    expected by GPAW code paths.", "tools": [{"function": {"description": "Matrix multiplication using BLAS GEMM for GPAW linear-algebra operations.\n\nThis function performs the operation out <- alpha * a . b + beta * out using a BLAS\ngemm routine (via the internal mmmx wrapper) and is intended for dense linear\nalgebra operations that occur throughout GPAW (for example, multiplying blocks\nof arrays that represent coefficients, density matrices, or intermediate\noperators in DFT calculations). The implementation maps inputs with arbitrary\nleading dimensions to 2D matrices, calls a high-performance GEMM implementation,\nand then reshapes the result back to the appropriate multi-dimensional shape\nexpected by GPAW code paths.", "name": "gpaw_utilities_blas_gemmdot", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "Left operand array. Must be a NumPy array. The matrix\nmultiplication sums over the last axis of a. For example, if a has\nshape (..., M, K) it will be mapped to a 2D matrix with shape\n(M_prod, K) before calling GEMM, where M_prod is the product of the\nleading dimensions of a prior to the last axis. The dtype of a\ndetermines the dtype of any newly allocated output array when out is\nNone. In the special vector-vector case (a.ndim == 1 and b.ndim == 1)\na is treated as a 1D vector and a scalar result is returned (see\nReturns).", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "Right operand array. Must be a NumPy array. The index\nused for the product-sum depends on the trans parameter: if trans\n== 'n' the sum is over the first axis of b; if trans == 't' or\ntrans == 'c' the sum is over the last axis of b. For example, if\ntrans == 'n' and b has shape (K, N, ...) it will be mapped so that\nK matches a's last axis and the result has trailing dimensions\ncorresponding to b's remaining axes. The array is reshaped internally\nto a 2D matrix before calling GEMM.", "default": ""}, "alpha": {"type": "float", "description": "Scalar multiplier for the product term. Defaults to 1.0.\nThe product a . b is computed and multiplied by alpha before being\nadded to the beta-scaled out. This follows the standard BLAS GEMM\ninterface used for scaling the product term.", "default": 1.0}, "beta": {"type": "float", "description": "Scalar multiplier for the existing out array. Defaults to\n1.0. When out is provided, its current contents are scaled by beta\nand then the scaled product (alpha * a . b) is added. If out is None,\na zero array is created and the beta factor has no effect other than\nthe documented semantics.", "default": 1.0}, "out": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Output array to store the result. If provided, it\nwill be reshaped and overwritten in place according to the required\n2D GEMM shape, and then reshaped back to the multidimensional result\nshape before being returned. If out is None (the default), a new\nNumPy array of zeros with the appropriate shape and dtype a.dtype is\nallocated. Note: in the 1D-1D vector-vector special case an out value\nmust not be provided (the implementation asserts out is None) and a\nscalar is returned instead.", "default": null}, "trans": {"type": "string", "description": "Specifies whether b is used transposed/conjugate-transposed\nor not. Accepted values are 'n' for no transpose, 't' for transpose,\nand 'c' for conjugate-transpose. If trans == 'n' the contraction is\nover the first axis of b; if trans == 't' or 'c' the contraction is\nover the last axis of b. If trans == 'c' the complex conjugate of b\nis used in the multiplication. Defaults to \"n\".", "default": "n"}}, "required": ["a", "b", "out", "trans", "alpha", "beta"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a GPAW dense linear-algebra micro-kernel where the coefficient block A is 2×3 and the operator block B is stored in the usual 3×2 layout. Initially I was going to apply B exactly as stored (no transpose), but I just realized that in this code path the operator is generated in the opposite convention (it’s meant to be applied as its transpose relative to how it’s laid out in memory). So keep A as-is, but apply B in the orientation that swaps its axes before multiplying. Then do the BLAS-style update out ← 0.8·A·(that oriented B) + 0.2·out, starting from the existing 2×2 out = [[1.0, 0.0],[0.0, 1.0]]. Use the same numeric A and B values as before: A = [[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]] and B = [[1.0, 0.0],[0.0, 1.0],[1.0, 1.0]].", "answers": "[{\"name\": \"gpaw_utilities_blas_gemmdot\", \"arguments\": {\"a\": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], \"b\": [[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], \"alpha\": 0.8, \"beta\": 0.2, \"out\": [[1.0, 0.0], [0.0, 1.0]], \"trans\": \"t\"}}]"}
{"func_name": "gpaw_utilities_blas_to2d", "func_desc": "gpaw.utilities.blas.to2d returns a 2D NumPy array view (or copy when necessary) by preserving the original array's first axis and flattening all remaining axes into the second axis. This utility is used in GPAW's BLAS-related code to present multidimensional data (for example, blocks of wavefunction coefficients, density matrices, or batched vectors) as 2D matrices suitable for BLAS/ScaLAPACK routines and other linear-algebra operations required by the DFT calculations described in the README.", "tools": [{"function": {"description": "gpaw.utilities.blas.to2d returns a 2D NumPy array view (or copy when necessary) by preserving the original array's first axis and flattening all remaining axes into the second axis. This utility is used in GPAW's BLAS-related code to present multidimensional data (for example, blocks of wavefunction coefficients, density matrices, or batched vectors) as 2D matrices suitable for BLAS/ScaLAPACK routines and other linear-algebra operations required by the DFT calculations described in the README.\n", "name": "gpaw_utilities_blas_to2d", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "any"}, "description": "Input n-dimensional array to be reshaped for BLAS-style matrix operations. The function treats array.shape[0] as the leading dimension (number of rows) and combines all remaining axes array.shape[1:] into a single trailing dimension (number of columns). The input must be a NumPy ndarray and must have at least one dimension (ndim >= 1). Typical use in GPAW: convert arrays of shape (N, A, B, ...) representing N independent vectors/blocks each with A*B*... degrees of freedom into shape (N, A*B*...) so they can be passed to BLAS routines.", "default": ""}}, "required": ["array"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a GPAW BLAS call for a small batch of wavefunction-coefficient blocks. I initially thought I could just `reshape(2, 12)`, but I realized that one of the nested lists is a malformed/experimental artifact from a truncated I/O record: in the second k-point/spin block, the very last coefficient is a non-numeric placeholder. For the BLAS path I need a clean 2D matrix that preserves the first axis as rows and flattens all remaining axes into columns (row-major), but only after removing the bad coefficient so the remaining data form a consistent rectangular block. After dropping that single invalid trailing entry, run `gpaw.utilities.blas.to2d` on the repaired array and return the resulting 2D matrix.\n\nData (note: the final entry in the last row of the second block is invalid):\n[\n  [\n    [0.12, -0.05, 1.3, 0.0],\n    [0.7, 0.2, -0.1, 0.9],\n    [1.1, -0.4, 0.33, 0.08]\n  ],\n  [\n    [-0.6, 0.15, 0.0, 0.22],\n    [0.05, 1.25, -0.75, 0.4],\n    [0.9, 0.0, -0.2, \"NaN\"]\n  ]\n]\n\nImportant constraint: don’t replace the invalid value with 0 or an imputed number—remove that single trailing artifact so the block is rectangular again, then convert to 2D with `to2d`.", "answers": "[{\"name\":\"gpaw_utilities_blas_to2d\",\"arguments\":{\"array\":[[[0.12,-0.05,1.3,0.0],[0.7,0.2,-0.1,0.9],[1.1,-0.4,0.33,0.08]],[[-0.6,0.15,0.0,0.22],[0.05,1.25,-0.75,0.4],[0.9,0.0,-0.2]]]}}]"}
{"func_name": "gpaw_utilities_divrl", "func_desc": "gpaw.utilities.divrl: divide a radial-array by r**l with safe extrapolation to the origin.\n    \n    This utility is used in GPAW (a DFT code based on the PAW method and real-space/ radial grids) to convert a radial quantity that scales as r**l into a regular function at the origin by dividing by r**l on the nonzero radial grid points and estimating the value at the origin by a small-r extrapolation. In practical GPAW workflows this is useful for processing radial parts of partial waves, projector functions, augmentation quantities or any spherical-harmonic component that behaves like r**l times a smooth function near r = 0.", "tools": [{"function": {"description": "gpaw.utilities.divrl: divide a radial-array by r**l with safe extrapolation to the origin.\n\nThis utility is used in GPAW (a DFT code based on the PAW method and real-space/ radial grids) to convert a radial quantity that scales as r**l into a regular function at the origin by dividing by r**l on the nonzero radial grid points and estimating the value at the origin by a small-r extrapolation. In practical GPAW workflows this is useful for processing radial parts of partial waves, projector functions, augmentation quantities or any spherical-harmonic component that behaves like r**l times a smooth function near r = 0.", "name": "gpaw_utilities_divrl", "parameters": {"properties": {"a_g": {"type": "array", "items": {"type": "any"}, "description": "Array of radial values sampled on the radial grid r_g. This array is treated as the numerator to be divided by r**l on indices 1:; index 0 is treated as the origin and is not directly divided to avoid division-by-zero. The function returns a copy of this array with elementwise division applied (for l > 0) and with index 0 set by an extrapolation. The caller should provide a 1D numeric numpy.ndarray whose shape matches r_g; the dtype and shape are preserved in the returned array.", "default": ""}, "l": {"type": "integer", "description": "Integer angular-momentum-like exponent. When l > 0 the routine divides a_g[1:] by r_g[1:]**l and computes a_g[0] by an extrapolation that assumes the resulting function b(r) = a(r)/r**l has the small-r expansion b(r) ≈ A + B r**2. When l <= 0 the function makes no division and simply returns a copy of a_g. This parameter controls the physical operation of removing the r**l factor from radial quantities in GPAW workflows.", "default": ""}, "r_g": {"type": "array", "items": {"type": "any"}, "description": "Radial grid values corresponding to a_g. r_g[0] is interpreted as the origin (typically 0.0) and r_g[1:], r_g[2], ... are the positive grid points used for elementwise division and extrapolation. This must be a 1D numeric numpy.ndarray of the same length as a_g. The routine uses r_g[1:3] to compute the extrapolated value at the origin, so at least three grid points (len(r_g) >= 3) are required.", "default": ""}}, "required": ["a_g", "l", "r_g"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a single spherical-harmonic channel from a PAW augmentation dataset in a GPAW-like workflow. I originally thought this channel was the d-like one, so I was going to call `gpaw.utilities.divrl` with the corresponding angular index. But then I noticed the near-origin scaling: for the nonzero grid points, the ratio a(r)/r^2 is constant to numerical precision, while a(r)/r^3 is not. So the angular index must match the smallest power of r that makes a(r)/r^l smooth and finite at the origin.\n\nUse `gpaw.utilities.divrl` to divide safely and extrapolate the value at r=0.\n\nRadial grid and samples:\n- r_g = [0.0, 0.05, 0.10, 0.20, 0.40]\n- a_g = [0.0, 0.00025, 0.0010, 0.0040, 0.0160]\n\n(Please infer the correct l from the scaling constraint above rather than using my initial assumption.)", "answers": "[{\"name\": \"gpaw_utilities_divrl\", \"arguments\": {\"a_g\": [0.0, 0.00025, 0.001, 0.004, 0.016], \"l\": 2, \"r_g\": [0.0, 0.05, 0.1, 0.2, 0.4]}}]"}
{"func_name": "gpaw_utilities_dos_delta", "func_desc": "Return a normalized approximation to the Dirac delta function centered at x0 on a numerical grid.\n    \n    This function is used in GPAW's DOS (density of states) and spectral-broadening utilities to convert discrete eigenvalues into a smooth spectral distribution. Given a numerical grid x (for example an energy grid in eV when using ASE/GPAW energy units), delta(x, x0, width, mode) produces a normalized kernel (area = 1) located at x0 with a controllable broadening width and choice of functional form. The default mode 'Gauss' is the common Gaussian broadening used to smooth DOS; 'Lorentz' gives the Lorentzian (Cauchy) shape often used to model lifetime broadening.", "tools": [{"function": {"description": "Return a normalized approximation to the Dirac delta function centered at x0 on a numerical grid.\n\nThis function is used in GPAW's DOS (density of states) and spectral-broadening utilities to convert discrete eigenvalues into a smooth spectral distribution. Given a numerical grid x (for example an energy grid in eV when using ASE/GPAW energy units), delta(x, x0, width, mode) produces a normalized kernel (area = 1) located at x0 with a controllable broadening width and choice of functional form. The default mode 'Gauss' is the common Gaussian broadening used to smooth DOS; 'Lorentz' gives the Lorentzian (Cauchy) shape often used to model lifetime broadening.", "name": "gpaw_utilities_dos_delta", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Numerical grid where the kernel is evaluated. In DFT/DOS contexts this is typically an energy grid (e.g. in eV when energies are expressed in ASE/GPAW units). The returned array has the same shape as x. x is expected to be a numpy.ndarray; values that are NaN or Inf in x will propagate to the result.", "default": ""}, "x0": {"type": "float", "description": "Center position of the kernel on the same axis and in the same units as x (e.g. an eigenvalue in eV). This is the location of the approximated delta peak.", "default": ""}, "width": {"type": "float", "description": "Broadening parameter (positive float) controlling the kernel width, in the same units as x and x0. For Gaussian mode this parameter is the standard-width scaling used in the implementation so that the returned Gaussian is normalized to unit area; for Lorentz mode the implementation uses width so that the Lorentzian half-width at half-maximum equals width/2 and the kernel is normalized to unit area. The function does not validate width; width <= 0 will lead to division-by-zero or invalid values.", "default": ""}, "mode": {"type": "string", "description": "Shape of the broadening kernel. Accepted values implemented in the code are:\n'Gauss' — Gaussian kernel: exp(-((x - x0) / width)**2) / (sqrt(pi) * width). This is the default and yields a normalized Gaussian with unit area.\n'Lorentz' — Lorentzian (Cauchy) kernel: (2 / pi / width) / (((x - x0) / (width / 2))**2 + 1). In the implementation this form is algebraically equivalent to the standard normalized Lorentzian with half-width at half-maximum = width/2.", "default": "Gauss"}}, "required": ["x", "x0", "width", "mode"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a GPAW DOS where a few quasi-bound states sit close to the Fermi level, so I started by planning to apply lifetime broadening on a coarse energy grid. Use an energy grid spanning −5.0 eV to 5.0 eV in 0.5 eV steps and place a normalized delta-like peak at 1.2 eV. I was going to use the lifetime-broadening lineshape, but then I realized I need the kernel to have noticeably shorter tails to avoid bleeding spectral weight into the band gap on this coarse grid. Keep the same numerical broadening parameter (0.4 eV) and switch to the tails-suppressing lineshape instead, while preserving normalization (area = 1).", "answers": "[{\"name\": \"gpaw_utilities_dos_delta\", \"arguments\": {\"x\": [-5.0, -4.5, -4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0], \"x0\": 1.2, \"width\": 0.4, \"mode\": \"Gauss\"}}]"}
{"func_name": "gpaw_utilities_ekin_dekindecut", "func_desc": "gpaw.utilities.ekin.dekindecut: Compute a linear interpolation of the array de at the reciprocal-space magnitude corresponding to a plane-wave kinetic-energy cutoff and return the value normalized by that magnitude. This function is used in GPAW's plane-wave related utilities to estimate the slope/value of a spectral quantity (de) at the cutoff boundary defined by ecut; in the GPAW codebase this helps determine contributions at the kinetic energy cutoff when working with plane-wave expansions and related extrapolations.", "tools": [{"function": {"description": "gpaw.utilities.ekin.dekindecut: Compute a linear interpolation of the array de at the reciprocal-space magnitude corresponding to a plane-wave kinetic-energy cutoff and return the value normalized by that magnitude. This function is used in GPAW's plane-wave related utilities to estimate the slope/value of a spectral quantity (de) at the cutoff boundary defined by ecut; in the GPAW codebase this helps determine contributions at the kinetic energy cutoff when working with plane-wave expansions and related extrapolations.\n", "name": "gpaw_utilities_ekin_dekindecut", "parameters": {"properties": {"G": {"type": "array", "items": {"type": "any"}, "description": "1D array of reciprocal-space magnitudes (G values) sampled for the spectral quantity de. G is expected to be a monotonically increasing, evenly spaced 1D array with at least two elements so that spacing dG = G[1] is meaningful. The caller must ensure G and ecut share a consistent unit system so that G0 = sqrt(2 * ecut) is numerically compatible with the values in G.", "default": ""}, "de": {"type": "array", "items": {"type": "any"}, "description": "1D array of the same shape as G containing the spectral quantity values corresponding to each entry in G. In the GPAW plane-wave context, de typically represents a derivative or density-like quantity sampled as a function of |G|; it is used here only to produce a local linear interpolation near the cutoff. The function will use de[g:g+2] together with G[g:g+2] to form a linear fit, so de must provide at least two entries around the cutoff index.", "default": ""}, "ecut": {"type": "float", "description": "Plane-wave kinetic-energy cutoff value used to define the cutoff magnitude G0 = sqrt(2 * ecut). ecut must be provided in the same energy units consistent with the G array and the calling code (no automatic unit conversion is performed). This parameter controls which interval of G is used for the linear interpolation.", "default": ""}}, "required": ["G", "de", "ecut"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW plane-wave convergence sanity check and want to use `gpaw.utilities.ekin.dekindecut` to estimate the spectral-derivative contribution right at the kinetic-energy cutoff boundary. I initially planned to use the nominal cutoff from my input file, but then I realized the code’s effective cutoff is slightly lower because the highest-|G| shell in my sampled grid is known to be systematically overestimated by one uniform G-step (it’s a binning artifact from how I computed |G| magnitudes). So: take the same sampled data G = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0] and de = [0.0, 0.8, 1.6, 2.1, 2.4, 2.5], but before calling the function, correct the cutoff so that the resulting reciprocal-space boundary magnitude is exactly one G-step smaller than the nominal boundary magnitude implied by ecut = 6.125. Using that corrected cutoff, what value does `dekindecut` return (i.e., the linearly interpolated de at the cutoff boundary, normalized by the corresponding boundary magnitude)?", "answers": "[{\"name\":\"gpaw_utilities_ekin_dekindecut\",\"arguments\":{\"G\":[0.0,1.0,2.0,3.0,4.0,5.0],\"de\":[0.0,0.8,1.6,2.1,2.4,2.5],\"ecut\":3.125}}]"}
{"func_name": "gpaw_utilities_h2gpts", "func_desc": "Convert a desired real-space grid spacing into a number of grid points per cell direction used by GPAW.\n    \n    This utility function is part of gpaw.utilities and is used by GPAW (a DFT code using real-space uniform grids and multigrid methods) to convert a target real-space grid spacing (h) and a Cartesian unit cell (cell_cv) into an integer number of grid points along each of the three cell directions. The returned grid-point counts are rounded to the nearest multiple of idiv and are enforced to be at least idiv. This is important in GPAW for constructing real-space grids that match desired spatial resolution, for ensuring compatibility with multigrid coarsening/solver levels and with algorithms that require grid dimensions divisible by a small integer block size.", "tools": [{"function": {"description": "Convert a desired real-space grid spacing into a number of grid points per cell direction used by GPAW.\n\nThis utility function is part of gpaw.utilities and is used by GPAW (a DFT code using real-space uniform grids and multigrid methods) to convert a target real-space grid spacing (h) and a Cartesian unit cell (cell_cv) into an integer number of grid points along each of the three cell directions. The returned grid-point counts are rounded to the nearest multiple of idiv and are enforced to be at least idiv. This is important in GPAW for constructing real-space grids that match desired spatial resolution, for ensuring compatibility with multigrid coarsening/solver levels and with algorithms that require grid dimensions divisible by a small integer block size.", "name": "gpaw_utilities_h2gpts", "parameters": {"properties": {"h": {"type": "float", "description": "Desired real-space grid spacing in the same length units as cell_cv. This is the target distance between adjacent grid points in Cartesian space. h must be positive; if h is zero or negative the function will perform a division by h which leads to invalid or undefined results.", "default": ""}, "cell_cv": {"type": "array", "items": {"type": "any"}, "description": "3x3 array representing the unit cell in Cartesian coordinates (each column/row orientation follows the convention used in calling code). The function uses numpy.linalg.inv(cell_cv) to compute characteristic lengths and therefore requires a square, non-singular 3x3 array. If cell_cv is not invertible or does not have shape (3, 3), numpy.linalg.inv or subsequent array operations will raise a numpy.linalg.LinAlgError or a shape-related exception.", "default": ""}, "idiv": {"type": "integer", "description": "Positive integer divisor (default 4) used to force the number of grid points in each direction to be a multiple of idiv. Typical choices (such as the default 4) are used in GPAW to ensure grid sizes compatible with multigrid levels and certain FFT/blocking requirements. idiv must be a positive integer; non-positive or non-integer values will produce incorrect results or runtime errors.", "default": 4}}, "required": ["h", "cell_cv", "idiv"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a GPAW real-space calculation for a tetragonal slab with cell vectors (Å): [[5.43, 0.0, 0.0], [0.0, 5.43, 0.0], [0.0, 0.0, 20.0]]. Initially I was going to target a grid spacing of 0.18 Å and round the grid sizes to a multigrid-friendly factor of 8. But I just realized the Poisson/multigrid level coarsening in my workflow needs one *additional* coarsening level beyond what I planned, so the grid dimensions must be divisible by twice my original block factor (otherwise the restriction/prolongation chain breaks). Please convert the same target spacing into the grid-point counts per direction under that updated divisibility constraint (and still enforce the minimum points rule).", "answers": "[{\"name\":\"gpaw_utilities_h2gpts\",\"arguments\":{\"h\":0.18,\"cell_cv\":[[5.43,0.0,0.0],[0.0,5.43,0.0],[0.0,0.0,20.0]],\"idiv\":16}}]"}
{"func_name": "gpaw_utilities_hardware_dhms", "func_desc": "gpaw.utilities.hardware.dhms converts a duration given in seconds into a four-element list representing days, hours, minutes and seconds. This utility is intended for formatting and reporting elapsed wall-clock or CPU time (for example in GPAW calculation logs and performance messages) where a human-readable decomposition of a floating-point seconds value is required.", "tools": [{"function": {"description": "gpaw.utilities.hardware.dhms converts a duration given in seconds into a four-element list representing days, hours, minutes and seconds. This utility is intended for formatting and reporting elapsed wall-clock or CPU time (for example in GPAW calculation logs and performance messages) where a human-readable decomposition of a floating-point seconds value is required.\n", "name": "gpaw_utilities_hardware_dhms", "parameters": {"properties": {"secs": {"type": "float", "description": "Elapsed time in seconds. This value is treated as a numeric duration (typically reported by timers or perf counters in GPAW). The function accepts fractional seconds and uses integer arithmetic to compute whole days, hours and minutes; the final seconds value is rounded to the nearest integer. No validation is performed to enforce non-negativity; for negative inputs the computation uses Python's floor-division and modulus semantics which may yield a negative days component combined with non-negative hours/minutes/seconds computed from the modular remainder.", "default": ""}}, "required": ["secs"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a messy GPAW benchmark log and want a human-readable runtime summary. Initially I was going to feed the *reported* total wall-clock time directly into the dhms formatter, but I realized the log line includes a fixed profiling overhead that must be removed first to make runs comparable. The log reports total wall-clock = 93784.6 s, and the profiler overhead is exactly 2 minutes 30.6 seconds. Subtract the overhead from the reported total, then convert the corrected duration into [days, hours, minutes, seconds] using gpaw.utilities.hardware.dhms so I can print a consistent summary.", "answers": "[{\"name\": \"gpaw_utilities_hardware_dhms\", \"arguments\": {\"secs\": 93634.0}}]"}
{"func_name": "gpaw_utilities_hardware_hms_string", "func_desc": "Return a human-readable hours:minutes:seconds string for a duration given in seconds.\n    \n    This utility is used throughout GPAW's timing and logging utilities to present elapsed wall-clock or CPU time in a compact, human-readable form (for example in progress output, log files, and test diagnostics). The function converts a duration expressed in seconds into integer hours, minutes and seconds using the internal helper hms(secs) and then formats each component as a two-digit field when appropriate.", "tools": [{"function": {"description": "Return a human-readable hours:minutes:seconds string for a duration given in seconds.\n\nThis utility is used throughout GPAW's timing and logging utilities to present elapsed wall-clock or CPU time in a compact, human-readable form (for example in progress output, log files, and test diagnostics). The function converts a duration expressed in seconds into integer hours, minutes and seconds using the internal helper hms(secs) and then formats each component as a two-digit field when appropriate.", "name": "gpaw_utilities_hardware_hms_string", "parameters": {"properties": {"secs": {"type": "float", "description": "Duration in seconds to format. This value is interpreted as a floating-point number of seconds (elapsed time). The caller in GPAW typically supplies wall-clock or CPU elapsed time measured by timers. The function relies on the internal hms(secs) helper to compute integer (hours, minutes, seconds) from this input; if secs is not a valid float suitable for hms, the underlying hms or comparisons will raise an exception which will propagate to the caller.", "default": ""}}, "required": ["secs"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a messy GPAW timing log from a DFT convergence test. I was going to format the *total* elapsed time that the timer printed (9876.5 s) into an hours:minutes:seconds string for the run summary. But I just realized that number includes a fixed 2-minute pre-run warm-up (MPI init + cache priming) that shouldn’t be counted in the scientific wall-clock time we report. Please format only the *net* elapsed time after removing that warm-up, using the same GPAW-style duration-to-HH:MM:SS formatter.", "answers": "[{\"name\": \"gpaw_utilities_hardware_hms_string\", \"arguments\": {\"secs\": 9756.5}}]"}
{"func_name": "gpaw_utilities_method", "func_desc": "gpaw.utilities.method: Pack off-diagonal sum used by GPAW to compress Hermitian 2D arrays (for example density or overlap matrices) into a 1D vector by summing symmetric off-diagonal elements. This packing is used in GPAW (a density-functional theory code) to reduce storage and to prepare matrix data for communication, serialization, or algorithms that operate on the independent elements of a Hermitian matrix.\n    \n    This function expects a square, two-dimensional numpy array that represents a Hermitian matrix (real or complex). It transforms an n x n matrix A into a 1D numpy.ndarray of length n*(n+1)//2 by iterating over the upper triangular part (including diagonal) and, for off-diagonal elements, storing the sum a_ij + a_ji. For example, the 3x3 matrix\n       / a00 a01 a02        | a10 a11 a12 |\n       \\ a20 a21 a22 /\n    is transformed to the vector\n       (a00, a01 + a10, a02 + a20, a11, a12 + a21, a22)\n    which corresponds to scanning rows i = 0..n-1 and for each i appending elements for j = i..n-1: a_ij + a_ji (with diagonal elements appearing unchanged).", "tools": [{"function": {"description": "gpaw.utilities.method: Pack off-diagonal sum used by GPAW to compress Hermitian 2D arrays (for example density or overlap matrices) into a 1D vector by summing symmetric off-diagonal elements. This packing is used in GPAW (a density-functional theory code) to reduce storage and to prepare matrix data for communication, serialization, or algorithms that operate on the independent elements of a Hermitian matrix.\n\nThis function expects a square, two-dimensional numpy array that represents a Hermitian matrix (real or complex). It transforms an n x n matrix A into a 1D numpy.ndarray of length n*(n+1)//2 by iterating over the upper triangular part (including diagonal) and, for off-diagonal elements, storing the sum a_ij + a_ji. For example, the 3x3 matrix\n   / a00 a01 a02        | a10 a11 a12 |\n   \\ a20 a21 a22 /\nis transformed to the vector\n   (a00, a01 + a10, a02 + a20, a11, a12 + a21, a22)\nwhich corresponds to scanning rows i = 0..n-1 and for each i appending elements for j = i..n-1: a_ij + a_ji (with diagonal elements appearing unchanged).", "name": "gpaw_utilities_method", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "any"}, "description": "A 2-dimensional, square numpy array representing a Hermitian matrix used in GPAW DFT workflows (for example a density matrix). The function asserts A.ndim == 2, A.shape[0] == A.shape[1], and A.dtype in [float, complex]; if any of these conditions fail an AssertionError is raised. The dtype check follows the source code's exact check and therefore expects numpy dtypes that are recognized as matching Python float or complex in the current build.", "default": ""}}, "required": ["A"], "type": "any"}}, "type": "function"}], "query": "I’m about to pack a 4×4 overlap matrix from a GPAW run for MPI communication using gpaw.utilities.method (the off-diagonal-summed Hermitian packing). I initially planned to pack it exactly as-is, but then I realized the matrix I exported includes a redundant global scale factor from a unit-conversion step in the workflow. So before packing, uniformly rescale the entire matrix by the same factor that would bring the largest diagonal entry down to exactly half of its current value (leave symmetry intact). Then pack the rescaled matrix into the 1D vector using the off-diagonal-summed upper-triangular scan convention. The raw matrix is [[1.0, 0.2, -0.1, 0.0],[0.2, 2.0, 0.3, 0.4],[-0.1, 0.3, 3.0, -0.2],[0.0, 0.4, -0.2, 4.0]].", "answers": "[{\"name\":\"gpaw_utilities_method\",\"arguments\":{\"A\":[[0.5,0.1,-0.05,0.0],[0.1,1.0,0.15,0.2],[-0.05,0.15,1.5,-0.1],[0.0,0.2,-0.1,2.0]]}}]"}
{"func_name": "gpaw_utilities_pack_hermitian", "func_desc": "Pack Hermitian matrices into a compact 1D representation used in GPAW for\n    storage and transport of Hermitian operators (for example density matrices,\n    Hamiltonians, overlap matrices) where only the independent upper-triangle\n    elements and diagonals are needed. This function is used in the GPAW code\n    base to reduce memory and communication costs by converting an n x n Hermitian\n    matrix into a packed vector of length n*(n+1)//2. When given a 3D array it\n    applies the same packing to each 2D slice (useful for batches of matrices,\n    e.g., k-point or spin channels).", "tools": [{"function": {"description": "Pack Hermitian matrices into a compact 1D representation used in GPAW for\nstorage and transport of Hermitian operators (for example density matrices,\nHamiltonians, overlap matrices) where only the independent upper-triangle\nelements and diagonals are needed. This function is used in the GPAW code\nbase to reduce memory and communication costs by converting an n x n Hermitian\nmatrix into a packed vector of length n*(n+1)//2. When given a 3D array it\napplies the same packing to each 2D slice (useful for batches of matrices,\ne.g., k-point or spin channels).", "name": "gpaw_utilities_pack_hermitian", "parameters": {"properties": {"M2": {"type": "array", "items": {"type": "any"}, "description": "A square 2D NumPy array (n x n) representing a matrix\nthat is expected to be Hermitian (M2[r, c] == conj(M2[c, r])) or a\n3D NumPy array (k x n x n) representing a stack of k such square\nmatrices. The dtype of M2 determines the dtype of the returned array\n(for complex-valued operators use a complex dtype). Each 2D slice is\npacked by emitting elements in row-major triangular order: for r from\n0 to n-1 emit the diagonal element M2[r, r] followed by the averaged\nupper-triangle elements (M2[r, c] + conj(M2[c, r]))/2 for c > r.\nThe function requires square matrices; passing a non-square array\nwill raise an IndexError during indexing.", "default": ""}, "tolerance": {"type": "float", "description": "Absolute tolerance used to check Hermiticity. For each\noff-diagonal pair (r, c) the absolute difference\nabs(M2[r, c] - conj(M2[c, r])) is computed and must be strictly\nless than this tolerance, otherwise an AssertionError is raised.\nDefault is 1e-10. This parameter controls how much numerical\nnon-Hermiticity (for example from rounding or I/O) is tolerated before\nthe packing fails.", "default": 1e-10}}, "required": ["M2", "tolerance"], "type": "any"}}, "type": "function"}], "query": "I’m about to ship two k-point Hamiltonians (each 3×3) through MPI in a GPAW-style workflow, so I need them packed into the compact upper-triangle 1D representation (length n*(n+1)//2 per matrix), using the batch mode (3D array: k×n×n). Initially I planned to enforce an extremely tight Hermiticity check, but I just realized these matrices came from a mixed-precision SCF loop and pick up tiny antisymmetric roundoff at about the scale of machine epsilon for float64. So: pack them as a batch, and set the Hermiticity tolerance to the **smallest decimal power-of-ten that is still safely above float64 machine epsilon** (so it won’t spuriously reject numerical noise, but is still as strict as possible).\n\nMatrices (k=2):\nH0 = [[1.0, 0.2, -0.1],[0.2, 2.0, 0.3],[-0.1, 0.3, 3.0]]\nH1 = [[0.5, -0.4, 0.0],[-0.4, 1.5, 0.25],[0.0, 0.25, 2.5]]", "answers": "[{\"name\":\"gpaw_utilities_pack_hermitian\",\"arguments\":{\"M2\":[[[1.0,0.2,-0.1],[0.2,2.0,0.3],[-0.1,0.3,3.0]],[[0.5,-0.4,0.0],[-0.4,1.5,0.25],[0.0,0.25,2.5]]],\"tolerance\":1e-15}}]"}
{"func_name": "gpaw_utilities_packed_index", "func_desc": "gpaw.utilities.packed_index returns a single integer index corresponding to a pair of matrix indices (i1, i2) when a symmetric ni x ni matrix is stored in packed one-dimensional form. This function is used within GPAW (a density-functional theory code) to reduce memory and improve cache locality by storing only the triangular part of symmetric or Hermitian matrices (for example, overlap, density or Hamiltonian blocks) in a linear array. The mapping is symmetric: the pair (i1, i2) and (i2, i1) produce the same packed index so that a single stored value represents both symmetric matrix elements.", "tools": [{"function": {"description": "gpaw.utilities.packed_index returns a single integer index corresponding to a pair of matrix indices (i1, i2) when a symmetric ni x ni matrix is stored in packed one-dimensional form. This function is used within GPAW (a density-functional theory code) to reduce memory and improve cache locality by storing only the triangular part of symmetric or Hermitian matrices (for example, overlap, density or Hamiltonian blocks) in a linear array. The mapping is symmetric: the pair (i1, i2) and (i2, i1) produce the same packed index so that a single stored value represents both symmetric matrix elements.\n", "name": "gpaw_utilities_packed_index", "parameters": {"properties": {"i1": {"type": "integer", "description": "First matrix index (row index). This is the integer position along the first matrix axis and, in the expected use within GPAW, should be in the range 0..ni-1. The function does not perform bounds checking; passing indices outside the expected range may produce a result that is not meaningful for packed storage.", "default": ""}, "i2": {"type": "integer", "description": "Second matrix index (column index). This is the integer position along the second matrix axis and, in the expected use within GPAW, should be in the range 0..ni-1. The ordering of i1 and i2 does not affect the returned packed index: packed_index(i1, i2, ni) == packed_index(i2, i1, ni).", "default": ""}, "ni": {"type": "integer", "description": "Size of the square matrix (number of rows and columns). This determines the total number of elements in the full matrix (ni*ni) and the number of stored packed elements (ni*(ni+1)//2 for triangular storage). ni is used in the arithmetic formula that maps a sorted pair of indices to a unique linear offset. The function does not validate ni; negative or zero values are permitted by the code but will not produce a meaningful packed index for matrix storage.", "default": ""}}, "required": ["i1", "i2", "ni"], "type": "any"}}, "type": "function"}], "query": "I'm debugging a GPAW memory optimization where we store only one triangular half of a real symmetric overlap block in packed 1D form. I was about to look up the packed index for the element at (row 6, col 2) in an 8×8 block, but then I realized those indices came from a log that sometimes switches between 0-based Python indexing and 1-based Fortran-style indexing. Assume the log line I’m using is 1-based, and I need the packed index corresponding to the *same physical matrix element* after converting it to the indexing convention expected by gpaw.utilities.packed_index. What packed-array index should I use (it must be identical if I swap the two matrix indices)?", "answers": "[{\"name\": \"gpaw_utilities_packed_index\", \"arguments\": {\"i1\": 5, \"i2\": 1, \"ni\": 8}}]"}
{"func_name": "gpaw_utilities_tools_gram_schmidt", "func_desc": "gpaw.utilities.tools.gram_schmidt: Orthonormalize the columns of a matrix U in-place using the classical Gram-Schmidt procedure.\n    \n    This function implements the classical Gram-Schmidt algorithm to produce a set of orthonormal column vectors from the columns of the input array U. In the context of GPAW (a density-functional theory code that works with wavefunction coefficient matrices, basis-function coefficients, and real-space discretizations), this routine is used to convert a collection of (possibly non-orthogonal) vectors into an orthonormal basis suitable for subsequent linear-algebra operations such as projections, subspace diagonalization, or construction of orthonormalized wavefunctions. The implementation uses the complex-conjugate inner product (numpy.dot(col2.conj(), col)) and normalizes with numpy.linalg.norm, so it supports real or complex-valued arrays.", "tools": [{"function": {"description": "gpaw.utilities.tools.gram_schmidt: Orthonormalize the columns of a matrix U in-place using the classical Gram-Schmidt procedure.\n\nThis function implements the classical Gram-Schmidt algorithm to produce a set of orthonormal column vectors from the columns of the input array U. In the context of GPAW (a density-functional theory code that works with wavefunction coefficient matrices, basis-function coefficients, and real-space discretizations), this routine is used to convert a collection of (possibly non-orthogonal) vectors into an orthonormal basis suitable for subsequent linear-algebra operations such as projections, subspace diagonalization, or construction of orthonormalized wavefunctions. The implementation uses the complex-conjugate inner product (numpy.dot(col2.conj(), col)) and normalizes with numpy.linalg.norm, so it supports real or complex-valued arrays.", "name": "gpaw_utilities_tools_gram_schmidt", "parameters": {"properties": {"U": {"type": "array", "items": {"type": "any"}, "description": "A writable NumPy array whose columns are the vectors to be orthonormalized. The function expects U to be a two-dimensional array with shape (n, m) where each column 0..m-1 is a length-n vector. The function operates in-place: the columns of U are modified so that after the call they are orthonormal with respect to the standard complex inner product (conjugate on the first argument). The array must be mutable and have a numeric dtype (real or complex). No new array is returned.", "default": ""}}, "required": ["U"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a small DFT subspace in GPAW from three trial wavefunction coefficient vectors (columns of a 4×3 matrix). I was going to orthonormalize them exactly as-is with classical Gram–Schmidt, but I just realized the third “trial vector” is contaminated by an obvious I/O artifact: it’s entirely real and has no weight on the first basis function, even though it should share support there like the other two. So please do this instead: keep the first two columns unchanged, then replace the third column by the equal-weight superposition of the original third column and the first trial vector (use a real mixing coefficient, same weight from each), and only then orthonormalize all three columns in-place using the classical Gram–Schmidt procedure (complex-conjugate inner product). Use the original matrix U below as the starting point:\n[[1.0, 1.0, 0.0],\n [0.0, 1.0, 1.0],\n [1.0, 0.0, 1.0],\n [0.0, 0.0, 1.0]]", "answers": "[{\"name\":\"gpaw_utilities_tools_gram_schmidt\",\"arguments\":{\"U\":[[1.0,1.0,0.5],[0.0,1.0,1.0],[1.0,0.0,1.0],[0.0,0.0,1.0]]}}]"}
{"func_name": "gpaw_utilities_tools_lowdin", "func_desc": "gpaw.utilities.tools.lowdin orthonormalizes the columns of a coefficient matrix U using the Löwdin (symmetric) orthogonalization procedure. In the GPAW density-functional-theory (DFT) context this is used to convert a set of non-orthogonal basis-function coefficient vectors or molecular-orbital coefficient columns into a symmetric orthonormal set suitable for subsequent linear-algebra operations (for example, when working with PAW projector/basis representations or overlap-corrected coefficient sets).\n    \n    If an overlap matrix S is provided it is used directly; otherwise the overlap is formed from U as S = dagger(U) @ U, where dagger denotes the conjugate-transpose used elsewhere in GPAW. The implementation computes the Hermitian eigen-decomposition of S, forms the symmetric inverse square root of S from the eigenvectors and eigenvalues, and left-multiplies U by that inverse square root so that the columns of U become orthonormal with respect to the standard inner product.", "tools": [{"function": {"description": "gpaw.utilities.tools.lowdin orthonormalizes the columns of a coefficient matrix U using the Löwdin (symmetric) orthogonalization procedure. In the GPAW density-functional-theory (DFT) context this is used to convert a set of non-orthogonal basis-function coefficient vectors or molecular-orbital coefficient columns into a symmetric orthonormal set suitable for subsequent linear-algebra operations (for example, when working with PAW projector/basis representations or overlap-corrected coefficient sets).\n\nIf an overlap matrix S is provided it is used directly; otherwise the overlap is formed from U as S = dagger(U) @ U, where dagger denotes the conjugate-transpose used elsewhere in GPAW. The implementation computes the Hermitian eigen-decomposition of S, forms the symmetric inverse square root of S from the eigenvectors and eigenvalues, and left-multiplies U by that inverse square root so that the columns of U become orthonormal with respect to the standard inner product.", "name": "gpaw_utilities_tools_lowdin", "parameters": {"properties": {"U": {"type": "array", "items": {"type": "any"}, "description": "2-D array whose columns represent the vectors to be orthonormalized. This array is modified in-place: after the call the columns of U are Löwdin-orthonormalized. In GPAW workflows U typically contains basis-function coefficients or orbital expansion coefficients; the function operates directly on these coefficient matrices so no copy is returned.", "default": ""}, "S": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Overlap matrix corresponding to the columns of U. If provided, S is used as the Hermitian overlap matrix in the Löwdin procedure. Default: None, in which case S is computed internally as dagger(U) @ U (the conjugate-transpose of U multiplied by U). S must be a Hermitian (symmetric) matrix consistent with U for correct results.", "default": null}}, "required": ["U", "S"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW post-processing step where I need to Löwdin-orthonormalize three MO coefficient columns (3 basis functions × 3 orbitals). I was going to feed the overlap matrix from the PAW overlap operator directly, but I just realized the matrix I pasted includes a tiny non-Hermitian artifact from a float-rounding copy/paste step. Since Löwdin uses a Hermitian eigendecomposition anyway, first replace the provided overlap with its symmetrized (Hermitian) form obtained by averaging it with its conjugate-transpose, and then perform the symmetric Löwdin orthonormalization of U using that corrected overlap (do not recompute overlap from U). Use:\n\nU = [[1.0, 0.2, 0.0],\n     [0.0, 1.0, 0.3],\n     [0.1, 0.0, 1.0]]\n\nS_raw = [[1.02, 0.10, 0.00],\n         [0.11, 1.05, 0.08],\n         [0.00, 0.08, 1.01]]\n\n(So the off-diagonal (1,2) and (2,1) entries don’t match; fix it via symmetrization as described, then run Löwdin.)", "answers": "[{\"name\":\"gpaw_utilities_tools_lowdin\",\"arguments\":{\"U\":[[1.0,0.2,0.0],[0.0,1.0,0.3],[0.1,0.0,1.0]],\"S\":[[1.02,0.10500000000000001,0.0],[0.10500000000000001,1.05,0.08],[0.0,0.08,1.01]]}}]"}
{"func_name": "gpaw_utilities_tools_normalize", "func_desc": "Normalize columns of U.\n    \n    This function normalizes each column vector of the input array U in place using the Euclidean (L2) norm. In the GPAW DFT codebase (which represents physical quantities such as wavefunction coefficient matrices, basis-function coefficients, or other column-wise vector sets as NumPy arrays), this utility ensures each column has unit length, which is often required before or after linear-algebra operations (for example, when preparing coefficient matrices for orthonormalization, projection, or numerical stabilization in plane-wave, atom-centered, or real-space grid representations).", "tools": [{"function": {"description": "Normalize columns of U.\n\nThis function normalizes each column vector of the input array U in place using the Euclidean (L2) norm. In the GPAW DFT codebase (which represents physical quantities such as wavefunction coefficient matrices, basis-function coefficients, or other column-wise vector sets as NumPy arrays), this utility ensures each column has unit length, which is often required before or after linear-algebra operations (for example, when preparing coefficient matrices for orthonormalization, projection, or numerical stabilization in plane-wave, atom-centered, or real-space grid representations).", "name": "gpaw_utilities_tools_normalize", "parameters": {"properties": {"U": {"type": "array", "items": {"type": "any"}, "description": "Array whose columns are treated as vectors to normalize. For a typical 2-D array of shape (M, N), each of the N column vectors of length M is divided in place by its Euclidean norm computed with numpy.linalg.norm. The function operates by iterating over U.T and performing in-place division on each column view, so U is modified directly; no copy is returned. It is intended for floating-point arrays representing collections of column vectors (e.g., wavefunction coefficient matrices). If U has integer dtype, in-place true division by a float norm will raise a NumPy casting error. For arrays with nonstandard shapes (e.g., 1-D arrays, higher-rank arrays), behavior follows NumPy iteration semantics but may not correspond to column-wise normalization and should be used with care.", "default": ""}}, "required": ["U"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a tiny set of GPAW trial wavefunction coefficients for an orthonormalization step and I was going to just normalize each column of my 3x2 float coefficient matrix U in place. But I just noticed one of the columns in my copy is in arbitrary units and needs to be rescaled so that its L2 norm becomes exactly half of what it currently is before normalization (otherwise it will overweight that trial state in the preconditioning test). Apply that rescaling to the appropriate column (the one whose raw L2 norm is the larger of the two), then run the in-place column normalization on the resulting U. Start from the same raw columns as before: first column [3.0, 0.0, 4.0], second column [1.0, 2.0, 2.0].", "answers": "[{\"name\":\"gpaw_utilities_tools_normalize\",\"arguments\":{\"U\":[[1.5,1.0],[0.0,2.0],[2.0,2.0]]}}]"}
{"func_name": "gpaw_utilities_tools_split_formula", "func_desc": "gpaw.utilities.tools.split_formula: Expand a chemical formula string into a flat list of element symbols by repeating each element according to its numeric count. This function is used in GPAW utilities to translate simple chemical formulas (used e.g. in tests, quick input parsing, or stoichiometry checks) into an explicit list of atom symbols that can be passed to ASE/GPAW routines that expect per-atom entries.", "tools": [{"function": {"description": "gpaw.utilities.tools.split_formula: Expand a chemical formula string into a flat list of element symbols by repeating each element according to its numeric count. This function is used in GPAW utilities to translate simple chemical formulas (used e.g. in tests, quick input parsing, or stoichiometry checks) into an explicit list of atom symbols that can be passed to ASE/GPAW routines that expect per-atom entries.\n", "name": "gpaw_utilities_tools_split_formula", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula to parse, expressed as a contiguous string of element symbols and counts. Element symbols must begin with an uppercase ASCII letter and may include one or more lowercase ASCII letters immediately following (e.g. 'C', 'Mg', 'He'). Counts are represented by non-letter characters encountered after an element symbol; the character is evaluated with Python's eval() and interpreted as the total multiplicity for the immediately preceding element. Because the implementation appends the element once when its symbol is first parsed, a numeric count n will result in n total occurrences of that element (the code performs an extend with (eval(c) - 1) additional copies). The function processes the input left-to-right without permitting separators or whitespace; any characters that are not ASCII letters are passed to eval() and therefore must be valid Python expressions that evaluate to integers. Example valid input: 'C2H3Mg' -> expands to two 'C', three 'H', and one 'Mg'.", "default": ""}}, "required": ["formula"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a quick ASE/GPAW sanity test where the input generator wants an explicit per-atom symbol list. Initially I was going to feed it alumina as the usual compact string, but I just realized my templating step auto-inserts a leading coefficient when copying from my lab notebook (it’s written in the conventional “2×(formula)” style). That would incorrectly double the atom list if I expand it naively. So: expand the alumina formula **after stripping any overall leading multiplicative coefficient** (i.e., treat the written line as a scaled formula and reduce it back to the primitive composition before expanding). Then do the same expansion for zirconia, but here’s the catch: the zirconia line in my notes is written in the oxygen-deficient form and I need the **stoichiometric dioxide** (the one whose oxygen count is exactly twice the zirconium count) before expanding. Please output the explicit symbol lists for both materials.", "answers": "[{\"name\": \"gpaw_utilities_tools_split_formula\", \"arguments\": {\"formula\": \"Al2O3\"}}, {\"name\": \"gpaw_utilities_tools_split_formula\", \"arguments\": {\"formula\": \"ZrO2\"}}]"}
{"func_name": "gpaw_utilities_tools_symmetrize", "func_desc": "Symmetrize input matrix by replacing it with its Hermitian (conjugate-transpose) average.\n    \n    This function is used in GPAW (a DFT code) to enforce the physical requirement that many operator matrices\n    (e.g., Hamiltonian, density matrix, overlap) are Hermitian (for complex-valued arrays) or symmetric\n    (for real-valued arrays). It performs the operation matrix <- (matrix + matrix^†) / 2 in-place, where\n    matrix^† is the conjugate transpose of matrix (computed by dagger(matrix) in the source). The implementation\n    uses NumPy in-place operations (numpy.add and numpy.multiply) to avoid extra allocations, which is important\n    for large matrices common in plane-wave, real-space, or atom-centered basis computations in GPAW.", "tools": [{"function": {"description": "Symmetrize input matrix by replacing it with its Hermitian (conjugate-transpose) average.\n\nThis function is used in GPAW (a DFT code) to enforce the physical requirement that many operator matrices\n(e.g., Hamiltonian, density matrix, overlap) are Hermitian (for complex-valued arrays) or symmetric\n(for real-valued arrays). It performs the operation matrix <- (matrix + matrix^†) / 2 in-place, where\nmatrix^† is the conjugate transpose of matrix (computed by dagger(matrix) in the source). The implementation\nuses NumPy in-place operations (numpy.add and numpy.multiply) to avoid extra allocations, which is important\nfor large matrices common in plane-wave, real-space, or atom-centered basis computations in GPAW.", "name": "gpaw_utilities_tools_symmetrize", "parameters": {"properties": {"matrix": {"type": "array", "items": {"type": "any"}, "description": "The input array to be symmetrized. This should be a square 2-D NumPy array\nrepresenting a matrix stored in GPAW as an operator or similar quantity. The function treats the\narray as a matrix A and replaces its contents with (A + A^†)/2, where A^† is the conjugate\ntranspose. The operation is performed in-place: the original array object is overwritten and no\nseparate copy is returned. The dtype and shape of the array are preserved by the in-place operations.\nPassing a non-square array will generally lead to a NumPy error due to shape mismatch when computing\nthe conjugate transpose. Passing a non-NumPy object or an array-like structure that does not support\nthe required NumPy ufuncs may raise TypeError or produce undefined behavior.", "default": ""}}, "required": ["matrix"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up an overlap matrix coming out of a GPAW post-processing script. I was going to just average it with its plain transpose because I thought it was purely real, but I realized one of the upstream steps can inject tiny imaginary round-off terms (e.g., from phase conventions in a k-point projection), so I need to enforce the physically correct condition: make it equal to its conjugate-transpose average, in-place, using the same Hermitian symmetrization GPAW uses.\n\nUse this 3×3 matrix (note: the imaginary parts are small and should be handled correctly by conjugation, not dropped):\n[[1.0+0.0j, 0.20+1e-12j, -0.10-2e-12j],\n [0.25-1e-12j, 1.0+0.0j, 0.05+3e-12j],\n [-0.12+2e-12j, 0.04-3e-12j, 1.0+0.0j]].\n\nSymmetrize it in-place so the result is (A + A^†)/2 (Hermitian average), not just (A + A^T)/2.", "answers": "[{\"name\":\"gpaw_utilities_tools_symmetrize\",\"arguments\":{\"matrix\":[[1.0,0.2,-0.1],[0.25,1.0,0.05],[-0.12,0.04,1.0]]}}]"}
{"func_name": "gpaw_utilities_unpack_hermitian", "func_desc": "gpaw.utilities.unpack_hermitian unpacks a 1D packed representation of a Hermitian matrix (produced by the companion pack_hermitian routine) into a full 2D numpy.ndarray. This routine is used in GPAW to reconstruct full Hermitian matrices that are compressed for storage or communication (for example, when storing triangular matrix elements such as overlap, density, or Hamiltonian blocks in DFT workflows). The function accepts either a single packed 1D array or a 2D array whose rows are independent packed arrays and returns the corresponding full Hermitian matrix or a stack of matrices.", "tools": [{"function": {"description": "gpaw.utilities.unpack_hermitian unpacks a 1D packed representation of a Hermitian matrix (produced by the companion pack_hermitian routine) into a full 2D numpy.ndarray. This routine is used in GPAW to reconstruct full Hermitian matrices that are compressed for storage or communication (for example, when storing triangular matrix elements such as overlap, density, or Hamiltonian blocks in DFT workflows). The function accepts either a single packed 1D array or a 2D array whose rows are independent packed arrays and returns the corresponding full Hermitian matrix or a stack of matrices.\n", "name": "gpaw_utilities_unpack_hermitian", "parameters": {"properties": {"M": {"type": "array", "items": {"type": "any"}, "description": "Input array holding packed Hermitian data. If M.ndim == 1, M is interpreted as a single packed representation of a Hermitian matrix and must be contiguous in memory (is_contiguous(M) must be True). The length of M must be consistent with a triangular packing: there must exist an integer n such that len(M) == n*(n+1)/2; the code computes n as int(sqrt(0.25 + 2.0 * len(M))). If M.ndim == 2, each row of M is treated as an independent packed 1D array and the function returns a stacked array of full matrices (one matrix per row). The function detects the scalar kind of M (real or complex) and dispatches to specialized unpack routines: for complex-valued arrays it calls cgpaw.unpack_complex to preserve complex entries; for real-valued arrays it calls cgpaw.unpack.", "default": ""}}, "required": ["M"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing Hermitian Hamiltonian blocks in a GPAW DFT run after MPI communication. I first thought the payload was a single packed lower-triangular vector for one 3×3 complex Hermitian block, but I just realized the MPI buffer actually contains two consecutive blocks (same size) concatenated back-to-back. Also, the sender used the convention that the first element of each packed block is the real-valued (0,0) diagonal entry, so if there’s any tiny imaginary part there it should be treated as numerical noise and dropped upon reconstruction. Please use gpaw.utilities.unpack_hermitian on the correctly segmented input so I get a stack of full 3×3 Hermitian matrices suitable for subsequent matrix–vector products.\n\nPacked buffer (concatenated): [1.2, 0.3, 2.1, 0.1, 0.05, 3.0, 0.9, -0.2, 1.7, 0.0, 0.04, 2.6].", "answers": "[{\"name\": \"gpaw_utilities_unpack_hermitian\", \"arguments\": {\"M\": [[1.2, 0.3, 2.1, 0.1, 0.05, 3.0], [0.9, -0.2, 1.7, 0.0, 0.04, 2.6]]}}]"}
{"func_name": "gpaw_xc_ri_spherical_hse_kernel_Hn", "func_desc": "Compute the H_n function used in the spherical HSE (Heyd–Scuseria–Ernzerhof) screened-exchange kernel (Eq. 24 in the implementation). This helper implements the closed-form combination of powers and complementary error functions used by the spherical HSE kernel in gpaw.xc.ri.spherical_hse_kernel. In the GPAW DFT codebase this function appears as an inner radial integrand evaluator for constructing the short-range screened exchange kernel on spherical coordinates; it therefore plays the practical role of producing elementwise numerical values of H_n for arrays of radial or transformed variables used in spherical integration.", "tools": [{"function": {"description": "Compute the H_n function used in the spherical HSE (Heyd–Scuseria–Ernzerhof) screened-exchange kernel (Eq. 24 in the implementation). This helper implements the closed-form combination of powers and complementary error functions used by the spherical HSE kernel in gpaw.xc.ri.spherical_hse_kernel. In the GPAW DFT codebase this function appears as an inner radial integrand evaluator for constructing the short-range screened exchange kernel on spherical coordinates; it therefore plays the practical role of producing elementwise numerical values of H_n for arrays of radial or transformed variables used in spherical integration.\n", "name": "gpaw_xc_ri_spherical_hse_kernel_Hn", "parameters": {"properties": {"n": {"type": "integer", "description": "Integer order of the H_n function. In the spherical HSE kernel this selects the power of Xi and xi that appear in the numerator (powers 2*n+1) and the exponent of the denominator factor (xi*Xi)^(n+1). In practice n is used as a non-negative integer index when expanding spherical contributions to the screened-exchange kernel; passing values outside the expected integer range may produce mathematically valid results but is uncommon in the HSE context.", "default": ""}, "Xi": {"type": "array", "items": {"type": "any"}, "description": "Array of Xi arguments for the H_n expression. Xi corresponds to one of the radial or transformed variables used in the spherical HSE kernel evaluation and is used elementwise in powers Xi**(2*n+1) and in the complementary error function arguments safeerfc(Xi +/- xi). Xi must be a numpy.ndarray; its shape must be broadcast-compatible with xi. The function performs elementwise operations and returns an array shaped according to numpy broadcasting rules applied to Xi and xi.", "default": ""}, "xi": {"type": "array", "items": {"type": "any"}, "description": "Array of xi arguments for the H_n expression. xi corresponds to the other radial or transformed variable in the spherical HSE integrand and appears both in powers xi**(2*n+1) and in the denominator factor (xi*Xi)**(n+1) as well as in the safeerfc(Xi +/- xi) arguments. xi must be a numpy.ndarray; its shape must be broadcast-compatible with Xi. In the physical context of spherical radial integration xi is typically a non-negative radial variable, but the function accepts any numeric values representable in the provided numpy arrays.", "default": ""}}, "required": ["n", "Xi", "xi"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a spherical HSE screened-exchange kernel regression test in GPAW and need the H_n helper evaluated on a tiny broadcast grid. Initially I was going to use order n=2, but then I realized the test is meant to probe the *next* even moment beyond the quadratic term, so the order should be the smallest even integer strictly larger than 2. Keep the same broadcast-compatible radial-variable arrays Xi=[0.2, 0.6, 1.0] and xi=[[0.1],[0.4]]. Compute the elementwise H_n values on the resulting 2×3 grid (broadcasting xi over Xi) and return the full array.", "answers": "[{\"name\": \"gpaw_xc_ri_spherical_hse_kernel_Hn\", \"arguments\": {\"n\": 4, \"Xi\": [0.2, 0.6, 1], \"xi\": [[0.1], [0.4]]}}]"}
{"func_name": "gpaw_xc_sic_matrix_exponential", "func_desc": "gpaw.xc.sic.matrix_exponential computes the matrix exponential U = exp(i * dlt * G)\n    for an anti-Hermitian generator G used in GPAW's self-interaction-correction (sic)\n    code path. In the GPAW DFT/PAW context this routine is used to construct a unitary\n    rotation operator from an anti-Hermitian generator matrix G_nn (for example to\n    rotate orbitals or apply small unitary updates in SIC procedures). The routine\n    diagonalizes a Hermitian matrix derived from G_nn, forms phase factors exp(i*dlt*w)\n    from the real eigenvalues w, and reconstructs the exponential by back-transforming\n    the diagonal phase matrix with the eigenvectors.", "tools": [{"function": {"description": "gpaw.xc.sic.matrix_exponential computes the matrix exponential U = exp(i * dlt * G)\nfor an anti-Hermitian generator G used in GPAW's self-interaction-correction (sic)\ncode path. In the GPAW DFT/PAW context this routine is used to construct a unitary\nrotation operator from an anti-Hermitian generator matrix G_nn (for example to\nrotate orbitals or apply small unitary updates in SIC procedures). The routine\ndiagonalizes a Hermitian matrix derived from G_nn, forms phase factors exp(i*dlt*w)\nfrom the real eigenvalues w, and reconstructs the exponential by back-transforming\nthe diagonal phase matrix with the eigenvectors.", "name": "gpaw_xc_sic_matrix_exponential", "parameters": {"properties": {"G_nn": {"type": "array", "items": {"type": "any"}, "description": "Anti-Hermitian (skew-Hermitian) matrix representing the\ngenerator G in the expression U = exp(i * dlt * G). The function expects\na two-dimensional square array with shape (n, n) where n = G_nn.shape[1].\nThe implementation treats complex and real dtypes differently: when\nG_nn.dtype == complex the code uses both real and imaginary parts to form\na Hermitian matrix for diagonalization; when G_nn.dtype is not complex the\ncode uses only the real part. For mathematically correct/unitary results,\nG_nn should satisfy G_nn.conj().T == -G_nn (anti-Hermitian). If G_nn is not\nsquare, contains non-finite values (NaN or Inf), or strongly violates the\nanti-Hermitian property the numerical result may be invalid or the linear\nalgebra routines may raise errors (for example numpy.linalg.LinAlgError).", "default": ""}, "dlt": {"type": "float", "description": "Scalar scaling factor that multiplies the generator inside the\nexponential. Physically this can represent a small rotation angle or a\ntimestep-like parameter controlling the magnitude of the unitary update.\nThe value is used exactly as provided in exp(i * dlt * w) where w are the\nreal eigenvalues obtained from diagonalizing the Hermitian matrix derived\nfrom G_nn. Very large magnitudes of dlt can produce rapidly oscillatory\nphases and increase numerical sensitivity.", "default": ""}}, "required": ["G_nn", "dlt"], "type": "any"}}, "type": "function"}], "query": "I’m implementing a GPAW SIC orbital-rotation micro-step and need the unitary update U = exp(i * dlt * G) from a nominally anti-Hermitian 2×2 generator. I was going to feed the raw coupling matrix directly, but I realized the one I pulled from logs is contaminated by a tiny symmetric (Hermitian) numerical drift, so using it as-is would break the unitary assumption. Please first project the generator onto its anti-Hermitian part by averaging it with the negative conjugate transpose (for this real-valued test case, that reduces to taking (G − G^T)/2), then compute U with the same small step size as before. Use the following noisy generator:\nG = [[0.0, 0.2004], [-0.1996, 0.0]] and dlt = 0.05.", "answers": "[{\"name\":\"gpaw_xc_sic_matrix_exponential\",\"arguments\":{\"G_nn\":[[0.0,0.2],[-0.2,0.0]],\"dlt\":0.05}}]"}
{"func_name": "gpaw_xc_sic_ortho", "func_desc": "gpaw.xc.sic.ortho: Orthogonalize column vectors of a matrix using the symmetric Löwdin procedure.\n    \n    Performs Symmetric-Löwdin orthogonalization of the column vectors contained in a square numpy.ndarray. This function is intended for use within GPAW's self-interaction-correction (gpaw.xc.sic) code paths where a set of (typically localized) orbital-like column vectors must be transformed into an orthonormal set before further processing. The routine computes the Hermitian overlap matrix O = W_nn @ W_nn.T.conj() and either applies a second-order perturbative approximation of O^{-1/2} when O is already close to the identity (faster, avoids diagonalization), or computes the exact inverse square root via diagonalization of O (stable, more expensive). The returned matrix has the same shape as W_nn and its columns are orthonormal within numerical tolerance: the overlap of the returned columns with their Hermitian conjugates is (approximately) the identity.", "tools": [{"function": {"description": "gpaw.xc.sic.ortho: Orthogonalize column vectors of a matrix using the symmetric Löwdin procedure.\n\nPerforms Symmetric-Löwdin orthogonalization of the column vectors contained in a square numpy.ndarray. This function is intended for use within GPAW's self-interaction-correction (gpaw.xc.sic) code paths where a set of (typically localized) orbital-like column vectors must be transformed into an orthonormal set before further processing. The routine computes the Hermitian overlap matrix O = W_nn @ W_nn.T.conj() and either applies a second-order perturbative approximation of O^{-1/2} when O is already close to the identity (faster, avoids diagonalization), or computes the exact inverse square root via diagonalization of O (stable, more expensive). The returned matrix has the same shape as W_nn and its columns are orthonormal within numerical tolerance: the overlap of the returned columns with their Hermitian conjugates is (approximately) the identity.", "name": "gpaw_xc_sic_ortho", "parameters": {"properties": {"W_nn": {"type": "array", "items": {"type": "any"}, "description": "Square input array whose column vectors are to be orthogonalized. In the source code this array is treated as having ndim = W_nn.shape[1] and the Hermitian overlap is computed as O_nn = W_nn @ W_nn.T.conj(), so W_nn must be laid out such that this product yields an (ndim x ndim) Hermitian matrix. The practical significance in GPAW is that W_nn typically contains orbital or projector coefficients that must be orthonormalized for stable self-interaction-correction or related operations.", "default": ""}, "maxerr": {"type": "float", "description": "Threshold for choosing the perturbative (approximate) orthogonalization instead of explicit diagonalization of the overlap. If the L1 deviation from identity of the overlap matrix, computed as sum(abs(O_nn - I)), is strictly less than maxerr, the routine uses the perturbative Symmetric-Löwdin approximation X = 1.5*I - 0.5*O_nn to obtain an approximate O^{-1/2} and avoid diagonalization. If the deviation is greater than or equal to maxerr, the routine diagonalizes O_nn with a Hermitian eigensolver (eigh), forms nsqrt_n = diag(1/sqrt(eigenvalues)), and constructs X = U @ nsqrt_n @ U.T.conj() to obtain the exact inverse square root. The default value 1e-10 is chosen to balance performance and numerical stability in typical GPAW calculations; set this parameter smaller to force diagonalization more often, or larger to favor the perturbative shortcut.", "default": 1e-10}}, "required": ["W_nn", "maxerr"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a GPAW self-interaction-correction (SIC) step where three localized orbital-like coefficient vectors are stored as columns of a 3×3 matrix. I was going to force the **fast** Symmetric-Löwdin path by using a relatively loose error threshold, but then I realized the pipeline sometimes hands me orbitals that are *not* as close to orthonormal as I assumed, and the perturbative inverse-square-root can become unreliable.\n\nSo: first, compute the Hermitian overlap O = W @ W^† and check how close it is to the identity. If the largest absolute deviation of O from I is **above** the previously intended looseness threshold, switch to the **more stable exact inverse-square-root route** (the one that diagonalizes O), using the corresponding stricter setting that forces that behavior; otherwise keep the faster route.\n\nUse this matrix (real-valued, so † is just transpose):\n[[1.0, 0.02, 0.0],\n [0.01, 0.99, 0.03],\n [0.0, -0.01, 1.01]]\n\nFinally, Symmetric-Löwdin orthogonalize the columns accordingly.", "answers": "[{\"name\": \"gpaw_xc_sic_ortho\", \"arguments\": {\"W_nn\": [[1.0, 0.02, 0.0], [0.01, 0.99, 0.03], [0.0, -0.01, 1.01]], \"maxerr\": 1e-08}}]"}
{"func_name": "gpaw_xc_vdw_phi_asymptotic", "func_desc": "gpaw.xc.vdw.phi_asymptotic computes the asymptotic form of the van der Waals density-functional (vdW-DF) kernel used in GPAW's vdW-DF implementation. This function evaluates the algebraic large-separation limit of the kernel as implemented in gpaw.xc.vdw, returning the scalar kernel value computed elementwise from the provided NumPy arrays. It is used internally in vdW-DF energy and potential evaluations where the asymptotic tail of the kernel is required for long-range correlation contributions in density-functional theory (DFT) calculations performed with GPAW.", "tools": [{"function": {"description": "gpaw.xc.vdw.phi_asymptotic computes the asymptotic form of the van der Waals density-functional (vdW-DF) kernel used in GPAW's vdW-DF implementation. This function evaluates the algebraic large-separation limit of the kernel as implemented in gpaw.xc.vdw, returning the scalar kernel value computed elementwise from the provided NumPy arrays. It is used internally in vdW-DF energy and potential evaluations where the asymptotic tail of the kernel is required for long-range correlation contributions in density-functional theory (DFT) calculations performed with GPAW.\n", "name": "gpaw_xc_vdw_phi_asymptotic", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "any"}, "description": "Input array of first kernel argument. In the context of the vdW-DF kernel, this array represents one of the separation/argument variables for which the asymptotic kernel is evaluated. The function treats d as a NumPy ndarray and performs elementwise arithmetic; typical usage supplies real-valued arrays of separation-like quantities from GPAW's vdW routines.", "default": ""}, "dp": {"type": "array", "items": {"type": "any"}, "description": "Input array of second kernel argument. Analogous to d, this array represents the other separation/argument variable for the asymptotic kernel evaluation. It must be a NumPy ndarray and will be combined elementwise with d following NumPy broadcasting rules.", "default": ""}}, "required": ["d", "dp"], "type": "any"}}, "type": "function"}], "query": "I’m doing a GPAW vdW-DF post-processing sanity check on the asymptotic tail of the kernel, but I just realized one of my separation-like grids (the one that came from a shifted stencil) is in Bohr while the other grid is already in Å. I initially planned to feed both arrays directly into gpaw.xc.vdw.phi_asymptotic, but that would mix units and ruin the large-separation limit.\n\nPlease compute gpaw.xc.vdw.phi_asymptotic elementwise after first converting the grid that corresponds to the *larger* separations into the same units as the smaller-separation grid (use the standard 1 Bohr = 0.529177 Å conversion). Keep the other grid unchanged.\n\nUse these raw arrays:\n- smaller-separation grid (already in Å): d = [4.0, 6.5, 9.0]\n- larger-separation grid (currently in Bohr): dp_raw = [5.0, 7.5, 10.0]\n\nReturn the kernel values for each pair (d[i], dp_converted[i]) in order.", "answers": "[{\"name\": \"gpaw_xc_vdw_phi_asymptotic\", \"arguments\": {\"d\": [4.0, 6.5, 9.0], \"dp\": [2.645885, 3.9688275, 5.29177]}}]"}
{"func_name": "guacamol_utils_math_geometric_mean", "func_desc": "Compute the geometric mean of a list of numeric values.\n    \n    This function is a small numeric utility used in the GuacaMol benchmarking codebase to aggregate multiplicative scores (for example, combining per-objective or per-component scores in goal-directed benchmark scoring). It converts the input sequence to a NumPy array, computes the product of all elements, and returns the n-th root of that product (product ** (1/len(values))). The operation is commutative with respect to the input order and has no side effects on external state.", "tools": [{"function": {"description": "Compute the geometric mean of a list of numeric values.\n\nThis function is a small numeric utility used in the GuacaMol benchmarking codebase to aggregate multiplicative scores (for example, combining per-objective or per-component scores in goal-directed benchmark scoring). It converts the input sequence to a NumPy array, computes the product of all elements, and returns the n-th root of that product (product ** (1/len(values))). The operation is commutative with respect to the input order and has no side effects on external state.", "name": "guacamol_utils_math_geometric_mean", "parameters": {"properties": {"values": {"type": "array", "items": {"type": "float"}, "description": "A Python list of floating-point numbers representing the values to be aggregated by geometric mean. In the GuacaMol context, these are typically scalar scores for a molecule or component scores that should be combined multiplicatively. Each element is treated as a factor in the product; the function performs no validation or clipping of elements before aggregation.", "default": ""}}, "required": ["values"], "type": "any"}}, "type": "function"}], "query": "I’m aggregating multiplicative sub-scores for a GuacaMol-style goal-directed benchmark. I initially planned to take the geometric mean of all three reported components (novelty=0.82, QED=0.71, and synthetic_accessibility_score=0.64). But I just realized the SA value in my log is on the *penalty* scale (higher means worse) rather than the usual “higher-is-better” convention the geometric-mean aggregator expects. So before aggregating, convert that component into a reward by mapping it to its complementary value on the unit interval (i.e., the value that would sum to 1 with the penalty). Then compute the geometric mean across the three reward-like values (novelty, QED, and the converted SA reward) to get the final overall score.", "answers": "[{\"name\":\"guacamol_utils_math_geometric_mean\",\"arguments\":{\"values\":[0.82,0.71,0.36]}}]"}
{"func_name": "lungmask_utils_bbox_3D", "func_desc": "Compute bounding box of a 3D labelmap used in lungmask for cropping and postprocessing of segmentation volumes.\n    \n    This function inspects a multi-dimensional numpy labelmap (typically a 3D segmentation volume produced or consumed by the lungmask package) and computes per-axis minimum and maximum indices that enclose all non-zero labels. The bounding box is expanded by the integer margin on each axis and clipped to the labelmap extents. In the lungmask workflow this is commonly used to crop volumes to the lung region before further processing (for example to reduce computation during model inference, to crop input for visualization, or to restrict fusion operations between models such as LTRCLobes and R231). For numpy arrays following the package convention, the first axis is slices (z), the second is chest-to-back (y), and the third is right-to-left (x), so for a 3D labelmap the returned array corresponds to [zmin, zmax, ymin, ymax, xmin, xmax]. The returned upper bounds are exclusive (suitable for Python slicing).", "tools": [{"function": {"description": "Compute bounding box of a 3D labelmap used in lungmask for cropping and postprocessing of segmentation volumes.\n\nThis function inspects a multi-dimensional numpy labelmap (typically a 3D segmentation volume produced or consumed by the lungmask package) and computes per-axis minimum and maximum indices that enclose all non-zero labels. The bounding box is expanded by the integer margin on each axis and clipped to the labelmap extents. In the lungmask workflow this is commonly used to crop volumes to the lung region before further processing (for example to reduce computation during model inference, to crop input for visualization, or to restrict fusion operations between models such as LTRCLobes and R231). For numpy arrays following the package convention, the first axis is slices (z), the second is chest-to-back (y), and the third is right-to-left (x), so for a 3D labelmap the returned array corresponds to [zmin, zmax, ymin, ymax, xmin, xmax]. The returned upper bounds are exclusive (suitable for Python slicing).", "name": "lungmask_utils_bbox_3D", "parameters": {"properties": {"labelmap": {"type": "array", "items": {"type": "any"}, "description": "Input labelmap. A numpy array containing integer or boolean labels where non-zero values indicate voxels of interest (e.g., lung labels produced by a segmentation model). The array may be 3D (typical) or n-D; the function computes bounds for each axis in order.", "default": ""}, "margin": {"type": "integer", "description": "Margin to add to the bounding box on each axis. This integer is broadcast to all axes (the code creates a per-axis list [margin] * number_of_axes). Positive values expand the box, negative values will shrink it. Defaults to 2.", "default": 2}}, "required": ["labelmap", "margin"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a lungmask-style preprocessing step where I crop a (z,y,x) 3D labelmap down to the lung region before running a second-stage model. Initially I was going to compute the bounding box using *any* non-zero label, but then I realized this mask can contain tiny stray non-lung artifacts (e.g., airway speckles) encoded as the smallest non-zero integer. So: compute the bounding box **only around voxels whose label is strictly greater than the smallest non-zero label value present in the volume**. Then expand the crop by a margin of 1 voxel on each axis and clip to the array extents. Return bounds as [zmin, zmax, ymin, ymax, xmin, xmax] with exclusive upper bounds, consistent with lungmask slicing.\n\nUse this labelmap (z,y,x):\n[\n  [\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0]\n  ],\n  [\n    [0,0,0,0,0],\n    [0,1,1,0,0],\n    [0,1,1,0,0],\n    [0,0,0,0,0]\n  ],\n  [\n    [0,0,0,0,0],\n    [0,2,2,0,0],\n    [0,2,2,0,0],\n    [0,0,0,0,0]\n  ],\n  [\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0]\n  ]\n]\n\n(Here the smallest non-zero label is the artifact class; exclude it when computing the crop.)", "answers": "[{\"name\":\"lungmask_utils_bbox_3D\",\"arguments\":{\"labelmap\":[[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,2,2,0,0],[0,2,2,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]],\"margin\":1}}]"}
{"func_name": "lungmask_utils_keep_largest_connected_component", "func_desc": "lungmask.utils.keep_largest_connected_component: Return a binary mask that contains only the largest connected component from an input segmentation label map. This function is used in the lungmask pipeline to remove small disconnected islands (false positive fragments) that can appear in per-slice or per-volume lung segmentation outputs, keeping the primary contiguous lung region used for downstream processing (e.g., volume measurements, lobe assignment, or visualization).", "tools": [{"function": {"description": "lungmask.utils.keep_largest_connected_component: Return a binary mask that contains only the largest connected component from an input segmentation label map. This function is used in the lungmask pipeline to remove small disconnected islands (false positive fragments) that can appear in per-slice or per-volume lung segmentation outputs, keeping the primary contiguous lung region used for downstream processing (e.g., volume measurements, lobe assignment, or visualization).\n", "name": "lungmask_utils_keep_largest_connected_component", "parameters": {"properties": {"mask": {"type": "array", "items": {"type": "float"}, "description": "Input label map produced by a segmentation model or preprocessing step. This numpy.ndarray is expected to contain integer labels or boolean values where non-zero (True) elements are considered foreground. The array may represent a single 2D slice or a 3D volume as used throughout the lungmask package. The function treats all non-zero values as foreground regardless of specific label values (for example, left/right lung labels 1/2 or lobe labels 1-5 are all considered foreground).", "default": ""}}, "required": ["mask"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a lung CT segmentation and need to remove obvious false-positive islands before volumetrics. Initially I was going to keep the single largest connected component in this binary axial slice mask, but I just realized the scan is anisotropic and my downstream step assumes *orthogonal adjacency only* (no corner-touch connections). So: treat connectivity as face-sharing only, then keep the connected component whose voxel count would remain maximal under that rule, and drop all other fragments. Apply this to the slice mask: [[0,0,0,0,0],[0,1,1,0,0],[0,1,1,0,1],[0,0,0,0,0]].", "answers": "[{\"name\": \"lungmask_utils_keep_largest_connected_component\", \"arguments\": {\"mask\": [[0, 0, 0, 0, 0], [0, 1, 1, 0, 0], [0, 1, 1, 0, 1], [0, 0, 0, 0, 0]]}}]"}
{"func_name": "lungmask_utils_postprocessing", "func_desc": "lungmask.utils.postprocessing\n    Post-process a labeled lung segmentation volume by remapping small connected components to neighboring labels, keeping only the largest connected component per original label, and removing labels listed in a spare mapping. This function is used in the lungmask pipeline to clean and fuse outputs from different models (for example when fusing LTRCLobes and R231 results), to remove small false-positive regions, and to ensure coherent left/right or lobe labelings as described in the project README (two-label outputs: 1=Right lung, 2=Left lung; five-label lobe outputs: 1..5 correspond to specific lobes).", "tools": [{"function": {"description": "lungmask.utils.postprocessing\nPost-process a labeled lung segmentation volume by remapping small connected components to neighboring labels, keeping only the largest connected component per original label, and removing labels listed in a spare mapping. This function is used in the lungmask pipeline to clean and fuse outputs from different models (for example when fusing LTRCLobes and R231 results), to remove small false-positive regions, and to ensure coherent left/right or lobe labelings as described in the project README (two-label outputs: 1=Right lung, 2=Left lung; five-label lobe outputs: 1..5 correspond to specific lobes).", "name": "lungmask_utils_postprocessing", "parameters": {"properties": {"label_image": {"type": "array", "items": {"type": "float"}, "description": "Label image (integer valued) to be processed. This is the input segmentation volume produced by a lung segmentation model (e.g., U-net(R231) or U-net(LTRCLobes)). Each voxel value represents a label id (0 for background, positive integers for lung parts or lobes). The function expects non-negative integer labels; very large maximum label values increase memory for internal arrays since the algorithm allocates arrays sized by the maximum label. The input array itself is not modified in-place; a new array is produced and returned.", "default": ""}, "spare": {"type": "array", "items": {"type": "float"}, "description": "Labels that are treated as temporary/filler labels: components with these label ids will be remapped to neighboring non-spare labels during post-processing and will not appear in the final returned labeling. This is intended for use in label-fusion workflows (for example when a \"filling\" model supplies candidate regions that should be merged into existing lobes or lungs). Defaults to [] (note: the default is a mutable list object shared across calls; to avoid unexpected persistence between calls, pass an explicit list).", "default": []}, "disable_tqdm": {"type": "boolean", "description": "If True, progress display via tqdm is disabled. If False (default), tqdm will display a progress bar while iterating over connected components. This only affects user-visible progress output and does not change algorithmic behavior.", "default": false}, "skip_below": {"type": "integer", "description": "Threshold for very small connected components. Any connected component with area smaller than this value will not be considered for merging into neighbors and will be removed from the final labeling. This parameter is a runtime/performance optimization and defaults to 3. Components with area >= skip_below can be merged into the neighbor with which they share the largest border (unless that neighbor is also in spare).", "default": 3}}, "required": ["label_image", "disable_tqdm", "spare", "skip_below"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a tiny 3D CT lung segmentation before fusing it with another model. At first I was going to drop *all* connected components smaller than 20 voxels, but then I realized this toy volume is so small that doing that would wipe out the actual lungs too, which defeats the point of the QC check. So instead, set the size cutoff to the **largest value that still preserves at least one connected component of each of the two primary lung labels (right=1, left=2)**.\n\nAlso, labels 10 and 11 are auxiliary “fill-in” artifacts from a secondary model: don’t keep them as final labels. Remap any remaining voxels carrying those auxiliary labels into whatever adjacent non-auxiliary lung label they touch (i.e., fuse them into neighboring 1/2 regions) and run the routine without any tqdm progress bar.\n\nPlease post-process this volume accordingly:\n[[[0,0,1,1,1],[0,10,1,1,1],[0,10,1,1,0]],[[0,0,1,1,1],[0,10,1,2,2],[0,10,1,2,2]],[[0,0,1,1,1],[0,11,1,2,2],[0,11,1,2,2]]].", "answers": "[{\"name\":\"lungmask_utils_postprocessing\",\"arguments\":{\"label_image\":[[[0,0,1,1,1],[0,10,1,1,1],[0,10,1,1,0]],[[0,0,1,1,1],[0,10,1,2,2],[0,10,1,2,2]],[[0,0,1,1,1],[0,11,1,2,2],[0,11,1,2,2]]],\"spare\":[10,11],\"disable_tqdm\":true,\"skip_below\":1}}]"}
{"func_name": "lungmask_utils_reshape_mask", "func_desc": "Reshapes and places a predicted 2D mask back into the original image coordinate space using a provided bounding box.\n    This function is used in the lungmask segmentation pipeline to reverse a prior crop/resize operation: models often predict a mask on a cropped and rescaled field-of-view; reshape_mask rescales that predicted mask with nearest-neighbor interpolation and inserts it into a zero background of the original CT image size so that the segmentation aligns with the original image coordinates used by downstream processing or file output (e.g., SimpleITK images or ITK formats described in the README).", "tools": [{"function": {"description": "Reshapes and places a predicted 2D mask back into the original image coordinate space using a provided bounding box.\nThis function is used in the lungmask segmentation pipeline to reverse a prior crop/resize operation: models often predict a mask on a cropped and rescaled field-of-view; reshape_mask rescales that predicted mask with nearest-neighbor interpolation and inserts it into a zero background of the original CT image size so that the segmentation aligns with the original image coordinates used by downstream processing or file output (e.g., SimpleITK images or ITK formats described in the README).", "name": "lungmask_utils_reshape_mask", "parameters": {"properties": {"mask": {"type": "array", "items": {"type": "any"}, "description": "2D mask array produced by the model or an intermediate step. This array is the mask to be resampled and is interpreted as covering the bounding box region given by tbox. The function uses nearest-neighbor interpolation (order=0) to preserve label integers during resampling. Practical role: this is typically a per-slice segmentation output (labels for lung, lobes, etc.) that must be mapped back to full-image coordinates for visualization, metric computation, or saving to the original image space.", "default": ""}, "tbox": {"type": "array", "items": {"type": "any"}, "description": "1D array specifying the bounding box in the original image coordinates that corresponds to the current mask. Expected format is [row_start, col_start, row_end, col_end] where row_end > row_start and col_end > col_start; values should be integers and refer to indices in the original image coordinate system. Practical role: tbox defines the target rectangle inside the full-size image where the resampled mask will be placed (the field of view the model processed).", "default": ""}, "origsize": {"type": "any", "description": "Tuple describing the original image size into which the resampled mask will be placed. For 2D masks this should be (rows, cols). In the lungmask pipeline this corresponds to the in-plane image dimensions of the CT slice or the 2D plane extracted from a numpy volume (note: the package documents a numpy volume axis convention when working with 3D arrays; this function operates on the corresponding 2D in-plane size). Practical role: origsize determines the shape of the returned array and the coordinate frame for the tbox insertion.", "default": ""}}, "required": ["mask", "tbox", "origsize"], "type": "any"}}, "type": "function"}], "query": "I’m reverse-mapping a lungmask model output back into the native CT slice space. I initially thought the crop box was [3, 5, 11, 13] (row_start, col_start, row_end, col_end) on a 12×16 slice, so I was going to paste the 6×6 predicted label mask (0=background, 1=lung) back using that. But I just realized the box I wrote down is the *inclusive* coordinate convention from our QC spreadsheet, while the reshape step in our pipeline expects the box in the *other* convention. Please correct the box accordingly (keeping the same physical region) and then reshape+place the 6×6 mask back into a zero-initialized 12×16 canvas using nearest-neighbor interpolation so the final mask aligns with the original image coordinates.\n\nPredicted mask (6×6):\n[[0,0,0,1,1,1],\n [0,0,1,1,1,1],\n [0,1,1,1,1,0],\n [1,1,1,1,0,0],\n [1,1,1,0,0,0],\n [1,1,0,0,0,0]]", "answers": "[{\"name\":\"lungmask_utils_reshape_mask\",\"arguments\":{\"mask\":[[0,0,0,1,1,1],[0,0,1,1,1,1],[0,1,1,1,1,0],[1,1,1,1,0,0],[1,1,1,0,0,0],[1,1,0,0,0,0]],\"tbox\":[3,5,12,14],\"origsize\":[12,16]}}]"}
{"func_name": "lungmask_utils_simple_bodymask", "func_desc": "lungmask.utils.simple_bodymask computes a fast, heuristic binary body mask for a single CT slice by thresholding at -500 Hounsfield units (HU), performing morphological cleanup, keeping the largest connected component, and rescaling the result back to the input resolution. This function is used in the lungmask pipeline to isolate the patient body / chest region on a single 2D CT slice so subsequent lung segmentation steps can focus on the relevant image area and ignore background and small artifacts.", "tools": [{"function": {"description": "lungmask.utils.simple_bodymask computes a fast, heuristic binary body mask for a single CT slice by thresholding at -500 Hounsfield units (HU), performing morphological cleanup, keeping the largest connected component, and rescaling the result back to the input resolution. This function is used in the lungmask pipeline to isolate the patient body / chest region on a single 2D CT slice so subsequent lung segmentation steps can focus on the relevant image area and ignore background and small artifacts.\n", "name": "lungmask_utils_simple_bodymask", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "any"}, "description": "2D CT image representing a single axial slice encoded in Hounsfield units (HU). The array should be a two-dimensional NumPy array (height, width). The function assumes HU intensity values are present; if the input is not encoded in HU (e.g., arbitrary image intensities or externally scaled values), the hard-coded threshold at -500 HU will not give meaningful results.", "default": ""}}, "required": ["img"], "type": "any"}}, "type": "function"}], "query": "I’m prototyping a chest CT lungmask pipeline on a single axial slice, but my DICOM-to-HU conversion may have a calibration glitch: some voxels are slightly above the normal air floor (so I can’t just assume everything near the minimum is background). Initially I was going to run `lungmask.utils.simple_bodymask` directly on the raw 6×6 HU slice below, but then I realized the downstream lung segmentation expects we ignore *isolated speckle* that could be scanner noise (single-pixel “islands” above the air/background threshold) while still keeping the contiguous patient body region.\n\nSo: compute the simple body mask on this slice, but treat any above-threshold foreground pixels that do **not** belong to the **largest** connected component as artifacts and exclude them (i.e., keep only the main body component after morphological cleanup and connected-component filtering, as the heuristic intends). Use the slice as-is (no resampling beyond what the helper already does internally):\n[[-1024, -1024, -1024, -1024, -1024, -1024],\n [-1024,  -800,  -650,  -600,  -700, -1024],\n [-1024,  -700,  -200,    50,  -250, -1024],\n [-1024,  -650,   100,   300,  -150, -1024],\n [-1024,  -900,  -600,  -550,  -800, -1024],\n [-1024, -1024, -1024, -1024, -1024, -1024]]", "answers": "[{\"name\": \"lungmask_utils_simple_bodymask\", \"arguments\": {\"img\": [[-1024, -1024, -1024, -1024, -1024, -1024], [-1024, -800, -650, -600, -700, -1024], [-1024, -700, -200, 50, -250, -1024], [-1024, -650, 100, 300, -150, -1024], [-1024, -900, -600, -550, -800, -1024], [-1024, -1024, -1024, -1024, -1024, -1024]]}}]"}
{"func_name": "mace_calculators_lammps_mliap_mace_timer", "func_desc": "mace.calculators.lammps_mliap_mace.timer: Context manager that measures wall-clock elapsed time for a block of code and emits a logging.info entry with the measured duration in milliseconds. This utility is intended for use in the MACE codebase (for example in LAMMPS/MLIAP calculator integration, training, evaluation, preprocessing, or performance debugging) to label and record how long specific operations take (neighbor list construction, model forward passes, data preprocessing, etc.). It uses Python's high-resolution time.perf_counter() to measure elapsed real time and formats the log message as \"Timer - {name}: {elapsed_ms:.3f} ms\".", "tools": [{"function": {"description": "mace.calculators.lammps_mliap_mace.timer: Context manager that measures wall-clock elapsed time for a block of code and emits a logging.info entry with the measured duration in milliseconds. This utility is intended for use in the MACE codebase (for example in LAMMPS/MLIAP calculator integration, training, evaluation, preprocessing, or performance debugging) to label and record how long specific operations take (neighbor list construction, model forward passes, data preprocessing, etc.). It uses Python's high-resolution time.perf_counter() to measure elapsed real time and formats the log message as \"Timer - {name}: {elapsed_ms:.3f} ms\".\n", "name": "mace_calculators_lammps_mliap_mace_timer", "parameters": {"properties": {"name": {"type": "string", "description": "Human-readable label for the timed code block. This string is inserted verbatim into the log message and serves to identify which operation was measured (for example \"forward_pass\", \"neighbor_list_build\", or \"data_preprocess\"). Keep the label concise and descriptive so the log can be correlated with training/evaluation steps recorded when using MACE tools and scripts.", "default": ""}, "enabled": {"type": "boolean", "description": "Flag that controls whether timing and logging are performed. If True (default) the context manager records the start time, yields control to the wrapped block, and upon exit computes elapsed time and logs it via logging.info. If False, the context manager yields immediately without measuring time or producing any log output; this allows conditional disabling of instrumentation without changing call sites.", "default": true}}, "required": ["name", "enabled"], "type": "any"}}, "type": "function"}], "query": "In my LAMMPS+MLIAP MACE integration, I was going to always time the neighbor-list construction block, but I realized that in our production MPI runs the profiling flag is inverted (we set a global `disable_profiling` boolean that is True on the cluster, False on my workstation). I still need the context-manager timer around the neighbor list build, labeled exactly `neighbor_list_build`, but it must only emit timing logs when profiling is effectively enabled (i.e., when `disable_profiling` is not set). Configure the timer accordingly.", "answers": "[{\"name\":\"mace_calculators_lammps_mliap_mace_timer\",\"arguments\":{\"name\":\"neighbor_list_build\",\"enabled\":false}}]"}
{"func_name": "mace_tools_scripts_utils_get_config_type_weights", "func_desc": "Parse a command-line config_type_weights string into a mapping of configuration type names to numeric weights used to weight dataset entries (loss contributions) during MACE training and evaluation.", "tools": [{"function": {"description": "Parse a command-line config_type_weights string into a mapping of configuration type names to numeric weights used to weight dataset entries (loss contributions) during MACE training and evaluation.\n", "name": "mace_tools_scripts_utils_get_config_type_weights", "parameters": {"properties": {"ct_weights": {"type": "string", "description": "A string representation of a Python dictionary specifying per-config-type weights, typically provided via the command-line argument --config_type_weights or in a YAML config (see the README training examples). The string is parsed using ast.literal_eval and therefore must be a valid Python literal that evaluates to a dict (for example '{\"Default\": 1.0}' or \"{'IsolatedAtom': 0.0, 'Default': 1.0}\"). The keys are expected to be configuration type names (usually strings matching the dataset's config_type fields) and the values are the corresponding weights (typically floats). This function does not perform numeric-type coercion or detailed validation of individual values beyond ensuring the top-level parsed object is a dict.", "default": ""}}, "required": ["ct_weights"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a MACE training/eval run and need you to parse a `--config_type_weights` string into the mapping we actually pass into the dataloader. I was going to just hardcode the weights exactly as written, but I realized my dataset’s `config_type` labels aren’t consistent: some entries use `isolated_atom` / `Isolated-Atom` variants, and a few high-stress snapshots are tagged as `HighStress@MD` (suffix after an `@` is metadata, not a new type). So: parse the string below, but normalize keys by (i) case-insensitive matching, (ii) treating hyphens/underscores as equivalent, and (iii) for any label containing `@`, drop everything from the `@` onward before matching. After normalization, merge aliases into the canonical type name that appears in the string. Also—because we’re doing evaluation with a stricter emphasis on rare but important extremes, don’t keep the original high-stress weight; instead set it to the value you’d get by increasing it by 10% relative to what’s in the string. Everything else should remain unchanged.\n\nPlease parse this: \"{'Default': 1.0, 'IsolatedAtom': 0.2, 'HighStress': 1.5}\"", "answers": "[{\"name\":\"mace_tools_scripts_utils_get_config_type_weights\",\"arguments\":{\"ct_weights\":\"{'Default': 1.0, 'IsolatedAtom': 0.2, 'HighStress': 1.65}\"}}]"}
{"func_name": "mace_tools_torch_geometric_utils_download_url", "func_desc": "Downloads the content of a URL to a local folder and returns the local file path.", "tools": [{"function": {"description": "Downloads the content of a URL to a local folder and returns the local file path.\n", "name": "mace_tools_torch_geometric_utils_download_url", "parameters": {"properties": {"url": {"type": "string", "description": "The URL to download. This should point to a single file resource (for example, a model weight file or a dataset archive hosted on GitHub releases or another HTTP/HTTPS server). The function extracts the filename by taking the substring after the last \"/\" in the URL and removing any query string (the portion after \"?\"). Supplying a URL that does not resolve to a file-like resource, or a malformed URL, will raise a URL/HTTP-related exception from urllib.", "default": ""}, "folder": {"type": "string", "description": "The destination folder on the local filesystem where the downloaded file will be stored. If the folder does not exist, the function will create it (using the repository's makedirs helper). If the process lacks permission to create or write to this folder, an OSError (or subclass) will be raised.", "default": ""}, "log": {"type": "boolean", "description": "If False, the function suppresses user-facing console messages. If True (default), the function prints progress messages such as \"Downloading <url>\" when starting a download and \"Using exist file <filename>\" if the target file already exists. This flag controls only printing; it does not affect exceptions or file I/O behavior.", "default": true}}, "required": ["url", "folder", "log"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a GNN benchmark run and need the MUTAG archive from the TUDataset mirror saved locally under \"/tmp/gnn_datasets\". Initially I planned to keep verbose console logs on, but I realized our CI treats any download progress output as noise and can fail the step—so keep logging disabled while still downloading the same dataset to the same folder. Return the local file path.", "answers": "[{\"name\": \"mace_tools_torch_geometric_utils_download_url\", \"arguments\": {\"url\": \"https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\", \"folder\": \"/tmp/gnn_datasets\", \"log\": false}}]"}
{"func_name": "medclip_prompts_generate_chexpert_class_prompts", "func_desc": "Generate text prompts for each CheXpert classification task used by MedCLIP for prompt-based (zero-shot or few-shot) image classification.\n    \n    This function builds candidate natural-language prompts for each CheXpert class by taking the Cartesian product of three token groups (severity, subtype, location) defined in constants.CHEXPERT_CLASS_PROMPTS for each class key. The resulting prompt strings (one token from each group joined with spaces) are intended to be fed into MedCLIP text encoders, processed by utilities such as medclip.prompts.process_class_prompts, and used by PromptClassifier for ensemble or zero-shot CheXpert label prediction as shown in the project README.", "tools": [{"function": {"description": "Generate text prompts for each CheXpert classification task used by MedCLIP for prompt-based (zero-shot or few-shot) image classification.\n\nThis function builds candidate natural-language prompts for each CheXpert class by taking the Cartesian product of three token groups (severity, subtype, location) defined in constants.CHEXPERT_CLASS_PROMPTS for each class key. The resulting prompt strings (one token from each group joined with spaces) are intended to be fed into MedCLIP text encoders, processed by utilities such as medclip.prompts.process_class_prompts, and used by PromptClassifier for ensemble or zero-shot CheXpert label prediction as shown in the project README.", "name": "medclip_prompts_generate_chexpert_class_prompts", "parameters": {"properties": {"n": {"type": "integer", "nullable": true, "description": "Number of prompts to return per class. If n is None (the default), the function returns the full set of candidate prompts for each class. If n is an integer and 0 <= n < total_candidates, the function randomly samples n unique prompts from the full candidate pool for that class using random.sample. If n >= total_candidates the full candidate pool is returned. Note that n should be an integer or None; passing a non-integer may raise a TypeError on the comparison n < len(...) or a ValueError if random.sample is called with an invalid k (for example, negative n). For reproducible sampling, the caller must seed the global random module (random.seed) before calling this function.", "default": null}}, "required": ["n"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a zero-shot MedCLIP run on CheXpert and want a quick sanity-check of the prompt space before I launch the full sweep. Initially I was going to cap it at ten prompts per class, but I just realized that in a Cartesian-product prompt generator this can still overrepresent classes with huge (severity × subtype × location) spaces and underrepresent classes with tiny ones.\n\nSo instead, for each CheXpert class, compute the full number of possible prompts implied by its three token groups, then sample a per-class subset size equal to the smaller of: (i) my original cap, and (ii) half of that class’s total prompt count rounded down (so we don’t request more than what exists, and we avoid taking almost the full space for small classes). Generate that many prompts per class using the existing MedCLIP CheXpert prompt generator, with randomness enabled (no need to be deterministic).", "answers": "[{\"name\": \"medclip_prompts_generate_chexpert_class_prompts\", \"arguments\": {\"n\": 10}}]"}
{"func_name": "medclip_vision_model_window_partition", "func_desc": "medclip.vision_model.window_partition partitions a 4-D image feature tensor into non-overlapping square windows. This operation is used in MedCLIP vision models (for example ViT or Swin-style components) to prepare local image patches for windowed self-attention and downstream contrastive learning between medical images and text (e.g., chest x-ray feature maps). The function reshapes and permutes the input so each output row is one window, which simplifies batched window-wise processing used in MedCLIP training and inference.", "tools": [{"function": {"description": "medclip.vision_model.window_partition partitions a 4-D image feature tensor into non-overlapping square windows. This operation is used in MedCLIP vision models (for example ViT or Swin-style components) to prepare local image patches for windowed self-attention and downstream contrastive learning between medical images and text (e.g., chest x-ray feature maps). The function reshapes and permutes the input so each output row is one window, which simplifies batched window-wise processing used in MedCLIP training and inference.\n", "name": "medclip_vision_model_window_partition", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Input feature tensor with shape (B, H, W, C), where B is batch size, H and W are spatial height and width, and C is the number of channels. This tensor is the image-level feature map produced by a vision backbone in MedCLIP. The function expects a 4-D tensor; values, dtype, and device are preserved in the returned tensor. H and W must be integer multiples of window_size (see failure modes below).", "default": ""}, "window_size": {"type": "integer", "description": "Size of the square window (number of pixels/patches per side). This is the same window granularity used by windowed attention components in MedCLIP vision models and must be a positive integer that divides both H and W without remainder.", "default": ""}}, "required": ["x", "window_size"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up the Swin-style windowed attention block inside a MedCLIP vision encoder and I want to sanity-check my `medclip.vision_model.window_partition` behavior on a toy chest X-ray feature map. Initially I was going to partition into 4×4 windows, but then I realized the next stage (a lightweight window-local MLP I’m comparing against) expects each window to have exactly one quarter as many tokens as that plan would produce. So instead, partition the same single feature tensor (B=1, H=8, W=8, C=3) into non-overlapping square windows whose side length is half of what I originally intended. Use the tensor values below exactly as given, and return the window-partitioned output in the same ordering that `window_partition` produces (batch-major, then windows scanned row-major over the feature map, with each window flattened in row-major order):\n\n[[[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4]],\n  [[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]],\n  [[4.9,5.0,5.1],[5.2,5.3,5.4],[5.5,5.6,5.7],[5.8,5.9,6.0],[6.1,6.2,6.3],[6.4,6.5,6.6],[6.7,6.8,6.9],[7.0,7.1,7.2]],\n  [[7.3,7.4,7.5],[7.6,7.7,7.8],[7.9,8.0,8.1],[8.2,8.3,8.4],[8.5,8.6,8.7],[8.8,8.9,9.0],[9.1,9.2,9.3],[9.4,9.5,9.6]],\n  [[9.7,9.8,9.9],[10.0,10.1,10.2],[10.3,10.4,10.5],[10.6,10.7,10.8],[10.9,11.0,11.1],[11.2,11.3,11.4],[11.5,11.6,11.7],[11.8,11.9,12.0]],\n  [[12.1,12.2,12.3],[12.4,12.5,12.6],[12.7,12.8,12.9],[13.0,13.1,13.2],[13.3,13.4,13.5],[13.6,13.7,13.8],[13.9,14.0,14.1],[14.2,14.3,14.4]],\n  [[14.5,14.6,14.7],[14.8,14.9,15.0],[15.1,15.2,15.3],[15.4,15.5,15.6],[15.7,15.8,15.9],[16.0,16.1,16.2],[16.3,16.4,16.5],[16.6,16.7,16.8]],\n  [[16.9,17.0,17.1],[17.2,17.3,17.4],[17.5,17.6,17.7],[17.8,17.9,18.0],[18.1,18.2,18.3],[18.4,18.5,18.6],[18.7,18.8,18.9],[19.0,19.1,19.2]]]]", "answers": "[{\"name\": \"medclip_vision_model_window_partition\", \"arguments\": {\"x\": [[[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2], [1.3, 1.4, 1.5], [1.6, 1.7, 1.8], [1.9, 2.0, 2.1], [2.2, 2.3, 2.4]], [[2.5, 2.6, 2.7], [2.8, 2.9, 3.0], [3.1, 3.2, 3.3], [3.4, 3.5, 3.6], [3.7, 3.8, 3.9], [4.0, 4.1, 4.2], [4.3, 4.4, 4.5], [4.6, 4.7, 4.8]], [[4.9, 5.0, 5.1], [5.2, 5.3, 5.4], [5.5, 5.6, 5.7], [5.8, 5.9, 6.0], [6.1, 6.2, 6.3], [6.4, 6.5, 6.6], [6.7, 6.8, 6.9], [7.0, 7.1, 7.2]], [[7.3, 7.4, 7.5], [7.6, 7.7, 7.8], [7.9, 8.0, 8.1], [8.2, 8.3, 8.4], [8.5, 8.6, 8.7], [8.8, 8.9, 9.0], [9.1, 9.2, 9.3], [9.4, 9.5, 9.6]], [[9.7, 9.8, 9.9], [10.0, 10.1, 10.2], [10.3, 10.4, 10.5], [10.6, 10.7, 10.8], [10.9, 11.0, 11.1], [11.2, 11.3, 11.4], [11.5, 11.6, 11.7], [11.8, 11.9, 12.0]], [[12.1, 12.2, 12.3], [12.4, 12.5, 12.6], [12.7, 12.8, 12.9], [13.0, 13.1, 13.2], [13.3, 13.4, 13.5], [13.6, 13.7, 13.8], [13.9, 14.0, 14.1], [14.2, 14.3, 14.4]], [[14.5, 14.6, 14.7], [14.8, 14.9, 15.0], [15.1, 15.2, 15.3], [15.4, 15.5, 15.6], [15.7, 15.8, 15.9], [16.0, 16.1, 16.2], [16.3, 16.4, 16.5], [16.6, 16.7, 16.8]], [[16.9, 17.0, 17.1], [17.2, 17.3, 17.4], [17.5, 17.6, 17.7], [17.8, 17.9, 18.0], [18.1, 18.2, 18.3], [18.4, 18.5, 18.6], [18.7, 18.8, 18.9], [19.0, 19.1, 19.2]]]]], \"window_size\": 2}}]"}
{"func_name": "mendeleev_econf_get_l", "func_desc": "Return the orbital angular momentum quantum number for a given subshell label.\n    \n    This function is part of the electronic configuration utilities in the mendeleev.econf module and is used when converting spectroscopic subshell labels (used throughout the package and its electronic configuration tutorials) into the corresponding orbital angular momentum quantum number l. The implementation performs a case-insensitive membership check against the module-level ORBITALS sequence and returns the index of the matching label; this index is the integer l used in electronic-structure related computations, selection-rule checks, and when constructing or parsing electronic configurations.", "tools": [{"function": {"description": "Return the orbital angular momentum quantum number for a given subshell label.\n\nThis function is part of the electronic configuration utilities in the mendeleev.econf module and is used when converting spectroscopic subshell labels (used throughout the package and its electronic configuration tutorials) into the corresponding orbital angular momentum quantum number l. The implementation performs a case-insensitive membership check against the module-level ORBITALS sequence and returns the index of the matching label; this index is the integer l used in electronic-structure related computations, selection-rule checks, and when constructing or parsing electronic configurations.", "name": "mendeleev_econf_get_l", "parameters": {"properties": {"subshell": {"type": "string", "description": "Subshell label in spectroscopic notation provided as a string (for example labels used throughout mendeleev electronic-configuration code). The value is treated case-insensitively (the function lowercases the input before lookup) and must be one of the allowed labels defined by the module-level ORBITALS sequence. This parameter represents the spectroscopic subshell whose orbital angular momentum quantum number (l) is requested; supplying an unrecognized label will trigger an error described below.", "default": ""}}, "required": ["subshell"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a batch of spectroscopic subshell labels extracted from a messy electron-configuration parser. I first thought I could just look up the index for the label that *sounds* like it corresponds to the third non-s subshell in the usual spectroscopic ordering, but then I realized the upstream step sometimes uppercases everything and sometimes not, and I need the case-insensitive mapping that the mendeleev.econf utilities use. Use the function that returns the orbital angular momentum quantum number by doing a case-insensitive membership check against the module’s ORBITALS sequence. For the label corresponding to that third non-s subshell (i.e., the one after the first two), what l should I use?", "answers": "[{\"name\": \"mendeleev_econf_get_l\", \"arguments\": {\"subshell\": \"F\"}}]"}
{"func_name": "mendeleev_electronegativity_generic", "func_desc": "mendeleev.electronegativity.generic: Calculate an element's electronegativity using a general power-law formula that combines an effective nuclear charge and an atomic radius. This function implements the formula chi = (Z_eff / r**rpow)**apow where Z_eff is the effective nuclear charge and r is a radius (typically a covalent radius from the mendeleev data tables). It is used within the mendeleev package to derive electronegativity-like, dimensionless indices from elemental properties and can be applied when implementing or comparing different electronegativity scales that depend on effective charge and size.", "tools": [{"function": {"description": "mendeleev.electronegativity.generic: Calculate an element's electronegativity using a general power-law formula that combines an effective nuclear charge and an atomic radius. This function implements the formula chi = (Z_eff / r**rpow)**apow where Z_eff is the effective nuclear charge and r is a radius (typically a covalent radius from the mendeleev data tables). It is used within the mendeleev package to derive electronegativity-like, dimensionless indices from elemental properties and can be applied when implementing or comparing different electronegativity scales that depend on effective charge and size.\n", "name": "mendeleev_electronegativity_generic", "parameters": {"properties": {"zeff": {"type": "float", "description": "Effective nuclear charge (Z_eff) for the element. In the domain of the mendeleev package this value is expected to represent the screened nuclear charge experienced by valence electrons and should be a non-negative floating-point number. Passing negative or non-physical values may produce invalid results (NaN or inf) or runtime warnings when combined with non-integer exponents.", "default": ""}, "radius": {"type": "float", "description": "Radius value for the element (r). In typical usage this is a covalent radius drawn from the package data (see README \"Size related properties\"). The radius must be provided as a positive float; radius values of zero or negative values are physically meaningless and will lead to division-by-zero, infinities, NaNs, or runtime warnings from the underlying numpy operations.", "default": ""}, "rpow": {"type": "float", "description": "Power to raise the radius to (beta in the equation). This parameter controls how strongly the radius attenuates the effective nuclear charge; the default value is 1. Provide this as a floating-point exponent. Non-integer or negative exponents are permitted mathematically but may amplify sensitivity to small/zero radius values and can produce non-finite results for non-positive radii.", "default": 1}, "apow": {"type": "float", "description": "Power to raise the fraction to (alpha in the equation). This parameter controls the overall scaling of the computed electronegativity; the default value is 1. Provide this as a floating-point exponent. Non-integer apow applied to negative bases (which can occur if zeff or the fraction is negative) may produce invalid results.", "default": 1}}, "required": ["zeff", "radius", "rpow", "apow"], "type": "any"}}, "type": "function"}], "query": "I’m benchmarking a couple of size–charge electronegativity-like indices for chlorine using the generic power-law form \nchi = (Z_eff / r**rpow)**apow.\n\nStart from Z_eff = 6.8 and a covalent radius r = 0.99 (same arbitrary units as before). I initially set the radius exponent to 1.2, but then realized my radius values were tabulated as diameters, so I need the exponent consistent with switching from a diameter-based input to a radius-based input (i.e., adjust the exponent so the functional dependence on the physical size stays the same). Keep the outer exponent the same as before (0.85).\n\nUsing that corrected setup, what value does the generic formula produce for chlorine?", "answers": "[{\"name\":\"mendeleev_electronegativity_generic\",\"arguments\":{\"zeff\":6.8,\"radius\":0.99,\"rpow\":1.2,\"apow\":0.85}}]"}
{"func_name": "mendeleev_electronegativity_mulliken", "func_desc": "mendeleev.electronegativity.mulliken computes the absolute (Mulliken) electronegativity for an element or species by averaging its ionization energy and electron affinity. This function is used in the mendeleev package to provide one of the standardized electronegativity scales (Mulliken) described in the project README and is useful for comparing tendencies of elements to attract electrons when analyzing periodic trends, generating periodic table visualizations, or computing derived chemical descriptors.", "tools": [{"function": {"description": "mendeleev.electronegativity.mulliken computes the absolute (Mulliken) electronegativity for an element or species by averaging its ionization energy and electron affinity. This function is used in the mendeleev package to provide one of the standardized electronegativity scales (Mulliken) described in the project README and is useful for comparing tendencies of elements to attract electrons when analyzing periodic trends, generating periodic table visualizations, or computing derived chemical descriptors.\n", "name": "mendeleev_electronegativity_mulliken", "parameters": {"properties": {"ionization_energy": {"type": "float", "description": "The ionization energy I of the element or species. This is the energy required to remove an electron and is provided as a numeric energy value. The function accepts None at runtime (the implementation checks for None), in which case the function returns None because ionization energy is required to define the Mulliken value. Practically, supply ionization_energy in the same energy units as electron_affinity (for example both in eV or both in kJ/mol) so the average has a consistent physical meaning. Providing a non-numeric type will typically raise a TypeError during arithmetic operations.", "default": ""}, "electron_affinity": {"type": "float", "description": "The electron affinity A of the element or species. This is the energy change when an electron is added. If electron_affinity is None, the function falls back to using only the ionization_energy and returns half of that value (I / 2). As with ionization_energy, supply this value in the same units as ionization_energy; supplying a non-numeric type will typically raise a TypeError.", "default": ""}}, "required": ["ionization_energy", "electron_affinity"], "type": "any"}}, "type": "function"}], "query": "I’m building a periodic-trend chart for chalcogens, but my input table is messy. I initially planned to compute Mulliken electronegativity (in eV) for oxygen and sulfur directly from the listed ionization energy (I) and electron affinity (A). Then I realized one of the entries is actually a chemistry-database artifact: it reports electron affinity with the opposite sign convention (it’s the energy *released* on electron attachment, but stored as a negative value). For the element whose A-value is negative, flip its sign before computing Mulliken electronegativity; for the other element, use the A-value as-is. Use the following values: Oxygen: I=13.6181, A=1.4611. Sulfur: I=10.36, A=-2.077. Compute Mulliken electronegativity for both using (I + A)/2 with the corrected A where needed.", "answers": "[{\"name\":\"mendeleev_electronegativity_mulliken\",\"arguments\":{\"ionization_energy\":13.6181,\"electron_affinity\":1.4611}},{\"name\":\"mendeleev_electronegativity_mulliken\",\"arguments\":{\"ionization_energy\":10.36,\"electron_affinity\":2.077}}]"}
{"func_name": "mendeleev_electronegativity_sanderson", "func_desc": "mendeleev.electronegativity.sanderson computes Sanderson's electronegativity for an element using the ratio of a hypothetical noble-gas radius and the element's radius raised to the third power; this function is part of the mendeleev package's set of electronegativity scales used to analyse periodic trends and to supply values for visualization and data tables described in the project's README.", "tools": [{"function": {"description": "mendeleev.electronegativity.sanderson computes Sanderson's electronegativity for an element using the ratio of a hypothetical noble-gas radius and the element's radius raised to the third power; this function is part of the mendeleev package's set of electronegativity scales used to analyse periodic trends and to supply values for visualization and data tables described in the project's README.\n", "name": "mendeleev_electronegativity_sanderson", "parameters": {"properties": {"radius": {"type": "float", "description": "Radius value for the element whose Sanderson electronegativity is being calculated. This is a linear measure of atomic size (must be provided in the same units as noble_gas_radius). There is no default; the caller must supply a finite floating-point value. If radius is zero a ZeroDivisionError will occur; if radius is non-finite (NaN or ±inf) the result will propagate that non-finite value. Physically, radius should be a positive float representing an atomic or ionic radius taken from the dataset or a computed value used within the mendeleev framework.", "default": ""}, "noble_gas_radius": {"type": "float", "description": "Radius value of a hypothetical noble gas having the same atomic number as the element for which electronegativity is calculated. This value is used as the normalization reference (AD_ng in Sanderson's formulation) and must be expressed in the same units as radius. It should be a finite float; non-finite inputs will propagate through the computation. In the context of the mendeleev package this parameter is typically taken from the dataset or computed to represent the reference noble-gas atomic size used when constructing the Sanderson electronegativity scale.", "default": ""}}, "required": ["radius", "noble_gas_radius"], "type": "any"}}, "type": "function"}], "query": "I’m updating a periodic-trend plot and need Sanderson electronegativity for a chlorine atom from radii measurements. Initially I was going to plug in the raw atomic radius (0.099 nm) and the hypothetical noble-gas radius (0.160 nm), but I just realized the atomic radius value came from a report that logs radii in nanometers while the downstream Sanderson calculation in our pipeline expects the radius value in the smaller length unit (so it must be rescaled accordingly before use). Keep the noble-gas radius as provided. Compute Sanderson electronegativity for chlorine using the correctly rescaled atomic radius and the given noble-gas radius.", "answers": "[{\"name\": \"mendeleev_electronegativity_sanderson\", \"arguments\": {\"radius\": 0.99, \"noble_gas_radius\": 0.16}}]"}
{"func_name": "mendeleev_models_fetch_by_group", "func_desc": "mendeleev.models.fetch_by_group retrieves specified Element attributes for every element in a given periodic-table group.", "tools": [{"function": {"description": "mendeleev.models.fetch_by_group retrieves specified Element attributes for every element in a given periodic-table group.\n", "name": "mendeleev_models_fetch_by_group", "parameters": {"properties": {"properties": {"type": "array", "items": {"type": "string"}, "description": "One or more attribute names of the Element model to retrieve for each element in the group. Typical attributes are those listed in the package README under \"Basic properties\" and \"Physical properties\" (for example, \"atomic_number\", \"name\", \"atomic_weight\", \"melting_point\", \"ionization_energies\"). The function also accepts a single attribute as a str (the implementation checks isinstance(properties, str) and will convert it to a one-item list). The order of attributes in this sequence determines the order of values in each returned row. If \"atomic_number\" is not included in this sequence it will be automatically prepended so that results include the atomic number and are ordered reliably.", "default": ""}, "group": {"type": "integer", "description": "Periodic-table group number used to filter elements (Element.group_id == group). This corresponds to the chemical group/column in the periodic table; the default value is 18 (the noble gases). Use this parameter to request data for all elements that belong to the specified group.", "default": 18}}, "required": ["properties", "group"], "type": "any"}}, "type": "function"}], "query": "I’m drafting the methods appendix for a kinetics lab and need a clean table of the **group directly to the right of the chalcogens** (i.e., the halogen family). Pull **atomic number, symbol, name, and atomic weight** for every element in that group — but don’t include any entries that are clearly not naturally occurring elements (we only want the ones you’d realistically handle in a standard wet lab context).", "answers": "[{\"name\":\"mendeleev_models_fetch_by_group\",\"arguments\":{\"properties\":[\"atomic_number\",\"symbol\",\"name\",\"atomic_weight\"],\"group\":17}}]"}
{"func_name": "mendeleev_utils_coeffs", "func_desc": "mendeleev.utils.coeffs: Compute integer stoichiometric coefficients for a binary compound from two oxidation states.\n    \n    Computes the smallest integer ratio of atoms for two elements (or ions) required to balance their charges when forming a binary compound. This function is used within the mendeleev package to convert integer oxidation states (as found in element data and periodic-table-based computations) into stoichiometric coefficients by computing the least common multiple (LCM) of the two oxidation states and dividing by each oxidation state. The implementation uses lcm = abs(a * b) // math.gcd(a, b) and returns lcm // a, lcm // b. The returned pair corresponds to the multiplicities of the first and second element, in that order, as determined directly from the provided oxidation states.", "tools": [{"function": {"description": "mendeleev.utils.coeffs: Compute integer stoichiometric coefficients for a binary compound from two oxidation states.\n\nComputes the smallest integer ratio of atoms for two elements (or ions) required to balance their charges when forming a binary compound. This function is used within the mendeleev package to convert integer oxidation states (as found in element data and periodic-table-based computations) into stoichiometric coefficients by computing the least common multiple (LCM) of the two oxidation states and dividing by each oxidation state. The implementation uses lcm = abs(a * b) // math.gcd(a, b) and returns lcm // a, lcm // b. The returned pair corresponds to the multiplicities of the first and second element, in that order, as determined directly from the provided oxidation states.", "name": "mendeleev_utils_coeffs", "parameters": {"properties": {"a": {"type": "integer", "description": "Oxidation state of the first element. This integer represents the formal charge per atom used to determine charge balance in the resulting compound. Negative values represent negatively charged ions (anions) and positive values represent positively charged ions (cations). The value must be a non-zero integer; passing zero will lead to a division-by-zero error in the current implementation. Non-integer types (e.g., float, str) will raise a TypeError because the underlying math.gcd operation requires integers.", "default": ""}, "b": {"type": "integer", "description": "Oxidation state  of the second element. Same semantic meaning as `a`. Defaults to 2. The default is provided for convenience when one oxidation state is commonly +2; callers should supply an explicit integer oxidation state for accurate results.", "default": 2}}, "required": ["a", "b"], "type": "any"}}, "type": "function"}], "query": "I’m batch-generating neutral binary oxide formulas from oxidation-state tables, but I realized my pipeline sometimes feeds the anion oxidation state with the wrong sign (e.g., treating oxide as if it were a cation). For the specific test case of aluminum and oxide: start by computing coefficients using the oxidation states exactly as they appear in my raw feed (+3 for Al and +2 for the oxide entry). Then—because we must end with a neutral compound—correct the computation by flipping the sign of whichever oxidation state must be negative to represent the anion, and recompute the smallest whole-number stoichiometric coefficients (in the function’s return order: first element then second). What coefficients should I use after that correction?", "answers": "[{\"name\": \"mendeleev_utils_coeffs\", \"arguments\": {\"a\": 3, \"b\": -2}}]"}
{"func_name": "molmass_format_charge", "func_desc": "Return a standardized string representation of an ionic charge for use in\n    chemical formula display, mass/charge (m/z) annotations, and related output in\n    the molmass library. This function is used by higher-level objects (for\n    example Formula and spectrum formatting) to append or display the net ionic\n    charge of a molecule or particle in a compact, human- and machine-readable\n    form.\n    \n    The formatting rules implemented here follow the conventions used in the\n    molmass codebase and README examples: a zero charge is represented by the\n    single character '0'; a charge of magnitude one is represented only by its\n    sign ('+' or '-'); charges with absolute value greater than one are rendered\n    as the absolute count immediately followed by the sign, optionally preceded\n    by a single-character prefix when provided (for example, formatting -2 with\n    prefix '_' yields '_2-'). The function has no side effects and returns a new\n    string; it does not modify external state.", "tools": [{"function": {"description": "Return a standardized string representation of an ionic charge for use in\nchemical formula display, mass/charge (m/z) annotations, and related output in\nthe molmass library. This function is used by higher-level objects (for\nexample Formula and spectrum formatting) to append or display the net ionic\ncharge of a molecule or particle in a compact, human- and machine-readable\nform.\n\nThe formatting rules implemented here follow the conventions used in the\nmolmass codebase and README examples: a zero charge is represented by the\nsingle character '0'; a charge of magnitude one is represented only by its\nsign ('+' or '-'); charges with absolute value greater than one are rendered\nas the absolute count immediately followed by the sign, optionally preceded\nby a single-character prefix when provided (for example, formatting -2 with\nprefix '_' yields '_2-'). The function has no side effects and returns a new\nstring; it does not modify external state.", "name": "molmass_format_charge", "parameters": {"properties": {"charge": {"type": "integer", "description": "Net ionic charge of the molecule or particle being\nformatted. In the chemical domain this is the integer net charge (for\nexample +1 for ammonium NH4+, -2 for sulfate SO4(2-)). The function\ntreats 0 specially and returns the literal '0'. Callers should pass\nan integer; passing a non-integer may raise TypeError or lead to\nundefined formatting behavior.", "default": ""}, "prefix": {"type": "string", "description": "Single-character prefix placed before the numeric magnitude\nwhen the absolute value of charge is greater than 1. This allows\nuse-cases in the molmass ecosystem where a separator or marker is\ndesired before the magnitude (for example '_' to produce '_2-'). The\ndefault is the empty string, which results in no prefix. If prefix is\nnon-empty but abs(charge) is 0 or 1, the prefix is ignored and not\nincluded in the result (e.g., prefix '_' with charge 1 returns '+').", "default": ""}}, "required": ["charge", "prefix"], "type": "any"}}, "type": "function"}], "query": "I’m generating annotated labels for ions in a mass spectrum plot, and I need to format the net charge string the same way molmass does. Initially I was going to include an underscore separator before the magnitude for negative charges (so the label would clearly separate the magnitude from the rest of the fragment name), but I just realized this label will be consumed by a downstream exporter that treats underscores as field delimiters and will break the annotation. So: format the charge for a fragment ion with net charge of −3 using the molmass_format_charge function, but use the simplest separator that is guaranteed not to be interpreted as a delimiter by most parsers (i.e., remove the problematic separator entirely while keeping molmass’ conventions for magnitude/sign ordering). What exact function call should I make?", "answers": "[{\"name\":\"molmass_format_charge\",\"arguments\":{\"charge\":-3,\"prefix\":\"\"}}]"}
{"func_name": "molmass_from_peptide", "func_desc": "Return chemical formula for a peptide polymer built from unmodified amino acids.\n    \n    This function converts an amino acid sequence into a chemical formula string that represents the corresponding peptide polymer. It is intended for use with the molmass library to compute molecular mass, elemental composition, and mass distribution by passing its output to molmass.Formula. The function accepts typical single-letter amino acid sequences, tolerates embedded spaces for readability, and recognizes trailing ionic charge descriptors as accepted by the package (for example, rdkit-style descriptors such as \"_2+\"). Internally the function removes whitespace, extracts any trailing charge with split_charge, maps residues to their residue formulas using the AMINOACIDS mapping via from_sequence, appends \"H2O\" to represent the polymer termini/hydration stoichiometry, and then reattaches any parsed charge with join_charge. The returned string uses the same formula syntax expected by molmass.Formula (parentheses, optional bracketed charge notation).", "tools": [{"function": {"description": "Return chemical formula for a peptide polymer built from unmodified amino acids.\n\nThis function converts an amino acid sequence into a chemical formula string that represents the corresponding peptide polymer. It is intended for use with the molmass library to compute molecular mass, elemental composition, and mass distribution by passing its output to molmass.Formula. The function accepts typical single-letter amino acid sequences, tolerates embedded spaces for readability, and recognizes trailing ionic charge descriptors as accepted by the package (for example, rdkit-style descriptors such as \"_2+\"). Internally the function removes whitespace, extracts any trailing charge with split_charge, maps residues to their residue formulas using the AMINOACIDS mapping via from_sequence, appends \"H2O\" to represent the polymer termini/hydration stoichiometry, and then reattaches any parsed charge with join_charge. The returned string uses the same formula syntax expected by molmass.Formula (parentheses, optional bracketed charge notation).", "name": "molmass_from_peptide", "parameters": {"properties": {"sequence": {"type": "string", "description": "Amino acid sequence for which to build a peptide chemical formula. The sequence may contain embedded spaces (they are removed) and may include a trailing charge descriptor (for example \"_2+\" or \"2+\"); such a descriptor will be parsed and encoded in the returned formula. The sequence must use standard single-letter amino acid symbols that are present in the AMINOACIDS residue mapping used by molmass; sequences containing unknown symbols will cause the underlying from_sequence routine to raise an error (typically FormulaError or ValueError) indicating invalid residue symbols. The function does not modify the original sequence object; it only returns a new formula string.", "default": ""}}, "required": ["sequence"], "type": "any"}}, "type": "function"}], "query": "I’m batching a few MS-ready peptide IDs and I want a molmass-compatible formula for the *actual polymer* (unmodified residues, with the usual termini hydration stoichiometry) so I can feed it into molmass.Formula. I was going to just convert the literal string as-is, but I realized the sequence field coming from our LIMS sometimes contains readability whitespace *and* an rdkit-style trailing charge tag, and in this particular case the underscore-charge tag got duplicated during copy/paste.\n\nPlease take this input exactly: \"A  C D E K_2+_2+\" (note the inconsistent spacing and the repeated trailing charge descriptor). Treat the peptide as the unmodified sequence (letters only, ignoring whitespace), and for the charge: don’t sum duplicated tags—keep only the *single terminal charge descriptor corresponding to the unique charge state implied by the intended tag* (i.e., the one you would retain after deduplicating exact repeats at the end). Then return the molmass-compatible peptide polymer formula string with that charge preserved.", "answers": "[{\"name\":\"molmass_from_peptide\",\"arguments\":{\"sequence\":\"A  C D E K_2+_2+\"}}]"}
{"func_name": "molmass_from_sequence", "func_desc": "Convert a biological sequence (DNA, RNA, or peptide) to a chemical formula string in Hill notation.", "tools": [{"function": {"description": "Convert a biological sequence (DNA, RNA, or peptide) to a chemical formula string in Hill notation.\n", "name": "molmass_from_sequence", "parameters": {"properties": {"sequence": {"type": "string", "description": "A sequence of single-character item codes representing monomers in a biological polymer (for example, single-letter amino acid codes for peptides or single-letter nucleotide codes for DNA/RNA). The function iterates the string one character at a time; each character must be a key in the groups mapping. The sequence may include an ionic charge using rdkit-style or other supported charge notation; any such charge is parsed and preserved (see notes on charge handling below). Examples of valid domain usage are converting a peptide like 'ACDE' or a nucleotide sequence like 'ATG' into a concatenated chemical formula for downstream molecular mass, elemental composition, or mass spectrum calculations performed by the molmass library.", "default": ""}, "groups": {"type": "dict", "additionalProperties": {"type": "string"}, "description": "Mapping from sequence item (single-character string) to its chemical formula given in Hill notation (string). Typical values for this argument are the module mappings DEOXYNUCLEOTIDES, NUCLEOTIDES, or AMINOACIDS which map item codes (e.g., 'A', 'C', 'G', 'T', amino-acid single letters) to the corresponding group chemical formula strings. The function requires that every character in sequence exists as a key in this mapping; keys and values must be strings.", "default": ""}}, "required": ["sequence", "groups"], "type": "any"}}, "type": "function"}], "query": "I’m re-checking the elemental composition for a short RNA oligo, but my wet-lab note is messy. The sequence I wrote down is “AUGC”, and the group mapping is the same as before (A→\"C10H12N5O6P\", U→\"C9H11N2O8P\", G→\"C10H12N5O7P\", C→\"C9H12N3O7P\").\n\nInitially I was going to just convert it directly, but I realized the last character in my note is likely a contamination/formatting artifact from the instrument export: it’s the only base in the string that would disappear if you truncate the sequence to the longest prefix that contains no repeated nucleotide letters (our pipeline flags repeats as possible concatenation artifacts for this particular run). \n\nSo: before converting to a chemical formula (Hill notation), first correct the input by truncating it to that longest no-repeat prefix, then compute the formula using the same group mapping.", "answers": "[{\"name\":\"molmass_from_sequence\",\"arguments\":{\"sequence\":\"AUG\",\"groups\":{\"A\":\"C10H12N5O6P\",\"U\":\"C9H11N2O8P\",\"G\":\"C10H12N5O7P\",\"C\":\"C9H12N3O7P\"}}}]"}
{"func_name": "molmass_join_charge", "func_desc": "Return a chemical formula string with an ionic charge suffix suitable for use\n    in molmass calculations, web output, and textual representations of charged\n    species.", "tools": [{"function": {"description": "Return a chemical formula string with an ionic charge suffix suitable for use\nin molmass calculations, web output, and textual representations of charged\nspecies.", "name": "molmass_join_charge", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula without charge. This is the core formula\nstring describing the molecular composition (for example \"C8H10N4O2\")\nthat will be augmented with an ionic charge notation. The string is\ninserted either inside square brackets (when separator is the empty\nstring) or left as-is (when a non-empty separator such as '_' is\nprovided). The function does not modify element ordering or validate\nchemical correctness of the formula; it only modifies the textual\nrepresentation to include the charge.", "default": ""}, "charge": {"type": "integer", "description": "Charge number of the species. Zero indicates a neutral\nspecies and leaves the input formula unchanged. Non-zero integers are\nformatted as a charge suffix where the sign character is '+' for\npositive charges and '-' for negative charges. The magnitude '1' is\nomitted when formatting a single charge (e.g. 1 -> '+', -1 -> '-'),\nwhile magnitudes greater than 1 are shown before the sign\n(e.g. 2 -> '2+', -2 -> '2-'). This integer value is used directly to\nproduce the textual suffix appended to the formula and is the standard\nionic notation used elsewhere in the molmass library for charged ions.", "default": ""}, "separator": {"type": "string", "description": "Character separating the formula from the formatted charge.\nAllowed values are the empty string '' (the default) or '_' as used in\nmolmass code and outputs. If separator is the empty string, the input\nformula is wrapped in square brackets and the formatted charge is\nappended immediately after the closing bracket (for example\n'[Formula]2+'); if separator is '_' the separator is inserted between\nthe formula and the formatted charge without adding brackets\n(for example 'Formula_2-'). The default empty string produces the\nbracketed representation commonly used for ionic formulas.", "default": ""}}, "required": ["formula", "charge"], "type": "any"}}, "type": "function"}], "query": "I’m generating molmass-ready strings for an ion panel from an LC–MS method sheet, but the charge column is messy. For the sulfate entry the base composition is the tetravalent oxyanion of sulfur (no waters, no counterions). I was going to hard-code the charge as the common value, but then I realized our exporter already encodes the sign in a separate boolean and the magnitude in an integer field. For sulfate, the magnitude is 2 and the boolean indicates it’s an anion, so the net charge should follow from that (don’t assume the sign from the name). Please return the chemical formula string with the default bracketed ionic-charge suffix appropriate for that net charge.", "answers": "[{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO4\",\"charge\":-2}}]"}
{"func_name": "molmass_molmass_precision_digits", "func_desc": "molmass.molmass.precision_digits returns the recommended number of digits after the decimal point to print a floating point value f so that the formatted number fits within a field of width characters. This helper is used by the molmass library when formatting numeric outputs (for example average mass, monoisotopic_mass, nominal_mass, m/z values in spectra, and intensity percentages shown in the console, web UI, and dataframes) so that numbers are presented compactly and consistently within fixed-width displays.", "tools": [{"function": {"description": "molmass.molmass.precision_digits returns the recommended number of digits after the decimal point to print a floating point value f so that the formatted number fits within a field of width characters. This helper is used by the molmass library when formatting numeric outputs (for example average mass, monoisotopic_mass, nominal_mass, m/z values in spectra, and intensity percentages shown in the console, web UI, and dataframes) so that numbers are presented compactly and consistently within fixed-width displays.\n", "name": "molmass_molmass_precision_digits", "parameters": {"properties": {"f": {"type": "float", "description": "Floating point number to be displayed. The function uses the base-10 logarithm of the absolute value of f to estimate the length of the integer portion of the number and to determine how many characters remain for the fractional part. Negative values are supported (the sign consumes one character). Passing f == 0 will raise a ValueError because log10(0) is undefined. Behavior for non-finite floats (inf, -inf, nan) is not specified and may raise a math domain error or produce unexpected results.", "default": ""}, "width": {"type": "integer", "description": "Maximum allowed total length in characters for the printed number, including any sign, digits before the decimal point, the decimal point itself, and digits after the decimal point. This is an integer number of characters as used by fixed-width formatting routines. If width is too small to accommodate the integer part plus sign and decimal point, the function still returns at least 1 (it guarantees at least one fractional digit), but the formatted string may exceed the requested width.", "default": ""}}, "required": ["f", "width"], "type": "any"}}, "type": "function"}], "query": "I’m printing a monoisotopic m/z into a fixed-width table for an LC–MS report. Initially I was going to allocate a 9-character field because that’s what our legacy template uses, but I just realized the exported table includes an explicit leading sign for alignment (so positive values still consume one extra character). Keep the same m/z value (445.1234567), but recompute the number of digits after the decimal so that, after reserving space for that always-present sign, the formatted number still fits in the original field width. How many digits after the decimal should I print?", "answers": "[{\"name\":\"molmass_molmass_precision_digits\",\"arguments\":{\"f\":445.1234567,\"width\":8}}]"}
{"func_name": "monai_data_utils_collate_meta_tensor", "func_desc": "Collate a sequence of meta-tensor containers into a batched meta-tensor structure suitable for MONAI data pipelines.\n    \n    This function is used in MONAI (a PyTorch-based medical imaging framework) to assemble per-sample data items produced by a Dataset into a single batch that preserves both tensor data and associated metadata (for example spatial affine, origin, orientation stored in MONAI MetaTensor/MetaObj objects). It recursively inspects the first element of the provided sequence to determine how to collate: if elements are MONAI MetaObj instances, it delegates to collate_meta_tensor_fn to produce a single batched meta-tensor; if elements are mapping/dictionary-like it builds a dictionary whose values are the collated results for each key; if elements are tuples/lists it returns a list of collated results for each position; otherwise it falls back to torch.utils.data.dataloader.default_collate to produce a conventional tensor batch. This behavior enables DataLoader-style batching while preserving domain-specific metadata required for healthcare imaging workflows and downstream models.", "tools": [{"function": {"description": "Collate a sequence of meta-tensor containers into a batched meta-tensor structure suitable for MONAI data pipelines.\n\nThis function is used in MONAI (a PyTorch-based medical imaging framework) to assemble per-sample data items produced by a Dataset into a single batch that preserves both tensor data and associated metadata (for example spatial affine, origin, orientation stored in MONAI MetaTensor/MetaObj objects). It recursively inspects the first element of the provided sequence to determine how to collate: if elements are MONAI MetaObj instances, it delegates to collate_meta_tensor_fn to produce a single batched meta-tensor; if elements are mapping/dictionary-like it builds a dictionary whose values are the collated results for each key; if elements are tuples/lists it returns a list of collated results for each position; otherwise it falls back to torch.utils.data.dataloader.default_collate to produce a conventional tensor batch. This behavior enables DataLoader-style batching while preserving domain-specific metadata required for healthcare imaging workflows and downstream models.", "name": "monai_data_utils_collate_meta_tensor", "parameters": {"properties": {"batch": {"type": "array", "items": {"type": "any"}, "description": "A list (sequence) of per-sample objects to collate into a batch. Each element in this list is expected to be one of:\n- A MetaObj instance (MONAI metadata wrapper): when the first element is a MetaObj, the function will call collate_meta_tensor_fn(batch) to combine all MetaObj items into a single batched meta-tensor that preserves per-sample metadata (affine, spacing, keys used in imaging pipelines).\n- A Mapping (e.g., dict): when the first element is mapping-like, the function assumes all elements share the same keys and returns a dict where each key maps to the result of recursively collating the list of values for that key. Note: if a later element is missing a key present in the first element, a KeyError will be raised when attempting to access d[k].\n- A tuple or list: when the first element is a tuple/list, the function returns a list whose i-th entry is the result of recursively collating the i-th entries from each element of batch. This preserves ordered container structures commonly used to store (image, label) pairs or multi-modal inputs.\n- Any other object type: if none of the above cases match (and no MetaObj is found during the recursive checks), the function falls back to torch.utils.data.dataloader.default_collate(batch) to produce a standard PyTorch-style batched tensor or nested structure.\nThe function requires that the provided batch is a Sequence; if the argument is not a Sequence the function raises NotImplementedError. The function performs recursive inspection starting from the first element and recurses into mappings and sequences to locate MetaObj instances; if it reaches leaf objects without finding MetaObj it uses default_collate. No mutation of the input list is performed by this function, but the collated outputs may be new tensors/containers allocated in memory.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a MONAI DataLoader batch collation for a 2D segmentation experiment and I thought I could just collate everything in a nested dict, but then I realized one of the per-sample fields is a string identifier and default_collate will choke unless we filter out obvious acquisition artifacts first.\n\nPlease collate ONLY the valid samples from the following 3 dataset outputs into a single batch, preserving the nested structure for imaging + metadata. Treat a sample as invalid if its in-plane spacing is non-physical for clinical images (i.e., any spacing component is <= 0). After filtering, collate what remains using MONAI’s meta-aware collation so tensors are batched and metadata dictionaries are collated recursively.\n\n- sample A: {\"image\": [[1.0, 2.0],[3.0, 4.0]], \"label\": 0, \"meta\": {\"patient_id\": \"P001\", \"spacing\": [1.0, 1.0]}}\n- sample B: {\"image\": [[5.0, 6.0],[7.0, 8.0]], \"label\": 1, \"meta\": {\"patient_id\": \"P002\", \"spacing\": [0.0, 1.0]}}   \n- sample C: {\"image\": [[9.0, 10.0],[11.0, 12.0]], \"label\": 2, \"meta\": {\"patient_id\": \"P003\", \"spacing\": [1.0, 1.0]}}\n\n(So you must first exclude the artifact sample based on the spacing rule, then collate the remaining ones into a single batch.)", "answers": "[{\"name\":\"monai_data_utils_collate_meta_tensor\",\"arguments\":{\"batch\":[{\"image\":[[1.0,2.0],[3.0,4.0]],\"label\":0,\"meta\":{\"patient_id\":\"P001\",\"spacing\":[1.0,1.0]}},{\"image\":[[9.0,10.0],[11.0,12.0]],\"label\":2,\"meta\":{\"patient_id\":\"P003\",\"spacing\":[1.0,1.0]}}]}}]"}
{"func_name": "monai_data_utils_remove_extra_metadata", "func_desc": "Remove extra metadata keys from a MONAI metadata dictionary in-place.\n    \n    This function is part of the monai.data.utils utilities and is used in MONAI preprocessing and data handling workflows to remove keys that are considered \"extra metadata\" according to MONAI conventions. It determines which keys to remove by calling get_extra_metadata_keys(), then delegates the actual removal to remove_keys(data=meta, keys=keys). Typical uses include cleaning a sample's metadata before serialization, logging, or passing the metadata into training/evaluation components so that large, temporary, or implementation-specific entries do not propagate through the pipeline.", "tools": [{"function": {"description": "Remove extra metadata keys from a MONAI metadata dictionary in-place.\n\nThis function is part of the monai.data.utils utilities and is used in MONAI preprocessing and data handling workflows to remove keys that are considered \"extra metadata\" according to MONAI conventions. It determines which keys to remove by calling get_extra_metadata_keys(), then delegates the actual removal to remove_keys(data=meta, keys=keys). Typical uses include cleaning a sample's metadata before serialization, logging, or passing the metadata into training/evaluation components so that large, temporary, or implementation-specific entries do not propagate through the pipeline.", "name": "monai_data_utils_remove_extra_metadata", "parameters": {"properties": {"meta": {"type": "any", "description": "A dictionary containing metadata for a medical imaging sample or batch, following MONAI's metadata conventions. This mapping is modified in-place: any top-level keys in meta that match the set returned by get_extra_metadata_keys() will be removed from this same dict object. The function does not return a new dict. The values and nested structures of remaining keys are preserved. If meta is empty, the function performs no changes. The caller should provide a dict; passing an object that is not a dict may raise a TypeError or other exception from underlying operations.", "default": ""}}, "required": ["meta"], "type": "any"}}, "type": "function"}], "query": "I’m about to serialize a MONAI preprocessed sample’s metadata, but I realized my downstream caching step hashes the dict and will break if we keep any transient MONAI bookkeeping (like transform traces or patch sampling breadcrumbs). Initially I was going to just drop the obvious trace field, but that’s risky because MONAI may stash extras under multiple conventional keys depending on the transform chain. So: clean the metadata **in-place** by removing whatever keys MONAI itself classifies as “extra metadata,” while preserving the core spatial provenance (the path-like identifier, geometry transform matrix, and voxel size), plus any non-MONAI user annotations. Use this metadata object:\n\n{\"filename_or_obj\":\"/data/Task01/imagesTr/patient001.nii.gz\",\"affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]],\"spacing\":[1.25,1.25,2.5],\"original_channel_dim\":-1,\"patch_index\":[0,64,64],\"applied_operations\":[{\"class\":\"Spacingd\",\"orig_spacing\":[1.0,1.0,1.0],\"new_spacing\":[1.25,1.25,2.5]}],\"extra_info\":{\"note\":\"debug run\"}}.\n\nImportant constraint: don’t hardcode which keys are “extra”; rely on MONAI’s own convention-based discovery of extra metadata keys so the result is deterministic even if the transform chain changes.", "answers": "[{\"name\": \"monai_data_utils_remove_extra_metadata\", \"arguments\": {\"meta\": {\"filename_or_obj\": \"/data/Task01/imagesTr/patient001.nii.gz\", \"affine\": [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], \"spacing\": [1.25, 1.25, 2.5], \"original_channel_dim\": -1, \"patch_index\": [0, 64, 64], \"applied_operations\": [{\"class\": \"Spacingd\", \"orig_spacing\": [1.0, 1.0, 1.0], \"new_spacing\": [1.25, 1.25, 2.5]}], \"extra_info\": {\"note\": \"debug run\"}}}}]"}
{"func_name": "monai_data_utils_remove_keys", "func_desc": "monai.data.utils.remove_keys removes one or more keys from a mapping in-place without returning a value.\n    \n    This utility function is intended for MONAI data-processing workflows (for example, preprocessing and transform pipelines in medical imaging) where sample dictionaries carry images, labels, and metadata and specific keys need to be discarded before further processing or saving. The function iterates over the provided list of keys and calls dict.pop(key, None) for each key so that missing keys are ignored silently. Because it modifies the input mapping directly, callers should not expect a new dictionary or any return value.", "tools": [{"function": {"description": "monai.data.utils.remove_keys removes one or more keys from a mapping in-place without returning a value.\n\nThis utility function is intended for MONAI data-processing workflows (for example, preprocessing and transform pipelines in medical imaging) where sample dictionaries carry images, labels, and metadata and specific keys need to be discarded before further processing or saving. The function iterates over the provided list of keys and calls dict.pop(key, None) for each key so that missing keys are ignored silently. Because it modifies the input mapping directly, callers should not expect a new dictionary or any return value.", "name": "monai_data_utils_remove_keys", "parameters": {"properties": {"data": {"type": "any", "description": "The dictionary to be modified. In MONAI this is typically a sample or batch dictionary containing imaging tensors and associated metadata (e.g., image, label, meta_dict). The function operates on this object in-place; no copy is made. If an object that does not implement dict.pop is provided, a runtime exception (for example, AttributeError or TypeError) may be raised by the interpreter.", "default": ""}, "keys": {"type": "array", "items": {"type": "string"}, "description": "An ordered collection of string keys to remove from data. For each key in this list, the function attempts to delete that key from data using data.pop(key, None). If a key is not present in data, it is ignored and no error is raised. The operation is performed in the order of this list; duplicate entries in keys will cause repeated pop attempts but have no additional effect after the first removal.", "default": ""}}, "required": ["data", "keys"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning a MONAI-style sample dict right before writing it to disk for a de-identified QA set. Initially I was going to drop the prediction tensor plus the feature blob and the obvious identifier field. Wait—our downstream reviewer still needs the prediction tensor for overlay QC, so we must keep that one. Instead, remove only the remaining two discard targets (ignore if either is absent) from the mapping *in-place* using `monai.data.utils.remove_keys`:\n\nsample = {\n  \"image\": [[[0.0, 1.0], [2.0, 3.0]]],\n  \"label\": [[[1, 0], [0, 1]]],\n  \"pred\": [[[0.2, 0.8], [0.6, 0.4]]],\n  \"intermediate_features\": [0.12, 0.34, 0.56],\n  \"patient_id\": \"PT-1042\",\n  \"meta_dict\": {\"spacing\": [1.0, 1.0, 1.5], \"modality\": \"CT\"}\n}", "answers": "[{\"name\":\"monai_data_utils_remove_keys\",\"arguments\":{\"data\":{\"image\":[[[0.0,1.0],[2.0,3.0]]],\"label\":[[[1,0],[0,1]]],\"pred\":[[[0.2,0.8],[0.6,0.4]]],\"intermediate_features\":[0.12,0.34,0.56],\"patient_id\":\"PT-1042\",\"meta_dict\":{\"spacing\":[1.0,1.0,1.5],\"modality\":\"CT\"}},\"keys\":[\"intermediate_features\",\"patient_id\"]}}]"}
{"func_name": "monai_handlers_utils_stopping_fn_from_metric", "func_desc": "Returns a stopping function that reads a metric value by name from an Ignite Engine's state for use with ignite.handlers.EarlyStopping.\n    \n    This helper is used in MONAI training and evaluation workflows (medical imaging deep learning) to construct the score function required by ignite.handlers.EarlyStopping. MONAI engines (ignite.engine.Engine) commonly populate engine.state.metrics with validation or training metrics (for example \"val_loss\", \"val_dice\") after each epoch or iteration. The callable returned by this function simply accesses engine.state.metrics[metric_name] and returns that value, so the EarlyStopping handler can decide whether to stop based on that metric.", "tools": [{"function": {"description": "Returns a stopping function that reads a metric value by name from an Ignite Engine's state for use with ignite.handlers.EarlyStopping.\n\nThis helper is used in MONAI training and evaluation workflows (medical imaging deep learning) to construct the score function required by ignite.handlers.EarlyStopping. MONAI engines (ignite.engine.Engine) commonly populate engine.state.metrics with validation or training metrics (for example \"val_loss\", \"val_dice\") after each epoch or iteration. The callable returned by this function simply accesses engine.state.metrics[metric_name] and returns that value, so the EarlyStopping handler can decide whether to stop based on that metric.", "name": "monai_handlers_utils_stopping_fn_from_metric", "parameters": {"properties": {"metric_name": {"type": "string", "description": "The name of the metric key to retrieve from engine.state.metrics. This string should match a metric key that MONAI or user code has stored in the Ignite Engine's state.metrics mapping (for example \"val_loss\" or \"val_mean_dice\"). This parameter has no default and must be provided; it determines which metric the returned stopping function will read when called.", "default": ""}}, "required": ["metric_name"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up `ignite.handlers.EarlyStopping` inside a MONAI validation workflow and I initially planned to monitor the metric key I *thought* was written by my Dice aggregator (the one that sounds like it’s already averaged over the validation set). But I just realized my evaluator writes **two** Dice-related keys into `engine.state.metrics`: one is the per-epoch aggregate, and the other is the per-iteration / per-batch variant. Early stopping should use the **stable epoch-level aggregate** (the one that would be appropriate to log once per epoch), not the noisier per-batch value. Please construct the stopping score function using the correct metric name from `engine.state.metrics` for that epoch-level aggregate Dice, so EarlyStopping can use it as its `score_function`.", "answers": "[{\"name\": \"monai_handlers_utils_stopping_fn_from_metric\", \"arguments\": {\"metric_name\": \"val_mean_dice\"}}]"}
{"func_name": "monai_losses_perceptual_medicalnet_intensity_normalisation", "func_desc": "monai.losses.perceptual.medicalnet_intensity_normalisation: Normalize a medical image volume to zero mean and unit variance following the MedicalNet preprocessing convention.\n    \n    This function implements the intensity normalization used in the MedicalNet project (see referenced source in original implementation). It computes the global arithmetic mean and standard deviation of all voxels in the provided n-dimensional medical image volume (for example, a 3D MRI or CT scan stored as a NumPy array) and returns a new array where each voxel intensity is shifted and scaled to have zero mean and unit variance. This normalization is commonly applied as a preprocessing step in deep learning workflows for medical imaging (as in MONAI) to stabilize training, make network weights more comparable across inputs, and reduce sensitivity to absolute intensity scales between studies or scanners.", "tools": [{"function": {"description": "monai.losses.perceptual.medicalnet_intensity_normalisation: Normalize a medical image volume to zero mean and unit variance following the MedicalNet preprocessing convention.\n\nThis function implements the intensity normalization used in the MedicalNet project (see referenced source in original implementation). It computes the global arithmetic mean and standard deviation of all voxels in the provided n-dimensional medical image volume (for example, a 3D MRI or CT scan stored as a NumPy array) and returns a new array where each voxel intensity is shifted and scaled to have zero mean and unit variance. This normalization is commonly applied as a preprocessing step in deep learning workflows for medical imaging (as in MONAI) to stabilize training, make network weights more comparable across inputs, and reduce sensitivity to absolute intensity scales between studies or scanners.", "name": "monai_losses_perceptual_medicalnet_intensity_normalisation", "parameters": {"properties": {"volume": {"type": "array", "items": {"type": "float"}, "description": "An n-dimensional NumPy array containing the medical image intensities to normalize. The array represents voxel or pixel intensity values across spatial dimensions (and optionally channels). This parameter is used as the source data from which the mean and standard deviation are computed globally (over all elements of the array). The function does not modify this input in-place; it reads the values to compute statistics and produces a separate normalized array as output.", "default": ""}}, "required": ["volume"], "type": "any"}}, "type": "function"}], "query": "I’m preprocessing a tiny 3D brain MRI toy volume (NumPy array, shape (1, 4, 4, 4): first dim is channel). Initially I was going to run MedicalNet-style intensity normalisation over the whole tensor as-is, but I just noticed there’s an acquisition artifact: any voxel that hits the scanner’s floor value (exactly 0.0) should be treated as background padding and must NOT influence the global mean/std. So: compute the arithmetic mean and standard deviation using only the non-background voxels (i.e., voxels whose intensity is strictly above that floor value), then apply the same affine transform to *all* voxels in the volume (including the background ones) so the non-background set ends up with zero mean and unit variance. Use the MONAI MedicalNet convention (global stats over the selected voxels, not per-slice/per-channel). Here’s the volume:\n\n[[[[0.0, 15.2, 23.5, 40.1],\n   [5.0, 18.3, 27.6, 35.0],\n   [10.2, 20.0, 30.5, 45.3],\n   [2.5, 12.0, 22.0, 32.0]],\n\n  [[3.0, 14.5, 25.0, 36.5],\n   [6.5, 19.0, 28.0, 38.0],\n   [11.0, 21.5, 31.0, 41.0],\n   [4.0, 13.0, 24.0, 34.0]],\n\n  [[1.0, 16.0, 26.0, 37.0],\n   [7.0, 17.5, 29.0, 39.5],\n   [9.5, 22.5, 33.0, 43.0],\n   [3.5, 11.5, 21.0, 31.5]],\n\n  [[2.0, 8.0, 18.0, 28.0],\n   [6.0, 16.0, 26.0, 36.0],\n   [12.0, 24.0, 32.0, 44.0],\n   [5.5, 9.0, 20.0, 30.0]]]]", "answers": "[{\"name\":\"monai_losses_perceptual_medicalnet_intensity_normalisation\",\"arguments\":{\"volume\":[[[[0.0,15.2,23.5,40.1],[5.0,18.3,27.6,35.0],[10.2,20.0,30.5,45.3],[2.5,12.0,22.0,32.0]],[[3.0,14.5,25.0,36.5],[6.5,19.0,28.0,38.0],[11.0,21.5,31.0,41.0],[4.0,13.0,24.0,34.0]],[[1.0,16.0,26.0,37.0],[7.0,17.5,29.0,39.5],[9.5,22.5,33.0,43.0],[3.5,11.5,21.0,31.5]],[[2.0,8.0,18.0,28.0],[6.0,16.0,26.0,36.0],[12.0,24.0,32.0,44.0],[5.5,9.0,20.0,30.0]]]]}}]"}
{"func_name": "monai_metrics_froc_compute_froc_score", "func_desc": "Compute the CAMELYON-style FROC score (average sensitivity at predefined false positive rates per image).\n    \n    This function is modified from the official CAMELYON16 challenge evaluation code and implements the challenge's second evaluation metric: the average sensitivity (true positive rate) evaluated at a set of predefined false positive rates per whole-slide image. It is intended for use in medical imaging detection pipelines (for example, lesion/metastasis detection on whole-slide histopathology images) where model outputs are aggregated at multiple detection thresholds to produce per-threshold average false positives per image and corresponding sensitivities. The function linearly interpolates the provided sensitivity curve at the requested false-positive-per-image thresholds and returns the arithmetic mean of those interpolated sensitivities. The implementation reverses the input arrays before interpolation to satisfy numpy.interp's requirement that the interpolation x-coordinates be in increasing order.", "tools": [{"function": {"description": "Compute the CAMELYON-style FROC score (average sensitivity at predefined false positive rates per image).\n\nThis function is modified from the official CAMELYON16 challenge evaluation code and implements the challenge's second evaluation metric: the average sensitivity (true positive rate) evaluated at a set of predefined false positive rates per whole-slide image. It is intended for use in medical imaging detection pipelines (for example, lesion/metastasis detection on whole-slide histopathology images) where model outputs are aggregated at multiple detection thresholds to produce per-threshold average false positives per image and corresponding sensitivities. The function linearly interpolates the provided sensitivity curve at the requested false-positive-per-image thresholds and returns the arithmetic mean of those interpolated sensitivities. The implementation reverses the input arrays before interpolation to satisfy numpy.interp's requirement that the interpolation x-coordinates be in increasing order.", "name": "monai_metrics_froc_compute_froc_score", "parameters": {"properties": {"fps_per_image": {"type": "array", "items": {"type": "any"}, "description": "A one-dimensional numeric array containing the average number of false positives per image computed at a series of detection thresholds. Each element corresponds to a particular detection threshold and represents the expected false positives per whole-slide image for that threshold. This array must have the same length as total_sensitivity and represent the x-axis values for interpolation. In typical MONAI/CAMELYON workflows these values are produced by aggregating per-image false positive counts across a validation/test set at multiple detection thresholds.", "default": ""}, "total_sensitivity": {"type": "array", "items": {"type": "any"}, "description": "A one-dimensional numeric array of the same length as fps_per_image containing the sensitivities (true positive rates) corresponding to each detection threshold. Each element is the fraction of true lesions correctly detected at the matching threshold. This array provides the y-axis values for interpolation. Both fps_per_image and total_sensitivity are reversed internally (via [::-1]) before interpolation so that numpy.interp receives increasing x-coordinates.", "default": ""}, "eval_thresholds": {"type": "any", "description": "A tuple of numeric false-positive-per-image target rates at which the function will evaluate (by linear interpolation) the sensitivity curve defined by fps_per_image and total_sensitivity. Defaults to (0.25, 0.5, 1, 2, 4, 8), which is the canonical set used in the CAMELYON16 challenge for reporting the averaged sensitivity metric. The tuple elements are treated as the x-coordinates at which to sample the sensitivity curve.", "default": [0.25, 0.5, 1, 2, 4, 8]}}, "required": ["fps_per_image", "total_sensitivity"], "type": "any"}}, "type": "function"}], "query": "I’m evaluating a metastasis/lesion detector on 120 whole-slide images using the CAMELYON-style FROC (mean sensitivity at predefined FP-per-image operating points). I was going to pass the sensitivity curve exactly as exported from my threshold sweep, but I just realized the sweep logged FP/image in the *opposite* direction than what my plotting script assumes, and it also included one extra end-point that is a duplicated artifact (a repeated FP/image value from two adjacent thresholds). Please first remove the duplicate by keeping the entry with the higher sensitivity for that repeated FP/image, then compute the CAMELYON-style FROC score at the *default CAMELYON operating points* using linear interpolation as in the official code. Here are the raw arrays from the sweep (in the order they were logged):\n\nfps_per_image = [0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 8.0, 16.0]\n\ntotal_sensitivity = [0.50, 0.61, 0.70, 0.78, 0.84, 0.90, 0.93, 0.94, 0.97]\n\nCompute the final CAMELYON-style FROC score after that cleanup.", "answers": "[{\"name\":\"monai_metrics_froc_compute_froc_score\",\"arguments\":{\"fps_per_image\":[0.125,0.25,0.5,1.0,2.0,4.0,8.0,16.0],\"total_sensitivity\":[0.5,0.61,0.7,0.78,0.84,0.9,0.94,0.97]}}]"}
{"func_name": "monai_networks_layers_convutils_polyval", "func_desc": "monai.networks.layers.convutils.polyval evaluates a polynomial at a given variable using Horner's method; this utility is part of MONAI's convolution-related utilities (monai.networks.layers.convutils) and can be used wherever a polynomial defined by coefficient sequences must be computed in medical imaging pipelines, for example when mapping parameters or constructing analytic filter responses.", "tools": [{"function": {"description": "monai.networks.layers.convutils.polyval evaluates a polynomial at a given variable using Horner's method; this utility is part of MONAI's convolution-related utilities (monai.networks.layers.convutils) and can be used wherever a polynomial defined by coefficient sequences must be computed in medical imaging pipelines, for example when mapping parameters or constructing analytic filter responses.\n", "name": "monai_networks_layers_convutils_polyval", "parameters": {"properties": {"coef": {"type": "array", "items": {"type": "float"}, "description": "Coefficients of the polynomial, provided as a 1D sequence (for example a Python list or a 1D torch.Tensor) where coef[0] is the coefficient of the highest-degree term and coef[-1] is the constant term. In the context of MONAI's convutils, these coefficients define the polynomial to be evaluated (e.g., polynomial kernels or parameter transforms). The function converts coef to a torch.Tensor with dtype torch.float and will use the device of x (if x is a torch.Tensor) when creating this tensor. If coef is zero-dimensional or has length < 1, the routine attempts to return a zero tensor matching x.shape (see Failure modes below).", "default": ""}, "x": {"type": "float", "description": "The variable(s) at which to evaluate the polynomial. This can be a Python scalar float, a sequence/array of floats, or a torch.Tensor. The function converts x to a torch.Tensor with dtype torch.float; if x is already a torch.Tensor, its device is used for the computation and for creating the coef tensor. The returned tensor will have a shape that corresponds to x after conversion (so supplying a tensor-shaped x produces an output of the same shape).", "default": ""}}, "required": ["coef", "x"], "type": "any"}}, "type": "function"}], "query": "I’m calibrating a 3D MRI bias-field/intensity correction stage and need to evaluate an analytic correction curve using MONAI’s `monai.networks.layers.convutils.polyval` (Horner’s method). Initially I was going to use the polynomial written in the lab note as `2x^3 - 0.5x^2 + 3x - 7` at `x = 1.25`, but I just realized the scanner exports intensities in a normalized scale where the value we should feed into the calibration curve is the **original x shifted upward by half of the quadratic coefficient’s magnitude** (this accounts for the pre-centering step). Keep the polynomial itself exactly as written, but evaluate it at the adjusted variable value described above, and return the MONAI `polyval` call arguments.", "answers": "[{\"name\": \"monai_networks_layers_convutils_polyval\", \"arguments\": {\"coef\": [2.0, -0.5, 3.0, -7.0], \"x\": 1.5}}]"}
{"func_name": "monai_networks_layers_factories_split_args", "func_desc": "monai.networks.layers.factories.split_args normalizes an argument specification into a (type, kwargs) pair suitable for MONAI factory-style layer construction utilities.\n    \n    This function is part of MONAI's layer factory utilities used by network/layer factories to accept flexible type specifications from configuration or code. It accepts either a single string naming a layer/type (for example a key used with monai.networks.layers.Act) or an explicit pair consisting of a name or callable that identifies the object to instantiate and a dict of keyword arguments to pass to that object's constructor. The normalized output is intended for direct use with factory mappings that construct PyTorch/MONAI layer objects from a type-specifier and keyword arguments.", "tools": [{"function": {"description": "monai.networks.layers.factories.split_args normalizes an argument specification into a (type, kwargs) pair suitable for MONAI factory-style layer construction utilities.\n\nThis function is part of MONAI's layer factory utilities used by network/layer factories to accept flexible type specifications from configuration or code. It accepts either a single string naming a layer/type (for example a key used with monai.networks.layers.Act) or an explicit pair consisting of a name or callable that identifies the object to instantiate and a dict of keyword arguments to pass to that object's constructor. The normalized output is intended for direct use with factory mappings that construct PyTorch/MONAI layer objects from a type-specifier and keyword arguments.", "name": "monai_networks_layers_factories_split_args", "parameters": {"properties": {"args": {"type": "any", "description": "input arguments to be parsed. This must be either:\n- a string that names the desired type (for example \"PRELU\" as used by monai.networks.layers.Act), in which case the function returns that string and an empty dict of kwargs; or\n- a two-element tuple (name_obj, name_args) where name_obj is either a string naming the object type or a callable (for example a class or factory function) and name_args is a dict of keyword arguments to pass when constructing the object. The tuple form is used to provide explicit constructor parameters (for example (\"PRELU\", {\"num_parameters\": 1, \"init\": 0.25}) when instantiating an activation with specific settings).", "default": ""}}, "required": ["args"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a config-driven MONAI UNet where activations are specified inconsistently across experiments, and I need to normalize the activation spec before it hits the layer factory. Initially I was going to pass the activation as a tuple with both the name and kwargs, but I just realized the training pipeline now enforces a shared activation hyperparameter: it reads a single scalar `alpha=0.2` from the experiment manifest and requires that scalar to be injected as the initialization value for the activation, while keeping the parameter-count consistent with the single-channel setting we’re testing. So: take the activation spec that starts from the same base idea as the tuple form (name + kwargs) but ensure the kwargs reflect (1) a single learnable parameter for the activation and (2) the initialization set from that shared scalar. Then run `monai.networks.layers.factories.split_args` to normalize it into the (type, kwargs) pair for factory construction.", "answers": "[{\"name\": \"monai_networks_layers_factories_split_args\", \"arguments\": {\"args\": [\"PRELU\", {\"num_parameters\": 1, \"init\": 0.2}]}}]"}
{"func_name": "monai_networks_nets_efficientnet_get_efficientnet_image_size", "func_desc": "monai.networks.nets.efficientnet.get_efficientnet_image_size returns the required input image spatial size (single dimension) for a specified EfficientNet model variant used in MONAI's imaging networks.", "tools": [{"function": {"description": "monai.networks.nets.efficientnet.get_efficientnet_image_size returns the required input image spatial size (single dimension) for a specified EfficientNet model variant used in MONAI's imaging networks.\n", "name": "monai_networks_nets_efficientnet_get_efficientnet_image_size", "parameters": {"properties": {"model_name": {"type": "string", "description": "Name of the EfficientNet variant to query. This must match a key in the module-level efficientnet_params mapping used by the EfficientNet network constructors (for example \"efficientnet-b0\", ..., \"efficientnet-b7\"). In the MONAI medical-imaging context this string is used when selecting a pretrained or custom EfficientNet backbone for 2D image classification or encoding tasks so that input images can be resized or cropped to the correct resolution expected by the model.", "default": ""}}, "required": ["model_name"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a MONAI 2D medical image classifier with an EfficientNet backbone and I initially planned to standardize all slices to the input side length used by the *smallest* EfficientNet variant to keep GPU memory low. But I just realized my checkpoint was trained with a *larger* variant, and mismatching the expected spatial size will break weight loading and/or degrade performance. Please tell me the required single-dimension input image side length for the variant that is exactly one step larger than the smallest b-variant in the EfficientNet family used by MONAI, so I can set my resize/crop preprocessing correctly.", "answers": "[{\"name\": \"monai_networks_nets_efficientnet_get_efficientnet_image_size\", \"arguments\": {\"model_name\": \"efficientnet-b1\"}}]"}
{"func_name": "monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args", "func_desc": "get_medicalnet_pretrained_resnet_args: Return the configuration tuple (bias_downsample, shortcut_type) that matches the MedicalNet pretrained ResNet weight conventions for a given ResNet depth.\n    \n    This function is used in MONAI (a PyTorch-based medical imaging deep learning framework) when constructing ResNet variants that will load pretrained weights from MedicalNet/model zoo. It encodes the known conventions for whether the downsample convolution uses a bias term and which residual shortcut variant (\"A\" or \"B\") was used when the MedicalNet weights were produced. Using the values returned by this function ensures the model architecture matches the weight parameter shapes and naming expected by the pretrained checkpoint, avoiding mismatches when loading weights for medical imaging tasks (classification, segmentation, or feature extraction).", "tools": [{"function": {"description": "get_medicalnet_pretrained_resnet_args: Return the configuration tuple (bias_downsample, shortcut_type) that matches the MedicalNet pretrained ResNet weight conventions for a given ResNet depth.\n\nThis function is used in MONAI (a PyTorch-based medical imaging deep learning framework) when constructing ResNet variants that will load pretrained weights from MedicalNet/model zoo. It encodes the known conventions for whether the downsample convolution uses a bias term and which residual shortcut variant (\"A\" or \"B\") was used when the MedicalNet weights were produced. Using the values returned by this function ensures the model architecture matches the weight parameter shapes and naming expected by the pretrained checkpoint, avoiding mismatches when loading weights for medical imaging tasks (classification, segmentation, or feature extraction).", "name": "monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args", "parameters": {"properties": {"resnet_depth": {"type": "integer", "description": "The ResNet depth identifier (for example 18, 34, 50, 101, 152, 200) that specifies which ResNet variant is being constructed. This integer is used to select the MedicalNet weight convention: ResNet-18 and ResNet-34 use the alternative shortcut/type and bias convention encoded by this function, while other common depths use the default convention. The parameter is expected to be an integer corresponding to the network depth; passing a value of a different type may lead to unexpected membership behavior or TypeError if the object is unhashable.", "default": ""}}, "required": ["resnet_depth"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a MONAI 3D ResNet to load MedicalNet pretrained weights for a tumor classification baseline. I initially planned to use the same depth as the common mid-sized variant (the one people pick between the shallow and the very deep models), but I just realized my GPU memory budget forces me to step down by exactly one standard depth tier while still staying in the “basic block” family (not the bottleneck one). To avoid checkpoint shape/name mismatches, tell me the MedicalNet-convention settings I must match: whether the downsample convolution uses a bias term and which residual shortcut variant was used for that depth.", "answers": "[{\"name\": \"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\", \"arguments\": {\"resnet_depth\": 18}}]"}
{"func_name": "monai_networks_nets_swin_unetr_compute_mask", "func_desc": "Computes an attention region mask for shifted-window self-attention as used by Swin Transformer and the Swin-UNETR model in MONAI.\n    \n    This function builds a binary-region index map over a 2D or 3D input grid (height/width or depth/height/width) and converts that map into an attention mask that prevents cross-window attention after a cyclic shift. The implementation follows the approach in Liu et al., \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" and the reference implementation in the Microsoft Swin-Transformer repository. In MONAI this mask is used by swin_unetr network components to ensure attention is computed only within the appropriate local windows for medical image segmentation tasks, and it is intended to be added to attention logits before softmax so masked positions receive a large negative bias (here -100.0) and contribute near-zero probability after softmax.", "tools": [{"function": {"description": "Computes an attention region mask for shifted-window self-attention as used by Swin Transformer and the Swin-UNETR model in MONAI.\n\nThis function builds a binary-region index map over a 2D or 3D input grid (height/width or depth/height/width) and converts that map into an attention mask that prevents cross-window attention after a cyclic shift. The implementation follows the approach in Liu et al., \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" and the reference implementation in the Microsoft Swin-Transformer repository. In MONAI this mask is used by swin_unetr network components to ensure attention is computed only within the appropriate local windows for medical image segmentation tasks, and it is intended to be added to attention logits before softmax so masked positions receive a large negative bias (here -100.0) and contribute near-zero probability after softmax.", "name": "monai_networks_nets_swin_unetr_compute_mask", "parameters": {"properties": {"dims": {"type": "any", "description": "Spatial dimension sizes of the input image/volume. For 3D volumes this must be a tuple of three integers (depth, height, width). For 2D images this must be a tuple of two integers (height, width). The function branches on len(dims) == 3 or len(dims) == 2; any other length is unsupported and will lead to an exception or undefined behavior.", "default": ""}, "window_size": {"type": "any", "description": "Local window size used to partition the input into non-overlapping windows before shifting. This must be a tuple of integers whose length matches len(dims). Each element corresponds to the window size along the respective spatial axis. Elements are used directly to create slicing ranges and must be valid integers for the given dims.", "default": ""}, "shift_size": {"type": "any", "description": "Shift size applied to create the shifted-window attention pattern. This must be a tuple of integers whose length matches len(dims) and corresponds elementwise to window_size. Values are used to compute the three-region slicing used to assign region indices. If shift_size and window_size lengths do not match dims length or contain invalid integers, the function will raise an error or behave incorrectly.", "default": ""}, "device": {"type": "string", "description": "Device specification passed to torch.zeros when allocating the intermediate img_mask tensor. This should be a device string accepted by PyTorch (for example 'cpu' or 'cuda') and determines where the returned attention mask tensor is allocated. The returned tensor will reside on the same device.", "default": ""}}, "required": ["dims", "window_size", "shift_size", "device"], "type": "any"}}, "type": "function"}], "query": "I’m setting up shifted-window self-attention masks for a 3D Swin-UNETR CT volume on CPU. Start by computing the mask on a grid of depth=32, height=96, width=96 with a window that tiles the grid evenly as (8, 6, 6), and use the usual half-window cyclic shift. Wait—our latest preprocessing sometimes crops the depth by one patch when the z-axis padding is odd, and for this case the depth is actually the largest value <32 that is still divisible by the depth-window. Keep the in-plane dims unchanged, keep the same window sizes, and set each shift component to exactly half of its corresponding window component. Compute the shifted-window attention mask on CPU for that corrected grid.", "answers": "[{\"name\":\"monai_networks_nets_swin_unetr_compute_mask\",\"arguments\":{\"dims\":[24,96,96],\"window_size\":[8,6,6],\"shift_size\":[4,3,3],\"device\":\"cpu\"}}]"}
{"func_name": "monai_networks_nets_swin_unetr_filter_swinunetr", "func_desc": "filter_swinunetr\n    Converts and filters parameter keys from a pretrained checkpoint (from the Disruptive Autoencoders / SSL weights referenced in the repository) so they can be loaded into the MONAI SwinUNETR model using utilities such as monai.networks.utils.copy_model_state. This function inspects the incoming state-dict key (string) and either returns a renamed key paired with the original tensor value or returns None to indicate the parameter should be skipped. It is intended to be used when adapting checkpoint key naming conventions (for example, checkpoints that use an \"encoder.\" prefix) to MONAI's SwinUNETR naming convention (which expects a \"swinViT.\" prefix and slightly different internal key layout). This function does not modify the tensor contents, move tensors across devices, or validate tensor shapes; those checks and any loading errors are handled by the caller (e.g., copy_model_state or torch.nn.Module.load_state_dict).", "tools": [{"function": {"description": "filter_swinunetr\nConverts and filters parameter keys from a pretrained checkpoint (from the Disruptive Autoencoders / SSL weights referenced in the repository) so they can be loaded into the MONAI SwinUNETR model using utilities such as monai.networks.utils.copy_model_state. This function inspects the incoming state-dict key (string) and either returns a renamed key paired with the original tensor value or returns None to indicate the parameter should be skipped. It is intended to be used when adapting checkpoint key naming conventions (for example, checkpoints that use an \"encoder.\" prefix) to MONAI's SwinUNETR naming convention (which expects a \"swinViT.\" prefix and slightly different internal key layout). This function does not modify the tensor contents, move tensors across devices, or validate tensor shapes; those checks and any loading errors are handled by the caller (e.g., copy_model_state or torch.nn.Module.load_state_dict).", "name": "monai_networks_nets_swin_unetr_filter_swinunetr", "parameters": {"properties": {"key": {"type": "string", "description": "The key name from the source state dictionary. In practice this is a parameter name from a pretrained SSL checkpoint (for example keys that begin with \"encoder.\" or the exact keys listed below). The function uses this string to decide whether to skip the parameter or to produce a renamed key compatible with MONAI SwinUNETR. Passing a non-string will raise a TypeError in typical usage because string operations (slicing and equality) are applied.", "default": ""}, "value": {"type": "array", "items": {"type": "float"}, "description": "The tensor value associated with the source state-dict key. This is the weight or bias tensor to be copied into the target model if the key is accepted. The function does not alter this tensor (it returns the same object when returning a (key, value) pair). The function does not check device, dtype, or shape; those compatibility checks are left to the loader that applies the returned mapping.", "default": ""}}, "required": ["key", "value"], "type": "any"}}, "type": "function"}], "query": "I’m adapting a messy SSL checkpoint from a Swin-style encoder into MONAI SwinUNETR, but I just realized not every key with the old encoder prefix should be blindly renamed: if the key is for the Swin transformer trunk it needs the MONAI SwinUNETR trunk prefix, and anything that clearly belongs to a projection / head should be skipped (return None) because SwinUNETR won’t have a matching parameter. \n\nGiven this single example key, treat it as belonging to the transformer trunk (it’s inside the first stage’s first block’s first norm), so do the rename that swaps the old trunk prefix to the MONAI SwinUNETR trunk prefix while keeping the rest of the path intact and keeping the tensor value unchanged. Show exactly what filter_swinunetr would return for:\n- key: \"encoder.layers.0.blocks.0.norm1.weight\"\n- value: [[0.0123, -0.0456, 0.0789, -0.1011], [0.2022, 0.3033, -0.4044, 0.5055]]", "answers": "[{\"name\": \"monai_networks_nets_swin_unetr_filter_swinunetr\", \"arguments\": {\"key\": \"encoder.layers.0.blocks.0.norm1.weight\", \"value\": [[0.0123, -0.0456, 0.0789, -0.1011], [0.2022, 0.3033, -0.4044, 0.5055]]}}]"}
{"func_name": "monai_networks_nets_swin_unetr_window_partition", "func_desc": "window partition operation used by Swin Transformer–based models (for example Swin UNETR in MONAI) to split an input feature map or volume into non-overlapping local windows. This function implements the partitioning described in \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" (Liu et al.) and is intended for use in medical imaging deep-learning pipelines where local windowed self-attention is applied to 2D feature maps or 3D volumes.", "tools": [{"function": {"description": "window partition operation used by Swin Transformer–based models (for example Swin UNETR in MONAI) to split an input feature map or volume into non-overlapping local windows. This function implements the partitioning described in \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" (Liu et al.) and is intended for use in medical imaging deep-learning pipelines where local windowed self-attention is applied to 2D feature maps or 3D volumes.\n", "name": "monai_networks_nets_swin_unetr_window_partition", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Input tensor representing a batch of feature maps or volumes produced by a preceding network layer. For 3D volumes the expected shape is (B, D, H, W, C) where B is batch size, D is depth (slices), H is height, W is width and C is channel/features per voxel. For 2D feature maps the expected shape is (B, H, W, C). The function only supports inputs with rank 5 (3D volume) or rank 4 (2D map); other ranks will cause the internal reshape/permutation to fail (typically raising a runtime error). The input tensor is not modified in-place; the function returns a new tensor containing the partitioned windows (the implementation uses view/permute/contiguous operations and may return a new contiguous tensor sharing or copying memory as required by PyTorch).", "default": ""}, "window_size": {"type": "any", "description": "Local window size expressed as a tuple of integers matching the spatial dimensions of x. For a 3D input (B, D, H, W, C) provide (Wd, Wh, Ww) where each element divides the corresponding spatial dimension (D, H, W) exactly. For a 2D input (B, H, W, C) provide (Wh, Ww) where each element divides the corresponding spatial dimension (H, W) exactly. If the spatial dimensions are not divisible by the corresponding window sizes, the reshape/view operations will fail (raising a runtime error).", "default": ""}}, "required": ["x", "window_size"], "type": "any"}}, "type": "function"}], "query": "I’m validating the 3D window-partition step for a Swin UNETR-style pipeline on a toy CT feature volume, but I realized my downstream attention block expects the windows laid out in the same order it would see after a cyclic shift along all three spatial axes by exactly half of each window edge (so the partition must be applied *after* that shift). Please take the same single-volume tensor (B=1, D=4, H=4, W=4, C=2) with the exact values below, perform the cyclic shift implied by that constraint, and then partition into non-overlapping local windows whose edge lengths evenly tile the volume and whose each edge is exactly half of the corresponding full spatial extent in this toy example. Return the window tensor so I can manually spot-check the first and last windows.\n\nUse this exact x:\n[[[[[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0]], [[8.0, 9.0], [10.0, 11.0], [12.0, 13.0], [14.0, 15.0]], [[16.0, 17.0], [18.0, 19.0], [20.0, 21.0], [22.0, 23.0]], [[24.0, 25.0], [26.0, 27.0], [28.0, 29.0], [30.0, 31.0]]], [[[32.0, 33.0], [34.0, 35.0], [36.0, 37.0], [38.0, 39.0]], [[40.0, 41.0], [42.0, 43.0], [44.0, 45.0], [46.0, 47.0]], [[48.0, 49.0], [50.0, 51.0], [52.0, 53.0], [54.0, 55.0]], [[56.0, 57.0], [58.0, 59.0], [60.0, 61.0], [62.0, 63.0]]], [[[64.0, 65.0], [66.0, 67.0], [68.0, 69.0], [70.0, 71.0]], [[72.0, 73.0], [74.0, 75.0], [76.0, 77.0], [78.0, 79.0]], [[80.0, 81.0], [82.0, 83.0], [84.0, 85.0], [86.0, 87.0]], [[88.0, 89.0], [90.0, 91.0], [92.0, 93.0], [94.0, 95.0]]], [[[96.0, 97.0], [98.0, 99.0], [100.0, 101.0], [102.0, 103.0]], [[104.0, 105.0], [106.0, 107.0], [108.0, 109.0], [110.0, 111.0]], [[112.0, 113.0], [114.0, 115.0], [116.0, 117.0], [118.0, 119.0]], [[120.0, 121.0], [122.0, 123.0], [124.0, 125.0], [126.0, 127.0]]]]]", "answers": "[{\"name\":\"monai_networks_nets_swin_unetr_window_partition\",\"arguments\":{\"x\":[[[[[0.0,1.0],[2.0,3.0],[4.0,5.0],[6.0,7.0]],[[8.0,9.0],[10.0,11.0],[12.0,13.0],[14.0,15.0]],[[16.0,17.0],[18.0,19.0],[20.0,21.0],[22.0,23.0]],[[24.0,25.0],[26.0,27.0],[28.0,29.0],[30.0,31.0]]],[[[32.0,33.0],[34.0,35.0],[36.0,37.0],[38.0,39.0]],[[40.0,41.0],[42.0,43.0],[44.0,45.0],[46.0,47.0]],[[48.0,49.0],[50.0,51.0],[52.0,53.0],[54.0,55.0]],[[56.0,57.0],[58.0,59.0],[60.0,61.0],[62.0,63.0]]],[[[64.0,65.0],[66.0,67.0],[68.0,69.0],[70.0,71.0]],[[72.0,73.0],[74.0,75.0],[76.0,77.0],[78.0,79.0]],[[80.0,81.0],[82.0,83.0],[84.0,85.0],[86.0,87.0]],[[88.0,89.0],[90.0,91.0],[92.0,93.0],[94.0,95.0]]],[[[96.0,97.0],[98.0,99.0],[100.0,101.0],[102.0,103.0]],[[104.0,105.0],[106.0,107.0],[108.0,109.0],[110.0,111.0]],[[112.0,113.0],[114.0,115.0],[116.0,117.0],[118.0,119.0]],[[120.0,121.0],[122.0,123.0],[124.0,125.0],[126.0,127.0]]]]],\"window_size\":[2,2,2]}}]"}
{"func_name": "monai_networks_nets_swin_unetr_window_reverse", "func_desc": "monai.networks.nets.swin_unetr.window_reverse reconstructs a tensor of local windowed features back into the original spatial layout used by Swin Transformer style models (for example within the Swin-UNETR architecture used in medical imaging in MONAI). It reverses the window partitioning performed during windowed self-attention so that per-window feature vectors are rearranged into a contiguous volume or image that matches the original spatial dimensions.\n    \n    This function is used in the Swin-UNETR pipeline to reassemble processed local windows (the output of window-based attention or processing) into a full 2D image or 3D volume so downstream modules (decoders, up-samplers, loss computation) can operate on the restored spatial grid.", "tools": [{"function": {"description": "monai.networks.nets.swin_unetr.window_reverse reconstructs a tensor of local windowed features back into the original spatial layout used by Swin Transformer style models (for example within the Swin-UNETR architecture used in medical imaging in MONAI). It reverses the window partitioning performed during windowed self-attention so that per-window feature vectors are rearranged into a contiguous volume or image that matches the original spatial dimensions.\n\nThis function is used in the Swin-UNETR pipeline to reassemble processed local windows (the output of window-based attention or processing) into a full 2D image or 3D volume so downstream modules (decoders, up-samplers, loss computation) can operate on the restored spatial grid.", "name": "monai_networks_nets_swin_unetr_window_reverse", "parameters": {"properties": {"windows": {"type": "array", "items": {"type": "any"}, "description": "A tensor containing features extracted or processed per local window. This tensor is expected to be the result of a corresponding window partition operation and to be laid out so that it can be reshaped into blocks for each batch and spatial grid of windows. The last dimension is treated as the feature/channel dimension and is preserved. The tensor's device and dtype are preserved by this operation. If the memory layout or total number of elements does not match the expected grouping implied by window_size and dims, a runtime error (view/reshape mismatch) will occur.", "default": ""}, "window_size": {"type": "any", "description": "A tuple of integers specifying the size of each local window along each spatial axis. For a 3D volume this is (ws_d, ws_h, ws_w); for a 2D image this is (ws_h, ws_w). The lengths of window_size and the spatial portion of dims must be consistent: when dims has length 4 (b, d, h, w) window_size must have three elements; when dims has length 3 (b, h, w) window_size must have two elements. Mismatched lengths or values that do not evenly divide the corresponding spatial dimensions in dims will produce a runtime error during reshape.", "default": ""}, "dims": {"type": "any", "description": "A tuple describing the target output spatial dimensions including the batch size. For 3D volumes provide (b, d, h, w); for 2D images provide (b, h, w). The function branches based on len(dims): when len(dims) == 4 it reconstructs a 3D volume of shape (b, d, h, w, C); when len(dims) == 3 it reconstructs a 2D image of shape (b, h, w, C). dims must exactly match the spatial dimensions from which windows were originally partitioned; otherwise the reshaping logic will fail and raise a runtime exception (an undefined local variable or view/reshape error).", "default": ""}}, "required": ["windows", "window_size", "dims"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a Swin-UNETR-style 2D block for a single 4×4 medical image (batch size 1). I already have the per-window features after windowed self-attention, but I just realized the tensor I logged isn’t grouped by windows the way I first assumed: it’s a flat list of token feature vectors (3 channels each) that was produced by iterating windows in raster order over the 2×2 window grid, and within each window iterating tokens in row-major order. \n\nPlease reverse the window partitioning to reconstruct the full feature map in the original spatial layout. Use the same 2×2 window shape as the partition step.\n\nHere is the flat windows tensor (16 tokens total, 3 channels each):\n[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4],[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]].\n\nReturn the reconstructed tensor for the full 4×4 grid (with the batch dimension preserved).", "answers": "[{\"name\":\"monai_networks_nets_swin_unetr_window_reverse\",\"arguments\":{\"windows\":[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4],[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]],\"window_size\":[2,2],\"dims\":[1,4,4]}}]"}
{"func_name": "monai_networks_schedulers_rectified_flow_timestep_transform", "func_desc": "Applies a resolution-aware remapping to diffusion timesteps used by the rectified flow scheduler in MONAI. This function converts original timestep indices into adjusted timesteps that account for differences between the current input image voxel count and a reference (base) image voxel count. In the MONAI medical-imaging diffusion training workflow, this helps scale the effective progression through training timesteps when images have different spatial resolutions or sizes so that denoising or diffusion dynamics remain comparable across resolutions.", "tools": [{"function": {"description": "Applies a resolution-aware remapping to diffusion timesteps used by the rectified flow scheduler in MONAI. This function converts original timestep indices into adjusted timesteps that account for differences between the current input image voxel count and a reference (base) image voxel count. In the MONAI medical-imaging diffusion training workflow, this helps scale the effective progression through training timesteps when images have different spatial resolutions or sizes so that denoising or diffusion dynamics remain comparable across resolutions.\n", "name": "monai_networks_schedulers_rectified_flow_timestep_transform", "parameters": {"properties": {"t": {"type": "array", "items": {"type": "any"}, "description": "The original timestep(s) as a torch.Tensor of one or more scalar values representing discrete training timesteps or fractional timesteps. These are interpreted in the same units as num_train_timesteps (see below). The function first normalizes t by dividing by num_train_timesteps to obtain a fractional progress value in [0, 1] (for typical valid inputs).", "default": ""}, "input_img_size_numel": {"type": "array", "items": {"type": "any"}, "description": "The total number of spatial elements (H * W * D or other product of spatial axes) of the current input image as a torch.Tensor scalar. This represents the image resolution/size whose effect on diffusion dynamics we want to compensate for. It is compared to base_img_size_numel to compute a spatial scaling ratio.", "default": ""}, "base_img_size_numel": {"type": "integer", "description": "Reference total number of spatial elements used during training or as a canonical size (default 32768, equal to 32 * 32 * 32). This integer is the denominator in the spatial ratio and should be the same reference used when designing or calibrating the scheduler. If base_img_size_numel is zero, a division-by-zero error will occur.", "default": 32768}, "scale": {"type": "float", "description": "Additional multiplicative scaling factor applied to the spatial ratio before remapping timesteps (default 1.0). Use this to globally increase (>1.0) or decrease (<1.0) the influence of resolution differences on the timestep transform. A scale of 1.0 leaves the spatial ratio unmodified.", "default": 1.0}, "num_train_timesteps": {"type": "integer", "description": "Total number of discrete training timesteps used by the diffusion/rectified flow process (default 1000). This value is used to normalize input t into a fractional progress value and to re-scale the transformed fractional progress back into the original timestep units. If set inconsistently with the training schedule, the mapping between fractional progress and timestep indices will be incorrect.", "default": 1000}, "spatial_dim": {"type": "integer", "description": "Number of spatial dimensions in the image (default 3). This integer is used as the root exponent (1.0 / spatial_dim) when converting voxel-count ratios into linear spatial scale ratios. spatial_dim must be non-zero; a zero or negative spatial_dim will produce a ZeroDivisionError or an invalid root, and non-positive inputs to the root may produce non-real results.", "default": 3}}, "required": ["t", "input_img_size_numel", "scale", "num_train_timesteps", "spatial_dim", "base_img_size_numel"], "type": "any"}}, "type": "function"}], "query": "In a MONAI rectified-flow diffusion pipeline, I originally planned to remap timesteps assuming my fine-tuning crops are exactly 64×64×64 (so the voxel-count ratio versus the 32×32×32 reference is 8×). Wait—our data loader sometimes drops the last slice in one axis after random cropping when the requested patch size isn’t divisible by the stride, so the actual crop is the one that is **one slice smaller along a single axis** compared to the intended cube (the other two axes stay at the intended length). Using the same training setup (1000 training timesteps, scale=1.0, spatial_dim=3) and the same reference volume (32×32×32), remap the batch of original timesteps [0, 250, 500, 750, 999] using the **actual** fine-tuning crop voxel count implied by that constraint.", "answers": "[{\"name\": \"monai_networks_schedulers_rectified_flow_timestep_transform\", \"arguments\": {\"t\": [0.0, 250.0, 500.0, 750.0, 999.0], \"input_img_size_numel\": [258048.0], \"base_img_size_numel\": 32768, \"scale\": 1.0, \"num_train_timesteps\": 1000, \"spatial_dim\": 3}}]"}
{"func_name": "monai_networks_trt_compiler_get_dynamic_axes", "func_desc": "get_dynamic_axes(profiles) calculates a mapping of input/output tensor names to the axes that should be treated as dynamic when exporting a PyTorch model to ONNX, intended for use in MONAI workflows that convert models for TensorRT compilation. This function inspects a set of \"profiles\" describing minimum, optimal, and maximum sizes for each tensor dimension and returns the axis indices where the minimum and maximum differ. In the MONAI medical imaging domain, this allows onnx.export(..., dynamic_axes=...) to mark spatial or batch dimensions that can vary across inputs so downstream tools (ONNX runtime, TensorRT) can generate appropriate, optimized engines for variable-sized medical images.", "tools": [{"function": {"description": "get_dynamic_axes(profiles) calculates a mapping of input/output tensor names to the axes that should be treated as dynamic when exporting a PyTorch model to ONNX, intended for use in MONAI workflows that convert models for TensorRT compilation. This function inspects a set of \"profiles\" describing minimum, optimal, and maximum sizes for each tensor dimension and returns the axis indices where the minimum and maximum differ. In the MONAI medical imaging domain, this allows onnx.export(..., dynamic_axes=...) to mark spatial or batch dimensions that can vary across inputs so downstream tools (ONNX runtime, TensorRT) can generate appropriate, optimized engines for variable-sized medical images.\n", "name": "monai_networks_trt_compiler_get_dynamic_axes", "parameters": {"properties": {"profiles": {"type": "array", "items": {"type": "float"}, "description": "A list of profile dimension specifications. Each element is expected to be a mapping (iterable of key/value pairs in the form used by calling code) where each key is a tensor name (string) and the corresponding value is a three-element sequence [min, opt, max] representing shapes or sizes for that tensor at the minimum, nominal (optional), and maximum profile points. The function iterates each profile and each key within a profile, compares the min (vals[0]) and max (vals[2]) entries elementwise, and records the index i of any dimension where vals[0][i] != vals[2][i] as a dynamic axis. An empty list or other falsy value for profiles causes the function to return an empty mapping immediately. The function does not validate element types beyond indexability; if profile entries do not follow the expected three-element, indexable structure, Python indexing or type errors may be raised.", "default": ""}}, "required": ["profiles"], "type": "any"}}, "type": "function"}], "query": "I’m exporting a MONAI 3D UNet to ONNX for TensorRT and I’m trying to auto-generate `dynamic_axes` via `get_dynamic_axes(profiles)`. I initially assumed only the spatial dims should be dynamic, but I realized my two TensorRT optimization profiles are split by *different batch sizes*, and downstream engine selection depends on whether batch is treated as dynamic or static.\n\nHere are my two profiles (min/opt/max) for both `input_image` and `output_seg`:\n\nProfile A:\n- `input_image`: min=(2, 1, 64, 64, 64), opt=(2, 1, 128, 128, 128), max=(2, 1, 192, 192, 192)\n- `output_seg`: min=(2, 3, 64, 64, 64), opt=(2, 3, 128, 128, 128), max=(2, 3, 192, 192, 192)\n\nProfile B:\n- `input_image`: min=(4, 1, 64, 64, 64), opt=(4, 1, 160, 160, 160), max=(4, 1, 256, 256, 256)\n- `output_seg`: min=(4, 3, 64, 64, 64), opt=(4, 3, 160, 160, 160), max=(4, 3, 256, 256, 256)\n\nCompute the `dynamic_axes` mapping I should pass to `torch.onnx.export`, but with this constraint: treat as dynamic exactly those axes that vary across the *full set* of profiles (so if an axis is constant within a given profile but differs between the two profiles, it must still be marked dynamic).", "answers": "[{\"name\":\"monai_networks_trt_compiler_get_dynamic_axes\",\"arguments\":{\"profiles\":[{\"input_image\":[[2,1,64,64,64],[2,1,128,128,128],[2,1,192,192,192]],\"output_seg\":[[2,3,64,64,64],[2,3,128,128,128],[2,3,192,192,192]]},{\"input_image\":[[4,1,64,64,64],[4,1,160,160,160],[4,1,256,256,256]],\"output_seg\":[[4,3,64,64,64],[4,3,160,160,160],[4,3,256,256,256]]}]}}]"}
{"func_name": "monai_transforms_lazy_utils_is_compatible_apply_kwargs", "func_desc": "monai.transforms.lazy.utils.is_compatible_apply_kwargs checks whether two mappings of keyword arguments are compatible for combination by a lazy transform \"apply\" operation in MONAI's transform pipeline.\n    \n    This function is intended for use within MONAI's lazy transform system (used in medical imaging preprocessing and compositional transform APIs described in the project README) to decide if two per-call kwargs dictionaries can be merged or applied together when composing transforms. Each argument dictionary is expected to represent keyword arguments that would be passed to an individual transform's apply method during lazy execution (for example, per-item options carried through a composed sequence of transforms). The current implementation is a predicate function used by higher-level code that composes or merges kwargs before invoking apply.", "tools": [{"function": {"description": "monai.transforms.lazy.utils.is_compatible_apply_kwargs checks whether two mappings of keyword arguments are compatible for combination by a lazy transform \"apply\" operation in MONAI's transform pipeline.\n\nThis function is intended for use within MONAI's lazy transform system (used in medical imaging preprocessing and compositional transform APIs described in the project README) to decide if two per-call kwargs dictionaries can be merged or applied together when composing transforms. Each argument dictionary is expected to represent keyword arguments that would be passed to an individual transform's apply method during lazy execution (for example, per-item options carried through a composed sequence of transforms). The current implementation is a predicate function used by higher-level code that composes or merges kwargs before invoking apply.", "name": "monai_transforms_lazy_utils_is_compatible_apply_kwargs", "parameters": {"properties": {"kwargs_1": {"type": "any", "description": "First mapping of keyword arguments intended for a transform's apply call. This parameter represents per-transform or per-item options produced earlier in a lazy pipeline. The function expects a dict as provided by calling code; no mutation is performed on this object by the function itself.", "default": ""}, "kwargs_2": {"type": "any", "description": "Second mapping of keyword arguments intended for a transform's apply call. This parameter represents additional per-transform or per-item options that might be merged with kwargs_1 when composing transforms. The function expects a dict as provided by calling code; no mutation is performed on this object by the function itself.", "default": ""}}, "required": ["kwargs_1", "kwargs_2"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a MONAI lazy transform chain used for 3D brain MRI resampling + intensity normalization, and I need to decide whether two per-item `apply_kwargs` mappings can be merged into a single lazy `apply` call.\n\nInitially I thought they were compatible because they both specify the same interpolation mode. But I just noticed the upstream transform passes spatial metadata as a Python list-of-lists affine, while the downstream transform passes what should be the same affine but coming from a different loader as a NumPy array (still 4×4, same numeric values). I don’t want to force-cast either one because the predicate should decide compatibility as-is.\n\nAlso: one mapping has `keep_original_affine` set, the other does not mention it (so it should behave like an unspecified optional kwarg rather than an explicit conflict).\n\nGiven that, can these two kwargs mappings be combined safely for a single lazy apply call? Check compatibility between:\n1) {\"original_affine\": [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]], \"keep_original_affine\": true, \"mode\": \"bilinear\"}\n2) {\"original_affine\": \"(a 4x4 NumPy array with the same numeric values as in mapping 1)\", \"mode\": \"bilinear\", \"dtype\": \"float32\", \"padding_mode\": \"border\"}\n\nUse `monai.transforms.lazy.utils.is_compatible_apply_kwargs` to determine compatibility.", "answers": "[{\"name\": \"monai_transforms_lazy_utils_is_compatible_apply_kwargs\", \"arguments\": {\"kwargs_1\": {\"original_affine\": [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], \"keep_original_affine\": true, \"mode\": \"bilinear\"}, \"kwargs_2\": {\"original_affine\": \"(a 4x4 NumPy array with the same numeric values as in mapping 1)\", \"mode\": \"bilinear\", \"dtype\": \"float32\", \"padding_mode\": \"border\"}}}]"}
{"func_name": "monai_transforms_lazy_utils_kwargs_from_pending", "func_desc": "monai.transforms.lazy.utils.kwargs_from_pending extracts a dictionary of keyword arguments used to configure downstream lazy transforms from a \"pending\" transform item recorded in MONAI's lazy transform pipeline.\n    \n    This function is used in MONAI's lazy (deferred) transformation system for medical imaging preprocessing, where transform operations may collect metadata (such as interpolation mode, padding mode, target shape, and dtype) to be applied later when the actual tensor/image is materialized. The returned dictionary contains only the keys that should be forwarded as kwargs to the transform implementation (for example, resampling or resizing operations) so that composed, deferred transforms can be applied consistently across a preprocessing pipeline.", "tools": [{"function": {"description": "monai.transforms.lazy.utils.kwargs_from_pending extracts a dictionary of keyword arguments used to configure downstream lazy transforms from a \"pending\" transform item recorded in MONAI's lazy transform pipeline.\n\nThis function is used in MONAI's lazy (deferred) transformation system for medical imaging preprocessing, where transform operations may collect metadata (such as interpolation mode, padding mode, target shape, and dtype) to be applied later when the actual tensor/image is materialized. The returned dictionary contains only the keys that should be forwarded as kwargs to the transform implementation (for example, resampling or resizing operations) so that composed, deferred transforms can be applied consistently across a preprocessing pipeline.", "name": "monai_transforms_lazy_utils_kwargs_from_pending", "parameters": {"properties": {"pending_item": {"type": "any", "description": "A mapping that represents a pending transform item captured by MONAI's lazy transform mechanism. The mapping may contain the constants LazyAttr.INTERP_MODE and LazyAttr.PADDING_MODE (these are always checked and will appear in the result, possibly with value None), and may optionally contain LazyAttr.SHAPE and LazyAttr.DTYPE. This function does not validate the semantic correctness of the values (for example, it does not enforce that a shape is an integer sequence or that dtype is a valid data type); it only extracts and returns the relevant keys/values for later use. If pending_item is not a dict (for example, None or other type), the function treats it as absent and returns an empty dict. This parameter is central to MONAI's compositional preprocessing design because it carries transformation configuration between stages without immediately applying the operation.", "default": ""}}, "required": ["pending_item"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a MONAI lazy (deferred) preprocessing chain for 3D brain MRI resampling where the recorded pending item has a messy mix of metadata keys. I initially thought I could forward everything as-is, but I just realized some of the entries are bookkeeping (e.g., IDs, timestamps, notes) and a couple are clearly malformed artifacts from a logging hook (like an interpolation value with a typo and a dtype that isn’t a real torch/numpy dtype). Please extract only the kwargs that should actually be forwarded to the downstream resampling/resize implementation, using the valid interpolation setting (the one that corresponds to 3D linear interpolation), the valid padding mode that replicates edge values (not constant/zero padding), the target spatial size that is explicitly a 3-element list of positive ints, and the only dtype entry that is a standard 32-bit floating type. Here is the pending item:\n\n{\n  \"INTERP_MODE\": \"tri-linear\",\n  \"interp_mode\": \"trilinear\",\n  \"PADDING_MODE\": \"border\",\n  \"padding_mode\": \"zeros\",\n  \"SHAPE\": [128, 128, 64],\n  \"shape\": [128, 128],\n  \"DTYPE\": \"float32\",\n  \"dtype\": \"float32x\",\n  \"TRACE_ID\": \"case-00317\",\n  \"TIMESTAMP\": \"2026-02-04T10:11:12Z\",\n  \"NOTE\": \"resample before intensity norm\"\n}\n\nReturn the kwargs dictionary that kwargs_from_pending would produce for forwarding into the actual resampling transform.", "answers": "[{\"name\": \"monai_transforms_lazy_utils_kwargs_from_pending\", \"arguments\": {\"pending_item\": {\"INTERP_MODE\": \"tri-linear\", \"interp_mode\": \"trilinear\", \"PADDING_MODE\": \"border\", \"padding_mode\": \"zeros\", \"SHAPE\": [128, 128, 64], \"shape\": [128, 128], \"DTYPE\": \"float32\", \"dtype\": \"float32x\", \"TRACE_ID\": \"case-00317\", \"TIMESTAMP\": \"2026-02-04T10:11:12Z\", \"NOTE\": \"resample before intensity norm\"}}}]"}
{"func_name": "monai_transforms_lazy_utils_requires_interp", "func_desc": "monai.transforms.lazy.utils.requires_interp checks whether a given affine transformation matrix can be implemented by simple axis operations (flip, permutation, pad/slice) or whether it requires voxel-wise interpolation during resampling.\n    \n    This function is used in MONAI preprocessing and lazy transform code to decide whether a spatial transform represented by an affine matrix can be realized by cheap array operations (no interpolation, e.g., memory-only permutation/flip) or must be performed with interpolation (resampling voxels), which is more computationally expensive and can change image intensities. The function inspects the translation column and the top-left submatrix of the affine matrix to determine if the transform is an integer-translation plus axis-permutation/flip (returns a mapping) or requires interpolation (returns None). Internally the input is converted to a NumPy array for numeric checks; the function does not modify the provided matrix.", "tools": [{"function": {"description": "monai.transforms.lazy.utils.requires_interp checks whether a given affine transformation matrix can be implemented by simple axis operations (flip, permutation, pad/slice) or whether it requires voxel-wise interpolation during resampling.\n\nThis function is used in MONAI preprocessing and lazy transform code to decide whether a spatial transform represented by an affine matrix can be realized by cheap array operations (no interpolation, e.g., memory-only permutation/flip) or must be performed with interpolation (resampling voxels), which is more computationally expensive and can change image intensities. The function inspects the translation column and the top-left submatrix of the affine matrix to determine if the transform is an integer-translation plus axis-permutation/flip (returns a mapping) or requires interpolation (returns None). Internally the input is converted to a NumPy array for numeric checks; the function does not modify the provided matrix.", "name": "monai_transforms_lazy_utils_requires_interp", "parameters": {"properties": {"matrix": {"type": "array", "items": {"type": "any"}, "description": "The affine matrix to check. This is the (N+1)x(N+1) homogeneous affine matrix typically used in spatial transforms for medical images in MONAI, where the top-left N x N submatrix encodes axis scaling/rotation/flip and the last column encodes translation. The function uses the matrix values to determine whether the transform corresponds exactly (within tolerance) to axis flips/permutations and integer translations. The matrix is converted to a NumPy array internally for numeric comparisons; the function does not mutate the original matrix argument.", "default": ""}, "atol": {"type": "float", "description": "Absolute tolerance used for numerical comparisons. This tolerance is applied when checking whether translation components are close to integers and whether submatrix entries are close to -1, 0, or 1. The default is AFFINE_TOL (0.001 in the signature), meaning values within this absolute difference are treated as exact integers or exact -1/0/1 in the decision logic.", "default": 0.001}}, "required": ["matrix", "atol"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a MONAI lazy spatial pipeline for a 3D CT volume and I want to decide if my affine can be realized as pure axis ops (permute/flip plus integer pad/slice) or if it will force voxel-wise interpolation. Initially I was going to run `requires_interp` with the same tight tolerance we use for registration QA, but then I realized this affine comes from a DICOM-to-RAS conversion step that introduces tiny floating-point noise: some entries that should be exactly 0/±1 are off by ~1e-7 to 1e-8. So please use a tolerance that is just loose enough to treat those near-integers as exact, but still strict enough to reject half-voxel (non-integer) translations.\n\nHere’s the affine (note: it swaps X and Y, flips Z, and the translation is intended to be integer-valued even though it’s stored with float noise):\n[[0.0, 1.00000000005, 0.0, 5.0000000002],\n [0.99999999995, 0.0, 0.0, -2.0000000001],\n [0.0, 0.0, -1.0, 0.0],\n [0.0, 0.0, 0.0, 1.0]]\n\nCan `requires_interp` classify this as no-interp (cheap axis ops), and return the axis mapping if so?", "answers": "[{\"name\":\"monai_transforms_lazy_utils_requires_interp\",\"arguments\":{\"matrix\":[[0.0,1.00000000005,0.0,5.0000000002],[0.99999999995,0.0,0.0,-2.0000000001],[0.0,0.0,-1.0,0.0],[0.0,0.0,0.0,1.0]],\"atol\":1e-06}}]"}
{"func_name": "monai_transforms_utils_create_transform_ims_get_stacked_2d_ims", "func_desc": "monai.transforms.utils_create_transform_ims.get_stacked_2d_ims: Extract the three orthogonal 2D views from a 3D medical image volume and return them as a stacked list of 2D images for downstream transforms, visualization, or model input preparation in MONAI pipelines.\n    \n    This function is used in MONAI pre-processing/transform utilities to produce the three orthogonal slices (one per spatial axis) from a volumetric numpy.ndarray. It calls get_2d_slice(im, i, is_label) for i in range(3) and returns the three resulting 2D images in axis order i = 0, 1, 2. The caller is expected to have ensured consistent image sizing (for example, by applying SpatialPadd earlier in a typical MONAI pipeline). The is_label flag is forwarded to get_2d_slice so that label-specific handling performed there (such as discrete processing or different interpolation rules) is preserved.", "tools": [{"function": {"description": "monai.transforms.utils_create_transform_ims.get_stacked_2d_ims: Extract the three orthogonal 2D views from a 3D medical image volume and return them as a stacked list of 2D images for downstream transforms, visualization, or model input preparation in MONAI pipelines.\n\nThis function is used in MONAI pre-processing/transform utilities to produce the three orthogonal slices (one per spatial axis) from a volumetric numpy.ndarray. It calls get_2d_slice(im, i, is_label) for i in range(3) and returns the three resulting 2D images in axis order i = 0, 1, 2. The caller is expected to have ensured consistent image sizing (for example, by applying SpatialPadd earlier in a typical MONAI pipeline). The is_label flag is forwarded to get_2d_slice so that label-specific handling performed there (such as discrete processing or different interpolation rules) is preserved.", "name": "monai_transforms_utils_create_transform_ims_get_stacked_2d_ims", "parameters": {"properties": {"im": {"type": "array", "items": {"type": "any"}, "description": "The input medical image or label volume provided as a NumPy array. In MONAI workflows this is typically a 3D (or higher with channel dimension) image volume from which orthogonal 2D slices are extracted. The array must have sufficient spatial dimensions so that three orthogonal views are meaningful; sizes must be consistent across axes as required by downstream processing (SpatialPadd in the pipeline normally guarantees this).", "default": ""}, "is_label": {"type": "boolean", "description": "A boolean flag indicating whether im represents a segmentation label map (True) or an intensity image (False). This flag is forwarded to get_2d_slice and controls label-specific handling performed by that function (for example, preserving discrete label values or using label-appropriate processing). It does not modify im in place; it only affects how each 2D slice is produced.", "default": ""}}, "required": ["im", "is_label"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a quick MONAI visualization sanity-check for a tiny abdominal CT crop that’s already been SpatialPadd’ed to a consistent cubic shape. I initially planned to treat this volume as an intensity image, but I realized this particular tensor is actually a discretized foreground mask that got accidentally saved with float values (it’s effectively categorical), and I need the downstream slice extraction to preserve label-like handling (i.e., whatever special-casing `get_2d_slice` does for labels). Using `monai.transforms.utils_create_transform_ims.get_stacked_2d_ims`, extract the three orthogonal 2D views (axis order 0,1,2) from this 3D volume (D×H×W = 3×3×3): [[[0.0,0.2,0.4],[0.6,0.8,1.0],[1.2,1.4,1.6]],[[1.8,2.0,2.2],[2.4,2.6,2.8],[3.0,3.2,3.4]],[[3.6,3.8,4.0],[4.2,4.4,4.6],[4.8,5.0,5.2]]].", "answers": "[{\"name\":\"monai_transforms_utils_create_transform_ims_get_stacked_2d_ims\",\"arguments\":{\"im\":[[[0.0,0.2,0.4],[0.6,0.8,1.0],[1.2,1.4,1.6]],[[1.8,2.0,2.2],[2.4,2.6,2.8],[3.0,3.2,3.4]],[[3.6,3.8,4.0],[4.2,4.4,4.6],[4.8,5.0,5.2]]],\"is_label\":true}}]"}
{"func_name": "monai_transforms_utils_img_bounds", "func_desc": "monai.transforms.utils.img_bounds computes the bounding indices of non-zero content along the first two axes of a NumPy array. It is intended for use in MONAI preprocessing pipelines (medical imaging) to find the first and last rows/columns (axis 0 and axis 1) that contain any foreground (non-zero) values so callers can derive a tight in-plane bounding box for cropping or centering operations.\n    \n    This function inspects axis 0 and axis 1 of the provided image array using numpy.any to detect non-zero elements. It returns a 1D numpy array of four integer indices: the minimum and maximum index where axis 0 contains any non-zero elements, followed by the minimum and maximum index where axis 1 contains any non-zero elements.", "tools": [{"function": {"description": "monai.transforms.utils.img_bounds computes the bounding indices of non-zero content along the first two axes of a NumPy array. It is intended for use in MONAI preprocessing pipelines (medical imaging) to find the first and last rows/columns (axis 0 and axis 1) that contain any foreground (non-zero) values so callers can derive a tight in-plane bounding box for cropping or centering operations.\n\nThis function inspects axis 0 and axis 1 of the provided image array using numpy.any to detect non-zero elements. It returns a 1D numpy array of four integer indices: the minimum and maximum index where axis 0 contains any non-zero elements, followed by the minimum and maximum index where axis 1 contains any non-zero elements.", "name": "monai_transforms_utils_img_bounds", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "any"}, "description": "Input image array used by MONAI preprocessing utilities. The array may be multi-dimensional; this function evaluates non-zero presence along the first two dimensions (axis 0 and axis 1). Elements are interpreted with NumPy truthiness (zero values are treated as background, non-zero values as foreground). This parameter is required and is not modified by the function.", "default": ""}}, "required": ["img"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a MONAI preprocessing pipeline and need to compute in-plane bounds for a segmentation mask, but my data are a bit messy. I initially planned to treat any non-zero label as foreground when calling `monai.transforms.utils.img_bounds`, since this function finds the first/last rows and columns (axes 0 and 1) containing any non-zero values. However, I just realized the mask includes a small negative-valued artifact (from a buggy resampling step) that should be treated as background, while only truly positive labels represent tumor.\n\nSo: before computing bounds, interpret the mask as foreground only where values are strictly greater than 0 (everything else should be considered background). Then compute the bounding indices along axis 0 and axis 1 using `img_bounds` on that cleaned/binarized mask.\n\nUse this mask:\n[[0,0,0,0,0,0],\n [0,-1,1,1,0,0],\n [0,0,1,1,0,0],\n [0,0,0,0,0,0],\n [0,0,0,0,0,0]]", "answers": "[{\"name\": \"monai_transforms_utils_img_bounds\", \"arguments\": {\"img\": [[0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]}}]"}
{"func_name": "monai_transforms_utils_is_positive", "func_desc": "monai.transforms.utils.is_positive returns a boolean mask indicating which elements of a medical-image array are strictly greater than zero.", "tools": [{"function": {"description": "monai.transforms.utils.is_positive returns a boolean mask indicating which elements of a medical-image array are strictly greater than zero.\n", "name": "monai_transforms_utils_is_positive", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "any"}, "description": "Input numeric image array containing voxel or pixel intensities (for example, a 2D/3D medical image used in MONAI preprocessing pipelines). The array represents image data where positive values typically indicate foreground signal (e.g., soft-tissue intensity in MRI or positive Hounsfield units in CT) and non-positive values indicate background or absence of signal. The function performs an elementwise comparison against zero and does not modify this input array in place.", "default": ""}}, "required": ["img"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a quick foreground mask for a toy CT slice, but I just realized my downstream QC step treats exact zeros as padding/background (air and missing-data), so I must NOT include them as foreground. Use `monai.transforms.utils.is_positive` on this 2D slice (in HU, already rescaled): [[-1024, -10, 0], [15.5, 200, -3], [0.1, -0.2, 42]]. Generate the boolean mask that flags only true tissue signal (i.e., values strictly above the background cutoff implied by that constraint).", "answers": "[{\"name\": \"monai_transforms_utils_is_positive\", \"arguments\": {\"img\": [[-1024, -10, 0], [15.5, 200, -3], [0.1, -0.2, 42]]}}]"}
{"func_name": "monai_transforms_utils_rand_choice", "func_desc": "monai.transforms.utils.rand_choice returns a boolean that is True with probability prob and False otherwise. It is intended for use in MONAI transform pipelines and data-augmentation logic to make stochastic decisions (for example, whether to apply a particular spatial or intensity augmentation during preprocessing or training of medical imaging models).", "tools": [{"function": {"description": "monai.transforms.utils.rand_choice returns a boolean that is True with probability prob and False otherwise. It is intended for use in MONAI transform pipelines and data-augmentation logic to make stochastic decisions (for example, whether to apply a particular spatial or intensity augmentation during preprocessing or training of medical imaging models).\n", "name": "monai_transforms_utils_rand_choice", "parameters": {"properties": {"prob": {"type": "float", "description": "The probability of returning True. This parameter represents the desired probability that the function yields a True outcome; the default value 0.5 implements a 50/50 chance. Internally the function draws a single pseudorandom float from Python's standard library random.random() (which produces values in [0.0, 1.0)) and returns True when that draw is less than or equal to prob. Typical usage in MONAI is to pass a float in the closed interval [0.0, 1.0] where 0.0 means almost always False (True only if random.random() happens to be exactly 0.0, an extremely unlikely event) and 1.0 means always True. If prob > 1.0 the comparison will always be True; if prob < 0.0 the comparison will always be False. If prob is not a numeric type comparable to a float, the comparison may raise a TypeError.", "default": 0.5}}, "required": ["prob"], "type": "any"}}, "type": "function"}], "query": "In my MONAI training pipeline for a brain MRI segmentation model, I initially planned to apply a strong elastic deformation with a 20% chance each iteration. But I just realized we are doing deep supervision with 5 output heads, and if I keep that rate, the expected number of augmented outputs per iteration becomes 1 (which is too aggressive for our stability constraints early in training). Instead, make the stochastic gate so that, in expectation, only one head per iteration sees the elastic deformation. Return the stochastic decision as a boolean for the augmentation gate (assume independence across iterations).", "answers": "[{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.2}}]"}
{"func_name": "monai_transforms_utils_scale_affine", "func_desc": "Compute a homogeneous affine scaling matrix that maps coordinates from an original spatial size to a new spatial size.\n    \n    This utility is used in MONAI preprocessing and spatial transforms for medical imaging workflows to build an (r+1) x (r+1) affine matrix that applies axis-wise scaling when an image or volume is resized. The function determines r as the maximum of the number of spatial dimensions in spatial_size and new_spatial_size, computes per-dimension scale factors as original_dim / max(new_dim, 1) to avoid division by zero, and constructs a square affine matrix suitable for homogeneous coordinate transformations used by downstream resampling and transform utilities. If centered is True (the default), the function adjusts the translation components so the scaling is performed about the image center; if False, scaling is performed about the origin/corner. The function does not modify input arguments and returns a new numeric matrix object.", "tools": [{"function": {"description": "Compute a homogeneous affine scaling matrix that maps coordinates from an original spatial size to a new spatial size.\n\nThis utility is used in MONAI preprocessing and spatial transforms for medical imaging workflows to build an (r+1) x (r+1) affine matrix that applies axis-wise scaling when an image or volume is resized. The function determines r as the maximum of the number of spatial dimensions in spatial_size and new_spatial_size, computes per-dimension scale factors as original_dim / max(new_dim, 1) to avoid division by zero, and constructs a square affine matrix suitable for homogeneous coordinate transformations used by downstream resampling and transform utilities. If centered is True (the default), the function adjusts the translation components so the scaling is performed about the image center; if False, scaling is performed about the origin/corner. The function does not modify input arguments and returns a new numeric matrix object.", "name": "monai_transforms_utils_scale_affine", "parameters": {"properties": {"spatial_size": {"type": "any", "description": "The original spatial size of the image or volume. Each element corresponds to the size (number of voxels or pixels) along a spatial axis in the same order used throughout the MONAI spatial transform pipeline. This tuple is used as the numerator when computing per-axis scale factors (original_dim / max(new_dim, 1)). Supplying non-numeric entries will raise a TypeError or ValueError when floats are constructed from the values.", "default": ""}, "new_spatial_size": {"type": "any", "description": "The target spatial size to which the image or volume will be scaled. Each element corresponds to the target size along a spatial axis. For any target dimension value of 0, the function uses max(new_dim, 1) to avoid division by zero (i.e., a zero entry is treated as 1 for the scale computation). The number of elements may differ from spatial_size; r is computed as max(len(spatial_size), len(new_spatial_size)) and the returned affine matrix has size (r+1) x (r+1).", "default": ""}, "centered": {"type": "boolean", "description": "Whether the scaling should be centered about the image center (True, default) or performed about the origin/corner (False). When True, the function modifies the translation components of the homogeneous affine matrix so that the geometric center of the image remains fixed after scaling. When False, the resulting affine applies pure scaling about the coordinate origin.", "default": true}}, "required": ["spatial_size", "new_spatial_size", "centered"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a MONAI preprocessing chain for a 3D MRI volume and need the homogeneous (r+1)x(r+1) affine used for the resize step. Initially I planned to scale about the image center (so downstream spatial transforms stay aligned), but then I realized this particular branch of the pipeline is doing patch-based training with a corner-anchored sampler, and a center-based translation would silently mis-register the sampled patches. So: compute the affine scaling matrix for resizing from 256×256×128 voxels to 192×192×96 voxels, but do the scaling in the way that keeps the corner/origin fixed (i.e., no center-shift translation terms).", "answers": "[{\"name\": \"monai_transforms_utils_scale_affine\", \"arguments\": {\"spatial_size\": [256, 256, 128], \"new_spatial_size\": [192, 192, 96], \"centered\": false}}]"}
{"func_name": "monai_transforms_utils_squarepulse", "func_desc": "monai.transforms.utils.squarepulse computes a periodic square-wave signal from an input time-like tensor using PyTorch operations. This implementation is intended for use in MONAI workflows (medical imaging preprocessing, synthetic signal generation, or augmentation pipelines) where a reproducible, PyTorch-native square pulse is required; it reproduces the behavior of scipy.signal.square by comparing the phase against a duty cycle fraction of a 2*pi period.", "tools": [{"function": {"description": "monai.transforms.utils.squarepulse computes a periodic square-wave signal from an input time-like tensor using PyTorch operations. This implementation is intended for use in MONAI workflows (medical imaging preprocessing, synthetic signal generation, or augmentation pipelines) where a reproducible, PyTorch-native square pulse is required; it reproduces the behavior of scipy.signal.square by comparing the phase against a duty cycle fraction of a 2*pi period.\n", "name": "monai_transforms_utils_squarepulse", "parameters": {"properties": {"sig": {"type": "array", "items": {"type": "any"}, "description": "Input tensor interpreted as a time variable (in radians). Each element of sig represents an instantaneous phase or time sample at which the square-wave value is evaluated. The function converts this input to a torch.Tensor (via convert_to_tensor) if it is not already one, and the output tensor y has exactly the same shape as sig so it can be used elementwise in downstream preprocessing or synthetic-data generation for medical imaging pipelines.", "default": ""}, "duty": {"type": "float", "description": "Fraction of the 2*pi period during which the output is high (value 1). duty is a scalar floating-point duty cycle with a default of 0.5 (50% duty cycle produces a symmetric square wave alternating between 1 and -1). Internally duty is converted to a tensor for broadcasting comparison; valid duty values are in the closed interval [0, 1] to produce the conventional square-wave values. If duty is outside [0, 1], the implementation treats those positions as invalid and leaves the corresponding output values at 0 (see Failure modes). The duty parameter controls the practical shape of the waveform used for synthetic temporal patterns or test signals in MONAI workflows.", "default": 0.5}}, "required": ["sig", "duty"], "type": "any"}}, "type": "function"}], "query": "I’m building a synthetic cardiac-gating trigger inside a MONAI augmentation pipeline and I want the pulse computed in a purely PyTorch/MONAI way. Start by computing a periodic square-pulse for these phase samples (radians): [0.0, 0.5, 1.0, 1.8, 3.2, 4.5, 5.9, 6.4]. I initially planned to use a fixed duty fraction, but I just realized our downstream gating QC expects the ‘on’ window to correspond to exactly the fraction of the 2π cycle implied by the integer percent setting in our protocol sheet. So: interpret the duty setting as that integer percent divided by 100 (not a decimal typed by hand), and use the protocol’s value for the ‘on’ window. Then run monai.transforms.utils.squarepulse on the phase samples with that duty configuration and return the resulting square-wave values.", "answers": "[{\"name\": \"monai_transforms_utils_squarepulse\", \"arguments\": {\"sig\": [0.0, 0.5, 1.0, 1.8, 3.2, 4.5, 5.9, 6.4], \"duty\": 0.3}}]"}
{"func_name": "monai_transforms_utils_zero_margins", "func_desc": "monai.transforms.utils.zero_margins returns True if all values within margin indices of the edges of img along dimensions 1 and 2 are zero, otherwise returns False. This utility is intended for MONAI preprocessing and transform checks in medical imaging workflows (for example verifying zero padding or cleared borders in channel-first image arrays used with PyTorch).", "tools": [{"function": {"description": "monai.transforms.utils.zero_margins returns True if all values within margin indices of the edges of img along dimensions 1 and 2 are zero, otherwise returns False. This utility is intended for MONAI preprocessing and transform checks in medical imaging workflows (for example verifying zero padding or cleared borders in channel-first image arrays used with PyTorch).\n", "name": "monai_transforms_utils_zero_margins", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "any"}, "description": "Input array representing an image or batch of images. The function treats axis 1 and axis 2 as the spatial edge dimensions to inspect (i.e., the second and third dimensions of the array). Axis 0 is iterated over in aggregation (commonly channel or slice/depth in MONAI data layouts such as (C, H, W) or (C, D, H, W) where the check applies to the H and W axes). The array is not modified by this function; it is read-only for this check.", "default": ""}, "margin": {"type": "integer", "description": "Non-negative integer number of indices measured from each edge along dimensions 1 and 2 to inspect for zeros. A margin of 0 results in an empty slice check and therefore returns True (no nonzero values found in zero-width margins). Negative margins are not the intended use; they will follow NumPy slicing semantics (which may produce unexpected results) and should be avoided. The function expects an int and will not validate non-integer types.", "default": ""}}, "required": ["img", "margin"], "type": "any"}}, "type": "function"}], "query": "I’m validating a MONAI preprocessing step for a channel-first CT slice tensor shaped (C, H, W). I initially planned to verify a 2-pixel zero border, but I just realized our in-house cropper can legally write nonzero values into the *outermost* pixel ring (edge-adjacent artifacts), while the *next* ring inward must still be completely zero to guarantee safe patch extraction. So: check whether the zero-only condition holds for the 1-pixel-thick margin that is immediately inside the outermost border (i.e., the margin thickness should be the original border width minus one). Use `monai.transforms.utils.zero_margins` on the image below and report whether the margins are all zeros across all channels.\n\nUse this image array:\n[\n  [\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,5,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0]\n  ],\n  [\n    [0,0,0,0,0],\n    [0,0,0,0,0],\n    [0,0,7,0,0],\n    [0,0,0,0,0],\n    [0,0,0,0,0]\n  ]\n]", "answers": "[{\"name\":\"monai_transforms_utils_zero_margins\",\"arguments\":{\"img\":[[[0,0,0,0,0],[0,0,0,0,0],[0,0,5,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,7,0,0],[0,0,0,0,0],[0,0,0,0,0]]],\"margin\":1}}]"}
{"func_name": "monai_utils_dist_string_list_all_gather", "func_desc": "Utility to gather a list of Python strings from all processes in a distributed job and return the concatenated list of strings from every rank.\n    \n    This function is used in MONAI distributed training and utilities to share small pieces of textual information (for example file names, subject IDs, small metadata or logging messages) across multiple processes when running multi-GPU or multi-node workflows. It implements the pattern documented by PyTorch-Ignite's all_gather for strings: each rank joins its local list of strings into one long UTF-8 encoded byte sequence using a delimiter, uses a tensor-based all-gather (via ignite distributed APIs when ignite is available or native torch.distributed when initialized) and then decodes and splits the gathered byte sequences back into a flattened Python list ordered by rank (rank 0 results first, then rank 1, etc.). If no distributed backend is active (world size <= 1), the input list is returned unchanged.", "tools": [{"function": {"description": "Utility to gather a list of Python strings from all processes in a distributed job and return the concatenated list of strings from every rank.\n\nThis function is used in MONAI distributed training and utilities to share small pieces of textual information (for example file names, subject IDs, small metadata or logging messages) across multiple processes when running multi-GPU or multi-node workflows. It implements the pattern documented by PyTorch-Ignite's all_gather for strings: each rank joins its local list of strings into one long UTF-8 encoded byte sequence using a delimiter, uses a tensor-based all-gather (via ignite distributed APIs when ignite is available or native torch.distributed when initialized) and then decodes and splits the gathered byte sequences back into a flattened Python list ordered by rank (rank 0 results first, then rank 1, etc.). If no distributed backend is active (world size <= 1), the input list is returned unchanged.", "name": "monai_utils_dist_string_list_all_gather", "parameters": {"properties": {"strings": {"type": "array", "items": {"type": "string"}, "description": "a local list of UTF-8 text strings on the current process/rank to be gathered across all ranks. In MONAI workflows this typically contains small textual items such as image file names, case identifiers, or other per-process metadata that must be collected on every rank. The function does not modify this list in place; it only reads its contents. Each element will be converted to UTF-8 bytes internally before gathering.", "default": ""}, "delimiter": {"type": "string", "description": "a single string used to join the local list into one string before encoding for transport and to split the gathered strings back into elements after decoding. Default is \"    \". The delimiter must be chosen so that it does not occur in the original string elements; if an element contains the delimiter, the split operation will produce extra elements and the reconstructed list will be incorrect. The delimiter is applied exactly as provided and is not validated or escaped by this function.", "default": "\t"}}, "required": ["strings", "delimiter"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a 4-GPU MONAI validation loop where each rank produces a small list of case IDs it just processed, and I want to gather them into one combined list (ordered by rank) so only rank 0 prints a single consolidated log line.\n\nInitially I was going to use the human-readable delimiter we’ve used in past runs, but I just realized our case IDs are autogenerated from a DICOM export and can occasionally contain doubled punctuation (including the same two-character sequence), which would silently corrupt the split on decode. Instead, pick a delimiter that is guaranteed not to occur in these IDs by construction: use the single ASCII control character that is commonly used as a non-printing “unit separator” in text protocols.\n\nOn *this* rank my local IDs are still:\n[\"val_case_010\",\"val_case_011\",\"val_case_019\"]\n\nGather these strings across all ranks using that safer delimiter (and if the job isn’t actually distributed, it should just return my local list unchanged).", "answers": "[{\"name\": \"monai_utils_dist_string_list_all_gather\", \"arguments\": {\"strings\": [\"val_case_010\", \"val_case_011\", \"val_case_019\"], \"delimiter\": \"\\u001f\"}}]"}
{"func_name": "monai_utils_misc_list_to_dict", "func_desc": "monai.utils.misc.list_to_dict: Convert a list of \"key=value\" string items into a Python dictionary suitable for simple configuration parsing in MONAI workflows (for example, parsing command-line or bundle parameter overrides used in training/evaluation pipelines).\n    \n    This function accepts a sequence of text tokens where each token is expected to represent either a bare key (\"a\") or a key/value assignment (\"key=value\"). It is commonly used in MONAI to convert simple string-based parameter specifications into native Python types so they can be applied to configuration dictionaries for model training, inference, or pre-/post-processing pipelines.\n    \n    Behavior:\n    - Each element in items is split at the first \"=\" into a key and a value. If an element contains no \"=\", the value for that key becomes None. This supports shorthand flags or presence indicators.\n    - Surrounding whitespace and single-quote characters are removed from both keys and values by stripping the characters \" \n\r\t'\".\n    - After splitting and stripping, the function attempts to convert the textual value to a native Python object using ast.literal_eval (so numeric literals, lists, dicts, tuples, and quoted strings will become their Python equivalents).\n    - If ast.literal_eval raises a ValueError, the function then attempts to interpret the value as a boolean using distutils.util._strtobool; if that succeeds the boolean value is returned.\n    - If both conversions fail, the original stripped string is used as the value.\n    - Duplicate keys are considered an error: the function raises KeyError when the same key is encountered more than once.\n    - If items is an empty list (no elements), an empty dictionary is returned.\n    \n    Limitations and failure modes:\n    - The parameter type is list; passing a non-list is not documented and may lead to unexpected behavior. An empty list yields {}.\n    - ast.literal_eval exceptions other than ValueError (for example, SyntaxError) are not explicitly caught by the implementation and will propagate to the caller.\n    - The boolean conversion relies on distutils.util._strtobool semantics; values not recognized by that helper will not be converted to booleans and will fall back to the raw string.\n    - Keys and values have only the characters \" \n\r\t'\" stripped; other surrounding characters are preserved.", "tools": [{"function": {"description": "monai.utils.misc.list_to_dict: Convert a list of \"key=value\" string items into a Python dictionary suitable for simple configuration parsing in MONAI workflows (for example, parsing command-line or bundle parameter overrides used in training/evaluation pipelines).\n\nThis function accepts a sequence of text tokens where each token is expected to represent either a bare key (\"a\") or a key/value assignment (\"key=value\"). It is commonly used in MONAI to convert simple string-based parameter specifications into native Python types so they can be applied to configuration dictionaries for model training, inference, or pre-/post-processing pipelines.\n\nBehavior:\n- Each element in items is split at the first \"=\" into a key and a value. If an element contains no \"=\", the value for that key becomes None. This supports shorthand flags or presence indicators.\n- Surrounding whitespace and single-quote characters are removed from both keys and values by stripping the characters \" \n     '\".\n- After splitting and stripping, the function attempts to convert the textual value to a native Python object using ast.literal_eval (so numeric literals, lists, dicts, tuples, and quoted strings will become their Python equivalents).\n- If ast.literal_eval raises a ValueError, the function then attempts to interpret the value as a boolean using distutils.util._strtobool; if that succeeds the boolean value is returned.\n- If both conversions fail, the original stripped string is used as the value.\n- Duplicate keys are considered an error: the function raises KeyError when the same key is encountered more than once.\n- If items is an empty list (no elements), an empty dictionary is returned.\n\nLimitations and failure modes:\n- The parameter type is list; passing a non-list is not documented and may lead to unexpected behavior. An empty list yields {}.\n- ast.literal_eval exceptions other than ValueError (for example, SyntaxError) are not explicitly caught by the implementation and will propagate to the caller.\n- The boolean conversion relies on distutils.util._strtobool semantics; values not recognized by that helper will not be converted to booleans and will fall back to the raw string.\n- Keys and values have only the characters \" \n     '\" stripped; other surrounding characters are preserved.", "name": "monai_utils_misc_list_to_dict", "parameters": {"properties": {"items": {"type": "array", "items": {"type": "any"}, "description": "A list of strings representing key or key=value entries. Each element should be a string such as \"lr=0.001\", \"use_amp=True\", or \"tag\". Keys are parsed into dictionary keys (strings). Values are parsed into Python objects when possible via ast.literal_eval, then boolean-converted via _strtobool if literal_eval fails, and otherwise left as the stripped string. If an element contains no \"=\", the corresponding dictionary value is None. Supplying an empty list returns an empty dict. This function is typically used in MONAI to parse simple configuration overrides for training, evaluation, or preprocessing pipelines.", "default": ""}}, "required": ["items"], "type": "any"}}, "type": "function"}], "query": "I’m overriding a MONAI bundle config from the command line and need to turn a messy token list into a dict with native Python types using `monai.utils.misc.list_to_dict`. Initially I planned to pass all tokens through as-is, but I just realized MONAI will treat repeated keys as a hard error, and I accidentally specified the same scalar hyperparameter twice (once early, once later). Don’t change the *meaning* of the overrides—keep only the occurrence that appears later in the list (it reflects the final tuned value) and drop the earlier duplicate before converting. Then run the conversion (split on the first \"=\", strip whitespace and single quotes, literal-eval when possible; otherwise fall back to bool parsing, otherwise string; bare keys become None).\n\nTokens (in order):\n[\n  \"trainer.max_epochs=250\",\n  \"optim.lr=3e-4\",\n  \"amp=TRUE\",\n  \"roi_size=(96,96,64)\",\n  \"class_weights=[1.0, 0.5, 2.0]\",\n  \"post_pred.transforms={'type':'AsDiscrete','argmax':True}\",\n  \"run_name='liver_ct_v2'\",\n  \"optim.lr=0.00025\",\n  \"dry_run\"\n]\n\nReturn the final dictionary after applying the de-duplication rule and parsing into native Python types.", "answers": "[{\"name\": \"monai_utils_misc_list_to_dict\", \"arguments\": {\"items\": [\"trainer.max_epochs=250\", \"amp=TRUE\", \"roi_size=(96,96,64)\", \"class_weights=[1.0, 0.5, 2.0]\", \"post_pred.transforms={'type':'AsDiscrete','argmax':True}\", \"run_name='liver_ct_v2'\", \"optim.lr=0.00025\", \"dry_run\"]}}]"}
{"func_name": "monai_utils_module_damerau_levenshtein_distance", "func_desc": "Calculates the Damerau–Levenshtein distance between two strings, returning the minimum number of single-character edits (insertions, deletions, substitutions, or adjacent transpositions) required to transform s1 into s2. In the MONAI context this routine is used for spelling correction and string similarity tasks that arise in medical-imaging workflows (for example, normalizing metadata keys, matching label names, correcting typographical errors in annotations or clinical text before model training and analysis).\n    \n    This implementation follows the Damerau–Levenshtein definition (see https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance) and is designed to be a small, dependency-free utility for preprocessing and validation steps in MONAI pipelines. It performs an exact computation using a dynamic programming table (implemented as a dict) and treats an adjacent transposition of two characters as a single edit operation.", "tools": [{"function": {"description": "Calculates the Damerau–Levenshtein distance between two strings, returning the minimum number of single-character edits (insertions, deletions, substitutions, or adjacent transpositions) required to transform s1 into s2. In the MONAI context this routine is used for spelling correction and string similarity tasks that arise in medical-imaging workflows (for example, normalizing metadata keys, matching label names, correcting typographical errors in annotations or clinical text before model training and analysis).\n\nThis implementation follows the Damerau–Levenshtein definition (see https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance) and is designed to be a small, dependency-free utility for preprocessing and validation steps in MONAI pipelines. It performs an exact computation using a dynamic programming table (implemented as a dict) and treats an adjacent transposition of two characters as a single edit operation.", "name": "monai_utils_module_damerau_levenshtein_distance", "parameters": {"properties": {"s1": {"type": "string", "description": "The source string to be transformed. In MONAI usage this typically represents a label, metadata key, filename, or token extracted from clinical/annotation text whose spelling or format should be compared against a canonical or target string. The function uses Python string semantics (len, indexing); passing a non-str value will raise a TypeError when the function attempts string operations.", "default": ""}, "s2": {"type": "string", "description": "The target string to compare against s1. In MONAI workflows this represents the canonical form, expected label, or corrected token to which s1 should be compared or aligned. Like s1, this must be a Python str; non-str inputs will cause exceptions.", "default": ""}}, "required": ["s1", "s2"], "type": "any"}}, "type": "function"}], "query": "In a MONAI label-normalization pass, I was going to flag a likely adjacent-transposition typo by comparing the observed token to the canonical organ label. But I just realized our ingestion step lowercases everything and also strips trailing punctuation, and one of the annotators sometimes appends a period at the end of a label. So: take the observed token that is the canonical label with its last two letters swapped, then add a trailing '.' artifact; normalize it the same way our pipeline does (lowercase + remove that trailing punctuation), and then compute the Damerau–Levenshtein distance between the normalized observed token and the canonical label. What distance do you get?", "answers": "[{\"name\": \"monai_utils_module_damerau_levenshtein_distance\", \"arguments\": {\"s1\": \"livre\", \"s2\": \"liver\"}}]"}
{"func_name": "ncbi_genome_download_core_get_genus_label", "func_desc": "ncbi_genome_download.core.get_genus_label returns the genus (first token of the organism name) for an NCBI assembly summary entry. This function is used by the ncbi-genome-download tool to extract the genus string from an assembly entry's organism_name field so that entries can be grouped, filtered, or matched against user-specified genera (for example the --genera option described in the README).", "tools": [{"function": {"description": "ncbi_genome_download.core.get_genus_label returns the genus (first token of the organism name) for an NCBI assembly summary entry. This function is used by the ncbi-genome-download tool to extract the genus string from an assembly entry's organism_name field so that entries can be grouped, filtered, or matched against user-specified genera (for example the --genera option described in the README).\n", "name": "ncbi_genome_download_core_get_genus_label", "parameters": {"properties": {"entry": {"type": "any", "description": "A single assembly summary entry as provided by the NCBI assembly summary files or by upstream parsing code in ncbi_genome_download. The entry is expected to be a mapping that contains the key 'organism_name' whose value is the taxonomic organism name string (for example \"Escherichia coli\"). The practical role of this parameter is to supply the full organism name from which the genus is extracted; the function does not validate or enrich taxonomy data beyond simple string parsing.", "default": ""}}, "required": ["entry"], "type": "any"}}, "type": "function"}], "query": "I’m batching NCBI assembly summary entries and using `ncbi_genome_download.core.get_genus_label` so I can group downloads by genus for a downstream diversity report. For one problematic entry, I was going to just split the `organism_name` on whitespace and take the first token, but then I realized the field sometimes comes in with accidental leading spaces from a munged TSV export—so splitting naïvely could yield an empty token and break grouping. Treat this entry as having possible leading/trailing whitespace and extract the genus label the tool would use (i.e., the first *non-empty* token of the organism name): organism_name = \"  Lactobacillus delbrueckii subsp. bulgaricus  \" (same record otherwise: assembly_accession GCF_000203855.3; assembly_level Complete Genome; ftp_path ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/203/855/GCF_000203855.3_ASM20385v3). What genus string should be returned for grouping?", "answers": "[{\"name\":\"ncbi_genome_download_core_get_genus_label\",\"arguments\":{\"entry\":{\"organism_name\":\"  Lactobacillus delbrueckii subsp. bulgaricus  \",\"assembly_accession\":\"GCF_000203855.3\",\"assembly_level\":\"Complete Genome\",\"ftp_path\":\"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/203/855/GCF_000203855.3_ASM20385v3\"}}}]"}
{"func_name": "ncbi_genome_download_core_get_name_and_checksum", "func_desc": "Extract a full filename and its checksum from a checksums list for a file whose name ends with the given suffix.\n\nThis function is used by the ncbi-genome-download tool to identify the exact archive or sequence file name and the associated checksum entry from a parsed checksums listing (the checksums metadata that accompanies NCBI assembly or FTP directory listings). The returned filename and checksum are intended for use when verifying a downloaded file against the authoritative checksum provided by NCBI. The function handles typical NCBI file-naming edge cases such as CDS and RNA FASTA files that themselves include the substring used by plain genomic FASTA names, avoiding false positive matches.", "tools": [{"function": {"description": "Extract a full filename and its checksum from a checksums list for a file whose name ends with the given suffix.\n\nThis function is used by the ncbi-genome-download tool to identify the exact archive or sequence file name and the associated checksum entry from a parsed checksums listing (the checksums metadata that accompanies NCBI assembly or FTP directory listings). The returned filename and checksum are intended for use when verifying a downloaded file against the authoritative checksum provided by NCBI. The function handles typical NCBI file-naming edge cases such as CDS and RNA FASTA files that themselves include the substring used by plain genomic FASTA names, avoiding false positive matches.", "name": "ncbi_genome_download_core_get_name_and_checksum", "parameters": {"properties": {"checksums": {"type": "array", "items": {"type": "any"}, "description": "A list of checksum entries as produced from parsing an NCBI checksums/MD5 file or equivalent metadata. Each entry is expected to be a mapping-like object (dictionary) with at least the keys 'file' and 'checksum', where 'file' is the filename string found on the NCBI FTP/assembly directory and 'checksum' is the corresponding checksum string. The function scans this list in order and returns the first entry whose 'file' value matches the suffix constraint described by end.", "default": ""}, "end": {"type": "string", "description": "The filename suffix to match (for example a file ending such as '.fna.gz' or a format-specific ending returned by NgdConfig.get_fileending()). This is matched using str.endswith, so it must be the literal trailing substring expected at the end of the target filename. The value is typically one of the file-ending identifiers used by ncbi_genome_download (for example the values returned from NgdConfig.get_fileending('cds-fasta') or NgdConfig.get_fileending('rna-fasta')).", "default": ""}}, "required": ["checksums", "end"], "type": "any"}}, "type": "function"}], "query": "I’m validating an NCBI assembly fetch using the checksums metadata, and I need the exact (filename, checksum) pair for the *primary genomic FASTA archive* entry. Initially I was going to match by the suffix that literally contains the word “genomic”, but I just realized that both the CDS and RNA FASTA artifacts also contain that substring and could slip through if anything upstream accidentally does substring/regex matching instead of strict suffix matching. So: from the parsed checksums list below, extract the entry whose filename ends with the **compressed nucleotide FASTA for the full genome assembly** (i.e., the one that is *not* the CDS-from-genome nor the RNA-from-genome derivative, and not an annotation file). Return the exact filename and checksum.\n\nChecksums entries:\n- file: \"GCF_000005845.2_ASM584v2_genomic.fna.gz\", checksum: \"b3a1b6a68d7b0d3d3ac7d7a54c6f3b2f\"\n- file: \"GCF_000005845.2_ASM584v2_cds_from_genomic.fna.gz\", checksum: \"2f1c0f3b5fcb3b8e4a9c7c1e0b2d6a7c\"\n- file: \"GCF_000005845.2_ASM584v2_rna_from_genomic.fna.gz\", checksum: \"7c6d5e4f3a2b1c0d9e8f7a6b5c4d3e2f\"\n- file: \"GCF_000005845.2_ASM584v2_genomic.gff.gz\", checksum: \"4e3d2c1b0a9f8e7d6c5b4a3928171615\"", "answers": "[{\"name\": \"ncbi_genome_download_core_get_name_and_checksum\", \"arguments\": {\"checksums\": [{\"file\": \"GCF_000005845.2_ASM584v2_genomic.fna.gz\", \"checksum\": \"b3a1b6a68d7b0d3d3ac7d7a54c6f3b2f\"}, {\"file\": \"GCF_000005845.2_ASM584v2_cds_from_genomic.fna.gz\", \"checksum\": \"2f1c0f3b5fcb3b8e4a9c7c1e0b2d6a7c\"}, {\"file\": \"GCF_000005845.2_ASM584v2_rna_from_genomic.fna.gz\", \"checksum\": \"7c6d5e4f3a2b1c0d9e8f7a6b5c4d3e2f\"}, {\"file\": \"GCF_000005845.2_ASM584v2_genomic.gff.gz\", \"checksum\": \"4e3d2c1b0a9f8e7d6c5b4a3928171615\"}], \"end\": \"_genomic.fna.gz\"}}]"}
{"func_name": "ncbi_genome_download_core_get_species_label", "func_desc": "ncbi_genome_download.core.get_species_label: Extract the species label (the species epithet) from an NCBI assembly summary entry.\n\nThis function is used in the ncbi-genome-download tool to derive a simple species identifier from the assembly summary \"organism_name\" field provided by NCBI. The typical \"organism_name\" value is a scientific name string such as \"Escherichia coli str. K-12\" or \"Streptomyces coelicolor A3(2)\". The function performs a simple whitespace split of that string and returns the second token (index 1), which represents the species epithet in common two-word scientific names. This lightweight parsing is useful for creating human-readable directory labels, grouping downloaded assemblies by species, or generating short species tags when building NCBI mirror-like directory structures as described in the project README.", "tools": [{"function": {"description": "ncbi_genome_download.core.get_species_label: Extract the species label (the species epithet) from an NCBI assembly summary entry.\n\nThis function is used in the ncbi-genome-download tool to derive a simple species identifier from the assembly summary \"organism_name\" field provided by NCBI. The typical \"organism_name\" value is a scientific name string such as \"Escherichia coli str. K-12\" or \"Streptomyces coelicolor A3(2)\". The function performs a simple whitespace split of that string and returns the second token (index 1), which represents the species epithet in common two-word scientific names. This lightweight parsing is useful for creating human-readable directory labels, grouping downloaded assemblies by species, or generating short species tags when building NCBI mirror-like directory structures as described in the project README.", "name": "ncbi_genome_download_core_get_species_label", "parameters": {"properties": {"entry": {"type": "any", "description": "A single assembly summary entry represented as a dictionary (as produced by parsing NCBI assembly_summary files in ncbi-genome-download). This dictionary MUST contain the key 'organism_name' whose value is expected to be the organism scientific name as a string. The function splits entry['organism_name'] on spaces and returns the second whitespace-separated token as the species label. If the provided entry is missing 'organism_name' or that value is not a string, attempting to call this function will raise the underlying Python exception (for example, KeyError if the key is absent or AttributeError/TypeError if the value does not support split). If 'organism_name' is present but contains fewer than two whitespace-separated tokens (for example an empty string or a single-word name), the function returns the placeholder string 'sp.' to indicate that no distinct species epithet could be extracted.", "default": ""}}, "required": ["entry"], "type": "any"}}, "type": "function"}], "query": "I’m building a mirror-like directory tree from NCBI assembly_summary lines and using `ncbi_genome_download.core.get_species_label` to generate the species-level folder name from `organism_name`. Initially I was going to feed in the raw `organism_name` as-is, but I realized some entries include strain/serovar/isolate tokens (sometimes with parentheses) after the binomial, which I do *not* want to affect the species label. So: take the entry whose `organism_name` begins with the genus *Streptomyces* and has the next word as the species epithet, followed by a strain designator containing parentheses. Using the function’s rule (whitespace split and take the second token), what species epithet label should the folder use?", "answers": "[{\"name\": \"ncbi_genome_download_core_get_species_label\", \"arguments\": {\"entry\": {\"organism_name\": \"Streptomyces coelicolor A3(2)\"}}}]"}
{"func_name": "ncbi_genome_download_core_get_strain", "func_desc": "ncbi_genome_download.core.get_strain: Extract a strain identifier string from a single NCBI assembly summary entry.\n\nThis function is used by the ncbi-genome-download tool to derive a human-meaningful strain label from an assembly summary entry returned by NCBI. The derived strain label is typically used for naming output files or organizing downloaded genomes by strain. The function examines the entry in a prioritized order to find the most specific available strain information: it first inspects the 'infraspecific_name' field (handling values of the form \"key=value\" by taking the value after the final '='), then the 'isolate' field, and finally, when the entry's 'organism_name' consists of more than two whitespace-separated words and the viral flag is False, it treats all words from the third onward as the strain (joining them with single spaces). If none of these produce a non-empty string, the function falls back to returning the assembly accession from the 'assembly_accession' field. Empty strings in the checked fields are treated as absent. The viral parameter disables the organism_name-based extraction because viral organism names should not be parsed in this way for strain derivation. The function performs no I/O and has no side effects; it operates purely on the provided dictionary and returns a string.", "tools": [{"function": {"description": "ncbi_genome_download.core.get_strain: Extract a strain identifier string from a single NCBI assembly summary entry.\n\nThis function is used by the ncbi-genome-download tool to derive a human-meaningful strain label from an assembly summary entry returned by NCBI. The derived strain label is typically used for naming output files or organizing downloaded genomes by strain. The function examines the entry in a prioritized order to find the most specific available strain information: it first inspects the 'infraspecific_name' field (handling values of the form \"key=value\" by taking the value after the final '='), then the 'isolate' field, and finally, when the entry's 'organism_name' consists of more than two whitespace-separated words and the viral flag is False, it treats all words from the third onward as the strain (joining them with single spaces). If none of these produce a non-empty string, the function falls back to returning the assembly accession from the 'assembly_accession' field. Empty strings in the checked fields are treated as absent. The viral parameter disables the organism_name-based extraction because viral organism names should not be parsed in this way for strain derivation. The function performs no I/O and has no side effects; it operates purely on the provided dictionary and returns a string.", "name": "ncbi_genome_download_core_get_strain", "parameters": {"properties": {"entry": {"type": "any", "description": "Assembly summary entry as a Python dictionary corresponding to one line/record from an NCBI assembly summary file. This dictionary is expected to contain at least the keys 'infraspecific_name', 'isolate', 'organism_name', and 'assembly_accession' (access via entry['key'] is performed). The practical significance is that these keys are the standard columns from NCBI assembly metadata used to determine strain-level labels for genome downloads; if any of these keys are missing, a KeyError will be raised by the function.", "default": ""}, "viral": {"type": "boolean", "description": "Flag indicating whether the entry represents a viral genome. When False (default), the function will attempt to extract a strain from 'organism_name' when that name contains more than two words by joining words from the third onward. When True, the function will not use 'organism_name' for strain extraction, which is important in the viral domain where organism_name tokenization does not reliably represent strain. This parameter has no side effects beyond controlling the extraction logic.", "default": false}}, "required": ["entry", "viral"], "type": "any"}}, "type": "function"}], "query": "I’m scripting a cleanup step for an ncbi-genome-download mirror where filenames must reflect the most specific strain label. Start by extracting the strain identifier for this single assembly summary entry assuming it’s **not viral**. Initially I was going to rely on the organism name parsing, but I just realized the entry already encodes an infraspecific descriptor in a key/value style string, and we should prefer the most specific value by stripping everything up to (and including) the final delimiter. Treat empty fields as absent and keep the tool’s usual priority rules. Entry: infraspecific_name=\"strain=K-12 substr. MG1655\", isolate=\"\", organism_name=\"Escherichia coli K-12 substr. MG1655\", assembly_accession=\"GCF_000005845.2\".", "answers": "[{\"name\":\"ncbi_genome_download_core_get_strain\",\"arguments\":{\"entry\":{\"infraspecific_name\":\"strain=K-12 substr. MG1655\",\"isolate\":\"\",\"organism_name\":\"Escherichia coli K-12 substr. MG1655\",\"assembly_accession\":\"GCF_000005845.2\"},\"viral\":false}}]"}
{"func_name": "ncbi_genome_download_core_get_strain_label", "func_desc": "Clean up and normalize a strain name extracted from an NCBI assembly metadata entry so it can be safely used in file names and directory names created by ncbi-genome-download.", "tools": [{"function": {"description": "Clean up and normalize a strain name extracted from an NCBI assembly metadata entry so it can be safely used in file names and directory names created by ncbi-genome-download.\n", "name": "ncbi_genome_download_core_get_strain_label", "parameters": {"properties": {"entry": {"type": "any", "description": "A dictionary representing a single assembly/record metadata entry as produced or consumed by ncbi-genome-download internal code paths (for example, parsed rows from an NCBI assembly summary). This function calls the package-internal get_strain(entry, viral) to extract the raw strain name from this metadata dictionary. The practical role of entry is to provide the original strain information as supplied by NCBI so that the returned label can be used when saving genome files or building human-readable directory structures described in the README.", "default": ""}, "viral": {"type": "boolean", "description": "When True, instructs the internal get_strain call to use viral-specific extraction logic (if any) for entries from viral groups; when False (the default) standard non-viral strain extraction is used. This flag mirrors the group-specific behavior used elsewhere in ncbi-genome-download when selecting and organizing viral versus non-viral genomes.", "default": false}}, "required": ["entry", "viral"], "type": "any"}}, "type": "function"}], "query": "I’m organizing a mixed bacterial/viral NCBI assembly download into per-strain folders and need a filename-safe strain label for one Salmonella enterica assembly. Initially I was going to normalize the strain label directly from the `infraspecific_name` field, but I just realized this particular record’s value contains annotation cruft (a prefix key/value pattern plus punctuation like parentheses and a path-like separator) that would create unsafe or misleading directory names. So: generate the strain label using the strain-designating content from that field *after removing the key portion and stripping or normalizing any filesystem-hostile characters*, and if that cleaned result collapses to something empty or ambiguous, fall back to the isolate value instead. Use this metadata entry: organism_name \"Salmonella enterica subsp. enterica serovar Typhimurium\", infraspecific_name \"strain=LT2 (Lab strain)/rev.3\", isolate \"LT2\", assembly_accession \"GCF_000006945.2\". This is not viral.", "answers": "[{\"name\":\"ncbi_genome_download_core_get_strain_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Salmonella enterica subsp. enterica serovar Typhimurium\",\"infraspecific_name\":\"strain=LT2 (Lab strain)/rev.3\",\"isolate\":\"LT2\",\"assembly_accession\":\"GCF_000006945.2\"},\"viral\":false}}]"}
{"func_name": "nnunetv2_preprocessing_cropping_cropping_create_nonzero_mask", "func_desc": "nnunetv2.preprocessing.cropping.cropping.create_nonzero_mask: Create a boolean foreground mask for cropping by marking spatial voxels/pixels where any input channel is nonzero and filling interior holes.", "tools": [{"function": {"description": "nnunetv2.preprocessing.cropping.cropping.create_nonzero_mask: Create a boolean foreground mask for cropping by marking spatial voxels/pixels where any input channel is nonzero and filling interior holes.\n", "name": "nnunetv2_preprocessing_cropping_cropping_create_nonzero_mask", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "A channel-first image array used in nnU-Net preprocessing. Expected shapes are either (C, X, Y, Z) for 3D cases or (C, X, Y) for 2D cases, where C is the number of input modalities/channels and (X, Y, Z) or (X, Y) are the spatial dimensions. Each channel corresponds to an input modality (e.g., different MR sequences or CT) as described in the nnU-Net README; nonzero values indicate signal/foreground in that modality. The function treats any nonzero value as foreground. The function does not modify this input array in-place.", "default": ""}}, "required": ["data"], "type": "any"}}, "type": "function"}], "query": "I’m setting up nnU-Net v2 preprocessing for a tiny 3D multimodal brain MRI toy example (channel-first, shape (3, 4, 4, 3)). I initially planned to generate the foreground mask by thresholding only the anatomical channel that looks cleanest, but then I realized that would silently drop lesion-only signal that appears in a different modality. So instead, generate the nnU-Net foreground mask using the built-in routine that treats a voxel as foreground if *any* channel is nonzero, and make sure the mask uses the hole-filling behavior (so a fully enclosed zero-valued cavity inside the foreground would be filled).\n\nUse this exact toy array as `data`:\n\nChannel 0 (T1):\n[[[0, 0, 0],\n  [0, 5, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 7, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]]]\n\nChannel 1 (T2):\n[[[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 9, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]]]\n\nChannel 2 (FLAIR):\n[[[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [3, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]],\n\n [[0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0],\n  [0, 0, 0]]]\n\nWhat nnU-Net API call would you use to generate that boolean nonzero foreground mask (with interior holes filled) from this data?", "answers": "[{\"name\": \"nnunetv2_preprocessing_cropping_cropping_create_nonzero_mask\", \"arguments\": {\"data\": [[[[0, 0, 0], [0, 5, 0], [0, 0, 0], [0, 0, 0]], [[0, 7, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]], [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 9, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]], [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [3, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]]]}}]"}
{"func_name": "nnunetv2_utilities_overlay_plots_select_slice_to_plot2", "func_desc": "nnunetv2.utilities.overlay_plots.select_slice_to_plot2 selects a single slice index (along the first axis) from a 3D image/segmentation pair for visualization. It is intended for nnU-Net overlay plotting utilities to choose a representative 2D slice that contains the largest amount of foreground across all non-background classes (useful when creating overlay plots of predicted or ground-truth segmentations for medical image segmentation tasks).", "tools": [{"function": {"description": "nnunetv2.utilities.overlay_plots.select_slice_to_plot2 selects a single slice index (along the first axis) from a 3D image/segmentation pair for visualization. It is intended for nnU-Net overlay plotting utilities to choose a representative 2D slice that contains the largest amount of foreground across all non-background classes (useful when creating overlay plots of predicted or ground-truth segmentations for medical image segmentation tasks).\n", "name": "nnunetv2_utilities_overlay_plots_select_slice_to_plot2", "parameters": {"properties": {"image": {"type": "array", "items": {"type": "any"}, "description": "The image volume used to determine the number of slices and coordinate convention. Expected to be 3D with shape (S, H, W) where S is the number of axial slices (the function also accepts a shape like (1, H, W) for a single-slice volume). The image array itself is not inspected for intensity content by this implementation; only image.shape[0] (the slice count) is used. It is the caller's responsibility to ensure image and segmentation correspond spatially (same first dimension and slice ordering).", "default": ""}, "segmentation": {"type": "array", "items": {"type": "any"}, "description": "The label volume with the same slice axis as image, expected to be 3D with shape (S, H, W). Segmentation values <= 0 are treated as background and ignored; positive distinct label values are treated as separate foreground classes. The function computes, for each positive class, the per-slice voxel counts and then converts these to relative proportions (percent of that class that is present in each slice). These per-class per-slice proportions are averaged across classes to obtain a single score per slice.", "default": ""}}, "required": ["image", "segmentation"], "type": "any"}}, "type": "function"}], "query": "I’m preparing an nnU-Net overlay QA figure for a tiny 3D brain MRI toy example (5 axial slices, each 4×4) with a predicted *multi-class* tumor mask. Initially I was going to pick the slice that maximizes the **total count of foreground voxels** (anything non-zero) because that’s quick. But then I realized that biases toward slices dominated by a single label and can miss the “representative” slice when different tumor subregions are spread across slices.\n\nSo instead, use `nnunetv2.utilities.overlay_plots.select_slice_to_plot2` to pick the slice index along the first axis that maximizes the **mean per-class foreground proportion across all non-background labels** (i.e., for each tumor label present in the task, compute its fraction of pixels in that slice, then average those fractions; background is excluded). Use exactly the arrays below; image intensities are arbitrary.\n\nImage (5×4×4) and predicted segmentation (5×4×4):\n- image = [[[0.0, 0.1, 0.2, 0.3], [0.1, 0.2, 0.3, 0.4], [0.2, 0.3, 0.4, 0.5], [0.3, 0.4, 0.5, 0.6]], [[0.5, 0.6, 0.7, 0.8], [0.6, 0.7, 0.8, 0.9], [0.7, 0.8, 0.9, 1.0], [0.8, 0.9, 1.0, 1.1]], [[1.0, 1.1, 1.2, 1.3], [1.1, 1.2, 1.3, 1.4], [1.2, 1.3, 1.4, 1.5], [1.3, 1.4, 1.5, 1.6]], [[1.5, 1.6, 1.7, 1.8], [1.6, 1.7, 1.8, 1.9], [1.7, 1.8, 1.9, 2.0], [1.8, 1.9, 2.0, 2.1]], [[2.0, 2.1, 2.2, 2.3], [2.1, 2.2, 2.3, 2.4], [2.2, 2.3, 2.4, 2.5], [2.3, 2.4, 2.5, 2.6]]]\n- segmentation = [[[0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 2]], [[0, 0, 0, 0], [0, 1, 1, 0], [0, 0, 0, 0], [0, 0, 2, 2]], [[0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 2, 2, 2]], [[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 0], [0, 0, 2, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n\nReturn the selected slice index for plotting (largest mean per-class foreground proportion across the non-background tumor labels).", "answers": "[{\"name\":\"nnunetv2_utilities_overlay_plots_select_slice_to_plot2\",\"arguments\":{\"image\":[[[0.0,0.1,0.2,0.3],[0.1,0.2,0.3,0.4],[0.2,0.3,0.4,0.5],[0.3,0.4,0.5,0.6]],[[0.5,0.6,0.7,0.8],[0.6,0.7,0.8,0.9],[0.7,0.8,0.9,1.0],[0.8,0.9,1.0,1.1]],[[1.0,1.1,1.2,1.3],[1.1,1.2,1.3,1.4],[1.2,1.3,1.4,1.5],[1.3,1.4,1.5,1.6]],[[1.5,1.6,1.7,1.8],[1.6,1.7,1.8,1.9],[1.7,1.8,1.9,2.0],[1.8,1.9,2.0,2.1]],[[2.0,2.1,2.2,2.3],[2.1,2.2,2.3,2.4],[2.2,2.3,2.4,2.5],[2.3,2.4,2.5,2.6]]],\"segmentation\":[[[0,0,0,0],[0,1,0,0],[0,0,0,0],[0,0,0,2]],[[0,0,0,0],[0,1,1,0],[0,0,0,0],[0,0,2,2]],[[0,0,0,0],[0,1,1,1],[0,0,0,0],[0,2,2,2]],[[0,0,0,0],[0,0,1,0],[0,0,0,0],[0,0,2,0]],[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]]}}]"}
{"func_name": "periodictable_cromermann_fxrayatstol", "func_desc": "Calculate x-ray scattering factors at a specified sin(theta)/lambda (stol) for an element or ion using the Cromer–Mann form-factor data in the periodictable package.\n    \n    This function is used in x-ray scattering calculations (see package README) to obtain the atomic scattering factor f(sin(theta)/lambda) for a resolved element or ion symbol. It resolves and normalizes the provided element/ion symbol, optionally applies an explicit integer ion charge override, looks up the Cromer–Mann parameters for that species via getCMformula, and evaluates the form factor at the provided stol value(s) by calling the resulting object's atstol method. The returned value is intended for use in computing x-ray scattering amplitudes and cross sections in crystallography and related applications.", "tools": [{"function": {"description": "Calculate x-ray scattering factors at a specified sin(theta)/lambda (stol) for an element or ion using the Cromer–Mann form-factor data in the periodictable package.\n\nThis function is used in x-ray scattering calculations (see package README) to obtain the atomic scattering factor f(sin(theta)/lambda) for a resolved element or ion symbol. It resolves and normalizes the provided element/ion symbol, optionally applies an explicit integer ion charge override, looks up the Cromer–Mann parameters for that species via getCMformula, and evaluates the form factor at the provided stol value(s) by calling the resulting object's atstol method. The returned value is intended for use in computing x-ray scattering amplitudes and cross sections in crystallography and related applications.", "name": "periodictable_cromermann_fxrayatstol", "parameters": {"properties": {"symbol": {"type": "string", "description": "Symbol of an element or ion to evaluate, e.g., \"Ca\" or \"Ca2+\". This input may include ionic suffixes such as \"+\", \"-\", \"3+\" etc. If the symbol ends with a single sign character and no trailing digit (for example \"Na+\" or \"Cl-\"), the function will normalize it to an explicit 1 charge (e.g., \"Na1+\", \"Cl1-\") before lookup. The resolved symbol determines which Cromer–Mann parameters are used for the scattering factor lookup and thus directly controls the physical species whose scattering is returned.", "default": ""}, "stol": {"type": "float", "description": "The value(s) of sin(theta)/lambda at which to evaluate the x-ray form factor, given in inverse angstroms (1/Å). The function accepts a scalar float for a single evaluation or a sequence (e.g., list or numpy array) of floats for vectorized evaluation; when a sequence is supplied, the function will return a numpy.ndarray of matching shape containing the scattering factor for each stol. These values are forwarded to the underlying cmf.atstol(stol) routine which computes the Cromer–Mann form factor.", "default": ""}, "charge": {"type": "integer", "nullable": true, "description": "Optional integer ion charge override. If provided (not None), this integer will override any ionic suffix present in symbol. The function strips trailing digits and sign characters from symbol and then appends an explicit charge suffix corresponding to this integer (formatted so that 2 becomes \"2+\", -1 becomes \"1-\", etc.). If charge is None (the default), the function uses any valence suffix already present in symbol (after the normalization rule described above).", "default": null}}, "required": ["symbol", "stol", "charge"], "type": "any"}}, "type": "function"}], "query": "I’m computing X-ray scattering amplitudes for an aqueous iron salt, but the speciation is messy. I initially thought I should just request the form factor for the trivalent cation at sin(theta)/lambda = 0.42 1/Å and pass an explicit charge override to remove ambiguity. Wait—our downstream refinement script already applies the oxidation state correction globally, so if I also hard-code a charge here I’ll double-count it. Please compute the Cromer–Mann atomic form factor at sin(theta)/lambda = 0.42 1/Å for the same iron species, but do it in the way that avoids imposing any additional charge beyond what the symbol itself implies when normalized/resolved.", "answers": "[{\"name\": \"periodictable_cromermann_fxrayatstol\", \"arguments\": {\"symbol\": \"Fe3+\", \"stol\": 0.42}}]"}
{"func_name": "periodictable_fasta_D2Omatch", "func_desc": "periodictable.fasta.D2Omatch computes the percent D2O in solvent required to match the neutron scattering length density (SLD) of a sample.\n    This function is used in neutron scattering contrast matching (for example small-angle neutron scattering or neutron reflectometry) to find the solvent D2O concentration that makes the neutron SLD of a mixed hydrogenated/deuterated sample equal to the neutron SLD of a H2O/D2O solvent mixture. It assumes linear mixing of SLDs between hydrogenated and deuterated forms of the sample and between H2O and D2O solvent, and uses the module constants H2O_SLD and D2O_SLD (water SLD values evaluated at 20 °C) for solvent SLDs. The deuterated sample SLD (Dsld) is defined to include exchangeable labile protons replaced by deuterons. Note: this function was deprecated in periodictable 1.5.3; prefer periodictable.nsf.D2O_match(formula) for newer code paths.", "tools": [{"function": {"description": "periodictable.fasta.D2Omatch computes the percent D2O in solvent required to match the neutron scattering length density (SLD) of a sample.\nThis function is used in neutron scattering contrast matching (for example small-angle neutron scattering or neutron reflectometry) to find the solvent D2O concentration that makes the neutron SLD of a mixed hydrogenated/deuterated sample equal to the neutron SLD of a H2O/D2O solvent mixture. It assumes linear mixing of SLDs between hydrogenated and deuterated forms of the sample and between H2O and D2O solvent, and uses the module constants H2O_SLD and D2O_SLD (water SLD values evaluated at 20 °C) for solvent SLDs. The deuterated sample SLD (Dsld) is defined to include exchangeable labile protons replaced by deuterons. Note: this function was deprecated in periodictable 1.5.3; prefer periodictable.nsf.D2O_match(formula) for newer code paths.", "name": "periodictable_fasta_D2Omatch", "parameters": {"properties": {"Hsld": {"type": "float", "description": "Neutron scattering length density (SLD) of the hydrogenated form of the material. This is the SLD for the sample containing protium (H) where labile protons have not been exchanged. The value must be expressed in the same units as the module-level H2O_SLD and D2O_SLD constants (the function does not convert units). Hsld represents the SLD contribution of the sample end-member that mixes linearly with its deuterated counterpart when varying sample deuteration.", "default": ""}, "Dsld": {"type": "float", "description": "Neutron scattering length density (SLD) of the deuterated form of the material. This SLD is for the sample form where all exchangeable labile protons have been replaced by deuterons (D). As with Hsld, Dsld must be given in the same units as H2O_SLD and D2O_SLD. Dsld represents the SLD of the fully (with respect to labile protons) deuterated sample end-member.", "default": ""}}, "required": ["Hsld", "Dsld"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a SANS contrast-matching run for a protein that’s partially deuterated, and I was going to plug the hydrogenated and fully deuterated SLD endpoints into the (deprecated) `periodictable.fasta.D2Omatch` helper. The catch is that our lab notebook reports SLDs in the older convention where the numeric values are written in “micro-Å⁻²” but the sign on the hydrogenated endpoint sometimes gets flipped during export. Assume the deuterated endpoint (which *already includes* replacement of all exchangeable/labile protons by deuterons) is reliable, but for the hydrogenated endpoint you must use the physically consistent sign given that typical hydrogenated proteins have *negative* SLD in H2O. Using the corrected hydrogenated SLD and the given deuterated SLD, compute the %D2O in the H2O/D2O solvent needed to match the sample at 20 °C (using the built-in H2O_SLD and D2O_SLD constants that the function assumes). The notebook lists endpoints as: hydrogenated magnitude 0.4 (in micro-Å⁻²) and deuterated 4.8 (in micro-Å⁻²).", "answers": "[{\"name\":\"periodictable_fasta_D2Omatch\",\"arguments\":{\"Hsld\":-4e-07,\"Dsld\":4.8e-06}}]"}
{"func_name": "periodictable_nsf_fix_number", "func_desc": "periodictable.nsf.fix_number converts a numeric string from NSF/periodictable data files into a plain Python float by removing uncertainty notation and a few special characters used in tabulated element data.\n    \n    This function is used throughout the periodictable.nsf data-handling code to normalise numeric fields found in tabular element and isotope data (for example neutron scattering lengths, cross sections, densities or activation-related values). It strips characters used in the source files ('<' for upper limits and '*' as an annotation), removes parenthetical uncertainty notation such as \"35.24(2)\", and returns only the central numeric value so downstream calculations (neutron scattering, x-ray scattering, or activation estimates described in the package README) receive a simple float without embedded uncertainty information.", "tools": [{"function": {"description": "periodictable.nsf.fix_number converts a numeric string from NSF/periodictable data files into a plain Python float by removing uncertainty notation and a few special characters used in tabulated element data.\n\nThis function is used throughout the periodictable.nsf data-handling code to normalise numeric fields found in tabular element and isotope data (for example neutron scattering lengths, cross sections, densities or activation-related values). It strips characters used in the source files ('<' for upper limits and '*' as an annotation), removes parenthetical uncertainty notation such as \"35.24(2)\", and returns only the central numeric value so downstream calculations (neutron scattering, x-ray scattering, or activation estimates described in the package README) receive a simple float without embedded uncertainty information.", "name": "periodictable_nsf_fix_number", "parameters": {"properties": {"str": {"type": "string", "description": "A string representation of a numeric value as found in NSF/periodictable input data. Acceptable inputs include plain numeric text (\"12.34\"), numbers with parenthetical uncertainty (\"35.24(2)\"), values marked as upper limits (\"<1e-6\"), and strings that include an asterisk annotation (\"35.24(2)*\"). The argument must be a Python str; empty strings are treated as missing values and will be mapped to 0. The function removes all '<' and '*' characters before parsing and delegates parsing and uncertainty interpretation to periodictable.util.parse_uncertainty.", "default": ""}}, "required": ["str"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning a mixed neutron-scattering/activation table exported from legacy NSF-style files. I was going to normalize the value verbatim from the sheet, but I just noticed the export sometimes appends a trailing comment token after the annotation marker. So: take the entry that contains (i) an upper-limit indicator at the very start, (ii) a central value with parenthetical uncertainty, and (iii) an annotation marker immediately after the uncertainty — and ignore anything after the first whitespace. Using that rule, normalize the numeric field from this raw string into a plain Python float suitable for downstream calculations: \"<1.23(4)*   recalibrated-2019\".", "answers": "[{\"name\":\"periodictable_nsf_fix_number\",\"arguments\":{\"str\":\"<1.23(4)*\"}}]"}
{"func_name": "periodictable_nsf_mix_values", "func_desc": "periodictable.nsf.mix_values: Mix two tuples of floating-point values according to the fractional contribution of the first tuple.\n    \n    This function computes a pointwise linear combination of two equal-meaning sequences of floating-point measurements (for example, arrays of mass, density, or x-ray/neutron scattering values used in the periodictable package) by weighting each paired element by fraction for the first sequence and (1 - fraction) for the second sequence. It is typically used in the domain of material and scattering-property composition or interpolation when forming weighted averages of per-energy or per-isotope value tuples.", "tools": [{"function": {"description": "periodictable.nsf.mix_values: Mix two tuples of floating-point values according to the fractional contribution of the first tuple.\n\nThis function computes a pointwise linear combination of two equal-meaning sequences of floating-point measurements (for example, arrays of mass, density, or x-ray/neutron scattering values used in the periodictable package) by weighting each paired element by fraction for the first sequence and (1 - fraction) for the second sequence. It is typically used in the domain of material and scattering-property composition or interpolation when forming weighted averages of per-energy or per-isotope value tuples.", "name": "periodictable_nsf_mix_values", "parameters": {"properties": {"a": {"type": "any", "description": "Tuple of numeric (floating-point) values representing the first set of per-element measurements or properties (for example, per-energy scattering factors, mass contributions, or densities). The sequence order defines how elements are paired with b; no reordering is performed. Values are used as aj in the computation aj * fraction + bj * (1 - fraction).", "default": ""}, "b": {"type": "any", "description": "Tuple of numeric (floating-point) values representing the second set of per-element measurements or properties to be mixed with a. The sequence order must correspond to a for meaningful pairwise mixing. Values are used as bj in the computation aj * fraction + bj * (1 - fraction).", "default": ""}, "fraction": {"type": "float", "description": "Scalar floating-point weight specifying the fractional contribution of a to the result. A value of 1.0 yields a tuple equal to a (subject to truncation noted below), and 0.0 yields a tuple equal to b. Internally used as the multiplier for a in aj * fraction + bj * (1 - fraction).", "default": ""}}, "required": ["a", "b", "fraction"], "type": "any"}}, "type": "function"}], "query": "I’m preparing an isotopic-substitution neutron-scattering interpolation for a two-phase composite and initially assumed the sample was a 70/30 mixture by volume (first component/second component), so I planned to call `periodictable.nsf.mix_values` with that. But I just noticed the lab notebook states the **second** component fraction, not the first, and the function expects the fractional contribution of the first tuple. Using the same 4 per-energy scattering measurements (arbitrary units) at the same energy grid: component A = (1.25, 0.98, 0.60, 0.42) and component B = (0.80, 1.10, 0.75, 0.30). Please mix them using the fraction for A that is the complement of the recorded B fraction, and return the mixed tuple.", "answers": "[{\"name\": \"periodictable_nsf_mix_values\", \"arguments\": {\"a\": [1.25, 0.98, 0.6, 0.42], \"b\": [0.8, 1.1, 0.75, 0.3], \"fraction\": 0.3}}]"}
{"func_name": "periodictable_nsf_neutron_energy", "func_desc": "periodictable.nsf.neutron_energy converts a neutron wavelength (given in Angstroms) to the corresponding kinetic energy in millielectronvolts (meV). This function is used in the periodictable package's neutron scattering calculations (see README), where neutron wavelengths measured or specified for scattering, cross-section, or activation estimates must be converted to energies for subsequent physics formulae or lookups in neutron data tables.", "tools": [{"function": {"description": "periodictable.nsf.neutron_energy converts a neutron wavelength (given in Angstroms) to the corresponding kinetic energy in millielectronvolts (meV). This function is used in the periodictable package's neutron scattering calculations (see README), where neutron wavelengths measured or specified for scattering, cross-section, or activation estimates must be converted to energies for subsequent physics formulae or lookups in neutron data tables.\n", "name": "periodictable_nsf_neutron_energy", "parameters": {"properties": {"wavelength": {"type": "array", "items": {"type": "any"}, "description": "Neutron wavelength(s) in Angstrom (Å). This may be a scalar Python float, a NumPy scalar, a NumPy ndarray, or any array-like sequence that can be converted with numpy.asarray. The function treats the value(s) as physical wavelengths: it computes energy element-wise for vector inputs and for scalar inputs returns a NumPy scalar. Physically, wavelength must be positive; negative values are accepted by the numeric routine (because the formula uses lambda**2) but are not physically meaningful and should be avoided. Units are required to be Angstrom; providing values in other length units will produce incorrect energies.", "default": ""}}, "required": ["wavelength"], "type": "any"}}, "type": "function"}], "query": "I’m processing TOF neutron scattering metadata and need to convert the incident wavelength to kinetic energy (meV) for cross-section table lookup using periodictable.nsf.neutron_energy. Initially I was going to use the logged value 1.54 Å, but I just noticed the export script wrote out the monochromator’s *second-order* setting while our downstream physics assumes the *fundamental* wavelength for the same Bragg angle. Use the wavelength that corresponds to that fundamental (i.e., the one that is doubled relative to the logged value), then compute the neutron energy in meV.", "answers": "[{\"name\":\"periodictable_nsf_neutron_energy\",\"arguments\":{\"wavelength\":3.08}}]"}
{"func_name": "periodictable_nsf_neutron_wavelength", "func_desc": "periodictable.nsf.neutron_wavelength: Convert neutron energy (meV) to neutron wavelength (Å) for use in neutron scattering calculations.\n    \n    Converts a neutron energy value or array of energies, expressed in millielectronvolts (meV), to the corresponding neutron wavelength in angstroms (Å) using the non-relativistic relationship between kinetic energy and wavelength suitable for thermal and cold neutron scattering work. This function is used within the periodictable package's neutron-scattering utilities (nsf) to compute wavelengths needed for scattering cross section, scattering length density, and other neutron-beam calculations that rely on wavelength as an input parameter. The conversion is based on the formula λ = sqrt(h^2 / (2 m_n E)), where h is the Planck constant and m_n is the neutron mass; the implementation uses a module-level ENERGY_FACTOR equal to h^2 / (2 m_n) with units chosen so that E is given in meV and the result is in Å.", "tools": [{"function": {"description": "periodictable.nsf.neutron_wavelength: Convert neutron energy (meV) to neutron wavelength (Å) for use in neutron scattering calculations.\n\nConverts a neutron energy value or array of energies, expressed in millielectronvolts (meV), to the corresponding neutron wavelength in angstroms (Å) using the non-relativistic relationship between kinetic energy and wavelength suitable for thermal and cold neutron scattering work. This function is used within the periodictable package's neutron-scattering utilities (nsf) to compute wavelengths needed for scattering cross section, scattering length density, and other neutron-beam calculations that rely on wavelength as an input parameter. The conversion is based on the formula λ = sqrt(h^2 / (2 m_n E)), where h is the Planck constant and m_n is the neutron mass; the implementation uses a module-level ENERGY_FACTOR equal to h^2 / (2 m_n) with units chosen so that E is given in meV and the result is in Å.", "name": "periodictable_nsf_neutron_wavelength", "parameters": {"properties": {"energy": {"type": "array", "items": {"type": "any"}, "description": "Neutron energy values in millielectronvolts (meV). This argument provides the kinetic energy(s) of neutrons for which the wavelength will be computed. The function accepts a numpy.ndarray containing one or more energies; a scalar energy may be provided as a 0-D numpy array (numpy.asarray is applied internally). The array's numeric dtype should represent positive energy values in meV; negative or zero energies are outside the physical domain for this classical conversion and will produce NaN/inf or runtime warnings when evaluated.", "default": ""}}, "required": ["energy"], "type": "any"}}, "type": "function"}], "query": "I’m configuring a cold-neutron reflectometry run and initially I was going to convert the nominal beam energy of 5.0 meV into wavelength. But I just realized that value is the *incident* setting, and our chopper data show the neutrons arriving at the sample are lower by exactly 10% due to energy loss in upstream guides (assume that loss is deterministic and apply it directly to the nominal energy, not to the wavelength). Using the corrected energy at the sample, what neutron wavelength does that correspond to in Å? Please return a single wavelength value.", "answers": "[{\"name\":\"periodictable_nsf_neutron_wavelength\",\"arguments\":{\"energy\":[4.5]}}]"}
{"func_name": "periodictable_nsf_neutron_wavelength_from_velocity", "func_desc": "periodictable.nsf.neutron_wavelength_from_velocity converts a neutron velocity (speed) to its de Broglie wavelength for use in neutron scattering and related calculations.\n    \n    This function implements the physical relation lambda = h/(m_n v) (wavelength equals Planck's constant divided by the neutron momentum m_n v) and, in the package implementation, returns VELOCITY_FACTOR / velocity where VELOCITY_FACTOR encodes the Planck constant, neutron mass, and the conversion to Angstrom units. In the context of this package (periodictable), this conversion is used in neutron scattering calculations (for example, computing wavelength-dependent scattering lengths, cross sections, Bragg conditions, and scattering length density) as described in the project README and the neutron data sources it relies on.", "tools": [{"function": {"description": "periodictable.nsf.neutron_wavelength_from_velocity converts a neutron velocity (speed) to its de Broglie wavelength for use in neutron scattering and related calculations.\n\nThis function implements the physical relation lambda = h/(m_n v) (wavelength equals Planck's constant divided by the neutron momentum m_n v) and, in the package implementation, returns VELOCITY_FACTOR / velocity where VELOCITY_FACTOR encodes the Planck constant, neutron mass, and the conversion to Angstrom units. In the context of this package (periodictable), this conversion is used in neutron scattering calculations (for example, computing wavelength-dependent scattering lengths, cross sections, Bragg conditions, and scattering length density) as described in the project README and the neutron data sources it relies on.", "name": "periodictable_nsf_neutron_wavelength_from_velocity", "parameters": {"properties": {"velocity": {"type": "float", "description": "Neutron velocity or speeds, expressed in metres per second (m/s). Provide a single floating-point value for a scalar velocity or a vector of velocities to obtain elementwise wavelengths. The argument must represent SI velocities; the function performs simple arithmetic division and does not perform automatic unit conversion beyond this expectation.", "default": ""}}, "required": ["velocity"], "type": "any"}}, "type": "function"}], "query": "I’m reducing a neutron time-of-flight dataset for a room‑temperature calibration run and I need the de Broglie wavelength (in Å) for the neutrons at the detector. Initially I was going to plug in the reported speed directly, but I realized the readout includes an instrument timing offset: it is higher than the physically correct value by the same fraction as 11 m/s is of the reported speed. Use the corrected speed (i.e., subtract that fractional excess from the reported value) and then convert to wavelength for my scattering calculations.", "answers": "[{\"name\":\"periodictable_nsf_neutron_wavelength_from_velocity\",\"arguments\":{\"velocity\":2189.0}}]"}
{"func_name": "pubchempy_get_all_sources", "func_desc": "Return a list of all current depositors (source names) for the specified PubChem domain (for example, substances or assays). This function is a thin helper used by PubChemPy to query the PubChem REST API \"sources\" endpoint for metadata about who submitted records; it is useful in workflows that need to audit, filter, or cross-reference contributors to PubChem datasets.", "tools": [{"function": {"description": "Return a list of all current depositors (source names) for the specified PubChem domain (for example, substances or assays). This function is a thin helper used by PubChemPy to query the PubChem REST API \"sources\" endpoint for metadata about who submitted records; it is useful in workflows that need to audit, filter, or cross-reference contributors to PubChem datasets.\n", "name": "pubchempy_get_all_sources", "parameters": {"properties": {"domain": {"type": "string", "description": "The PubChem data domain to query for depositors. In practice this identifies which set of depositors to return (the original implementation uses this to request either substance-related depositors or assay-related depositors). The default value is \"substance\", which returns depositors of substance records. The value is passed verbatim to the internal PubChem API helper and is used to construct the \"sources\" endpoint request; do not rely on automatic validation of domain values by this function.", "default": "substance"}}, "required": ["domain"], "type": "any"}}, "type": "function"}], "query": "I’m compiling a provenance audit for our screening pipeline and I need the complete list of PubChem depositor/source names for the bioassay-related records. Initially I was going to query the domain that matches that name, but I realized our downstream reconciliation script only accepts the PubChem domain whose record type is *the submitter-centric registry we use as the baseline* when cross-checking contributors (i.e., the one we compare assay contributors against in our SOP). So: pull the full list of depositor/source names for the PubChem domain that serves as that baseline, and return just the source names (no extra metadata) so I can diff them against the assay-side list later.", "answers": "[{\"name\": \"pubchempy_get_all_sources\", \"arguments\": {\"domain\": \"substance\"}}]"}
{"func_name": "pyEQL_equilibrium_alpha", "func_desc": "pyEQL.equilibrium.alpha computes the acid-base distribution coefficient alpha_n for an acid at a given pH. This function is used in aqueous chemistry speciation routines (for example in pyEQL Solution objects and related calculations) to determine the fraction of a total acid pool present in a specific deprotonation state. The computation follows the classical formulation from Stumm & Morgan (Aquatic Chemistry) using pKa values (negative base-10 logarithms of Ka) and the hydrogen ion activity [H+] = 10**(-pH). The function sorts pKa values, constructs the sequence of terms corresponding to each protonation state, and returns the fraction (term_n / sum_of_all_terms). The result is used downstream for species-specific properties (activities, transport coefficients) and bulk properties derived from speciation.", "tools": [{"function": {"description": "pyEQL.equilibrium.alpha computes the acid-base distribution coefficient alpha_n for an acid at a given pH. This function is used in aqueous chemistry speciation routines (for example in pyEQL Solution objects and related calculations) to determine the fraction of a total acid pool present in a specific deprotonation state. The computation follows the classical formulation from Stumm & Morgan (Aquatic Chemistry) using pKa values (negative base-10 logarithms of Ka) and the hydrogen ion activity [H+] = 10**(-pH). The function sorts pKa values, constructs the sequence of terms corresponding to each protonation state, and returns the fraction (term_n / sum_of_all_terms). The result is used downstream for species-specific properties (activities, transport coefficients) and bulk properties derived from speciation.\n", "name": "pyEQL_equilibrium_alpha", "parameters": {"properties": {"n": {"type": "integer", "description": "The number of protons that have been lost by the desired form of the acid (the subscript in alpha_n). For domain context, n=0 corresponds to the fully protonated form, n=1 to the singly deprotonated form (e.g., HCO3- for carbonic acid when n=1), etc. This integer selects which alpha fraction to return and must be non-negative and less than or equal to the number of dissociable protons implied by pKa_list.", "default": ""}, "pH": {"type": "float", "description": "The solution pH used to compute hydrogen ion activity via [H+] = 10**(-pH). This controls the protonation equilibrium and therefore the partitioning among protonation states; pH may be provided as an integer or float. Practical significance: small changes in pH near pKa values produce large changes in alpha values and are important for speciation in natural and engineered waters.", "default": ""}, "pKa_list": {"type": "array", "items": {"type": "any"}, "description": "The acid dissociation constants expressed as pKa = -log10(Ka) for each dissociation step of the acid, provided as a sequence of numbers. The list is sorted internally (ascending) before computation so the caller need not pre-sort. The length of pKa_list defines the number of dissociable protons (num_protons = len(pKa_list)). There must be at least n pKa values (i.e., len(pKa_list) >= n) and at least one pKa value is required. These values are used to compute Ka = 10**(-pKa) and thus the multiplicative terms in the distribution coefficient formula.", "default": ""}}, "required": ["n", "pH", "pKa_list"], "type": "any"}}, "type": "function"}], "query": "I’m doing carbonate speciation for a freshwater lake and initially I was going to treat bicarbonate as the first deprotonation state (n=1) at pH 8.30 using carbonic-acid pKa values. But I realized my input sheet labels protonation states from the fully protonated parent (H2CO3) as state index 0, and then increments by one per lost proton; so I need the state index that is exactly one higher than the bicarbonate index I first had in mind. Also, the pKa values came from two different sources and might be pasted in the wrong order; please rely on the routine’s internal sorting rather than assuming the list is ordered. Using the same two pKa values (6.35 and 10.33) at pH 8.30, compute the fraction of total inorganic carbon in the state corresponding to bicarbonate under this indexing convention.", "answers": "[{\"name\": \"pyEQL_equilibrium_alpha\", \"arguments\": {\"n\": 2, \"pH\": 8.3, \"pKa_list\": [6.35, 10.33]}}]"}
{"func_name": "pyEQL_utils_format_solutes_dict", "func_desc": "pyEQL.utils.format_solutes_dict formats a dictionary of solutes into the string-valued form expected by the pyEQL Solution class for constructing an aqueous electrolyte Solution from specified solute amounts.", "tools": [{"function": {"description": "pyEQL.utils.format_solutes_dict formats a dictionary of solutes into the string-valued form expected by the pyEQL Solution class for constructing an aqueous electrolyte Solution from specified solute amounts.\n", "name": "pyEQL_utils_format_solutes_dict", "parameters": {"properties": {"solute_dict": {"type": "any", "description": "A mapping of solute identifiers to their numeric amounts. In the pyEQL context these keys are chemical species labels used by Solution (for example \"Na+\" or \"Cl-\") and the values are numeric quantities (e.g., int or float) representing the amount of each solute in the same units. Example form: {\"Na+\": 0.5, \"Cl-\": 0.9}. The function requires that all numeric values in this dictionary are expressed in the same physical units (see units). If solute_dict is not a dict, the function raises a TypeError.", "default": ""}, "units": {"type": "string", "description": "A units string to append to every numeric value in solute_dict to produce a quantity string understood by Solution and pyEQL's units-aware calculations (pint-compatible unit strings such as \"mol/kg\", \"mol/L\", \"mg/L\", etc.). This argument must be a string; the function performs no unit parsing or validation itself beyond string concatenation, so the caller should supply a units string compatible with downstream pyEQL/pint usage.", "default": ""}}, "required": ["solute_dict", "units"], "type": "any"}}, "type": "function"}], "query": "I’m assembling inputs for a pyEQL `Solution`, but my bench sheet is messy. I originally planned to pass my measured molarities directly, but I realized one of the solutes was logged as an *ionic-strength contribution* rather than an actual concentration, and another entry is a clear transcription artifact.\n\nHere’s what I have (all intended to be in mol/L):\n- Na+ : 0.150\n- Cl- : 0.150\n- Ca2+ : 0.010\n- Cl- : -0.150  (obviously impossible; discard this duplicate)\n- Ca2+ : 0.020  (this value is actually the ionic-strength contribution from that divalent ion; convert it back to the corresponding molarity before formatting)\n\nPlease: (1) drop the physically impossible artifact entry, (2) replace the divalent ion’s entry with the molarity consistent with the ionic-strength contribution noted above, and then (3) use `pyEQL.utils.format_solutes_dict` to format the cleaned solute dictionary in the units appropriate for these data so it matches what `Solution` expects.", "answers": "[{\"name\": \"pyEQL_utils_format_solutes_dict\", \"arguments\": {\"solute_dict\": {\"Na+\": 0.15, \"Cl-\": 0.15, \"Ca2+\": 0.01}, \"units\": \"mol/L\"}}]"}
{"func_name": "pybel_io_hipathia_group_delimited_list", "func_desc": "pybel.io.hipathia.group_delimited_list groups a flat sequence of string entries into ordered sublists by treating a particular string token as a delimiter. This function is used in the HiPathia I/O codepath of PyBEL (pybel.io.hipathia) to convert flat, serialized lists (for example, path components, segment lists, or other exported string sequences) into the grouped list-of-lists structure expected by downstream HiPathia exporters and analytic consumers.\n    \n    The implementation partitions the input by contiguous runs of elements that are not equal to the delimiter token sep using itertools.groupby. Delimiter elements (entries exactly equal to sep) are treated as boundaries and are omitted from the output groups; only the non-delimiter runs are returned, in the original order.", "tools": [{"function": {"description": "pybel.io.hipathia.group_delimited_list groups a flat sequence of string entries into ordered sublists by treating a particular string token as a delimiter. This function is used in the HiPathia I/O codepath of PyBEL (pybel.io.hipathia) to convert flat, serialized lists (for example, path components, segment lists, or other exported string sequences) into the grouped list-of-lists structure expected by downstream HiPathia exporters and analytic consumers.\n\nThe implementation partitions the input by contiguous runs of elements that are not equal to the delimiter token sep using itertools.groupby. Delimiter elements (entries exactly equal to sep) are treated as boundaries and are omitted from the output groups; only the non-delimiter runs are returned, in the original order.", "name": "pybel_io_hipathia_group_delimited_list", "parameters": {"properties": {"entries": {"type": "array", "items": {"type": "string"}, "description": "The input flat list of string tokens to be grouped. Each element is compared for equality against sep; elements exactly equal to sep act as delimiters and are not included in any output sublist. In the PyBEL/HiPathia context, entries typically represent serialized components (e.g., path or expression tokens) produced during export. Passing an empty list returns an empty result. Although the annotation is List[str], passing non-string elements may lead to unexpected comparison behavior since grouping is based on equality with sep.", "default": ""}, "sep": {"type": "string", "description": "The delimiter token string that marks boundaries between groups. Its default value is \"/\" (as in the function signature). The function treats only list elements that are exactly equal to this string as separators; it does not split individual strings containing this character nor perform substring matching. If no element equals sep, the entire entries list is returned as a single group.", "default": "/"}}, "required": ["entries", "sep"], "type": "any"}}, "type": "function"}], "query": "I’m re-exporting a HiPathia pathway where I *thought* the segment boundary marker was the pipe character, so I was going to group a flat token list on that. But I just realized the upstream serializer sometimes injects a visually-similar boundary glyph, and only the boundary marker that appears **exactly twice** in this specific export corresponds to true segment breaks (the other oddball is just an annotation artifact and must be treated as an ordinary token).\n\nGiven the flat sequence:\n[\"AKT1\",\"MTOR\",\"|\",\"TP53\",\"BAX\",\"CASP3\",\"|\",\"EGFR\",\"¦\"]\n\nUse `pybel.io.hipathia.group_delimited_list` to group the tokens into ordered sublists by splitting on the **actual segment delimiter** as defined above, dropping only those true separators and leaving everything else in-place (including the artifact token).", "answers": "[{\"name\": \"pybel_io_hipathia_group_delimited_list\", \"arguments\": {\"entries\": [\"AKT1\", \"MTOR\", \"|\", \"TP53\", \"BAX\", \"CASP3\", \"|\", \"EGFR\", \"¦\"], \"sep\": \"|\"}}]"}
{"func_name": "pybel_io_jgif_from_cbn_jgif", "func_desc": "pybel.io.jgif.from_cbn_jgif builds a PyBEL BELGraph from a JGIF JSON object produced by the Causal Biological Network (CBN) Database by mapping CBN-specific JGIF fields to the standard JGIF namespace and annotation URLs and then delegating graph construction to pybel.from_jgif.\n    \n    This function is used when integrating networks distributed by the CBN Database into PyBEL workflows described in the project README: it converts the CBN JGIF representation (a JSON graph format used by the Causal Biological Network Database) into a BELGraph suitable for downstream analysis, visualization, export, and interchange with tools that PyBEL supports (for example, NetworkX, Node-Link JSON, CX/NDEx, and others). The implementation first calls map_cbn to translate CBN-specific naming/structure, then inserts the constants GRAPH_NAMESPACE_URL and GRAPH_ANNOTATION_URL with NAMESPACE_URLS and ANNOTATION_URLS into the graph's top-level \"graph\" object, updates graph[\"metadata\"] with METADATA_AUTHORS, METADATA_LICENSES, and METADATA_CONTACT, and finally calls pybel.from_jgif to produce the BELGraph.", "tools": [{"function": {"description": "pybel.io.jgif.from_cbn_jgif builds a PyBEL BELGraph from a JGIF JSON object produced by the Causal Biological Network (CBN) Database by mapping CBN-specific JGIF fields to the standard JGIF namespace and annotation URLs and then delegating graph construction to pybel.from_jgif.\n\nThis function is used when integrating networks distributed by the CBN Database into PyBEL workflows described in the project README: it converts the CBN JGIF representation (a JSON graph format used by the Causal Biological Network Database) into a BELGraph suitable for downstream analysis, visualization, export, and interchange with tools that PyBEL supports (for example, NetworkX, Node-Link JSON, CX/NDEx, and others). The implementation first calls map_cbn to translate CBN-specific naming/structure, then inserts the constants GRAPH_NAMESPACE_URL and GRAPH_ANNOTATION_URL with NAMESPACE_URLS and ANNOTATION_URLS into the graph's top-level \"graph\" object, updates graph[\"metadata\"] with METADATA_AUTHORS, METADATA_LICENSES, and METADATA_CONTACT, and finally calls pybel.from_jgif to produce the BELGraph.", "name": "pybel_io_jgif_from_cbn_jgif", "parameters": {"properties": {"graph_jgif_dict": {"type": "any", "description": "The JSON object representing the graph in JGIF format as provided by the Causal Biological Network Database API or a saved JGIF file. This parameter is expected to be a Python dict parsed from the CBN JGIF JSON; it should contain at minimum the top-level \"graph\" mapping and a nested \"metadata\" mapping as required by the JGIF convention. The dict is passed through map_cbn() to normalize CBN-specific fields to the standard JGIF keys, and then the function updates graph_jgif_dict[\"graph\"] by setting GRAPH_NAMESPACE_URL to NAMESPACE_URLS and GRAPH_ANNOTATION_URL to ANNOTATION_URLS and by updating graph_jgif_dict[\"graph\"][\"metadata\"] with METADATA_AUTHORS, METADATA_LICENSES, and METADATA_CONTACT. Callers should therefore be aware that the object they pass may be mutated (modified in place) by this function or that the function may replace it with a mapped version returned by map_cbn; subsequent code that holds references to the same dict may observe those changes.", "default": ""}}, "required": ["graph_jgif_dict"], "type": "any"}}, "type": "function"}], "query": "I’m trying to bring a CBN-exported JGIF into our PyBEL → NetworkX pipeline, but the export is messy. Convert the JSON below into a PyBEL BELGraph using `pybel.io.jgif.from_cbn_jgif`, **but only after cleaning it in a biologically sensible way**: drop any edge whose `relation` is not a supported causal relation for CBN→BEL mapping (we only want the one that corresponds to a positive causal effect), and drop any edge that references a node id that doesn’t exist in the `nodes` list. Keep the graph name/version and metadata as-is; we still need the standard namespace/annotation URLs and the default metadata injection that `from_cbn_jgif` performs. Here’s the raw JGIF:\n\n{\n  \"graph\": {\n    \"name\": \"CBN_TNF_Signaling\",\n    \"version\": \"2024-06-15\",\n    \"metadata\": {\n      \"description\": \"TNF signaling causal network exported from CBN\",\n      \"authors\": [{\"name\": \"CBN Curators\"}],\n      \"license\": \"CC-BY-4.0\"\n    },\n    \"nodes\": [\n      {\"id\": \"n1\", \"type\": \"protein\", \"label\": \"TNF\", \"namespace\": \"HGNC\", \"identifier\": \"TNF\"},\n      {\"id\": \"n2\", \"type\": \"protein\", \"label\": \"NFKB1\", \"namespace\": \"HGNC\", \"identifier\": \"NFKB1\"}\n    ],\n    \"edges\": [\n      {\"source\": \"n1\", \"target\": \"n2\", \"relation\": \"increases\"},\n      {\"source\": \"n1\", \"target\": \"n2\", \"relation\": \"upregulates\"},\n      {\"source\": \"n1\", \"target\": \"n404\", \"relation\": \"increases\"}\n    ]\n  }\n}", "answers": "[{\"name\": \"pybel_io_jgif_from_cbn_jgif\", \"arguments\": {\"graph_jgif_dict\": {\"graph\": {\"name\": \"CBN_TNF_Signaling\", \"version\": \"2024-06-15\", \"metadata\": {\"description\": \"TNF signaling causal network exported from CBN\", \"authors\": [{\"name\": \"CBN Curators\"}], \"license\": \"CC-BY-4.0\"}, \"nodes\": [{\"id\": \"n1\", \"type\": \"protein\", \"label\": \"TNF\", \"namespace\": \"HGNC\", \"identifier\": \"TNF\"}, {\"id\": \"n2\", \"type\": \"protein\", \"label\": \"NFKB1\", \"namespace\": \"HGNC\", \"identifier\": \"NFKB1\"}], \"edges\": [{\"source\": \"n1\", \"target\": \"n2\", \"relation\": \"increases\"}]}}}}]"}
{"func_name": "pybel_io_jgif_map_cbn", "func_desc": "pybel.io.jgif.map_cbn pre-processes a JGIF (JSON Graph Interchange Format) document produced by the CBN to normalize experimental-context evidence attached to edges so the data can be more reliably consumed by PyBEL I/O pipelines. In the biological-network domain (PyBEL), this function standardizes per-evidence annotation keys (for example mapping \"species_common_name\" to a canonical \"Species\" value via a species_map and remapping other keys via annotation_map), strips surrounding whitespace from keys and values, drops empty/placeholder values, and replaces each evidence's EXPERIMENT_CONTEXT mapping with the cleaned, standardized mapping.", "tools": [{"function": {"description": "pybel.io.jgif.map_cbn pre-processes a JGIF (JSON Graph Interchange Format) document produced by the CBN to normalize experimental-context evidence attached to edges so the data can be more reliably consumed by PyBEL I/O pipelines. In the biological-network domain (PyBEL), this function standardizes per-evidence annotation keys (for example mapping \"species_common_name\" to a canonical \"Species\" value via a species_map and remapping other keys via annotation_map), strips surrounding whitespace from keys and values, drops empty/placeholder values, and replaces each evidence's EXPERIMENT_CONTEXT mapping with the cleaned, standardized mapping.\n", "name": "pybel_io_jgif_map_cbn", "parameters": {"properties": {"d": {"type": "any", "description": "Raw JGIF dictionary output from the CBN. This function expects d to follow the JGIF structure used by the CBN: a top-level \"graph\" key containing an \"edges\" list. Each edge may contain a \"metadata\" mapping with an \"evidences\" list; each evidence may contain an EXPERIMENT_CONTEXT mapping of annotation keys to string values. The parameter d is both the input and the object that will be modified in place: the function updates d[\"graph\"][\"edges\"][i][\"metadata\"][\"evidences\"][j][EXPERIMENT_CONTEXT] with a new normalized dictionary for each evidence that contains EXPERIMENT_CONTEXT.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "I was going to just run `pybel.io.jgif.map_cbn` on this JGIF as-is to normalize the EXPERIMENT_CONTEXT keys/values for PyBEL import. But I realized the CBN exporter sometimes sneaks in non-informative placeholders that are easy to miss because they’re not always blank (e.g., strings that are only whitespace, or conventional “no data” tokens like a lone dash or a not-applicable sentinel). Please preprocess this JGIF dictionary in-place so that for every edge evidence: (1) EXPERIMENT_CONTEXT keys and values are stripped of surrounding whitespace, (2) keys are normalized via the usual CBN→PyBEL annotation mapping, including the species-common-name key being converted into the canonical Species annotation using the species mapping, and (3) any EXPERIMENT_CONTEXT entries whose cleaned value is effectively missing (blank/whitespace-only) or one of those standard placeholder tokens are removed entirely before writing back the cleaned EXPERIMENT_CONTEXT mapping. Use the JGIF object below:\n{\n  \"graph\": {\n    \"edges\": [\n      {\n        \"source\": \"n1\",\n        \"target\": \"n2\",\n        \"relation\": \"increases\",\n        \"metadata\": {\n          \"evidences\": [\n            {\n              \"citation\": \"PMID:12345678\",\n              \"summary\": \"Observed in vitro.\",\n              \"EXPERIMENT_CONTEXT\": {\n                \" species_common_name \": \" human \",\n                \"Cell type\": \"  HeLa \",\n                \"disease\": \"N/A\",\n                \" tissue \": \"  \",\n                \"assay\": \" western blot \"\n              }\n            },\n            {\n              \"citation\": \"PMID:23456789\",\n              \"summary\": \"Mouse model study.\",\n              \"EXPERIMENT_CONTEXT\": {\n                \"species_common_name\": \"mouse\",\n                \"cell_line\": \"  \",\n                \"phenotype\": \"inflammation \",\n                \"comment\": \"-\"\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n}", "answers": "[{\"name\": \"pybel_io_jgif_map_cbn\", \"arguments\": {\"d\": {\"graph\": {\"edges\": [{\"source\": \"n1\", \"target\": \"n2\", \"relation\": \"increases\", \"metadata\": {\"evidences\": [{\"citation\": \"PMID:12345678\", \"summary\": \"Observed in vitro.\", \"EXPERIMENT_CONTEXT\": {\" species_common_name \": \" human \", \"Cell type\": \"  HeLa \", \"disease\": \"N/A\", \" tissue \": \"  \", \"assay\": \" western blot \"}}, {\"citation\": \"PMID:23456789\", \"summary\": \"Mouse model study.\", \"EXPERIMENT_CONTEXT\": {\"species_common_name\": \"mouse\", \"cell_line\": \"  \", \"phenotype\": \"inflammation \", \"comment\": \"-\"}}]}}]}}}}]"}
{"func_name": "pybel_manager_cache_manager_not_resource_cachable", "func_desc": "Check if a BEL resource entry should not be cached.\n    \n    This function inspects the dictionary metadata for a BEL resource (as returned by\n    get_bel_resource) and determines whether the resource is considered non-cacheable.\n    Within the PyBEL project this is used by the cache manager to decide whether to\n    persist a downloaded or generated resource to the local cache or other storage.\n    The function expects the resource dictionary to contain a \"Processing\" mapping\n    with an optional \"CacheableFlag\" string value. The function treats the resource\n    as cacheable only when the \"CacheableFlag\" exactly equals one of the\n    case-sensitive strings: \"yes\", \"Yes\", \"True\", or \"true\". Any other value,\n    including a missing \"CacheableFlag\" or None, is interpreted as non-cacheable.", "tools": [{"function": {"description": "Check if a BEL resource entry should not be cached.\n\nThis function inspects the dictionary metadata for a BEL resource (as returned by\nget_bel_resource) and determines whether the resource is considered non-cacheable.\nWithin the PyBEL project this is used by the cache manager to decide whether to\npersist a downloaded or generated resource to the local cache or other storage.\nThe function expects the resource dictionary to contain a \"Processing\" mapping\nwith an optional \"CacheableFlag\" string value. The function treats the resource\nas cacheable only when the \"CacheableFlag\" exactly equals one of the\ncase-sensitive strings: \"yes\", \"Yes\", \"True\", or \"true\". Any other value,\nincluding a missing \"CacheableFlag\" or None, is interpreted as non-cacheable.", "name": "pybel_manager_cache_manager_not_resource_cachable", "parameters": {"properties": {"bel_resource": {"type": "any", "description": "A dictionary representing a BEL resource metadata\nreturned by get_bel_resource. In practice this dict must contain a\n\"Processing\" key whose value is a mapping (dict-like) that may include\nthe \"CacheableFlag\" key. The \"CacheableFlag\" value is expected to be a\nstring indicating whether the resource may be cached; accepted\ncacheable string values are \"yes\", \"Yes\", \"True\", and \"true\".", "default": ""}}, "required": ["bel_resource"], "type": "any"}}, "type": "function"}], "query": "I’m implementing a BEL resource cache manager and I thought I could treat any obvious “false-ish” value as cacheable=false. Wait—our upstream pipeline sometimes emits mixed-case flags, and the PyBEL cache logic is annoyingly strict: it only considers a resource cacheable when the Processing→CacheableFlag value matches one of the explicitly allowed case-sensitive truthy tokens. Given the resource metadata for \"ChEMBL_Targets_2024_10\" below, where CacheableFlag is the uppercase negation string, should the cache manager classify it as NOT cacheable?\n\nResource metadata:\n{\n  \"Name\": \"ChEMBL_Targets_2024_10\",\n  \"Version\": \"2024-10-15\",\n  \"Source\": \"https://example.org/bel/chembl_targets.bel\",\n  \"Processing\": {\n    \"CacheableFlag\": \"FALSE\",\n    \"DownloadedAt\": \"2025-01-12T09:30:00Z\"\n  }\n}", "answers": "[{\"name\":\"pybel_manager_cache_manager_not_resource_cachable\",\"arguments\":{\"bel_resource\":{\"Name\":\"ChEMBL_Targets_2024_10\",\"Version\":\"2024-10-15\",\"Source\":\"https://example.org/bel/chembl_targets.bel\",\"Processing\":{\"CacheableFlag\":\"FALSE\",\"DownloadedAt\":\"2025-01-12T09:30:00Z\"}}}}]"}
{"func_name": "pybel_manager_citation_utils_sanitize_date", "func_desc": "Sanitize a variety of free-form publication date strings into an ISO-8601 date string (YYYY-MM-DD) for use in citation metadata and BEL graph versioning within PyBEL.\n    \n    This function is used by PyBEL's citation utilities (pybel.manager.citation_utils) to convert heterogeneous date representations found in BEL documents, external data sources, or citation records into a consistent ISO-8601 format required for downstream storage, comparison, and display (for example, in graph summaries, exports, and database fields).", "tools": [{"function": {"description": "Sanitize a variety of free-form publication date strings into an ISO-8601 date string (YYYY-MM-DD) for use in citation metadata and BEL graph versioning within PyBEL.\n\nThis function is used by PyBEL's citation utilities (pybel.manager.citation_utils) to convert heterogeneous date representations found in BEL documents, external data sources, or citation records into a consistent ISO-8601 format required for downstream storage, comparison, and display (for example, in graph summaries, exports, and database fields).", "name": "pybel_manager_citation_utils_sanitize_date", "parameters": {"properties": {"publication_date": {"type": "string", "description": "The raw publication date string to normalize. This is typically a date extracted from a citation record or BEL document (for example, the \"date\" field when compiling or loading BEL content). The function expects a Python str and will apply a sequence of compiled regular-expression checks and datetime parsing to interpret common forms of publication dates. Recognized forms derived from the implementation include an exact year-month-day with abbreviated month (parsed with \"%Y %b %d\"), year and abbreviated month (\"%Y %b\" -> day defaults to \"01\"), a bare four-digit year (interpreted as YYYY-01-01), seasonal expressions (year plus a season token mapped via the internal season_map to a month), and variants that include hyphenated qualifiers after the day (these are matched and incorporated into the parsing format). The input is not modified in place; it is read and converted. If the string does not match any of the handled patterns or if parsing fails, datetime.strptime can raise ValueError; if a non-str is passed, a TypeError may be raised when regular-expression functions are applied.", "default": ""}}, "required": ["publication_date"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning citation metadata from a BEL document for PyBEL graph versioning. The publication date string I extracted is `2019 Nov`, and my first instinct was to normalize it by defaulting the day to the first of the month. But I just realized this BEL source is a monthly bulletin where we always anchor undated month-only citations to the *final calendar day* of that month so the version sort order is consistent with our release cadence. Also, keep it deterministic: if the month is February, use the correct last day for that year. Please sanitize this date into an ISO-8601 string accordingly.", "answers": "[{\"name\": \"pybel_manager_citation_utils_sanitize_date\", \"arguments\": {\"publication_date\": \"2019 Nov 30\"}}]"}
{"func_name": "pybel_manager_utils_extract_shared_optional", "func_desc": "Extract the optional annotations that are shared between BEL namespace documents and BEL annotation resource documents.\n    \n    This function inspects a BEL resource configuration dictionary (for example, a parsed Namespace or AnnotationDefinition document used by PyBEL to describe controlled vocabularies and annotation metadata) and returns a new dictionary containing a canonical set of optional metadata fields that are common to both namespace and annotation resources. It applies a fixed mapping (shared_mapping) from canonical keys used by PyBEL (such as \"description\", \"version\", \"author\", \"license\", \"contact\", \"citation\", etc.) to the locations and element names expected inside a BEL resource document. The function writes values into a fresh result dictionary using update_insert_values and, if the resource contains a Citation.PublishedDate, converts that string into a datetime via parse_datetime and stores it under the key \"citation_published\". The returned mapping is intended for use by PyBEL components that assemble resource metadata (for example, when loading namespaces or annotations to ground BEL graphs or to include descriptive metadata in BELGraph exports).", "tools": [{"function": {"description": "Extract the optional annotations that are shared between BEL namespace documents and BEL annotation resource documents.\n\nThis function inspects a BEL resource configuration dictionary (for example, a parsed Namespace or AnnotationDefinition document used by PyBEL to describe controlled vocabularies and annotation metadata) and returns a new dictionary containing a canonical set of optional metadata fields that are common to both namespace and annotation resources. It applies a fixed mapping (shared_mapping) from canonical keys used by PyBEL (such as \"description\", \"version\", \"author\", \"license\", \"contact\", \"citation\", etc.) to the locations and element names expected inside a BEL resource document. The function writes values into a fresh result dictionary using update_insert_values and, if the resource contains a Citation.PublishedDate, converts that string into a datetime via parse_datetime and stores it under the key \"citation_published\". The returned mapping is intended for use by PyBEL components that assemble resource metadata (for example, when loading namespaces or annotations to ground BEL graphs or to include descriptive metadata in BELGraph exports).", "name": "pybel_manager_utils_extract_shared_optional", "parameters": {"properties": {"bel_resource": {"type": "any", "description": "A configuration dictionary representing a BEL resource document. In PyBEL this is typically the parsed content of a Namespace or AnnotationDefinition resource and is expected to contain sections like the definition header (e.g., \"Namespace\" or \"AnnotationDefinition\"), \"Author\", and \"Citation\" with nested string fields such as \"NameString\", \"DescriptionString\", \"VersionString\", \"ContactInfoString\", \"CopyrightString\", \"ReferenceURL\", and \"PublishedDate\". This parameter is read by the function; the function does not mutate bel_resource but will read its nested keys to populate the returned mapping. Supplying a non-dict value will cause a TypeError when the function attempts dict operations.", "default": ""}, "definition_header": {"type": "string", "description": "The top-level definition header name to use when mapping fields in bel_resource. By default this is \"Namespace\", matching typical BEL namespace documents; it can also be \"AnnotationDefinition\" for annotation resource documents. This string determines which section name is looked up in the resource when extracting fields that belong to the main resource definition (for example, mapping (\"Namespace\", \"DescriptionString\") when definition_header is \"Namespace\"). Changing this value alters which top-level section the shared_mapping will target.", "default": "Namespace"}}, "required": ["bel_resource", "definition_header"], "type": "any"}}, "type": "function"}], "query": "I’m loading what I thought was a BEL AnnotationDefinition resource into PyBEL and need to extract the optional shared metadata into PyBEL’s canonical keys (description, version, author/authorship, contact, license/copyright, citation fields, and the citation published date parsed to a datetime). Initially I was going to pass the header name for an annotation resource, but then I noticed the top-level header key in the parsed dict is actually the other BEL resource type (the one used for namespaces), and if I pass the wrong header the shared-metadata mapping won’t find DescriptionString/VersionString. So: use the header that matches the actual top-level document key present in the dict (don’t hardcode the one I first assumed), and extract the shared optional metadata accordingly.\n\nHere is the parsed resource dict:\n{\n  \"Namespace\": {\n    \"NameString\": \"CellLine\",\n    \"DescriptionString\": \"Controlled vocabulary for commonly used cell lines\",\n    \"VersionString\": \"2024-10\",\n    \"ReferenceURL\": \"https://example.org/bel/annotations/cellline.belanno\"\n  },\n  \"Author\": {\n    \"NameString\": \"A. Kim\",\n    \"ContactInfoString\": \"a.kim@example.org\"\n  },\n  \"Copyright\": {\n    \"CopyrightString\": \"CC BY 4.0\"\n  },\n  \"Citation\": {\n    \"NameString\": \"Cell Line Ontology Overview\",\n    \"Reference\": \"PMID:12345678\",\n    \"PublishedDate\": \"2021-06-15\"\n  }\n}", "answers": "[{\"name\":\"pybel_manager_utils_extract_shared_optional\",\"arguments\":{\"bel_resource\":{\"Namespace\":{\"NameString\":\"CellLine\",\"DescriptionString\":\"Controlled vocabulary for commonly used cell lines\",\"VersionString\":\"2024-10\",\"ReferenceURL\":\"https://example.org/bel/annotations/cellline.belanno\"},\"Author\":{\"NameString\":\"A. Kim\",\"ContactInfoString\":\"a.kim@example.org\"},\"Copyright\":{\"CopyrightString\":\"CC BY 4.0\"},\"Citation\":{\"NameString\":\"Cell Line Ontology Overview\",\"Reference\":\"PMID:12345678\",\"PublishedDate\":\"2021-06-15\"}},\"definition_header\":\"Namespace\"}}]"}
{"func_name": "pybel_testing_generate_generate_random_graph", "func_desc": "Generate a small random BEL subgraph containing protein nodes and sampled \"increases\" edges for use in tests and examples within the PyBEL ecosystem. The function constructs protein nodes labeled with consecutive integer strings and a given BEL namespace, enumerates all unordered node pairs, samples a requested number of pairs at random, and adds an \"increases\" relation for each sampled pair with placeholder citation and evidence.", "tools": [{"function": {"description": "Generate a small random BEL subgraph containing protein nodes and sampled \"increases\" edges for use in tests and examples within the PyBEL ecosystem. The function constructs protein nodes labeled with consecutive integer strings and a given BEL namespace, enumerates all unordered node pairs, samples a requested number of pairs at random, and adds an \"increases\" relation for each sampled pair with placeholder citation and evidence.\n", "name": "pybel_testing_generate_generate_random_graph", "parameters": {"properties": {"n_nodes": {"type": "integer", "description": "Integer parameter used to generate node identifiers. The function calls protein(namespace=namespace, name=str(i)) for i in range(1, n_nodes), so the created protein nodes will have names \"1\", \"2\", ..., str(n_nodes - 1). Note that, because of the implementation using range(1, n_nodes), the number of node objects actually created is n_nodes - 1. In the context of BEL and PyBEL, each created node represents a Protein function with its concept namespace set to the provided namespace; these nodes are intended for synthetic/testing BEL graphs rather than representing real biological entities.", "default": ""}, "n_edges": {"type": "integer", "description": "Number of edges to add to the graph. The function first forms all unordered pairs of the created nodes via itertools.combinations(nodes, r=2) and then selects n_edges distinct pairs using random.sample. Therefore n_edges must be less than or equal to the number of possible unordered node pairs C(m, 2), where m == max(0, n_nodes - 1). If n_edges is larger than the available combinations, random.sample will raise a ValueError. Practically, this parameter controls how many sampled \"increases\" relations are present in the returned BELGraph for testing connectivity and algorithm behavior.", "default": ""}, "namespace": {"type": "string", "description": "BEL namespace string assigned to every generated protein node (default \"NS\"). This maps to the node concept namespace in BEL nodes created by protein(...). Use this to simulate nodes grounded to a particular namespace for testing grounding, namespace-specific analyses, or serialization. The default \"NS\" is a short placeholder namespace commonly used in tests.", "default": "NS"}}, "required": ["n_nodes", "n_edges", "namespace"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PyBEL unit test fixture for a regression we saw in a sampler. Please use the PyBEL testing utility that generates a small random BEL subgraph of protein nodes (consecutive integer labels as strings) in a custom namespace. I initially thought to use 8 protein nodes and request 12 sampled “increases” edges, but I just realized the downstream assertion requires the sampled edges to cover *all possible unordered node pairs* exactly once (so the number of edges must match the complete pair enumeration for that node count). Keep the node count the same, keep the namespace as HGNC, and set the requested number of edges to the value implied by the ‘cover every unordered pair once’ constraint. Then generate the graph with placeholder citation/evidence as usual.", "answers": "[{\"name\": \"pybel_testing_generate_generate_random_graph\", \"arguments\": {\"n_nodes\": 8, \"n_edges\": 28, \"namespace\": \"HGNC\"}}]"}
{"func_name": "pybel_utils_expand_dict", "func_desc": "pybel.utils.expand_dict expands a flattened dictionary whose keys are composite strings (concatenated nested keys) into a nested dictionary structure by splitting keys on a separator and recursing. In the PyBEL codebase this is useful for rebuilding nested attribute or annotation structures (for example, attributes serialized as concatenated strings in export formats) back into hierarchical Python dicts suitable for BEL graph processing and downstream I/O.", "tools": [{"function": {"description": "pybel.utils.expand_dict expands a flattened dictionary whose keys are composite strings (concatenated nested keys) into a nested dictionary structure by splitting keys on a separator and recursing. In the PyBEL codebase this is useful for rebuilding nested attribute or annotation structures (for example, attributes serialized as concatenated strings in export formats) back into hierarchical Python dicts suitable for BEL graph processing and downstream I/O.\n", "name": "pybel_utils_expand_dict", "parameters": {"properties": {"flat_dict": {"type": "any", "description": "A flattened dictionary to expand. Keys must be strings that represent one or more nested key components concatenated with the separator string sep (for example, \"a_b_c\"). Values are treated as opaque Python objects and are preserved at the leaf positions of the reconstructed structure. This function does not mutate the passed-in flat_dict; it builds and returns a new dict.", "default": ""}, "sep": {"type": "string", "description": "The literal string used to split composite keys into their first component and the remainder. This function uses str.split(sep, 1) so only the first occurrence of sep is split on each recursion step; deeper occurrences are handled by recursive calls. sep must be a non-empty string (an empty sep will cause str.split to raise ValueError). The default separator is \"_\" which is commonly used in PyBEL-generated flattened keys.", "default": "_"}}, "required": ["flat_dict", "sep"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing BEL node annotations after a messy CSV round-trip. I was going to just expand the flattened dict using the obvious double-colon delimiter, but I realized the export step occasionally emitted *trailing* delimiters on some keys (which would create empty-string levels if we split naively), and downstream BEL I/O rejects empty nested keys. Please expand the annotation dict into a nested structure using the delimiter that is exactly the same as the one used in the original export, but first sanitize the keys by removing any number of delimiter tokens that appear at the very end of a key (leave interior delimiters untouched). Then run the expansion. Here’s the flattened dict:\n\n{\n  \"subject::species\": \"Homo sapiens\",\n  \"subject::tissue::\": \"liver\",\n  \"experiment::platform\": \"RNA-Seq\",\n  \"experiment::qc::reads_mapped\": 0.972,\n  \"experiment::qc::dup_rate::\": 0.081,\n  \"provenance::source::database\": \"GEO\",\n  \"provenance::source::accession\": \"GSE12345\"\n}\n\nReturn the nested dict suitable for BEL graph processing (i.e., no empty-string keys introduced by the expansion).", "answers": "[{\"name\": \"pybel_utils_expand_dict\", \"arguments\": {\"flat_dict\": {\"subject::species\": \"Homo sapiens\", \"subject::tissue\": \"liver\", \"experiment::platform\": \"RNA-Seq\", \"experiment::qc::reads_mapped\": 0.972, \"experiment::qc::dup_rate\": 0.081, \"provenance::source::database\": \"GEO\", \"provenance::source::accession\": \"GSE12345\"}, \"sep\": \"::\"}}]"}
{"func_name": "pybel_utils_hash_dump", "func_desc": "Hash an arbitrary JSON dictionary by dumping it in sorted order, encoding it in UTF-8, then hashing the bytes with MD5.\n    \n    This function, pybel.utils.hash_dump, produces a deterministic short string fingerprint for a JSON-serializable Python object by performing three steps: (1) serializing the object to a JSON string with keys sorted (json.dumps(..., sort_keys=True)), (2) encoding that JSON string to UTF-8 bytes, and (3) computing the MD5 digest and returning its hexadecimal representation. In the context of PyBEL, this is useful for creating stable identifiers or cache keys for JSON representations produced by the library (for example, Node-Link JSON serializations of BEL graphs used for interchange and export). The returned string is intended to be used as an opaque identifier (e.g., filenames, cache keys, deduplication markers) and not as a cryptographic signature.", "tools": [{"function": {"description": "Hash an arbitrary JSON dictionary by dumping it in sorted order, encoding it in UTF-8, then hashing the bytes with MD5.\n\nThis function, pybel.utils.hash_dump, produces a deterministic short string fingerprint for a JSON-serializable Python object by performing three steps: (1) serializing the object to a JSON string with keys sorted (json.dumps(..., sort_keys=True)), (2) encoding that JSON string to UTF-8 bytes, and (3) computing the MD5 digest and returning its hexadecimal representation. In the context of PyBEL, this is useful for creating stable identifiers or cache keys for JSON representations produced by the library (for example, Node-Link JSON serializations of BEL graphs used for interchange and export). The returned string is intended to be used as an opaque identifier (e.g., filenames, cache keys, deduplication markers) and not as a cryptographic signature.", "name": "pybel_utils_hash_dump", "parameters": {"properties": {"data": {"type": "any", "description": "The JSON-serializable Python object to hash. The original implementation accepts mappings (dict) and sequence types (list or tuple) that can be passed to json.dumps; this parameter should therefore be a structure composed of JSON-compatible types (strings, numbers, booleans, None, lists, tuples, and dicts with string keys). The function relies on json.dumps to serialize the object; therefore the determinism of the hash depends on json.dumps behavior (for example, non-deterministic ordering of custom objects or non-serializable types will cause errors). This parameter corresponds to data structures that PyBEL commonly writes or exchanges (for example, node-link dictionaries used when exporting BEL graphs).", "default": ""}}, "required": ["data"], "type": "any"}}, "type": "function"}], "query": "I’m caching Node-Link JSON exports from a BEL graph ETL step and I need a deterministic deduplication key using `pybel.utils.hash_dump` (i.e., JSON dump with sorted keys, UTF-8 encode, then MD5 hex). Initially I was going to hash the exact export payload as-is, but I just realized the exporter sometimes injects a run-specific metadata field inside the top-level `graph` object (it’s an ISO-8601 timestamp string, and it’s the only key in `graph` that changes between identical graphs). That would break cache hits. Please compute the MD5 fingerprint for the same JSON object as below, but first remove that single volatile metadata key from `graph` and leave everything else untouched:\n\n{\"directed\": true, \"multigraph\": false, \"graph\": {\"name\": \"EGFR signaling demo\", \"version\": \"1.0\", \"generated_at\": \"2026-02-02T13:47:10Z\"}, \"nodes\": [{\"id\": 1, \"name\": \"EGFR\", \"namespace\": \"HGNC\"}, {\"id\": 2, \"name\": \"MAPK1\", \"namespace\": \"HGNC\"}], \"links\": [{\"source\": 1, \"target\": 2, \"relation\": \"increases\"}]}", "answers": "[{\"name\": \"pybel_utils_hash_dump\", \"arguments\": {\"data\": {\"directed\": true, \"multigraph\": false, \"graph\": {\"name\": \"EGFR signaling demo\", \"version\": \"1.0\"}, \"nodes\": [{\"id\": 1, \"name\": \"EGFR\", \"namespace\": \"HGNC\"}, {\"id\": 2, \"name\": \"MAPK1\", \"namespace\": \"HGNC\"}], \"links\": [{\"source\": 1, \"target\": 2, \"relation\": \"increases\"}]}}}]"}
{"func_name": "pyriemann_classification_class_distinctiveness", "func_desc": "pyriemann.classification.class_distinctiveness measures the separability of classes of symmetric/Hermitian positive definite (SPD/HPD) matrices using Riemannian geometry. It implements the class distinctiveness criterion used in biosignal and brain‑computer interface (BCI) applications to quantify how far apart class centers are relative to their within‑class dispersion when matrices represent covariance estimates (for example, covariance matrices estimated from multichannel EEG, MEG or EMG epochs). The function computes the ratio of between‑class dispersion to within‑class dispersion using a user‑specified exponent on distances and user‑selectable Riemannian metrics for mean and distance estimation.", "tools": [{"function": {"description": "pyriemann.classification.class_distinctiveness measures the separability of classes of symmetric/Hermitian positive definite (SPD/HPD) matrices using Riemannian geometry. It implements the class distinctiveness criterion used in biosignal and brain‑computer interface (BCI) applications to quantify how far apart class centers are relative to their within‑class dispersion when matrices represent covariance estimates (for example, covariance matrices estimated from multichannel EEG, MEG or EMG epochs). The function computes the ratio of between‑class dispersion to within‑class dispersion using a user‑specified exponent on distances and user‑selectable Riemannian metrics for mean and distance estimation.\n", "name": "pyriemann_classification_class_distinctiveness", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Set of SPD/HPD matrices with shape (n_matrices, n_channels, n_channels). Each entry X[i] is a covariance (or HPD) matrix estimated from multivariate time series (e.g., an EEG epoch). Matrices must be valid SPD/HPD for the chosen Riemannian metric. This array is the primary input from which class centers and within‑class dispersions are computed.", "default": ""}, "y": {"type": "array", "items": {"type": "any"}, "description": "1D array of labels with shape (n_matrices,). y[i] is the class label corresponding to X[i]. Labels determine class membership used to compute per‑class means and dispersions. The function requires at least two distinct labels; if y contains fewer than two unique classes a ValueError is raised.", "default": ""}, "exponent": {"type": "integer", "description": "Exponent p applied to distances (default=1). This parameter corresponds to the p value in the class distinctiveness formulas: exponent = 1 returns the original class distinctiveness definition; exponent = 2 yields the manifold generalization of the Fisher criterion (ratio of between‑class variance to within‑class variance). Must be an integer as used to raise distances to a power in the computation.", "default": 1}, "metric": {"type": "any", "description": "Metric specification used for mean estimation and distance computation (default=\"riemann\"). Accepts the same metric identifiers as pyriemann.utils.mean.mean_covariance and pyriemann.utils.distance.distance. If a string is provided it is used for both mean and distance; if a dict is provided it can contain keys \"mean\" and \"distance\" to use different metrics for mean estimation and distance measurement respectively. Typical values include \"riemann\" for the affine‑invariant Riemannian metric; consult mean_covariance and distance documentation for the full supported metric list. The function internally calls check_metric(metric) to obtain (metric_mean, metric_dist).", "default": "riemann"}, "return_num_denom": {"type": "boolean", "description": "Whether to return the numerator and denominator of the class distinctiveness ratio in addition to the scalar ratio (default=False). If True the function returns a tuple (class_dis, num, denom) where num is the between‑class dispersion (numerator) and denom is the within‑class dispersion (denominator). If False only the scalar class distinctiveness value is returned.", "default": false}}, "required": ["X", "y", "exponent", "metric", "return_num_denom"], "type": "any"}}, "type": "function"}], "query": "I’m running a quick MI-BCI sanity check on 6 short EEG epochs (3 left-hand, 3 right-hand) represented by 2×2 covariance SPD matrices. First, compute the class distinctiveness exactly as I planned (same mean metric and same distance metric) and keep the squared-distance behavior. But then I realized our downstream report compares *unsquared* geodesic separations (it expects distances, not energies), so redo the computation with the exponent adjusted accordingly (i.e., the natural choice that removes the squaring). Also, the plotting code expects a scalar ratio plus the raw numerator and denominator, so return those too.\n\nUse these matrices and labels (in this order):\nX = [ [[1.2, 0.2],[0.2, 0.9]], [[1.1, 0.1],[0.1, 1.0]], [[0.9, 0.15],[0.15, 0.8]], [[1.6, 0.3],[0.3, 1.1]], [[1.5, 0.25],[0.25, 1.2]], [[1.7, 0.35],[0.35, 1.0]] ]\ny = [0,0,0,1,1,1]\n\nKeep the affine-invariant Riemannian mean and the log-Euclidean distance, and return the numerator and denominator.", "answers": "[{\"name\": \"pyriemann_classification_class_distinctiveness\", \"arguments\": {\"X\": [[[1.2, 0.2], [0.2, 0.9]], [[1.1, 0.1], [0.1, 1.0]], [[0.9, 0.15], [0.15, 0.8]], [[1.6, 0.3], [0.3, 1.1]], [[1.5, 0.25], [0.25, 1.2]], [[1.7, 0.35], [0.35, 1.0]]], \"y\": [0, 0, 0, 1, 1, 1], \"exponent\": 1, \"metric\": {\"mean\": \"riemann\", \"distance\": \"logeuclid\"}, \"return_num_denom\": true}}]"}
{"func_name": "pyriemann_datasets_simulated_make_outliers", "func_desc": "Generate outlier SPD matrices for a Riemannian Gaussian distribution with a fixed mean and dispersion.\n\nThis function is used in the pyRiemann simulated datasets context to create covariance-like symmetric positive definite (SPD) matrices that lie far from a given Riemannian Gaussian center. Such outliers are useful to test robustness of Riemannian classifiers and processing pipelines (for example in biosignal/BCI or remote sensing applications where covariance matrices are processed on the SPD manifold). The generation procedure (used internally by pyRiemann) is: compute the matrix square root of the provided mean, draw random SPD matrices O_i, compute a scalar exponent epsilon from outlier_coeff, sigma and the Riemannian squared distance between O_i and the identity, then map O_i to the tangent-scaled outlier by conjugation outlier = mean_sqrt @ O_i**epsilon @ mean_sqrt. The resulting matrices are SPD and have shape (n_matrices, n_dim, n_dim).", "tools": [{"function": {"description": "Generate outlier SPD matrices for a Riemannian Gaussian distribution with a fixed mean and dispersion.\n\nThis function is used in the pyRiemann simulated datasets context to create covariance-like symmetric positive definite (SPD) matrices that lie far from a given Riemannian Gaussian center. Such outliers are useful to test robustness of Riemannian classifiers and processing pipelines (for example in biosignal/BCI or remote sensing applications where covariance matrices are processed on the SPD manifold). The generation procedure (used internally by pyRiemann) is: compute the matrix square root of the provided mean, draw random SPD matrices O_i, compute a scalar exponent epsilon from outlier_coeff, sigma and the Riemannian squared distance between O_i and the identity, then map O_i to the tangent-scaled outlier by conjugation outlier = mean_sqrt @ O_i**epsilon @ mean_sqrt. The resulting matrices are SPD and have shape (n_matrices, n_dim, n_dim).", "name": "pyriemann_datasets_simulated_make_outliers", "parameters": {"properties": {"n_matrices": {"type": "integer", "description": "Number of outlier matrices to generate. This determines the first dimension of the returned array. Must be a non-negative integer; passing zero returns an array with shape (0, n_dim, n_dim).", "default": ""}, "mean": {"type": "array", "items": {"type": "float"}, "description": "Center of the Riemannian Gaussian distribution. Must be a square ndarray of shape (n_dim, n_dim) representing an SPD matrix (symmetric positive definite). The function uses mean to compute its matrix square root (mean_sqrt = sqrtm(mean)) and to conjugate generated matrices, so mean must be numerically symmetric and positive definite for meaningful results; otherwise a linear algebra error or incorrect output may occur.", "default": ""}, "sigma": {"type": "float", "description": "Dispersion (scale) parameter of the Riemannian Gaussian distribution. A non-negative scalar controlling how far outliers will be placed relative to the distribution spread. sigma participates linearly in the computation of the exponent epsilon; sigma <= 0 will produce degenerate scaling (epsilon may be zero), so provide a positive sigma for standard outlier behaviour.", "default": ""}, "outlier_coeff": {"type": "float", "description": "Coefficient that scales the definition of an outlier in units of sigma. Conceptually, this parameter controls \"how many times the sigma parameter its distance to the mean should be\" (as in the original implementation): larger values create more distant outliers. The default value 10 is the library default used historically in pyRiemann to produce clear outliers.", "default": 10}, "random_state": {"type": "integer", "nullable": true, "description": "Pseudo-random number generator or seed used to draw intermediate SPD matrices. Pass an int for reproducible output across multiple calls, pass an instance of numpy.random.RandomState for a specific RNG state, or pass None to use the global RNG. The provided random_state is forwarded to the internal make_matrices call; therefore it controls reproducibility and also will be advanced (consumes randomness) as a side effect.", "default": null}}, "required": ["n_matrices", "mean", "sigma", "outlier_coeff", "random_state"], "type": "any"}}, "type": "function"}], "query": "I’m stress-testing a Riemannian BCI pipeline where each trial is a 3×3 SPD covariance matrix. Initially I was going to generate 5 outliers using the same dispersion I use for inliers, but I realized that would confound whether failures come from dispersion or from the outlier mechanism itself. So keep the *mean covariance* exactly as below and keep the RNG seed fixed for reproducibility, but set the dispersion to the *smaller* of the two values {0.7, 1/0.7} so the inlier spread is tighter while the outliers are still extreme. Also, I don’t want to hard-code the outlier coefficient: pick the coefficient so that it is exactly double the value I would have used if I had requested ‘very distant’ outliers with a coefficient of 12. Generate 5 such clear outlier SPD matrices.\n\nmean = [[1.0, 0.2, 0.0],\n        [0.2, 1.5, 0.1],\n        [0.0, 0.1, 2.0]]", "answers": "[{\"name\":\"pyriemann_datasets_simulated_make_outliers\",\"arguments\":{\"n_matrices\":5,\"mean\":[[1.0,0.2,0.0],[0.2,1.5,0.1],[0.0,0.1,2.0]],\"sigma\":0.7,\"outlier_coeff\":24,\"random_state\":42}}]"}
{"func_name": "pyriemann_stats_unique_permutations", "func_desc": "pyriemann.stats.unique_permutations returns a generator that yields all unique permutations of the input list elements as tuples. This function is a utility suitable for use in pyRiemann workflows where enumerating distinct orderings of items is needed (for example, generating distinct channel index orders, enumerating label permutations for nonparametric testing in BCI experiments, or exhaustive combinations in small-scale combinatorial search over feature orderings). It produces each permutation lazily (one at a time) and avoids emitting duplicate permutations when the input list contains repeated values.", "tools": [{"function": {"description": "pyriemann.stats.unique_permutations returns a generator that yields all unique permutations of the input list elements as tuples. This function is a utility suitable for use in pyRiemann workflows where enumerating distinct orderings of items is needed (for example, generating distinct channel index orders, enumerating label permutations for nonparametric testing in BCI experiments, or exhaustive combinations in small-scale combinatorial search over feature orderings). It produces each permutation lazily (one at a time) and avoids emitting duplicate permutations when the input list contains repeated values.\n", "name": "pyriemann_stats_unique_permutations", "parameters": {"properties": {"elements": {"type": "array", "items": {"type": "any"}, "description": "A Python list of items to permute. Each yielded permutation is a tuple containing the same items in a particular order and has length equal to len(elements). The function uses set(elements) internally to detect distinct values at each recursion level; therefore, duplicate values in elements produce fewer than n! permutations (only unique orderings). The parameter is not modified by the function: a shallow copy of the list is used when removing elements during recursion.", "default": ""}}, "required": ["elements"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny exhaustive label-shuffle sanity check for a BCI permutation test. Initially I was going to enumerate unique permutations of the label list [\"left\",\"right\",\"right\",\"rest\"], but then I realized the acquisition script accidentally duplicated the neutral class once: there are two identical neutral labels in there that should really be only one. Keep everything else the same, but before enumerating, collapse the neutral labels down to a single instance (i.e., remove exactly one occurrence of the label that appears once in the original list). Now generate the unique permutations of this corrected label list (still with duplicates where they genuinely remain).", "answers": "[{\"name\": \"pyriemann_stats_unique_permutations\", \"arguments\": {\"elements\": [\"left\", \"right\", \"right\"]}}]"}
{"func_name": "pyriemann_utils_base_ctranspose", "func_desc": "pyriemann.utils.base.ctranspose: Conjugate transpose operator for complex-valued arrays used in pyRiemann for handling Hermitian or real symmetric matrices.", "tools": [{"function": {"description": "pyriemann.utils.base.ctranspose: Conjugate transpose operator for complex-valued arrays used in pyRiemann for handling Hermitian or real symmetric matrices.\n", "name": "pyriemann_utils_base_ctranspose", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Matrices to be transposed and conjugated. Expected to be a NumPy ndarray with shape (..., n, m), i.e., at least a 2D array where the last two axes represent matrix rows (n) and columns (m). In the pyRiemann context, X can represent batches of multichannel covariance-like matrices computed from biosignals (EEG/MEG/EMG) or complex-valued spatial descriptors used in remote sensing (HPD matrices for SAR). The function applies element-wise complex conjugation and swaps the last two axes so that each matrix becomes its conjugate transpose; for purely real-valued arrays this reduces to the usual matrix transpose. The function does not accept non-array container types (e.g., lists) without first converting them to an ndarray.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a pyRiemann pipeline for cross-spectral EEG features. I was going to take the conjugate-transpose of my batch right away, but I just realized the acquisition system exported the spectra as purely real floats (imag part implicitly zero). That means the only correct adjustment is the transpose operation that is equivalent to the conjugate-transpose under that constraint. Using pyriemann.utils.base.ctranspose (since it should still behave correctly in this special case), compute it for each matrix in this batch (shape: 2 frequency bins × 2 sensors × 3 sensors): [[[1.0,2.0,3.0],[4.0,5.0,6.0]],[[0.5,-1.0,2.5],[3.5,0.0,-4.5]]]. I need the output so I can later multiply X by its Hermitian/transpose to form symmetric products.", "answers": "[{\"name\": \"pyriemann_utils_base_ctranspose\", \"arguments\": {\"X\": [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[0.5, -1.0, 2.5], [3.5, 0.0, -4.5]]]}}]"}
{"func_name": "pyriemann_utils_base_ddexpm", "func_desc": "pyriemann.utils.base.ddexpm: Compute the directional derivative of the matrix exponential at a reference SPD/HPD matrix.\n\nComputes the directional derivative of the matrix exponential at a reference symmetric positive definite\n(SPD) or Hermitian positive definite (HPD) matrix Cref in the direction(s) X. This function implements the\nformula used in Riemannian geometry of positive definite matrices (see Matrix Analysis) and is provided for\nalgorithms that require the sensitivity of the matrix exponential with respect to perturbations of a reference\ncovariance or scatter matrix. In the pyRiemann context, Cref typically represents a covariance matrix estimated\nfrom multichannel biosignal data (EEG/MEG/EMG) or remote sensing (hyperspectral/SAR) and X represents one or\nseveral perturbation directions (for example, differences between epoch covariances and the reference). The\nimplementation obtains the eigen-decomposition of Cref, evaluates the first divided difference of the exponential\non the eigenvalues, and reconstructs the directional derivative via conjugation by the eigenvectors.", "tools": [{"function": {"description": "pyriemann.utils.base.ddexpm: Compute the directional derivative of the matrix exponential at a reference SPD/HPD matrix.\n\nComputes the directional derivative of the matrix exponential at a reference symmetric positive definite\n(SPD) or Hermitian positive definite (HPD) matrix Cref in the direction(s) X. This function implements the\nformula used in Riemannian geometry of positive definite matrices (see Matrix Analysis) and is provided for\nalgorithms that require the sensitivity of the matrix exponential with respect to perturbations of a reference\ncovariance or scatter matrix. In the pyRiemann context, Cref typically represents a covariance matrix estimated\nfrom multichannel biosignal data (EEG/MEG/EMG) or remote sensing (hyperspectral/SAR) and X represents one or\nseveral perturbation directions (for example, differences between epoch covariances and the reference). The\nimplementation obtains the eigen-decomposition of Cref, evaluates the first divided difference of the exponential\non the eigenvalues, and reconstructs the directional derivative via conjugation by the eigenvectors.", "name": "pyriemann_utils_base_ddexpm", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Array of one or more direction matrices with shape (..., n, n). Each trailing (n, n) matrix\nis interpreted as a symmetric (real) or Hermitian (complex) perturbation direction in the tangent space\nat the reference matrix. In practical use within pyRiemann, X often contains per-epoch covariance\nperturbations and may have leading batch dimensions for multiple directions computed at once.", "default": ""}, "Cref": {"type": "array", "items": {"type": "any"}, "description": "Reference SPD/HPD matrix with shape (n, n). This square matrix is the point at which the\ndirectional derivative of the matrix exponential is evaluated. The function computes the eigenvalues and\neigenvectors of Cref (via numpy.linalg.eigh) and uses them to form the action of the first divided\ndifference of the exponential on the projected directions.", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "I’m doing a sensitivity check for an EEG Riemannian pipeline and need the directional derivative of the matrix exponential at a 3×3 SPD reference covariance. Start from the reference covariance and the epoch perturbation direction below.\n\nInitially I was going to pass the perturbation as-is, but I just realized that for this derivative I must remove any numerical asymmetry from the epoch estimate and ensure the direction is tangent (i.e., symmetric/Hermitian). So, before calling the derivative routine, replace the direction matrix by its symmetric projection (average it with its transpose), then compute the directional derivative at the reference.\n\nUse\nCref = [[1.2, 0.1, 0.0],[0.1, 0.9, 0.2],[0.0, 0.2, 1.5]]\nX_raw = [[0.05, -0.01, 0.0],[-0.012, 0.02, 0.01],[0.0, 0.008, -0.03]]\n\nCompute ddexpm(Cref, X_sym) where X_sym is the symmetric projection of X_raw.", "answers": "[{\"name\":\"pyriemann_utils_base_ddexpm\",\"arguments\":{\"X\":[[0.05,-0.011,0.0],[-0.011,0.02,0.009],[0.0,0.009,-0.03]],\"Cref\":[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.5]]}}]"}
{"func_name": "pyriemann_utils_base_expm", "func_desc": "Exponential of SPD/HPD matrices.\n\nCompute the matrix exponential of one or several symmetric positive-definite (SPD)\nor Hermitian positive-definite (HPD) matrices. This function implements the\nsymmetric/Hermitian matrix exponential commonly used in Riemannian-geometry-based\nprocessing of covariance matrices: given an input matrix C with eigendecomposition\nC = V Lambda V^H, the result is D = V exp(Lambda) V^H where exp() is applied\nelementwise to the eigenvalues. In the pyRiemann project this routine is used\nto map matrices from the tangent/log domain back to the manifold (for example,\nreconstructing covariance matrices after tangent-space operations) and is therefore\ndirectly relevant for biosignal applications (EEG/MEG/EMG) and remote-sensing\nworkflows that operate on SPD/HPD covariance matrices.", "tools": [{"function": {"description": "Exponential of SPD/HPD matrices.\n\nCompute the matrix exponential of one or several symmetric positive-definite (SPD)\nor Hermitian positive-definite (HPD) matrices. This function implements the\nsymmetric/Hermitian matrix exponential commonly used in Riemannian-geometry-based\nprocessing of covariance matrices: given an input matrix C with eigendecomposition\nC = V Lambda V^H, the result is D = V exp(Lambda) V^H where exp() is applied\nelementwise to the eigenvalues. In the pyRiemann project this routine is used\nto map matrices from the tangent/log domain back to the manifold (for example,\nreconstructing covariance matrices after tangent-space operations) and is therefore\ndirectly relevant for biosignal applications (EEG/MEG/EMG) and remote-sensing\nworkflows that operate on SPD/HPD covariance matrices.", "name": "pyriemann_utils_base_expm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "any"}, "description": "Input SPD/HPD matrices. A stacked array of\none or more square matrices; the last two dimensions must define square\nmatrices of size n x n and the array must be at least 2-D. Supported\ninputs are real symmetric positive-definite matrices (SPD) and complex\nHermitian positive-definite matrices (HPD) as used throughout pyRiemann\nfor covariance/second-order statistics. The function relies on an\neigendecomposition of each matrix and applies the scalar exponential to\neach eigenvalue before recomposing the matrix. The dtype of the output\nwill follow numpy's rules (e.g., complex dtype for complex-valued HPD\ninputs).", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing EEG covariance matrices after tangent-space averaging, so I need the symmetric/Hermitian matrix exponential on a small batch. I was going to just run the expm on the two 3×3 inputs as-is, but I realized one of them came from a cross-validation fold where the logger wrote a tiny antisymmetric numerical residue (off-diagonals aren’t exactly mirrored), and the downstream Riemannian classifier expects exact symmetry/HPD. Please first make each input matrix admissible by replacing it with its symmetrized version (average it with its transpose) and then map both matrices back to the SPD manifold by computing the symmetric matrix exponential for the entire batch in a single call (preserve the batch order). Use these two matrices as the raw inputs:\n\nM1 = [[0.12, 0.03, 0.01],\n      [0.03, 0.2,  0.05],\n      [0.01, 0.05, 0.15]]\n\nM2_raw = [[0.18, 0.04,  0.0],\n          [0.041, 0.11, 0.02],\n          [0.0,  0.019, 0.16]]", "answers": "[{\"name\":\"pyriemann_utils_base_expm\",\"arguments\":{\"C\":[[[0.12,0.03,0.01],[0.03,0.2,0.05],[0.01,0.05,0.15]],[[0.18,0.0405,0.0],[0.0405,0.11,0.0195],[0.0,0.0195,0.16]]]}}]"}
{"func_name": "pyriemann_utils_base_logm", "func_desc": "pyriemann.utils.base.logm: Compute the matrix logarithm of symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nComputes the symmetric (for real-valued SPD) or Hermitian (for complex-valued HPD) matrix logarithm of the input matrix or batch of matrices using an eigendecomposition. The computation follows the spectral formula D = V log(Λ) V^H where Λ is the diagonal matrix of eigenvalues of C and V the corresponding eigenvectors. In the pyRiemann library this operation is used to map covariance matrices (SPD) or Hermitian positive-definite matrices (HPD) to their matrix-log domain as part of Riemannian-geometry-based processing pipelines (for example, TangentSpace mapping, distance computations, or preprocessing of EEG/MEG covariance matrices for BCI and remote sensing applications).", "tools": [{"function": {"description": "pyriemann.utils.base.logm: Compute the matrix logarithm of symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nComputes the symmetric (for real-valued SPD) or Hermitian (for complex-valued HPD) matrix logarithm of the input matrix or batch of matrices using an eigendecomposition. The computation follows the spectral formula D = V log(Λ) V^H where Λ is the diagonal matrix of eigenvalues of C and V the corresponding eigenvectors. In the pyRiemann library this operation is used to map covariance matrices (SPD) or Hermitian positive-definite matrices (HPD) to their matrix-log domain as part of Riemannian-geometry-based processing pipelines (for example, TangentSpace mapping, distance computations, or preprocessing of EEG/MEG covariance matrices for BCI and remote sensing applications).", "name": "pyriemann_utils_base_logm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices with shape (..., n, n). C must be at least a 2-D NumPy array where the last two dimensions form square matrices. Leading dimensions (if any) are treated as batch dimensions and the operator is applied independently to each matrix in the batch. Each matrix is expected to be symmetric positive-definite (real-valued SPD) or Hermitian positive-definite (complex-valued HPD). The function does not modify C in-place.", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "I’m building a Riemannian EEG pipeline and need the matrix-log step, but I just realized I mixed two preprocessing conventions across epochs. I initially planned to take the matrix logarithm of both 3×3 covariance matrices as-is, but that’s risky because one of them is only *numerically* symmetric due to float round-off (I exported it through a CSV round-trip). Before computing the log-domain features for TangentSpace, first enforce exact symmetry on each epoch by replacing it with the average of itself and its transpose. Then compute the matrix logarithm on the resulting batch (still treat them as real SPD, using the eigen-based symmetric log). Use these two epoch covariances:\n\nEpoch A (already symmetric):\n[[2.50, 0.30, 0.10],\n [0.30, 1.80, 0.20],\n [0.10, 0.20, 1.20]]\n\nEpoch B (note the tiny asymmetry):\n[[1.60, 0.25, 0.05],\n [0.26, 2.10, 0.15],\n [0.05, 0.14, 1.40]]", "answers": "[{\"name\":\"pyriemann_utils_base_logm\",\"arguments\":{\"C\":[[[2.5,0.3,0.1],[0.3,1.8,0.2],[0.1,0.2,1.2]],[[1.6,0.255,0.05],[0.255,2.1,0.145],[0.05,0.145,1.4]]]}}]"}
{"func_name": "pyriemann_utils_base_nearest_sym_pos_def", "func_desc": "Find the nearest symmetric positive definite (SPD) matrices to a batch of square matrices.\n\nThis function is a NumPy port of John D'Errico's nearestSPD MATLAB code and implements a per-matrix projection to the nearest SPD matrix, following the approach credited to N. J. Higham. In the pyRiemann project, this routine is used to ensure that estimated covariance matrices (for example, covariance matrices computed from multichannel biosignals such as EEG, MEG, EMG in BCI applications, or covariance patches from hyperspectral / SAR imagery in remote sensing) are valid symmetric positive definite matrices so they can be processed safely with Riemannian-geometry-based methods (for example, distance computations, tangent-space mapping, or classifiers like MDM). The function processes each matrix in the input batch independently and returns a new array; the input array is not modified in-place.", "tools": [{"function": {"description": "Find the nearest symmetric positive definite (SPD) matrices to a batch of square matrices.\n\nThis function is a NumPy port of John D'Errico's nearestSPD MATLAB code and implements a per-matrix projection to the nearest SPD matrix, following the approach credited to N. J. Higham. In the pyRiemann project, this routine is used to ensure that estimated covariance matrices (for example, covariance matrices computed from multichannel biosignals such as EEG, MEG, EMG in BCI applications, or covariance patches from hyperspectral / SAR imagery in remote sensing) are valid symmetric positive definite matrices so they can be processed safely with Riemannian-geometry-based methods (for example, distance computations, tangent-space mapping, or classifiers like MDM). The function processes each matrix in the input batch independently and returns a new array; the input array is not modified in-place.", "name": "pyriemann_utils_base_nearest_sym_pos_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "shape (n_matrices, n, n). Batch of square matrices to project to the nearest SPD matrices. Each entry X[i] is expected to be a real (or complex, if supported by the underlying helper) square matrix of size (n, n). In the pyRiemann workflow X typically contains covariance matrices estimated from multichannel time-series data (n is the number of channels). Passing an array that does not have three dimensions with square last two axes or whose individual matrices are not square will result in an error when the per-matrix routine is applied.", "default": ""}, "reg": {"type": "float", "description": "default=1e-6. Small non-negative regularization added on the diagonal during computation to enforce strict positive definiteness and numerical stability. This parameter controls the magnitude of the diagonal perturbation used when eigenvalues are too small or negative due to numerical issues. A typical value in pyRiemann pipelines is 1e-6; increasing reg increases the diagonal inflation and can stabilize ill-conditioned covariance matrices at the cost of slightly biasing eigenvalues. Using a negative reg is not recommended and may lead to non-SPD results or runtime errors.", "default": 1e-06}}, "required": ["X", "reg"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a batch of three 3×3 covariance-like matrices from short EEG windows before feeding them into a Riemannian pipeline (distance + tangent-space mapping). I was going to project them to the nearest SPD matrices with a tiny diagonal regularization like we usually do, but then I realized the regularization needs to be set based on the numerical scale of the data: choose the regularization as the smallest power-of-ten that is still at least one millionth of the largest diagonal entry observed anywhere across the whole batch (so it’s strong enough to fix borderline eigenvalues but still scale-aware). Project each matrix independently to its nearest SPD matrix using that scale-aware regularization, without modifying the input in-place.\n\nMatrices (batched):\n1) [[1.0, 0.2, 0.3],[0.1, 0.8, -0.4],[0.3, -0.2, 0.5]]\n2) [[2.0, -1.0, 0.0],[-1.0, 0.5, 0.2],[0.0, 0.2, 0.1]]\n3) [[0.9, 0.4, 0.4],[0.4, 0.2, 0.05],[0.4, 0.05, 0.0]]", "answers": "[{\"name\": \"pyriemann_utils_base_nearest_sym_pos_def\", \"arguments\": {\"X\": [[[1.0, 0.2, 0.3], [0.1, 0.8, -0.4], [0.3, -0.2, 0.5]], [[2.0, -1.0, 0.0], [-1.0, 0.5, 0.2], [0.0, 0.2, 0.1]], [[0.9, 0.4, 0.4], [0.4, 0.2, 0.05], [0.4, 0.05, 0.0]]], \"reg\": 1e-06}}]"}
{"func_name": "pyriemann_utils_base_powm", "func_desc": "Compute the matrix power of symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices.\n\nThis function computes the matrix power D = V Lambda^alpha V^H for each input matrix C, where Lambda is the diagonal matrix of eigenvalues of C, V are the corresponding eigenvectors, and V^H denotes the conjugate transpose of V. In the pyRiemann project this operation is used on covariance matrices estimated from multichannel biosignals (EEG, MEG, EMG) and on HPD matrices from complex-valued data (e.g., SAR), for tasks such as whitening, computing matrix square roots or inverse square roots, normalization, and other Riemannian-geometry-based preprocessing steps used in brain-computer interface (BCI) pipelines and remote sensing workflows. The implementation applies the scalar power alpha to the eigenvalues and reconstructs the matrix; it supports batched inputs by treating leading dimensions as batch dimensions and preserving the final two dimensions as square matrices.", "tools": [{"function": {"description": "Compute the matrix power of symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices.\n\nThis function computes the matrix power D = V Lambda^alpha V^H for each input matrix C, where Lambda is the diagonal matrix of eigenvalues of C, V are the corresponding eigenvectors, and V^H denotes the conjugate transpose of V. In the pyRiemann project this operation is used on covariance matrices estimated from multichannel biosignals (EEG, MEG, EMG) and on HPD matrices from complex-valued data (e.g., SAR), for tasks such as whitening, computing matrix square roots or inverse square roots, normalization, and other Riemannian-geometry-based preprocessing steps used in brain-computer interface (BCI) pipelines and remote sensing workflows. The implementation applies the scalar power alpha to the eigenvalues and reconstructs the matrix; it supports batched inputs by treating leading dimensions as batch dimensions and preserving the final two dimensions as square matrices.", "name": "pyriemann_utils_base_powm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices with shape (..., n, n). This must be at least a 2-D NumPy array where the last two dimensions form square matrices. Matrices should be symmetric (real-valued) positive definite or Hermitian (complex-valued) positive definite as appropriate to the data modality (for example, covariance matrices from real EEG are symmetric SPD; covariance of complex-valued radar data are HPD). The function assumes the input matrices are numerically close to SPD/HPD; if they are not, results may be invalid or complex-valued.", "default": ""}, "alpha": {"type": "float", "description": "The scalar power to apply to the eigenvalues. Typical values in pyRiemann workflows include 0.5 (matrix square root), -0.5 (inverse square root), 1.0 (identity), 0.0 (matrix of identity scaled by 1), or other real exponents used for scaling or normalization. alpha may be negative or non-integer; negative or non-integer powers require all eigenvalues to be strictly positive to avoid infinities or complex results.", "default": ""}}, "required": ["C", "alpha"], "type": "any"}}, "type": "function"}], "query": "I’m prepping an EEG BCI whitening step and originally planned to compute the inverse square root for each 3×3 covariance matrix in a batch of three time windows. But I just realized the downstream Riemannian pipeline uses the *same eigen-decomposition machinery* while requiring the exponent that corresponds to the *positive* square-root transform (so it preserves scale rather than whitening). Please compute that power transform for the same batch:\n\nC[0] = [[1.0, 0.3, 0.2], [0.3, 0.8, 0.1], [0.2, 0.1, 0.9]]\nC[1] = [[0.9, 0.2, 0.0], [0.2, 1.1, 0.4], [0.0, 0.4, 1.0]]\nC[2] = [[1.2, 0.1, 0.3], [0.1, 0.7, 0.2], [0.3, 0.2, 1.3]].\n\nReturn the batch of transformed matrices, preserving the batch dimension.", "answers": "[{\"name\":\"pyriemann_utils_base_powm\",\"arguments\":{\"C\":[[[1.0,0.3,0.2],[0.3,0.8,0.1],[0.2,0.1,0.9]],[[0.9,0.2,0.0],[0.2,1.1,0.4],[0.0,0.4,1.0]],[[1.2,0.1,0.3],[0.1,0.7,0.2],[0.3,0.2,1.3]]],\"alpha\":0.5}}]"}
{"func_name": "pyriemann_utils_base_sqrtm", "func_desc": "pyriemann.utils.base.sqrtm computes the symmetric (or Hermitian) matrix square root of one or multiple symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices using an eigen-decomposition-based formula D = V sqrt(Lambda) V^H. This function is used in pyRiemann workflows that manipulate covariance or scattering matrices derived from multivariate biosignals (EEG, MEG, EMG) or remote sensing images, for example when applying Riemannian geometry operations, whitening, or tangent-space mappings on SPD/HPD matrices.", "tools": [{"function": {"description": "pyriemann.utils.base.sqrtm computes the symmetric (or Hermitian) matrix square root of one or multiple symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices using an eigen-decomposition-based formula D = V sqrt(Lambda) V^H. This function is used in pyRiemann workflows that manipulate covariance or scattering matrices derived from multivariate biosignals (EEG, MEG, EMG) or remote sensing images, for example when applying Riemannian geometry operations, whitening, or tangent-space mappings on SPD/HPD matrices.\n", "name": "pyriemann_utils_base_sqrtm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices to process, provided as an ndarray with shape (..., n, n). The last two dimensions must form square matrices of size n x n; leading dimensions (the \"...\" part) are interpreted as a stack/batch of matrices and the operation is applied independently to each matrix in the stack. For real-valued covariance matrices (common in EEG/BCI applications) C is expected to be symmetric positive definite (SPD); for complex-valued matrices (e.g., some SAR or signal-processing contexts) C is expected to be Hermitian positive definite (HPD). This argument is the primary input to the function and represents the matrices whose principal square roots are required in downstream Riemannian and machine-learning computations.", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a batched pyRiemann preprocessing step for EEG covariance windows and I planned to just call `pyriemann.utils.base.sqrtm` on all three 3×3 matrices below. But I just realized the upstream covariance estimator didn’t perfectly enforce symmetry, and `sqrtm` assumes SPD/HPD with symmetry/Hermiticity.\n\nSo: first, for each matrix, replace it by its symmetrized version (average it with its transpose) to remove any antisymmetric numerical junk. Then run a single batched `sqrtm` call on the symmetrized batch.\n\nUse these three raw covariance matrices (note: C2 has a tiny asymmetry between the (2,3) and (3,2) entries):\nC1 = [[1.0, 0.2, 0.1],\n      [0.2, 0.9, 0.3],\n      [0.1, 0.3, 1.1]]\n\nC2 = [[1.5, 0.1, 0.0],\n      [0.1, 1.3, 0.2],\n      [0.0, 0.19, 1.4]]\n\nC3 = [[0.8, 0.05, 0.02],\n      [0.05, 0.9, 0.04],\n      [0.02, 0.04, 0.95]]\n\nCompute the symmetric matrix square root of the symmetrized matrices in one batched call.", "answers": "[{\"name\":\"pyriemann_utils_base_sqrtm\",\"arguments\":{\"C\":[[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,1.1]],[[1.5,0.1,0.0],[0.1,1.3,0.195],[0.0,0.195,1.4]],[[0.8,0.05,0.02],[0.05,0.9,0.04],[0.02,0.04,0.95]]]}}]"}
{"func_name": "pyriemann_utils_covariance_coherence", "func_desc": "pyriemann.utils.covariance.coherence computes the squared coherence between all pairs of channels of a multichannel time series across frequency bins. This function is intended for spectral connectivity analysis of biosignals (for example EEG, MEG, EMG) commonly used in brain–computer interface (BCI) and related pipelines described in the pyRiemann README. It first estimates the cross-spectral density via an FFT-based windowed estimator (cross_spectrum), then builds a 3D array of squared coherence values for each channel pair and frequency. Different coherence variants (\"ordinary\", \"instantaneous\", \"lagged\", \"imaginary\") implement formulas used in connectivity analysis and correspond to the coherence types exposed by pyriemann.estimation.Coherences.", "tools": [{"function": {"description": "pyriemann.utils.covariance.coherence computes the squared coherence between all pairs of channels of a multichannel time series across frequency bins. This function is intended for spectral connectivity analysis of biosignals (for example EEG, MEG, EMG) commonly used in brain–computer interface (BCI) and related pipelines described in the pyRiemann README. It first estimates the cross-spectral density via an FFT-based windowed estimator (cross_spectrum), then builds a 3D array of squared coherence values for each channel pair and frequency. Different coherence variants (\"ordinary\", \"instantaneous\", \"lagged\", \"imaginary\") implement formulas used in connectivity analysis and correspond to the coherence types exposed by pyriemann.estimation.Coherences.\n", "name": "pyriemann_utils_covariance_coherence", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Multi-channel real-valued time series with shape (n_channels, n_times). This is the input biosignal (for example an EEG epoch) from which frequency-domain cross-spectra are estimated. The function does not modify X in-place.", "default": ""}, "window": {"type": "integer", "description": "Length of the FFT window (number of time samples) used for spectral estimation. Defaults to 128. A larger window gives finer frequency resolution but requires longer stationary segments of the signal.", "default": 128}, "overlap": {"type": "float", "description": "Fractional overlap between successive FFT windows, expressed between 0 and 1. Defaults to 0.75 (75% overlap). Higher overlap increases the number of averaged segments and reduces variance of spectral estimates at the cost of more computation.", "default": 0.75}, "fmin": {"type": "float", "nullable": true, "description": "Minimal frequency (in Hz) to include in the returned result. Defaults to None, meaning no explicit lower frequency cutoff is applied; selection is delegated to cross_spectrum. When provided, fmin restricts the frequency bins used and the shape of the returned frequency axis.", "default": null}, "fmax": {"type": "float", "nullable": true, "description": "Maximal frequency (in Hz) to include in the returned result. Defaults to None, meaning no explicit upper frequency cutoff is applied; selection is delegated to cross_spectrum. When provided, fmax restricts the frequency bins used and the shape of the returned frequency axis.", "default": null}, "fs": {"type": "float", "nullable": true, "description": "Sampling frequency of the input time series in Hz. Defaults to None. If fs is provided, the returned frequency axis (freqs) will contain frequencies in Hz and Nyquist/DC handling for the \"lagged\" coherence mode will use fs to detect and exclude DC (0 Hz) and Nyquist (fs/2) bins. If fs is None, the underlying cross_spectrum may return freqs as None; in that case frequency-aware exclusions fall back to excluding the first and last FFT bins by index.", "default": null}, "coh": {"type": "string", "description": "Coherence type to compute. Must be one of \"ordinary\", \"instantaneous\", \"lagged\", or \"imaginary\". Defaults to \"ordinary\".\n- \"ordinary\": squared magnitude coherence, computed as |S|^2 / (psd_i * psd_j), where S is the cross-spectral matrix and psd_i is the power spectral density of channel i. This is the standard magnitude-squared coherence used for general connectivity estimation.\n- \"instantaneous\": uses only the squared real part of S: (Re(S))^2 / (psd_i * psd_j), which emphasizes zero-phase (instantaneous) components.\n- \"imaginary\": uses the squared imaginary part of S: (Im(S))^2 / (psd_i * psd_j), often used to highlight phase-lagged interactions that are robust to volume conduction.\n- \"lagged\": isolates lagged (non-instantaneous) contributions by computing (Im(S))^2 / (psd_i*psd_j - (Re(S))^2). The diagonal real parts are forcibly set to zero for each frequency to prevent division by zero on the diagonal. Note that lagged coherence is not defined for DC and Nyquist bins; these bins are excluded (set to zero) and a warning is emitted. The coherence type meanings follow the conventions used in pyriemann.estimation.Coherences and are commonly applied in EEG/BCI connectivity studies.", "default": "ordinary"}}, "required": ["X", "fs", "window", "fmax", "overlap", "coh", "fmin"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick EMG connectivity sanity-check before a BCI feature-extraction run. I have the same 4-channel EMG snippet sampled at 200 Hz (forearm flexor, extensor, biceps, triceps) with 16 time samples (data matrix below).\n\nInitially I was going to compute squared imaginary coherence with an 8-sample FFT window and 50% overlap, keeping 10–80 Hz. But wait: with 200 Hz sampling and that window length, the frequency resolution is too coarse to even represent 10 Hz (the first non-DC bin lands above it). So instead, keep the window length and overlap unchanged but adjust the lower cutoff to the smallest strictly positive FFT bin available for that window, and keep the upper cutoff at 80 Hz.\n\nCompute the squared imaginary coherence between all channel pairs under those constraints.\n\nX = [\n  [0.12, 0.15, 0.14, 0.10, 0.08, 0.05, 0.02, 0.01, 0.03, 0.06, 0.09, 0.11, 0.13, 0.16, 0.18, 0.17],\n  [0.05, 0.07, 0.06, 0.04, 0.03, 0.02, 0.01, 0.00, 0.01, 0.02, 0.04, 0.05, 0.06, 0.08, 0.09, 0.08],\n  [0.20, 0.18, 0.16, 0.14, 0.12, 0.10, 0.08, 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19, 0.21, 0.22],\n  [0.09, 0.10, 0.11, 0.09, 0.07, 0.06, 0.05, 0.04, 0.05, 0.06, 0.08, 0.09, 0.10, 0.12, 0.13, 0.12]\n]", "answers": "[{\"name\": \"pyriemann_utils_covariance_coherence\", \"arguments\": {\"X\": [[0.12, 0.15, 0.14, 0.1, 0.08, 0.05, 0.02, 0.01, 0.03, 0.06, 0.09, 0.11, 0.13, 0.16, 0.18, 0.17], [0.05, 0.07, 0.06, 0.04, 0.03, 0.02, 0.01, 0.0, 0.01, 0.02, 0.04, 0.05, 0.06, 0.08, 0.09, 0.08], [0.2, 0.18, 0.16, 0.14, 0.12, 0.1, 0.08, 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19, 0.21, 0.22], [0.09, 0.1, 0.11, 0.09, 0.07, 0.06, 0.05, 0.04, 0.05, 0.06, 0.08, 0.09, 0.1, 0.12, 0.13, 0.12]], \"window\": 8, \"overlap\": 0.5, \"fmin\": 25.0, \"fmax\": 80.0, \"fs\": 200.0, \"coh\": \"imaginary\"}}]"}
{"func_name": "pyriemann_utils_covariance_covariance_sch", "func_desc": "Schaefer-Strimmer shrinkage covariance estimator for multichannel time series.\n\nThis function computes a shrunk estimate of the sample covariance matrix using the\nSchaefer-Strimmer method: the shrinkage estimator is a weighted average between\nthe sample covariance matrix (SCM) and a diagonal target matrix formed from the\nSCM diagonal. The optimal shrinkage intensity gamma is estimated using the\nauthors' analytical estimator and then clipped to [0, 1]. The implementation\ncenters the data (subtracts per-channel mean), uses the unbiased scaling\nfactor n_times / (n_times - 1) on the SCM, and returns the final covariance\nmatrix. This estimator is commonly used in domains supported by pyRiemann,\nincluding biosignal processing (EEG, MEG, EMG) for brain–computer interface\n(BCI) pipelines and in remote sensing for covariance estimation over spatial\nwindows; the shrunk covariance improves conditioning and downstream\nRiemannian-geometry-based classification or signal processing.", "tools": [{"function": {"description": "Schaefer-Strimmer shrinkage covariance estimator for multichannel time series.\n\nThis function computes a shrunk estimate of the sample covariance matrix using the\nSchaefer-Strimmer method: the shrinkage estimator is a weighted average between\nthe sample covariance matrix (SCM) and a diagonal target matrix formed from the\nSCM diagonal. The optimal shrinkage intensity gamma is estimated using the\nauthors' analytical estimator and then clipped to [0, 1]. The implementation\ncenters the data (subtracts per-channel mean), uses the unbiased scaling\nfactor n_times / (n_times - 1) on the SCM, and returns the final covariance\nmatrix. This estimator is commonly used in domains supported by pyRiemann,\nincluding biosignal processing (EEG, MEG, EMG) for brain–computer interface\n(BCI) pipelines and in remote sensing for covariance estimation over spatial\nwindows; the shrunk covariance improves conditioning and downstream\nRiemannian-geometry-based classification or signal processing.", "name": "pyriemann_utils_covariance_covariance_sch", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Multi-channel time-series data with shape\n(n_channels, n_times). Each row corresponds to one channel (sensor)\nand each column to a time sample. The array may contain real or\ncomplex values; complex-valued data are supported by the function.\nThe function subtracts the per-channel mean (centering) and computes\nthe sample covariance from the centered data. Practical significance:\nX is the raw multivariate signal from which a covariance matrix is\nestimated for use in Riemannian geometry based pipelines (for example,\nas input to pyRiemann estimators and classifiers). Requirements and\nfailure modes: n_times must be greater than 1 (otherwise divisions by\nn_times - 1 occur); channels with zero variance may produce divisions\nby zero and yield NaNs/infs in intermediate computations; the caller\nshould ensure input is finite and has sufficient time samples per\nchannel.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m building a Riemannian-geometry EEG motor-imagery pipeline and need a well-conditioned covariance per trial using the Schaefer–Strimmer shrinkage estimator. I initially planned to compute the shrinkage covariance on the entire 8-channel recording as-is, but I just noticed my data matrix is **channels × time**, whereas the downstream covariance code I’m using expects the orientation where the **other axis** is treated as variables. So before estimating covariance, transpose the data to the expected orientation (don’t rename channels; just fix the shape).\n\nAlso, this was a short trial and one channel was saturating intermittently (amplifier clipping). Since Schaefer–Strimmer uses a diagonal target built from the SCM diagonal, a clipped channel’s variance would dominate and distort the shrinkage. So: compute the per-channel peak-to-peak amplitude on the *original* channel traces and **drop exactly one channel**: the one with the largest peak-to-peak range (if there’s a tie, drop the channel that appears first in the given ordering). Then apply the Schaefer–Strimmer shrinkage covariance estimator to the remaining channels’ time series (with the correct orientation), using the method’s built-in mean-centering, unbiased SCM scaling, analytical gamma estimation, and clipping gamma to [0, 1].\n\nHere is a small excerpt of the trial (8×10; rows are channels in this order: C3, C4, Cz, F3, F4, P3, P4, Oz; columns are time points). Please return the final shrunk covariance matrix for the retained channels.\n\nX = [[12.1, 11.8, 12.4, 11.9, 12.0, 11.7, 12.3, 11.8, 12.2, 11.9],\n     [7.5, 7.7, 7.6, 7.8, 7.4, 7.5, 7.6, 7.7, 7.5, 7.6],\n     [3.2, 3.3, 3.1, 3.4, 3.2, 3.3, 3.2, 3.1, 3.3, 3.2],\n     [15.0, 14.8, 15.1, 14.9, 15.2, 14.7, 15.0, 14.9, 15.1, 14.8],\n     [9.0, 9.1, 8.9, 9.2, 9.0, 9.1, 8.8, 9.0, 9.1, 9.0],\n     [4.5, 4.6, 4.4, 4.7, 4.5, 4.6, 4.5, 4.4, 4.6, 4.5],\n     [6.0, 6.1, 5.9, 6.2, 6.0, 6.1, 5.9, 6.0, 6.1, 6.0],\n     [2.0, 2.1, 1.9, 2.2, 2.0, 2.1, 1.9, 2.0, 2.1, 2.0]].", "answers": "[{\"name\": \"pyriemann_utils_covariance_covariance_sch\", \"arguments\": {\"X\": [[7.5, 7.7, 7.6, 7.8, 7.4, 7.5, 7.6, 7.7, 7.5, 7.6], [3.2, 3.3, 3.1, 3.4, 3.2, 3.3, 3.2, 3.1, 3.3, 3.2], [15.0, 14.8, 15.1, 14.9, 15.2, 14.7, 15.0, 14.9, 15.1, 14.8], [9.0, 9.1, 8.9, 9.2, 9.0, 9.1, 8.8, 9.0, 9.1, 9.0], [4.5, 4.6, 4.4, 4.7, 4.5, 4.6, 4.5, 4.4, 4.6, 4.5], [6.0, 6.1, 5.9, 6.2, 6.0, 6.1, 5.9, 6.0, 6.1, 6.0], [2.0, 2.1, 1.9, 2.2, 2.0, 2.1, 1.9, 2.0, 2.1, 2.0]]}}]"}
{"func_name": "pyriemann_utils_covariance_get_nondiag_weight", "func_desc": "Compute non-diagonality weights of a set of square matrices.\n\nThis function computes a scalar weight for each square matrix provided in X that quantifies how non-diagonal the matrix is, following Eq(B.1) in [1]. In the context of pyRiemann and covariance-based multivariate analysis (for example EEG/MEG covariance matrices used in brain-computer interface pipelines and approximate joint diagonalization algorithms), these weights measure the relative energy of the off-diagonal elements compared to the diagonal elements and can be used to down-weight matrices that are nearly diagonal during joint-diagonalization or other aggregation steps.", "tools": [{"function": {"description": "Compute non-diagonality weights of a set of square matrices.\n\nThis function computes a scalar weight for each square matrix provided in X that quantifies how non-diagonal the matrix is, following Eq(B.1) in [1]. In the context of pyRiemann and covariance-based multivariate analysis (for example EEG/MEG covariance matrices used in brain-computer interface pipelines and approximate joint diagonalization algorithms), these weights measure the relative energy of the off-diagonal elements compared to the diagonal elements and can be used to down-weight matrices that are nearly diagonal during joint-diagonalization or other aggregation steps.", "name": "pyriemann_utils_covariance_get_nondiag_weight", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Set of square matrices. The last two axes of X must form square matrices of size n x n; any leading axes are treated as batch dimensions and are preserved in the output. Each matrix is treated elementwise (X**2 in the implementation), so X should be a numeric ndarray containing real or complex values as appropriate for the application (e.g., covariance/second-order statistics estimated from multichannel time series). The function expects at least a 2D ndarray; if X has additional leading dimensions they are interpreted as independent matrices.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a pyRiemann AJD run on four 3×3 EEG covariance estimates (one per trial), but I realized one of our preprocessing steps sometimes permutes channels on a subset of trials. I initially planned to compute non-diagonality weights directly, but that would be misleading unless each covariance is first put back into a consistent channel order.\n\nHere are the four trial covariances (3 channels each):\n\nTrial 1:\n[[1.0, 0.2, 0.0],\n [0.2, 0.9, 0.1],\n [0.0, 0.1, 1.1]]\n\nTrial 2:\n[[0.8, 0.01, 0.0],\n [0.01, 1.0, 0.02],\n [0.0, 0.02, 0.9]]\n\nTrial 3:\n[[1.2, 0.4, 0.3],\n [0.4, 1.1, 0.25],\n [0.3, 0.25, 1.3]]\n\nTrial 4:\n[[0.9, 0.0, 0.0],\n [0.0, 0.95, 0.0],\n [0.0, 0.0, 1.05]]\n\nChannel-order issue: only one trial has its first and last channels swapped relative to the others (i.e., apply the same simultaneous row/column swap for those two channels). Identify that trial by this property: after swapping those two channels, the (1,2) and (2,1) entries become exactly zero while preserving symmetry and leaving the diagonal entries unchanged as a multiset.\n\nOnce the correct trial has been re-ordered, stack all four 3×3 matrices into a single 3D array (n_trials×n_channels×n_channels) and compute the non-diagonality weight for each trial using Eq. (B.1) in the usual pyRiemann sense (relative off-diagonal energy vs diagonal energy). Return the four weights in trial order (with the corrected trial still reported in its original trial index position).", "answers": "[{\"name\": \"pyriemann_utils_covariance_get_nondiag_weight\", \"arguments\": {\"X\": [[[1.0, 0.2, 0.0], [0.2, 0.9, 0.1], [0.0, 0.1, 1.1]], [[0.8, 0.01, 0.0], [0.01, 1.0, 0.02], [0.0, 0.02, 0.9]], [[1.2, 0.4, 0.3], [0.4, 1.1, 0.25], [0.3, 0.25, 1.3]], [[0.9, 0.0, 0.0], [0.0, 0.95, 0.0], [0.0, 0.0, 1.05]]]}}]"}
{"func_name": "pyriemann_utils_distance_distance_kullback", "func_desc": "pyriemann.utils.distance.distance_kullback computes the left Kullback-Leibler divergence between two symmetric/Hermitian positive definite (SPD/HPD) matrices.\n\nThis function evaluates the information-theoretic left Kullback-Leibler divergence d(A,B) = 0.5*(tr(B^{-1}A) - n + log(det(B)/det(A))) for one or a batch of SPD/HPD matrices A and B. In the pyRiemann package this distance is commonly used to compare covariance matrices estimated from multichannel biosignals (e.g., EEG, MEG, EMG) for brain-computer interface (BCI) workflows and is also applicable to covariance-like matrices in remote sensing applications. The implementation computes the trace term via solving linear systems (effectively B^{-1}A), obtains signed log-determinants via numpy.linalg.slogdet, and returns the real, non-negative divergence values. The function accepts real-valued SPD and complex-valued Hermitian positive definite (HPD) matrices and supports broadcasting over leading batch dimensions to process multiple matrices at once.", "tools": [{"function": {"description": "pyriemann.utils.distance.distance_kullback computes the left Kullback-Leibler divergence between two symmetric/Hermitian positive definite (SPD/HPD) matrices.\n\nThis function evaluates the information-theoretic left Kullback-Leibler divergence d(A,B) = 0.5*(tr(B^{-1}A) - n + log(det(B)/det(A))) for one or a batch of SPD/HPD matrices A and B. In the pyRiemann package this distance is commonly used to compare covariance matrices estimated from multichannel biosignals (e.g., EEG, MEG, EMG) for brain-computer interface (BCI) workflows and is also applicable to covariance-like matrices in remote sensing applications. The implementation computes the trace term via solving linear systems (effectively B^{-1}A), obtains signed log-determinants via numpy.linalg.slogdet, and returns the real, non-negative divergence values. The function accepts real-valued SPD and complex-valued Hermitian positive definite (HPD) matrices and supports broadcasting over leading batch dimensions to process multiple matrices at once.", "name": "pyriemann_utils_distance_distance_kullback", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First input SPD/HPD matrix or stack of matrices with shape (..., n, n). In BCI and covariance-estimation contexts, A typically represents an estimated covariance matrix for one epoch, trial, or spatial window. The array must be at least 2-D, square on the last two axes, and contain SPD (for real data) or HPD (for complex data) matrices. The function will not modify A in-place.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second input SPD/HPD matrix or stack of matrices with the same shape contractibility as A: (..., n, n). B commonly represents a reference covariance matrix (for example a class prototype or a session/subject reference) against which A is compared. B must be square on the last two axes and positive definite so that inversion is possible; the function uses linear solves with B rather than explicit matrix inversion for numerical stability.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default), return the Kullback-Leibler divergence d(A, B) as defined above. If True, return the squared divergence (d(A, B)**2). The default behavior (squared=False) preserves the standard divergence scale used in many classification and distance-based algorithms in pyRiemann; setting squared=True may be useful where a squared-distance form is required by downstream algorithms.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m benchmarking a BCI covariance-matching step and need the left Kullback–Leibler divergence d(A,B)=0.5*(tr(B^{-1}A) − n + log(det(B)/det(A))) using pyriemann.utils.distance.distance_kullback. Start with A as the trial covariance [[1.2, 0.3, 0.1], [0.3, 0.9, 0.05], [0.1, 0.05, 0.7]] and B as the reference covariance [[1.0, 0.25, 0.0], [0.25, 1.1, 0.02], [0.0, 0.02, 0.8]]. Wait—our pipeline’s label convention means the reference covariance must be the one that has the larger determinant (more overall power/volume), and the other one should be treated as the trial. So compute the left KL divergence with the matrices assigned accordingly (trial as the smaller-determinant one, reference as the larger-determinant one), and return the standard (non-squared) divergence.", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_kullback\",\"arguments\":{\"A\":[[1.2,0.3,0.1],[0.3,0.9,0.05],[0.1,0.05,0.7]],\"B\":[[1.0,0.25,0.0],[0.25,1.1,0.02],[0.0,0.02,0.8]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_kullback_right", "func_desc": "Summary:\nCompute the right Kullback-Leibler divergence between two covariance matrices. This function is a thin wrapper that implements the \"right\" version of the Kullback-Leibler divergence used in pyRiemann for comparing symmetric (or Hermitian) positive definite (SPD/HPD) covariance matrices commonly estimated from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data. Concretely, it calls the underlying distance_kullback function with its arguments swapped so that the divergence is evaluated as D_right(A, B) = distance_kullback(B, A, squared=squared). The returned value quantifies how much the distribution characterized by A diverges from that characterized by B in the context of Riemannian geometry on SPD/HPD matrices and can be used as a dissimilarity measure in classification, clustering, or transfer-learning pipelines described in the project README.", "tools": [{"function": {"description": "Summary:\nCompute the right Kullback-Leibler divergence between two covariance matrices. This function is a thin wrapper that implements the \"right\" version of the Kullback-Leibler divergence used in pyRiemann for comparing symmetric (or Hermitian) positive definite (SPD/HPD) covariance matrices commonly estimated from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data. Concretely, it calls the underlying distance_kullback function with its arguments swapped so that the divergence is evaluated as D_right(A, B) = distance_kullback(B, A, squared=squared). The returned value quantifies how much the distribution characterized by A diverges from that characterized by B in the context of Riemannian geometry on SPD/HPD matrices and can be used as a dissimilarity measure in classification, clustering, or transfer-learning pipelines described in the project README.", "name": "pyriemann_utils_distance_distance_kullback_right", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "any"}, "description": "Left-hand matrix argument of the right Kullback-Leibler divergence. Expected to be a square numpy.ndarray representing a covariance matrix (symmetric positive definite for real-valued data or Hermitian positive definite for complex-valued data). In typical pyRiemann usage this is an estimated covariance matrix of shape (n_channels, n_channels). The function treats A as the first argument of the right divergence D_right(A, B) which is implemented by calling distance_kullback(B, A, ...).", "default": ""}, "B": {"type": "array", "items": {"type": "any"}, "description": "Right-hand matrix argument of the right Kullback-Leibler divergence. Expected to be a square numpy.ndarray with the same shape and type constraints as A (covariance / SPD / HPD matrix). In practical pipelines B often represents a reference or template covariance matrix (for example a class mean in Riemannian classification) against which A is compared.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default), return the standard Kullback-Leibler divergence value (a non-negative scalar). If True, return the squared form of the divergence if the underlying distance_kullback implementation supports a squared option. This flag is forwarded unchanged to the underlying distance_kullback call. Default behavior is squared=False.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an EEG transfer-learning pipeline where I compare a target-subject covariance to a source-template covariance using pyRiemann’s *right* KL divergence wrapper (the one that computes the KL with arguments swapped relative to the standard function). I was going to request the *raw* divergence, but I realized our downstream QC step expects the **squared** form when the matrices are small (here: 3 channels) to keep the scale consistent with the other squared Riemannian distances we log. \n\nPlease compute the right Kullback–Leibler divergence D_right(A, B) for these 3×3 SPD covariances, using whatever squared/non-squared setting is consistent with that QC expectation, and return the resulting scalar.\n\nA = [[1.20, 0.10, 0.05],[0.10, 0.90, 0.02],[0.05, 0.02, 0.70]]\nB = [[1.00, 0.08, 0.03],[0.08, 1.10, 0.04],[0.03, 0.04, 0.80]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_kullback_right\",\"arguments\":{\"A\":[[1.2,0.1,0.05],[0.1,0.9,0.02],[0.05,0.02,0.7]],\"B\":[[1.0,0.08,0.03],[0.08,1.1,0.04],[0.03,0.04,0.8]],\"squared\":true}}]"}
{"func_name": "pyriemann_utils_distance_distance_kullback_sym", "func_desc": "pyriemann.utils.distance.distance_kullback_sym computes the symmetrized Kullback-Leibler divergence (also known as the Jeffreys divergence) between two symmetric/Hermitian positive definite (SPD/HPD) matrices. This function is used in pyRiemann to quantify dissimilarity between covariance matrices arising from multivariate biosignals (e.g., EEG, MEG, EMG) and in other applications such as hyperspectral or SAR image processing where covariance/HPD matrices represent local statistics.", "tools": [{"function": {"description": "pyriemann.utils.distance.distance_kullback_sym computes the symmetrized Kullback-Leibler divergence (also known as the Jeffreys divergence) between two symmetric/Hermitian positive definite (SPD/HPD) matrices. This function is used in pyRiemann to quantify dissimilarity between covariance matrices arising from multivariate biosignals (e.g., EEG, MEG, EMG) and in other applications such as hyperspectral or SAR image processing where covariance/HPD matrices represent local statistics.\n", "name": "pyriemann_utils_distance_distance_kullback_sym", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "any"}, "description": "First SPD/HPD matrix or batch of matrices with shape (..., n, n). Each last-two-dimensional block must be a square symmetric (real) or Hermitian (complex) positive definite matrix. In the pyRiemann context, A typically represents an estimated covariance matrix for one epoch, window, or spatial patch. The function requires at least a 2-D array; higher-dimensional arrays are treated as batches of matrices indexed by the leading dimensions. If A is not square or its trailing dimensions do not match B, a ValueError is raised.", "default": ""}, "B": {"type": "array", "items": {"type": "any"}, "description": "Second SPD/HPD matrix or batch of matrices with the same shape as A, i.e., (..., n, n). B represents the comparison target (for instance, a reference covariance matrix, class mean, or other covariance estimate). A and B must have identical shapes; otherwise the function will raise an error. Both A and B must contain positive definite matrices; non-positive eigenvalues can cause numerical linear algebra errors (e.g., numpy.linalg.LinAlgError) from underlying computations.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default), return the symmetrized Kullback-Leibler divergence d between A and B. If True, return the squared divergence d**2. The squared option was added to provide compatibility with metric-based workflows that require squared distances (versionadded: 0.5). There are no side effects from toggling this flag; it only changes the returned numeric value.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m comparing two 3-channel EEG covariance matrices from separate 2-second epochs using pyRiemann’s symmetrized KL (Jeffreys) divergence. Initially I was going to pass them straight into `pyriemann.utils.distance.distance_kullback_sym` with `squared=False`, but I just realized one of the matrices is contaminated by a tiny rounding/serialization artifact: its (2,3) and (3,2) entries don’t exactly match even though it should be symmetric. Please first replace that inconsistent off-diagonal pair by their average so the matrix is exactly symmetric (leave all other entries unchanged), then compute the Jeffreys divergence between the cleaned matrix and the other matrix, returning the non-squared value.\n\nMatrix A (already symmetric) = [[1.20, 0.10, 0.05],[0.10, 0.90, 0.02],[0.05, 0.02, 1.10]]\nMatrix B (has the artifact) = [[1.00, 0.08, 0.03],[0.08, 1.05, 0.0402],[0.03, 0.04, 0.95]].", "answers": "[{\"name\": \"pyriemann_utils_distance_distance_kullback_sym\", \"arguments\": {\"A\": [[1.2, 0.1, 0.05], [0.1, 0.9, 0.02], [0.05, 0.02, 1.1]], \"B\": [[1.0, 0.08, 0.03], [0.08, 1.05, 0.0401], [0.03, 0.0401, 0.95]], \"squared\": false}}]"}
{"func_name": "pyriemann_utils_distance_distance_logchol", "func_desc": "Log-Cholesky distance between two symmetric/Hermitian positive definite (SPD/HPD)\nmatrices.\n\nThis function computes the Log-Cholesky distance used in pyRiemann to compare SPD/HPD\nmatrices such as covariance matrices estimated from multichannel biosignals (EEG,\nMEG, EMG) or from remote sensing data. The distance is computed by taking the\nCholesky decomposition of each input matrix, extracting the strictly lower\ntriangular entries and the diagonal entries, applying a natural logarithm to the\ndiagonal entries, and then computing the Euclidean norm of the concatenated\ndifferences. Concretely, for matrices A and B, if L_A and L_B are their (lower)\nCholesky factors, the squared distance equals the squared Frobenius norm of the\ndifference of the strictly lower triangular parts plus the squared Frobenius\nnorm of the difference of the elementwise logarithms of the diagonals. Returning\nthe non-squared distance takes the square root of this sum. This representation\nis useful in Riemannian processing pipelines (e.g., covariance estimation and\nclassification in BCI) because it provides a vectorized representation of SPD/HPD\nmatrices that can be used with standard Euclidean methods.", "tools": [{"function": {"description": "Log-Cholesky distance between two symmetric/Hermitian positive definite (SPD/HPD)\nmatrices.\n\nThis function computes the Log-Cholesky distance used in pyRiemann to compare SPD/HPD\nmatrices such as covariance matrices estimated from multichannel biosignals (EEG,\nMEG, EMG) or from remote sensing data. The distance is computed by taking the\nCholesky decomposition of each input matrix, extracting the strictly lower\ntriangular entries and the diagonal entries, applying a natural logarithm to the\ndiagonal entries, and then computing the Euclidean norm of the concatenated\ndifferences. Concretely, for matrices A and B, if L_A and L_B are their (lower)\nCholesky factors, the squared distance equals the squared Frobenius norm of the\ndifference of the strictly lower triangular parts plus the squared Frobenius\nnorm of the difference of the elementwise logarithms of the diagonals. Returning\nthe non-squared distance takes the square root of this sum. This representation\nis useful in Riemannian processing pipelines (e.g., covariance estimation and\nclassification in BCI) because it provides a vectorized representation of SPD/HPD\nmatrices that can be used with standard Euclidean methods.", "name": "pyriemann_utils_distance_distance_logchol", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First input SPD/HPD matrix or batch of matrices with\nshape (..., n, n). Must be at least 2D. In pyRiemann typical usage is to\npass covariance matrices estimated from multichannel time series (for\nexample, shape n_epochs x n_channels x n_channels). The function treats\nthe last two dimensions as the square matrix dimensions and computes the\nCholesky decomposition along those dimensions.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second input SPD/HPD matrix or batch of matrices with\nthe same shape as A: (..., n, n). The function computes the pairwise\nLog-Cholesky distance between corresponding matrices in A and B along\nthe leading dimensions. A and B must be compatible in shape; broadcasting\nis not performed by this function.", "default": ""}, "squared": {"type": "boolean", "description": "Default False. If True, return the squared Log-Cholesky\ndistance (the sum of squared differences of the strictly lower triangular\nparts and the log-diagonals) to avoid the square-root operation when a\nsquared metric is desired for numerical or performance reasons. If False,\nreturn the non-squared distance (the square root of that sum).", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a drift-detection step in an EEG BCI pipeline. I planned to compute the Log‑Cholesky distance (non‑squared) between two 3×3 covariance estimates from consecutive 1‑second epochs, but then I noticed the second matrix came from a covariance routine that sometimes returns the **channel order permuted** (same channels, different ordering) if the upstream montage metadata is missing. \n\nSo: first, re-order the channels of the second covariance so it matches the first epoch’s channel order (the permutation is the one that moves the channel with the **largest variance** in epoch 2 into the **last** position, and keeps the remaining two channels in their original relative order). After that alignment, compute the Log‑Cholesky distance (not squared) between epoch 1 covariance A and the aligned epoch 2 covariance.\n\nEpoch 1 covariance A = [[2.0, 0.3, 0.1],[0.3, 1.5, 0.2],[0.1, 0.2, 1.2]]\nEpoch 2 covariance (possibly permuted) B = [[1.8, 0.25, 0.05],[0.25, 1.6, 0.15],[0.05, 0.15, 1.1]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_logchol\",\"arguments\":{\"A\":[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]],\"B\":[[1.6,0.15,0.25],[0.15,1.1,0.05],[0.25,0.05,1.8]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_logdet", "func_desc": "Compute the log-det (Burg) distance between two symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nThis function implements the log-det distance d(A, B) = sqrt(log det((A + B) / 2) - 0.5 * log det(A B)) widely used in the Riemannian geometry of SPD/HPD matrices. In the pyRiemann library this distance is used to compare covariance matrices estimated from multivariate biosignals (e.g., EEG, MEG, EMG) in brain–computer interface and related applications, and forms a building block for classifiers and pipelines (for example, Minimum Distance to Mean (MDM) and other Riemannian methods). The implementation is numerically stable: it uses numpy.linalg.slogdet to compute logarithms of determinants and clips negative squared-distance values caused by floating-point rounding to zero before taking the square root.", "tools": [{"function": {"description": "Compute the log-det (Burg) distance between two symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nThis function implements the log-det distance d(A, B) = sqrt(log det((A + B) / 2) - 0.5 * log det(A B)) widely used in the Riemannian geometry of SPD/HPD matrices. In the pyRiemann library this distance is used to compare covariance matrices estimated from multivariate biosignals (e.g., EEG, MEG, EMG) in brain–computer interface and related applications, and forms a building block for classifiers and pipelines (for example, Minimum Distance to Mean (MDM) and other Riemannian methods). The implementation is numerically stable: it uses numpy.linalg.slogdet to compute logarithms of determinants and clips negative squared-distance values caused by floating-point rounding to zero before taking the square root.", "name": "pyriemann_utils_distance_distance_logdet", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "any"}, "description": "First SPD/HPD matrices. Expected to be at least 2D with shape (..., n, n), where the last two dimensions form a square positive-definite matrix (real symmetric for SPD or complex Hermitian for HPD). In the pyRiemann context A typically contains covariance matrices estimated from multichannel time series (e.g., shape n_epochs x n_channels x n_channels). The function calls _check_inputs(A, B) to validate shapes and basic compatibility; if the inputs are incompatible this preliminary check may raise an error.", "default": ""}, "B": {"type": "array", "items": {"type": "any"}, "description": "Second SPD/HPD matrices. Must have the same shape as A: (..., n, n). B plays the role of the comparison matrix to A in distance computations used by Riemannian classifiers and pipelines. As with A, entries are expected to define positive-definite matrices; non-positive-definite inputs can lead to invalid results (see Failure modes).", "default": ""}, "squared": {"type": "boolean", "description": "Whether to return the squared log-det distance. Default is False. If False (the default), the function returns the nonnegative distance d = sqrt(d2). If True, the function returns the nonnegative squared distance d2 = log det((A + B) / 2) - 0.5 * log det(A B). This option was added to allow callers to avoid an extra square-root operation when they need squared distances directly (versionadded: 0.5).", "default": false}}, "required": ["A", "B"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a Riemannian-geometry EEG pipeline where the classifier behaves oddly because I may be mixing covariance estimators. I initially planned to compute the non-squared log-det (Burg) distance directly between the two 3×3 covariance matrices A=[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.1]] and B=[[1.0,0.05,0.0],[0.05,1.3,0.1],[0.0,0.1,0.8]].\n\nWait—those were computed on different trial counts, so to make them comparable I need to shrink both toward the identity before computing the distance. Use the more conservative shrinkage weight (the one that pulls more strongly toward the identity) of 0.25, i.e., form A’=(1−α)A+αI and B’=(1−α)B+αI with that α, then compute the (non-squared) log-det (Burg) distance between A’ and B’ (using the numerically stable slogdet-based computation and clipping negative squared-distance to zero as usual).", "answers": "[{\"name\": \"pyriemann_utils_distance_distance_logdet\", \"arguments\": {\"A\": [[1.15, 0.07500000000000001, 0.0], [0.07500000000000001, 0.925, 0.15000000000000002], [0.0, 0.15000000000000002, 1.0750000000000002]], \"B\": [[1.0, 0.037500000000000006, 0.0], [0.037500000000000006, 1.225, 0.07500000000000001], [0.0, 0.07500000000000001, 0.8500000000000001]]}}]"}
{"func_name": "pyriemann_utils_distance_distance_mahalanobis", "func_desc": "pyriemann.utils.distance.distance_mahalanobis\nCompute the Mahalanobis distance between column vectors and a multivariate Gaussian distribution.\n\nThis function computes the Mahalanobis distance between each column vector x in X and a multivariate Gaussian distribution N(mu, C) defined by mean vector mu and covariance matrix C. The computation follows d(x, N(mu, C)) = sqrt((x - mu)^H C^{-1} (x - mu)). It is used in pyRiemann for comparing feature vectors (for example, signal or covariance-derived features from EEG/MEG/EMG in BCI pipelines or patch vectors in remote sensing) to a Gaussian model, which is a common operation in classification, outlier detection, and distance-based algorithms that operate on covariance or multivariate data.", "tools": [{"function": {"description": "pyriemann.utils.distance.distance_mahalanobis\nCompute the Mahalanobis distance between column vectors and a multivariate Gaussian distribution.\n\nThis function computes the Mahalanobis distance between each column vector x in X and a multivariate Gaussian distribution N(mu, C) defined by mean vector mu and covariance matrix C. The computation follows d(x, N(mu, C)) = sqrt((x - mu)^H C^{-1} (x - mu)). It is used in pyRiemann for comparing feature vectors (for example, signal or covariance-derived features from EEG/MEG/EMG in BCI pipelines or patch vectors in remote sensing) to a Gaussian model, which is a common operation in classification, outlier detection, and distance-based algorithms that operate on covariance or multivariate data.", "name": "pyriemann_utils_distance_distance_mahalanobis", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Vectors provided as a 2-D array of shape (n, n_vectors), where n is the dimensionality of each vector and n_vectors is the number of column vectors to compare to the distribution. Columns of X are treated as the individual vectors x. The array may be real- or complex-valued as in the mathematical formulation in the codebase. Note: if mean is not None, X is modified in-place via X -= mean; pass a copy of X if you need to preserve the original data.", "default": ""}, "cov": {"type": "array", "items": {"type": "float"}, "description": "Covariance matrix C of the multivariate Gaussian distribution, shaped (n, n). In the pyRiemann context this is typically an SPD (symmetric positive definite) matrix for real data or an HPD (Hermitian positive definite) matrix for complex data. cov must be invertible; if cov is singular, nearly singular, or not positive definite/Hermitian when expected, the internal matrix inverse square-root computation will fail or produce invalid results.", "default": ""}, "mean": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Mean vector mu of the multivariate Gaussian distribution, shaped (n, 1). If provided, this mean is subtracted from each column of X before distance computation, implementing the (x - mu) term. If None (default), the distribution is considered centered at zero and X is used as-is. Providing a mean with an incorrect shape (not matching n) will raise a broadcasting/shape error at runtime.", "default": null}, "squared": {"type": "boolean", "description": "If False (default), return the Euclidean Mahalanobis distances d = sqrt((x-mu)^H C^{-1} (x-mu)). If True, return the squared Mahalanobis distances d^2 = (x-mu)^H C^{-1} (x-mu) without taking the square root. This option can avoid a square root when downstream code only needs squared distances. This parameter was introduced in version 0.5 of the package.", "default": false}}, "required": ["X", "cov", "mean", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m doing EEG-based BCI feature QC and want to flag anomalous trials using Mahalanobis distance to a baseline Gaussian model. I have three 3D feature vectors (columns = trials) and a baseline mean/covariance estimated from clean resting data. \n\nInitially I was going to compute the plain (non-squared) Mahalanobis distances to the baseline. But I just realized the next step in my pipeline thresholds on the *energy-like* quadratic form (it compares distances additively across windows), so I need the version of the distance that removes the outer square-root.\n\nCompute the Mahalanobis distance of each column vector in X to N(mu, C) using the setting appropriate for this quadratic-form output.\n\nUse:\n- X (shape 3×3, columns are trials):\n  [[0.02, -0.01, 0.05],\n   [0.01,  0.00, 0.03],\n   [-0.02, 0.02, 0.04]]\n- mu (shape 3×1):\n  [[0.01],\n   [0.005],\n   [-0.015]]\n- C (shape 3×3):\n  [[0.0004, 0.0001, -0.00005],\n   [0.0001, 0.0003,  0.00002],\n   [-0.00005, 0.00002, 0.0005]]\n\nReturn the per-trial distances in the form produced by that quadratic-form setting (one value per column).", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_mahalanobis\",\"arguments\":{\"X\":[[0.02,-0.01,0.05],[0.01,0.0,0.03],[-0.02,0.02,0.04]],\"cov\":[[0.0004,0.0001,-5e-05],[0.0001,0.0003,2e-05],[-5e-05,2e-05,0.0005]],\"mean\":[[0.01],[0.005],[-0.015]],\"squared\":true}}]"}
{"func_name": "pyriemann_utils_distance_distance_wasserstein", "func_desc": "pyriemann.utils.distance.distance_wasserstein computes the Wasserstein (Bures) distance between two symmetric/Hermitian positive semidefinite (SPSD/HPSD) matrices. In the pyRiemann context this function is used to measure dissimilarity between covariance or covariance-like matrices (for example, covariance matrices estimated from EEG/MEG/EMG epochs in brain–computer interface pipelines or covariance estimates in remote sensing applications) and can be employed in classification, clustering, or pipeline evaluation where Riemannian-aware distances are required.", "tools": [{"function": {"description": "pyriemann.utils.distance.distance_wasserstein computes the Wasserstein (Bures) distance between two symmetric/Hermitian positive semidefinite (SPSD/HPSD) matrices. In the pyRiemann context this function is used to measure dissimilarity between covariance or covariance-like matrices (for example, covariance matrices estimated from EEG/MEG/EMG epochs in brain–computer interface pipelines or covariance estimates in remote sensing applications) and can be employed in classification, clustering, or pipeline evaluation where Riemannian-aware distances are required.\n", "name": "pyriemann_utils_distance_distance_wasserstein", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "any"}, "description": "First SPSD/HPSD matrices, at least 2-D ndarray with shape (..., n, n). Each trailing pair of dimensions represents one n-by-n SPSD (real symmetric) or HPSD (complex Hermitian) matrix. In practical pyRiemann use this typically contains covariance matrices computed per epoch or spatial window; leading dimensions allow broadcasting and batch computation of distances across multiple matrices.", "default": ""}, "B": {"type": "array", "items": {"type": "any"}, "description": "Second SPSD/HPSD matrices, same dtype and shape constraints as A and with identical trailing matrix dimensions (..., n, n). B plays the role of the second element in each pairwise comparison; when computing distances between corresponding matrices in A and B they must align on the leading shapes or be broadcastable according to numpy broadcasting rules that are accepted by the internal input checker.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default) the function returns the Wasserstein distance d(A,B) = sqrt(tr(A + B - 2 (B^{1/2} A B^{1/2})^{1/2})). If True the function returns the squared distance d^2 = tr(A + B - 2 (B^{1/2} A B^{1/2})^{1/2}) without taking the square root. The squared option was added in pyRiemann version 0.5 and can be useful to avoid an extra square-root operation when squared distances are needed for optimization or variance calculations.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "I’m comparing two EEG-epoch covariance-like 3×3 SPSD matrices using the Wasserstein/Bures distance via `pyriemann.utils.distance.distance_wasserstein`. Initially I was going to compute the non-squared distance directly, but I just realized my upstream covariance estimator outputs values in µV² and the downstream report expects the distance to be invariant to a global unit rescaling. So first, rescale BOTH matrices by the SAME positive scalar so that the larger of their traces becomes exactly 1 (leave the smaller one scaled by that same factor too). Then compute the Wasserstein (Bures) distance between the two rescaled matrices (again, not squared).\n\nA = [[1.20, 0.10, 0.00], [0.10, 0.80, 0.05], [0.00, 0.05, 0.50]]\nB = [[1.00, 0.05, 0.02], [0.05, 0.90, 0.00], [0.02, 0.00, 0.60]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_wasserstein\",\"arguments\":{\"A\":[[0.48,0.04,0.0],[0.04,0.32,0.02],[0.0,0.02,0.2]],\"B\":[[0.4,0.02,0.008],[0.02,0.36,0.0],[0.008,0.0,0.24]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_pairwise_distance", "func_desc": "pyriemann.utils.distance.pairwise_distance computes the full pairwise distance matrix between two sets of square matrices (typically covariance matrices used in pyRiemann for biosignals such as EEG/MEG/EMG in BCI or remote-sensing applications). It supports the Riemannian metric by default (\"riemann\") and several other metrics implemented in pyriemann.utils.distance, and returns either the matrix of distances between all pairs in X (when Y is None) or between each element of X and each element of Y.", "tools": [{"function": {"description": "pyriemann.utils.distance.pairwise_distance computes the full pairwise distance matrix between two sets of square matrices (typically covariance matrices used in pyRiemann for biosignals such as EEG/MEG/EMG in BCI or remote-sensing applications). It supports the Riemannian metric by default (\"riemann\") and several other metrics implemented in pyriemann.utils.distance, and returns either the matrix of distances between all pairs in X (when Y is None) or between each element of X and each element of Y.\n", "name": "pyriemann_utils_distance_pairwise_distance", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Array of input matrices with shape (n_matrices_X, n, n). Each entry X[i] is expected to be a square matrix (for pyRiemann use, typically a symmetric positive definite (SPD) matrix for real-valued data or Hermitian positive definite (HPD) for complex-valued data) representing an estimated covariance or similar. This argument supplies the first set of matrices for which pairwise distances will be computed. The function assumes X is a 3-D NumPy array; malformed shapes (not 3-D or not square on the last two axes) will lead to errors from internal checks or downstream distance computations.", "default": ""}, "Y": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional second array of matrices with shape (n_matrices_Y, n, n). Default is None, in which case Y is set to X to compute intra-set distances. When provided, Y must contain matrices with the same square dimension n as X. Passing None triggers symmetric optimization for metrics that are symmetric: when Y is None and the metric is symmetric (all supported metrics except some Kullback variants), the function computes only the upper-triangular block and mirrors it to produce a symmetric distance matrix. If Y is provided but has incompatible shape or dimension, a ValueError or an error from the underlying distance implementation will be raised.", "default": null}, "metric": {"type": "string", "description": "Metric identifier selecting the distance to use. Default is \"riemann\". Supported metrics implemented or dispatched explicitly by this function include \"riemann\", \"euclid\", \"harmonic\", \"logchol\", \"logeuclid\", and the Kullback variants used by pyRiemann (\"kullback\", \"kullback_right\") as documented in pyriemann.utils.distance.distance. The chosen metric determines the mathematical formula used to compare two matrices (for example, Riemannian distance between SPD matrices is commonly used for covariance matrices in BCI). If an unknown or unsupported metric is passed, the underlying distance dispatcher will raise an error (typically ValueError).", "default": "riemann"}, "squared": {"type": "boolean", "description": "If False (default), return the actual distances. If True, return squared distances. This flag is passed to the underlying distance computations; note that for some metrics the squared/non-squared semantics correspond to returning the squared value of the metric defined by pyRiemann. This parameter was added in pyRiemann v0.5.", "default": false}}, "required": ["X", "metric", "squared", "Y"], "type": "any"}}, "type": "function"}], "query": "I’m comparing covariance structure between two EEG conditions using pyRiemann. I have 3 matrices from resting-state (X) and 2 from motor imagery (Y), all intended to be 3×3 SPD. Compute the full pairwise distance matrix between X and Y.\n\nInitially I planned to use the log-Euclidean metric and square the distances, but then I realized our downstream classifier expects the metric that *reduces to the plain Euclidean distance after mapping each SPD matrix through the matrix logarithm* (so keep that mapping), yet it expects the distances in the *unsquared* form because it applies its own squaring internally. Please use that metric and return the non-squared distances.\n\nUse these matrices:\nX = [\n  [[1.2, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 1.1]],\n  [[0.8, -0.05, 0.02], [-0.05, 1.3, 0.04], [0.02, 0.04, 0.7]],\n  [[1.5, 0.2, -0.1], [0.2, 1.1, 0.0], [-0.1, 0.0, 0.95]]\n]\nY = [\n  [[1.0, 0.08, 0.03], [0.08, 1.4, -0.02], [0.03, -0.02, 0.85]],\n  [[0.9, 0.0, 0.06], [0.0, 1.0, 0.01], [0.06, 0.01, 1.25]]\n].", "answers": "[{\"name\": \"pyriemann_utils_distance_pairwise_distance\", \"arguments\": {\"X\": [[[1.2, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 1.1]], [[0.8, -0.05, 0.02], [-0.05, 1.3, 0.04], [0.02, 0.04, 0.7]], [[1.5, 0.2, -0.1], [0.2, 1.1, 0.0], [-0.1, 0.0, 0.95]]], \"Y\": [[[1.0, 0.08, 0.03], [0.08, 1.4, -0.02], [0.03, -0.02, 0.85]], [[0.9, 0.0, 0.06], [0.0, 1.0, 0.01], [0.06, 0.01, 1.25]]], \"metric\": \"logeuclid\", \"squared\": false}}]"}
{"func_name": "pyriemann_utils_geodesic_geodesic", "func_desc": "pyriemann.utils.geodesic.geodesic: Compute the matrix located at a given position along the geodesic between two square matrices according to a chosen metric.\n\nCompute the matrix at position alpha on the geodesic connecting matrices A and B according to a specified metric. This function is used in pyRiemann for interpolating or extrapolating between matrices that represent multivariate descriptors such as covariance matrices arising in biosignal processing (EEG/MEG/EMG) and remote sensing applications. The common practical use is to interpolate between symmetric/Hermitian positive definite (SPD/HPD) matrices on a Riemannian manifold (for example, when working with covariance matrices in brain-computer interface pipelines), but other metrics are available and a user-provided callable can be used to implement custom geodesics.", "tools": [{"function": {"description": "pyriemann.utils.geodesic.geodesic: Compute the matrix located at a given position along the geodesic between two square matrices according to a chosen metric.\n\nCompute the matrix at position alpha on the geodesic connecting matrices A and B according to a specified metric. This function is used in pyRiemann for interpolating or extrapolating between matrices that represent multivariate descriptors such as covariance matrices arising in biosignal processing (EEG/MEG/EMG) and remote sensing applications. The common practical use is to interpolate between symmetric/Hermitian positive definite (SPD/HPD) matrices on a Riemannian manifold (for example, when working with covariance matrices in brain-computer interface pipelines), but other metrics are available and a user-provided callable can be used to implement custom geodesics.", "name": "pyriemann_utils_geodesic_geodesic", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First matrices, with shape (..., n, n). Each trailing two dimensions must form a square matrix. In the pyRiemann context, these are typically covariance or HPD matrices estimated from multichannel time series; many metrics require these matrices to be symmetric/Hermitian positive definite. Supplying arrays with incompatible shapes (non-square trailing dimensions or mismatched batch shapes with B) will lead to an error when the chosen metric implementation is applied.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second matrices, with shape (..., n, n). B must be conformable with A so that a geodesic between corresponding matrices can be computed; typically A and B have identical batch shapes and the same n. As with A, values are expected to be appropriate for the chosen metric (for example, SPD for the \"riemann\" metric).", "default": ""}, "alpha": {"type": "float", "description": "Position on the geodesic. Common usage is alpha in the interval [0, 1] where alpha == 0 returns A and alpha == 1 returns B; intermediate values give interpolated matrices. Values outside [0, 1] constitute extrapolation and may be supported or may produce invalid results depending on the chosen metric implementation.", "default": ""}, "metric": {"type": "string", "description": "Metric used to compute the geodesic, default \"riemann\". Accepted string keys (resolved via an internal dispatcher) include \"euclid\", \"logchol\", \"logeuclid\", \"riemann\", and \"wasserstein\". Alternatively, metric can be a callable implementing the geodesic operation; in that case the callable is expected to accept the same arguments (A, B, alpha) and return a numpy.ndarray of shape (..., n, n). If a string is provided, an internal check_function resolves it to the corresponding implementation; providing an unknown string will raise an error from that resolution step. Different metrics have different domain requirements and numerical behaviors: for example, Riemannian and log-based metrics assume SPD/HPD matrices and will raise or produce invalid outputs if inputs violate those assumptions.", "default": "riemann"}}, "required": ["A", "B", "alpha"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an EEG covariance interpolation step (3 channels) before feeding features into an SPD-manifold classifier. I first planned to take the exact midpoint between the resting-state covariance A and task-state covariance B on the default Riemannian geodesic. \n\nBut I realized our preprocessing labels are slightly time-shifted, so the “task” covariance is probably too dominant if I pick the midpoint. Instead, I want the point that is closer to the resting-state end by a margin equal to the absolute difference between the off-diagonal (1,2) entries of A and B (using 0-based indexing), measured as a fraction of the full A→B path length. In other words: start from the halfway point, then shift toward A by that fraction. Use the default metric.\n\nMatrix A (resting-state covariance):\n[[1.0, 0.2, 0.1],\n [0.2, 0.9, 0.3],\n [0.1, 0.3, 1.1]]\n\nMatrix B (task-state covariance):\n[[1.3, 0.1, 0.0],\n [0.1, 1.0, 0.4],\n [0.0, 0.4, 1.4]]\n\nCompute the interpolated SPD matrix at that adjusted position along the geodesic.", "answers": "[{\"name\": \"pyriemann_utils_geodesic_geodesic\", \"arguments\": {\"A\": [[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 1.1]], \"B\": [[1.3, 0.1, 0.0], [0.1, 1.0, 0.4], [0.0, 0.4, 1.4]], \"alpha\": 0.4}}]"}
{"func_name": "pyriemann_utils_geodesic_geodesic_logchol", "func_desc": "pyriemann.utils.geodesic.geodesic_logchol computes the Log-Cholesky geodesic point between two symmetric/Hermitian positive definite matrices and reconstructs the corresponding SPD/HPD matrix at a given position alpha on that geodesic. This function implements the Log-Cholesky interpolation introduced in Z. Lin (2019) and is used in pyRiemann workflows to interpolate or extrapolate covariance/HPD matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG/MEG in BCI applications) in a manner consistent with Riemannian geometry.", "tools": [{"function": {"description": "pyriemann.utils.geodesic.geodesic_logchol computes the Log-Cholesky geodesic point between two symmetric/Hermitian positive definite matrices and reconstructs the corresponding SPD/HPD matrix at a given position alpha on that geodesic. This function implements the Log-Cholesky interpolation introduced in Z. Lin (2019) and is used in pyRiemann workflows to interpolate or extrapolate covariance/HPD matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG/MEG in BCI applications) in a manner consistent with Riemannian geometry.\n", "name": "pyriemann_utils_geodesic_geodesic_logchol", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First SPD/HPD matrix or batch of matrices with shape (..., n, n). In practical use within pyRiemann, A typically represents a covariance matrix estimated from multichannel time series (e.g., one epoch or one sensor configuration). The array must be symmetric (real) or Hermitian (complex) and positive definite so that a Cholesky factorization exists. The leading dimensions are treated as batch dimensions and are broadcasted consistently with B if needed; mismatched matrix shapes (different n) will raise an error.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second SPD/HPD matrix or batch of matrices with shape (..., n, n). B is the other endpoint of the geodesic and plays the role of the target covariance/HPD matrix in interpolation or extrapolation tasks (e.g., transferring covariance structure between sessions or subjects). Like A, B must be symmetric/Hermitian and positive definite to allow Cholesky decomposition. The function computes the geodesic point between A and B for the same last-two dimensions.", "default": ""}, "alpha": {"type": "float", "description": "Position on the geodesic, default=0.5. alpha = 0 returns A, alpha = 1 returns B, and values in (0, 1) interpolate along the log-Cholesky geodesic. Values outside [0, 1] produce extrapolation along the same formula. In practice, alpha is used to obtain intermediate covariance matrices for interpolation, averaging, transfer learning, or path generation in Riemannian-based classification pipelines. The function does not enforce bounds on alpha; numerical behavior for extreme magnitudes follows the implemented algebraic formula.", "default": 0.5}}, "required": ["A", "B", "alpha"], "type": "any"}}, "type": "function"}], "query": "I’m building a session-transfer template for a 3-channel EEG BCI pipeline using Log-Cholesky interpolation (pyriemann.utils.geodesic.geodesic_logchol). I have two covariance estimates from different sessions:\n\nA = [[1.0, 0.2, 0.1],\n     [0.2, 0.9, 0.05],\n     [0.1, 0.05, 0.8]]\n\nB = [[1.2, 0.1, 0.0],\n     [0.1, 1.1, 0.03],\n     [0.0, 0.03, 0.95]]\n\nInitially I planned to interpolate exactly halfway between sessions, but then I realized the template should be biased toward the *cleaner* session. Use the session whose covariance has the *smaller total off-diagonal magnitude* (sum of absolute off-diagonal entries) as the reference, and place the template one-quarter of the way from that reference toward the other session along the Log-Cholesky geodesic. Compute the resulting SPD matrix using geodesic_logchol (with alpha chosen accordingly).", "answers": "[{\"name\":\"pyriemann_utils_geodesic_geodesic_logchol\",\"arguments\":{\"A\":[[1.2,0.1,0.0],[0.1,1.1,0.03],[0.0,0.03,0.95]],\"B\":[[1.0,0.2,0.1],[0.2,0.9,0.05],[0.1,0.05,0.8]],\"alpha\":0.25}}]"}
{"func_name": "pyriemann_utils_mean_mean_harmonic", "func_desc": "pyriemann.utils.mean.mean_harmonic computes the harmonic mean of a collection of invertible square matrices.\n\nComputes the weighted harmonic mean defined by the matrix identity M = (sum_i w_i X_i^{-1})^{-1}. This function is intended for use in pyRiemann pipelines where sets of symmetric positive definite (SPD) covariance matrices (for example, covariance estimates from multichannel biosignals such as EEG, MEG, or EMG used in brain-computer interface workflows) need to be averaged using the harmonic operator. The implementation inverts each input matrix, computes the (weighted) Euclidean mean of these inverses via mean_euclid, then inverts the result to produce the harmonic mean. The operation returns a new numpy.ndarray and does not modify the input X in-place.", "tools": [{"function": {"description": "pyriemann.utils.mean.mean_harmonic computes the harmonic mean of a collection of invertible square matrices.\n\nComputes the weighted harmonic mean defined by the matrix identity M = (sum_i w_i X_i^{-1})^{-1}. This function is intended for use in pyRiemann pipelines where sets of symmetric positive definite (SPD) covariance matrices (for example, covariance estimates from multichannel biosignals such as EEG, MEG, or EMG used in brain-computer interface workflows) need to be averaged using the harmonic operator. The implementation inverts each input matrix, computes the (weighted) Euclidean mean of these inverses via mean_euclid, then inverts the result to produce the harmonic mean. The operation returns a new numpy.ndarray and does not modify the input X in-place.", "name": "pyriemann_utils_mean_mean_harmonic", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Set of invertible matrices with shape (n_matrices, n, n). Each X[i] must be a square, invertible matrix (for covariance applications these are typically SPD matrices). The first dimension indexes matrices to be averaged. The function will attempt to compute the inverse of each X[i]; if any matrix is singular or nearly singular, a numpy.linalg.LinAlgError or numerical instability may occur.", "default": ""}, "sample_weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Weights for each matrix, array of shape (n_matrices,). If None (the default), equal weights are used (i.e., all weights are 1/n_matrices). When provided, sample_weight[i] is multiplied with X[i]^{-1} in the summation. The length of sample_weight must match the number of matrices (first dimension of X). Negative or zero-sum weights are not explicitly checked here and may lead to unexpected results or exceptions from downstream linear algebra operations.", "default": null}}, "required": ["X", "sample_weight"], "type": "any"}}, "type": "function"}], "query": "I’m averaging SPD covariance matrices from three EEG trials using `pyriemann.utils.mean.mean_harmonic`, but there’s a wrinkle: I initially planned to weight them as “first two trials are twice as important as the third.” Then I realized our preprocessing notes flag the *third* trial as the most reliable (lowest artifact contamination), while the other two are equally reliable but less clean. So instead of the original plan, assign weights so that the cleanest trial gets double the weight of each of the other two, while keeping the total weight sum the same as in the original plan. Use those weights to compute the harmonic mean M = (\\sum_i w_i X_i^{-1})^{-1} for the following 3x3 covariance matrices:\nX[0] = [[1.2, 0.3, 0.0], [0.3, 0.9, 0.1], [0.0, 0.1, 1.5]]\nX[1] = [[1.0, 0.2, 0.1], [0.2, 1.1, 0.0], [0.1, 0.0, 1.3]]\nX[2] = [[0.8, 0.1, 0.0], [0.1, 0.7, 0.2], [0.0, 0.2, 1.0]].", "answers": "[{\"name\": \"pyriemann_utils_mean_mean_harmonic\", \"arguments\": {\"X\": [[[1.2, 0.3, 0.0], [0.3, 0.9, 0.1], [0.0, 0.1, 1.5]], [[1.0, 0.2, 0.1], [0.2, 1.1, 0.0], [0.1, 0.0, 1.3]], [[0.8, 0.1, 0.0], [0.1, 0.7, 0.2], [0.0, 0.2, 1.0]]], \"sample_weight\": [1.25, 1.25, 2.5]}}]"}
{"func_name": "pyriemann_utils_tangentspace_exp_map_riemann", "func_desc": "pyriemann.utils.tangentspace.exp_map_riemann projects matrices from a tangent space back onto the manifold of symmetric (SPD) or Hermitian (HPD) positive-definite matrices using the Riemannian exponential map; it is used in pyRiemann for converting tangent-space representations (e.g., produced by TangentSpace transformer) back to covariance-like SPD/HPD matrices for downstream algorithms (MDM, visualization, or re-injection into pipelines).", "tools": [{"function": {"description": "pyriemann.utils.tangentspace.exp_map_riemann projects matrices from a tangent space back onto the manifold of symmetric (SPD) or Hermitian (HPD) positive-definite matrices using the Riemannian exponential map; it is used in pyRiemann for converting tangent-space representations (e.g., produced by TangentSpace transformer) back to covariance-like SPD/HPD matrices for downstream algorithms (MDM, visualization, or re-injection into pipelines).\n", "name": "pyriemann_utils_tangentspace_exp_map_riemann", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Matrices in tangent space with shape (..., n, n). Each matrix is expected to be a real symmetric matrix (for SPD manifold) or a complex Hermitian matrix (for HPD manifold) representing a tangent vector at the reference point Cref. In typical pyRiemann workflows X is the output of a log map / tangent-space embedding and its entries represent elements of the matrix logarithm domain. If X does not satisfy the expected symmetry/Hermiticity, numerical results are undefined and the result may not lie on the SPD/HPD manifold.", "default": ""}, "Cref": {"type": "array", "items": {"type": "any"}, "description": "Reference SPD/HPD matrix with shape (n, n). This matrix is the reference point on the manifold at which the tangent space is defined (for example, a geometric mean or an epoch-specific covariance). Cref must be symmetric positive-definite (or Hermitian positive-definite for complex data); if Cref is not positive-definite the internal matrix square root and inverse-square-root computations will fail or produce invalid results.", "default": ""}, "Cm12": {"type": "boolean", "description": "If False (default), the function applies the exponential map using the formulation X_original = Cref^{1/2} exp(X) Cref^{1/2}. If True, the function applies the full Riemannian exponential map with congruence by the inverse square root of Cref: X_original = Cref^{1/2} exp(Cref^{-1/2} X Cref^{-1/2}) Cref^{1/2}. Use Cm12=True when X is expressed in the canonical Euclidean tangent coordinates that require congruence by Cref^{-1/2} before exponentiation (see Pennec et al., Section 3.4); use Cm12=False when X has already been transported/normalized relative to Cref so that direct exponentiation is appropriate.", "default": false}}, "required": ["X", "Cref", "Cm12"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing trial-level EEG covariance matrices from tangent-space features. Please map two 3×3 tangent-space matrices back to SPD covariances using `pyriemann.utils.tangentspace.exp_map_riemann` around the provided reference covariance.\n\nInitially I was going to use the inverse-square-root congruence normalization because that’s what our old pipeline did, but I just realized these tangent vectors were exported from a TangentSpace step configured with the *other* congruence convention (the one that uses the reference’s inverse square root on both sides before applying the matrix exponential and then maps back with the reference itself).\n\nSo: use the congruence setting consistent with that convention (i.e., do NOT apply the inverse-square-root congruence variant).\n\nTangent matrices (symmetric):\n1) [[0.10, 0.02, -0.01],[0.02, -0.05, 0.03],[-0.01, 0.03, 0.08]]\n2) [[-0.02, 0.01, 0.00],[0.01, 0.04, -0.02],[0.00, -0.02, -0.01]]\n\nReference Cref:\n[[1.20, 0.10, 0.05],[0.10, 0.90, 0.02],[0.05, 0.02, 1.10]]", "answers": "[{\"name\": \"pyriemann_utils_tangentspace_exp_map_riemann\", \"arguments\": {\"X\": [[[0.1, 0.02, -0.01], [0.02, -0.05, 0.03], [-0.01, 0.03, 0.08]], [[-0.02, 0.01, 0.0], [0.01, 0.04, -0.02], [0.0, -0.02, -0.01]]], \"Cref\": [[1.2, 0.1, 0.05], [0.1, 0.9, 0.02], [0.05, 0.02, 1.1]], \"Cm12\": false}}]"}
{"func_name": "pyriemann_utils_tangentspace_exp_map_wasserstein", "func_desc": "Project matrices back to SPD/HPD manifold using the Wasserstein exponential map.\n\nThis function implements the Wasserstein Riemannian exponential map that takes a perturbation in the tangent space at a reference symmetric (or Hermitian) positive definite (SPD/HPD) matrix and returns the corresponding matrix on the SPD/HPD manifold. In pyRiemann pipelines for biosignal processing and brain–computer interfaces (BCI), this is used to map tangent-space representations (for example, results of a TangentSpace transform or a gradient update) back to covariance matrices (SPD) or Hermitian positive definite matrices (HPD) so they can be interpreted or further processed as covariance estimates for multichannel time series. The implementation follows Eq.(36) in the referenced Wasserstein geometry paper and uses the eigen-decomposition of the reference matrix Cref to compute the map.", "tools": [{"function": {"description": "Project matrices back to SPD/HPD manifold using the Wasserstein exponential map.\n\nThis function implements the Wasserstein Riemannian exponential map that takes a perturbation in the tangent space at a reference symmetric (or Hermitian) positive definite (SPD/HPD) matrix and returns the corresponding matrix on the SPD/HPD manifold. In pyRiemann pipelines for biosignal processing and brain–computer interfaces (BCI), this is used to map tangent-space representations (for example, results of a TangentSpace transform or a gradient update) back to covariance matrices (SPD) or Hermitian positive definite matrices (HPD) so they can be interpreted or further processed as covariance estimates for multichannel time series. The implementation follows Eq.(36) in the referenced Wasserstein geometry paper and uses the eigen-decomposition of the reference matrix Cref to compute the map.", "name": "pyriemann_utils_tangentspace_exp_map_wasserstein", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Matrices in tangent space with shape (..., n, n). Each leading index corresponds to a tangent-space perturbation associated with the same reference matrix Cref. In practice X contains symmetric (or Hermitian) perturbation matrices computed from covariance estimators or operations performed in the tangent space. The last two dimensions must form square matrices of size n and must be consistent with Cref.", "default": ""}, "Cref": {"type": "array", "items": {"type": "any"}, "description": "Reference SPD/HPD matrix with shape (n, n). This matrix defines the base point on the manifold where the exponential map is applied. Cref must be symmetric (real) positive definite or Hermitian positive definite (complex) with strictly positive eigenvalues; it is typically a covariance matrix estimated from multichannel biosignal data (EEG/MEG/EMG) or spatial covariance in remote sensing applications.", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "I’m updating an EEG/BCI covariance model on the Wasserstein SPD manifold. Start by mapping my two 3×3 tangent-space gradient steps back to the manifold at the given reference covariance using the Wasserstein exponential map (Eq. 36-style, via eigendecomposition of the reference). However, I just realized one of the gradient steps was logged with the opposite sign convention (the optimizer recorded the *descent direction* instead of the *update to apply*). So: apply the exp map to the step that *increases* the objective exactly as-is, but for the other step, first flip it to represent the update actually applied, then map it. Use the same Cref for both, and treat the inputs as real-symmetric SPD (not complex/HPD). \n\nCref = [[1.2, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 0.7]]\n\nRaw tangent steps (in the order they were produced):\nX1 = [[0.02, -0.01, 0.0], [-0.01, 0.03, 0.005], [0.0, 0.005, -0.015]]\nX2 = [[-0.005, 0.002, 0.001], [0.002, -0.01, 0.0], [0.001, 0.0, 0.008]]\n\n(Only one of these needs sign flipping; it’s the one that is already a descent direction.)", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_exp_map_wasserstein\",\"arguments\":{\"X\":[[[0.02,-0.01,0.0],[-0.01,0.03,0.005],[0.0,0.005,-0.015]],[[0.005,-0.002,-0.001],[-0.002,0.01,0.0],[-0.001,0.0,-0.008]]],\"Cref\":[[1.2,0.1,0.0],[0.1,0.9,0.05],[0.0,0.05,0.7]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_log_map_logchol", "func_desc": "pyriemann.utils.tangentspace.log_map_logchol projects SPD/HPD matrices to the tangent space at a reference matrix using the log-Cholesky logarithmic map (see Lin 2019, Table 2). This implementation is intended for use with covariance or Hermitian positive definite matrices encountered in multivariate biosignal processing and remote sensing workflows (for example EEG/MEG covariance matrices used in BCI pipelines) where one needs a Euclidean representation (tangent vector) of SPD/HPD matrices relative to a chosen reference.", "tools": [{"function": {"description": "pyriemann.utils.tangentspace.log_map_logchol projects SPD/HPD matrices to the tangent space at a reference matrix using the log-Cholesky logarithmic map (see Lin 2019, Table 2). This implementation is intended for use with covariance or Hermitian positive definite matrices encountered in multivariate biosignal processing and remote sensing workflows (for example EEG/MEG covariance matrices used in BCI pipelines) where one needs a Euclidean representation (tangent vector) of SPD/HPD matrices relative to a chosen reference.\n", "name": "pyriemann_utils_tangentspace_log_map_logchol", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Matrices on the SPD/HPD manifold to project. Expected shape is (..., n, n), i.e., one or more square matrices where the last two dimensions are n-by-n. Leading dimensions are treated as independent samples/batch dimensions (for example, epochs of covariance matrices estimated from multichannel time series). X must contain real symmetric positive definite matrices or complex Hermitian positive definite matrices (SPD/HPD), since the algorithm relies on Cholesky decomposition.", "default": ""}, "Cref": {"type": "array", "items": {"type": "any"}, "description": "Reference SPD/HPD matrix that defines the tangent space origin. Expected shape is (n, n) and must match the last two dimensions of X. Cref is used as the base point at which the logarithmic map is computed (typical use: the geometric mean or class reference covariance in Riemannian BCI pipelines).", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a real EEG-BCI covariance feature step before classification. I initially planned to project all 3 epoch-wise 3×3 covariance matrices into the tangent space with the log‑Cholesky log map at an identity reference, but I realized the reference used downstream must be the *whitening matrix* estimated from a baseline: i.e., the diagonal matrix whose entries are the inverses of the channel-wise standard deviations from the baseline covariance (so it’s not full, just diagonal). Use the provided baseline covariance to construct that reference, then project the 3 epoch covariances with `pyriemann.utils.tangentspace.log_map_logchol`. (Assume everything is SPD and real-valued.)\n\nEpoch covariances (3 epochs, 3 channels):\n- E1 = [[1.2, 0.1, 0.0], [0.1, 1.1, 0.05], [0.0, 0.05, 0.9]]\n- E2 = [[1.0, 0.2, 0.1], [0.2, 1.3, 0.0], [0.1, 0.0, 1.1]]\n- E3 = [[0.95, 0.05, 0.02], [0.05, 1.05, 0.08], [0.02, 0.08, 1.2]]\n\nBaseline covariance (for constructing the diagonal whitening reference):\nCb = [[1.44, 0.0, 0.0], [0.0, 0.81, 0.0], [0.0, 0.0, 1.21]]", "answers": "[{\"name\": \"pyriemann_utils_tangentspace_log_map_logchol\", \"arguments\": {\"X\": [[[1.2, 0.1, 0.0], [0.1, 1.1, 0.05], [0.0, 0.05, 0.9]], [[1.0, 0.2, 0.1], [0.2, 1.3, 0.0], [0.1, 0.0, 1.1]], [[0.95, 0.05, 0.02], [0.05, 1.05, 0.08], [0.02, 0.08, 1.2]]], \"Cref\": [[0.8333333333333334, 0.0, 0.0], [0.0, 1.1111111111111112, 0.0], [0.0, 0.0, 0.9090909090909091]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_log_map_riemann", "func_desc": "Project SPD/HPD matrices to the tangent space at a reference matrix using the Riemannian logarithmic map.\n\nThis function implements the Riemannian logarithmic map used in pyRiemann to embed symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices into the tangent space associated with a reference SPD/HPD matrix. The mapping is commonly used in biosignal and remote-sensing workflows provided by pyRiemann (for example, to project covariance matrices estimated from multichannel EEG/MEG/EMG recordings into a vector space suitable for classical machine learning classifiers). Mathematically, for each matrix X in the input batch the default output is\nX_new = log(Cref^{-1/2} X Cref^{-1/2}),\nwhere log denotes the matrix logarithm and Cref^{-1/2} the inverse square root of the reference matrix. If C12 is True the function returns the full Riemannian logarithmic map\nX_new = Cref^{1/2} log(Cref^{-1/2} X Cref^{-1/2}) Cref^{1/2},\nwhich yields a matrix expressed in the original ambient space but carrying the tangent-space displacement relative to Cref. The function is pure (no in-place modification of inputs) and returns a new array.", "tools": [{"function": {"description": "Project SPD/HPD matrices to the tangent space at a reference matrix using the Riemannian logarithmic map.\n\nThis function implements the Riemannian logarithmic map used in pyRiemann to embed symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices into the tangent space associated with a reference SPD/HPD matrix. The mapping is commonly used in biosignal and remote-sensing workflows provided by pyRiemann (for example, to project covariance matrices estimated from multichannel EEG/MEG/EMG recordings into a vector space suitable for classical machine learning classifiers). Mathematically, for each matrix X in the input batch the default output is\nX_new = log(Cref^{-1/2} X Cref^{-1/2}),\nwhere log denotes the matrix logarithm and Cref^{-1/2} the inverse square root of the reference matrix. If C12 is True the function returns the full Riemannian logarithmic map\nX_new = Cref^{1/2} log(Cref^{-1/2} X Cref^{-1/2}) Cref^{1/2},\nwhich yields a matrix expressed in the original ambient space but carrying the tangent-space displacement relative to Cref. The function is pure (no in-place modification of inputs) and returns a new array.", "name": "pyriemann_utils_tangentspace_log_map_riemann", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Input matrices to project, with shape (..., n, n). Each trailing (n, n) matrix is expected to lie on the SPD (real symmetric positive definite) or HPD (complex Hermitian positive definite) manifold depending on data modality. In pyRiemann workflows, X typically contains covariance matrices estimated from multichannel time series (e.g., EEG epochs). The leading dimensions allow batching of multiple matrices.", "default": ""}, "Cref": {"type": "array", "items": {"type": "any"}, "description": "Reference SPD/HPD matrix with shape (n, n). This matrix defines the tangent space (the point on the manifold where the tangent space is attached). In practical use, Cref is often the geometric mean or a class mean covariance used to center matrices before applying tangent-space methods.", "default": ""}, "C12": {"type": "boolean", "description": "If False (default), return the tangent vector expressed in the reference tangent space using the canonical mapping log(Cref^{-1/2} X Cref^{-1/2}). If True, return the full mapped matrix Cref^{1/2} log(Cref^{-1/2} X Cref^{-1/2}) Cref^{1/2}, which relocates the tangent vector back into the ambient matrix space while preserving the Riemannian displacement relative to Cref. Use C12=True when the downstream algorithm expects outputs in the original matrix coordinate system; use False when working with tangent-space vectors (e.g., for vectorization and classical classifiers).", "default": false}}, "required": ["X", "Cref", "C12"], "type": "any"}}, "type": "function"}], "query": "I’m re-running a pyRiemann-style tangent-space embedding for an EEG epoch batch, but I realized my downstream classifier expects **tangent vectors in the whitened coordinate system**, not matrices transported back to the ambient space. So don’t use the version that re-applies the reference square-root on both sides.\n\nProject the following three 3×3 SPD covariance matrices into the tangent space at the given reference covariance using the Riemannian logarithmic map in the **whitened form** (i.e., the output should be the matrix logarithm of the congruence-transformed matrices).\n\nX = [\n  [[1.20, 0.10, 0.00], [0.10, 0.90, 0.20], [0.00, 0.20, 1.10]],\n  [[0.95, 0.05, 0.10], [0.05, 1.30, 0.15], [0.10, 0.15, 0.85]],\n  [[1.50, 0.20, 0.05], [0.20, 1.10, 0.25], [0.05, 0.25, 1.00]]\n]\nCref = [[1.00, 0.05, 0.00], [0.05, 1.10, 0.10], [0.00, 0.10, 0.95]]", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_log_map_riemann\",\"arguments\":{\"X\":[[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.1]],[[0.95,0.05,0.1],[0.05,1.3,0.15],[0.1,0.15,0.85]],[[1.5,0.2,0.05],[0.2,1.1,0.25],[0.05,0.25,1.0]]],\"Cref\":[[1.0,0.05,0.0],[0.05,1.1,0.1],[0.0,0.1,0.95]],\"C12\":false}}]"}
{"func_name": "pyriemann_utils_tangentspace_log_map_wasserstein", "func_desc": "pyriemann.utils.tangentspace.log_map_wasserstein: Project SPD/HPD matrices to the tangent space using the Wasserstein logarithmic map.", "tools": [{"function": {"description": "pyriemann.utils.tangentspace.log_map_wasserstein: Project SPD/HPD matrices to the tangent space using the Wasserstein logarithmic map.\n", "name": "pyriemann_utils_tangentspace_log_map_wasserstein", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Matrices in the SPD/HPD manifold to project. Expected shape (..., n, n), where the last two dimensions form square matrices. In the pyRiemann context these are typically covariance matrices estimated from multichannel biosignals (e.g., EEG, MEG, EMG) or spatial covariance estimates for remote sensing; each matrix represents a point on the manifold to be mapped to the tangent space at Cref. Elements may be real (SPD) or complex (HPD) depending on the application.", "default": ""}, "Cref": {"type": "array", "items": {"type": "float"}, "description": "Reference SPD/HPD matrix at which the tangent space is defined. Expected shape (n, n) and must have the same n as the last two dimensions of X. In practice Cref is often a reference covariance matrix (for example a Riemannian mean or session/subject-specific reference) used to linearize the manifold around that point for downstream Euclidean processing (vectorization, classifiers, transfer learning).", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "I’m processing 2×2 covariance (SPD) matrices from a wearable EMG session and want to project them into the tangent space using the Wasserstein logarithmic map. Initially I was going to use the provided reference covariance as-is, but I realized the downstream alignment step assumes the reference is the Bures–Wasserstein barycentric-type center of the batch we’re mapping (not an externally chosen matrix). So: compute the reference as the symmetric matrix obtained by element-wise averaging across the three covariances (and keep it symmetric), then apply `pyriemann.utils.tangentspace.log_map_wasserstein` to the original batch using that internally-derived reference.\n\nUse the batch\nX = [\n  [[1.2, 0.3],[0.3, 0.9]],\n  [[1.0, 0.1],[0.1, 0.8]],\n  [[0.95, 0.05],[0.05, 1.1]]\n].\n\nReturn the tangent-space projections for all three matrices.", "answers": "[{\"name\": \"pyriemann_utils_tangentspace_log_map_wasserstein\", \"arguments\": {\"X\": [[[1.2, 0.3], [0.3, 0.9]], [[1.0, 0.1], [0.1, 0.8]], [[0.95, 0.05], [0.05, 1.1]]], \"Cref\": [[1.05, 0.15], [0.15, 0.9333333333333333]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_transport", "func_desc": "pyriemann.utils.tangentspace.transport: Parallel transport of tangent-space matrices between two positive-definite base points under the affine-invariant metric.\n\nThis function implements the parallel transport of symmetric (real) or Hermitian (complex) matrices X that live in the tangent space of the manifold of symmetric/Hermitian positive definite (SPD/HPD) matrices. It moves tangent vectors defined at a source base point A to the tangent space at a target base point B along the geodesic given by the affine-invariant metric and Levi-Civita connection. In the pyRiemann workflow this is used when comparing or combining tangent-space representations of covariance/HPD matrices computed from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data across different sessions or subjects, where covariance matrices have been mapped to tangent space by a matrix logarithm and must be transported to a common reference base point before classification or transfer learning.", "tools": [{"function": {"description": "pyriemann.utils.tangentspace.transport: Parallel transport of tangent-space matrices between two positive-definite base points under the affine-invariant metric.\n\nThis function implements the parallel transport of symmetric (real) or Hermitian (complex) matrices X that live in the tangent space of the manifold of symmetric/Hermitian positive definite (SPD/HPD) matrices. It moves tangent vectors defined at a source base point A to the tangent space at a target base point B along the geodesic given by the affine-invariant metric and Levi-Civita connection. In the pyRiemann workflow this is used when comparing or combining tangent-space representations of covariance/HPD matrices computed from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data across different sessions or subjects, where covariance matrices have been mapped to tangent space by a matrix logarithm and must be transported to a common reference base point before classification or transfer learning.", "name": "pyriemann_utils_tangentspace_transport", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Symmetric (real) or Hermitian (complex) matrices in tangent space with shape (..., n, n). Each entry is a tangent vector (for example, the matrix logarithm of an SPD/HPD covariance matrix). The leading axes (...) allow transporting a batch of tangent matrices in one call. The function expects these inputs to already be in tangent space (i.e., obtained via a logarithmic map from SPD/HPD matrices); passing raw SPD/HPD matrices will produce incorrect results.", "default": ""}, "A": {"type": "array", "items": {"type": "any"}, "description": "Initial base SPD/HPD matrix of shape (n, n). This is the manifold point where the input tangent matrices X are currently based (for example, a reference covariance estimated from one subject/session). A must be symmetric/Hermitian positive definite and well-conditioned so that matrix inverse and square-root operations are numerically stable.", "default": ""}, "B": {"type": "array", "items": {"type": "any"}, "description": "Final base SPD/HPD matrix of shape (n, n). This is the target manifold point to which tangent matrices X will be transported (for example, a global reference covariance for alignment). B must be symmetric/Hermitian positive definite and compatible in dimension with A and the trailing dimensions of X.", "default": ""}}, "required": ["X", "A", "B"], "type": "any"}}, "type": "function"}], "query": "I’m aligning tangent-space EEG features across two sessions under the affine-invariant metric, but I realized my preprocessing script exported a mixed batch: one matrix is a valid 2×2 symmetric tangent vector at the source base point, and the other has a tiny asymmetry from float I/O (so it’s not exactly in the tangent space). Please take the batch and do the following in this order:\n\n1) Keep only the matrices that are physically valid tangent-space elements for SPD(2), i.e., those that are exactly symmetric as written (don’t “fix” anything by symmetrizing; treat any mismatch between X_ij and X_ji as an artifact to exclude).\n2) Parallel-transport the remaining valid tangent matrices from the source base point A to the target base point B using the affine-invariant parallel transport.\n\nData:\n- X (batch of two):\n  • X1 = [[0.10, 0.02], [0.02, -0.05]]\n  • X2 = [[-0.03, 0.01], [0.0100001, 0.08]]\n- Source base point A = [[2.0, 0.3], [0.3, 1.5]]\n- Target base point B = [[1.2, 0.1], [0.1, 2.5]]\n\nReturn the transported batch (only for the valid inputs).", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_transport\",\"arguments\":{\"X\":[[[0.1,0.02],[0.02,-0.05]]],\"A\":[[2.0,0.3],[0.3,1.5]],\"B\":[[1.2,0.1],[0.1,2.5]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_unupper", "func_desc": "Inverse upper triangular unpacking for symmetric/Hermitian matrices.\n\nThis function reconstructs symmetric (for real-valued data) or Hermitian\n(for complex-valued data) matrices from their weighted upper triangular\nvectorized representations. It is the inverse operation of pyriemann.utils.tangentspace.upper\nand is used in pyRiemann workflows that convert between matrix and vector\nrepresentations (for example, the TangentSpace transformer and any pipeline\nthat vectorizes SPD/HPD matrices for standard machine learning estimators).\nThe input T is expected to contain the upper-triangular entries where\noff-diagonal elements were previously multiplied by sqrt(2) (the symmetric\nweighting convention used by upper). This function divides those off-diagonal\nentries by sqrt(2) and fills the lower triangle as the conjugate transpose\nof the upper triangle to restore symmetry/Hermiticity.", "tools": [{"function": {"description": "Inverse upper triangular unpacking for symmetric/Hermitian matrices.\n\nThis function reconstructs symmetric (for real-valued data) or Hermitian\n(for complex-valued data) matrices from their weighted upper triangular\nvectorized representations. It is the inverse operation of pyriemann.utils.tangentspace.upper\nand is used in pyRiemann workflows that convert between matrix and vector\nrepresentations (for example, the TangentSpace transformer and any pipeline\nthat vectorizes SPD/HPD matrices for standard machine learning estimators).\nThe input T is expected to contain the upper-triangular entries where\noff-diagonal elements were previously multiplied by sqrt(2) (the symmetric\nweighting convention used by upper). This function divides those off-diagonal\nentries by sqrt(2) and fills the lower triangle as the conjugate transpose\nof the upper triangle to restore symmetry/Hermiticity.", "name": "pyriemann_utils_tangentspace_unupper", "parameters": {"properties": {"T": {"type": "array", "items": {"type": "any"}, "description": "Weighted upper triangular parts of symmetric/Hermitian matrices.\nExpected shape is (..., n * (n + 1) / 2), where the last axis contains\nthe flattened upper-triangular entries (including the diagonal) for\nmatrices of size n x n. The array may have any leading batch shape\n(denoted by ...). The dtype of T determines the dtype of the output;\ncomplex dtypes produce Hermitian matrices, real dtypes produce symmetric\nmatrices.", "default": ""}}, "required": ["T"], "type": "any"}}, "type": "function"}], "query": "I’m re-running a pyRiemann tangent-space pipeline where I need to invert the weighted upper-triangular vectorization back into symmetric/Hermitian matrices. I was going to treat the vectors as coming from real symmetric covariance matrices, but I realized our upstream export may include complex-valued entries in general (so the reconstruction must enforce the conjugate-symmetry rule when mirroring the upper triangle to the lower triangle). \n\nPlease reconstruct the original 3×3 matrices for this batch from the provided weighted upper-triangular vectors, using the same convention as `pyriemann.utils.tangentspace.upper`: the off-diagonal entries in the vector were scaled by √2 during vectorization, so they must be rescaled appropriately during reconstruction, and the lower triangle must be filled by the conjugate transpose of the upper triangle.\n\nUse these two weighted upper-triangular vectors (order: [a11, a12*√2, a13*√2, a22, a23*√2, a33]):\n1) [1.0, 0.70710678, -1.41421356, 2.0, 0.35355339, 3.0]\n2) [4.0, -2.82842712, 0.0, 5.0, 1.41421356, 6.0]", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_unupper\",\"arguments\":{\"T\":[[1.0,0.70710678,-1.41421356,2.0,0.35355339,3.0],[4.0,-2.82842712,0.0,5.0,1.41421356,6.0]]}}]"}
{"func_name": "pyriemann_utils_test_is_herm_pos_def", "func_desc": "Check if all matrices in X are Hermitian positive-definite (HPD).\n\nThis utility is used in pyRiemann to validate matrices (for example covariance or cross-spectral\nmatrices computed from multivariate biosignals such as EEG, MEG, EMG or from remote sensing data)\nbefore applying algorithms that assume Hermitian (conjugate-symmetric) and positive-definite\nstructure. The function returns True only when every square matrix in X is both Hermitian and\npositive-definite according to a numerical threshold; it composes the results of is_hermitian(X)\nand is_pos_def(X, tol=tol).", "tools": [{"function": {"description": "Check if all matrices in X are Hermitian positive-definite (HPD).\n\nThis utility is used in pyRiemann to validate matrices (for example covariance or cross-spectral\nmatrices computed from multivariate biosignals such as EEG, MEG, EMG or from remote sensing data)\nbefore applying algorithms that assume Hermitian (conjugate-symmetric) and positive-definite\nstructure. The function returns True only when every square matrix in X is both Hermitian and\npositive-definite according to a numerical threshold; it composes the results of is_hermitian(X)\nand is_pos_def(X, tol=tol).", "name": "pyriemann_utils_test_is_herm_pos_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of square matrices to test. Must be an ndarray with shape (..., n, n)\n(at least 2D) where the last two dimensions index square matrices of size n x n. Each\nmatrix is checked for Hermitian symmetry (A == A.conj().T) and for positive-definiteness\n(all eigenvalues > tol). In pyRiemann this commonly represents estimated covariance or\nHermitian covariance-like matrices used by Riemannian geometry methods for classification\nand signal processing.", "default": ""}, "tol": {"type": "float", "description": "Threshold below which eigenvalues are considered zero. Default 0.0. A value of\ntol = 0.0 enforces strict positive-definiteness (all eigenvalues must be strictly > 0).\nA small positive tol can be used to tolerate numerical round-off or near-singular matrices\nby treating eigenvalues <= tol as non-positive. This parameter is forwarded to the\nunderlying is_pos_def check.", "default": 0.0}}, "required": ["X", "tol"], "type": "any"}}, "type": "function"}], "query": "I’m about to run a Riemannian classifier on EEG cross-spectral density estimates, but my pipeline sometimes produces matrices that are *almost* Hermitian and only barely SPD due to finite-window FFT leakage. Please validate the same two 3×3 matrices below as Hermitian positive-definite, but don’t use the original tolerance I planned: I just realized my earlier setting was for single-precision export, while this run is native double-precision, so I should use the **stricter threshold that is one hundred times smaller than the previous one** to decide Hermitian-ness and positive-definiteness. Return whether the whole batch passes.\n\nX = [\n  [[2.0, 0.2, -0.1], [0.2, 1.5, 0.3], [-0.1, 0.3, 1.2]],\n  [[1.8, -0.05, 0.1], [-0.05, 1.1, -0.2], [0.1, -0.2, 1.4]]\n]", "answers": "[{\"name\":\"pyriemann_utils_test_is_herm_pos_def\",\"arguments\":{\"X\":[[[2.0,0.2,-0.1],[0.2,1.5,0.3],[-0.1,0.3,1.2]],[[1.8,-0.05,0.1],[-0.05,1.1,-0.2],[0.1,-0.2,1.4]]],\"tol\":1e-08}}]"}
{"func_name": "pyriemann_utils_test_is_herm_pos_semi_def", "func_desc": "Check whether every matrix in a collection is Hermitian positive semi-definite (HPSD).", "tools": [{"function": {"description": "Check whether every matrix in a collection is Hermitian positive semi-definite (HPSD).\n", "name": "pyriemann_utils_test_is_herm_pos_semi_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "The set of square matrices to test. This must be at least a 2-D numpy.ndarray with shape (..., n, n) where the last two dimensions index each n-by-n matrix. In the pyRiemann context, X typically contains covariance or cross-spectral matrices estimated from multichannel biosignals (EEG/MEG/EMG) or from spatial windows in remote sensing; these matrices may be real-valued symmetric (for SPD/real-valued covariance) or complex-valued Hermitian (for HPD/complex-valued covariance). The function verifies properties across the whole collection provided in X.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a batch of 3×3 cross-spectral covariance matrices (alpha-band EEG) before a Riemannian pipeline. Initially I was going to pass them in as-is, but I just realized one of the matrices is a quick-and-dirty export that only stored the upper triangle, so if I feed it directly the Hermitian test will be meaningless. Please do this instead: reconstruct each matrix into its Hermitian form by mirroring the strictly upper-triangular entries into the lower triangle using complex-conjugation (diagonal stays unchanged). Then check whether every reconstructed matrix is Hermitian positive semi-definite. Report whether the whole collection passes.\n\nHere are the two 3×3 matrices (each given as a full 3×3 numeric array, but treat the second one as “upper-triangle is authoritative; lower-triangle may be wrong”):\nMatrix A:\n[[2.0, 0.3, -0.1],\n [0.3, 1.5, 0.2],\n [-0.1, 0.2, 1.0]]\nMatrix B:\n[[1.0, 0.0, 0.2],\n [0.9, 0.8, 0.1],\n [0.4, 0.7, 0.5]]", "answers": "[{\"name\": \"pyriemann_utils_test_is_herm_pos_semi_def\", \"arguments\": {\"X\": [[[2.0, 0.3, -0.1], [0.3, 1.5, 0.2], [-0.1, 0.2, 1.0]], [[1.0, 0.0, 0.2], [0.0, 0.8, 0.1], [0.2, 0.1, 0.5]]]}}]"}
{"func_name": "pyriemann_utils_test_is_pos_semi_def", "func_desc": "pyriemann.utils.test.is_pos_semi_def checks whether every matrix contained in the input array X is positive semi-definite (PSD). This is a lightweight validator used in pyRiemann to verify covariance matrices or other square matrix collections before algorithms that assume non-negative eigenvalues (for example, Riemannian geometry routines that require SPD/PSD inputs).", "tools": [{"function": {"description": "pyriemann.utils.test.is_pos_semi_def checks whether every matrix contained in the input array X is positive semi-definite (PSD). This is a lightweight validator used in pyRiemann to verify covariance matrices or other square matrix collections before algorithms that assume non-negative eigenvalues (for example, Riemannian geometry routines that require SPD/PSD inputs).\n", "name": "pyriemann_utils_test_is_pos_semi_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The collection of square matrices to test. X must be a NumPy ndarray with shape (..., n, n), i.e., the last two axes represent n-by-n matrices and any leading axes index multiple matrices. In the pyRiemann context, X commonly holds estimated covariance matrices from multichannel biosignals (e.g., EEG epochs n_epochs x n_channels x n_channels) or spatial covariance blocks for remote sensing. The function first checks squareness via is_square(X); if X is not at least 2-D with square trailing dimensions, the function will return False. Elements are expected to be numeric (typically real-valued floats) because the check relies on eigenvalue computation. Note that floating-point round-off can produce small negative eigenvalues for matrices that are theoretically PSD; such numerical artifacts will cause this function to return False because the implementation tests eigenvalues with the comparison >= 0.0. The implementation delegates eigenvalue extraction to _get_eigenvals and does not modify X.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a batch of 3×3 EEG covariance estimates before pushing them into a Riemannian classifier. Initially I was going to validate all three matrices at once, but I just realized one epoch got corrupted by a transient amplifier saturation: it’s the one whose overall scale is noticeably larger than the other two (you can spot it because its diagonal entries are all well above 1, unlike the other epochs). Exclude that corrupted epoch and use `pyriemann.utils.test.is_pos_semi_def` to verify that every remaining matrix in the batch is positive semi-definite.\n\nMatrices (in epoch order):\n1) [[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 0.8]]\n2) [[0.5, -0.1, 0.0], [-0.1, 0.6, 0.2], [0.0, 0.2, 0.7]]\n3) [[2.0, 1.0, 0.5], [1.0, 1.5, 0.3], [0.5, 0.3, 1.2]]", "answers": "[{\"name\": \"pyriemann_utils_test_is_pos_semi_def\", \"arguments\": {\"X\": [[[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 0.8]], [[0.5, -0.1, 0.0], [-0.1, 0.6, 0.2], [0.0, 0.2, 0.7]]]}}]"}
{"func_name": "pyriemann_utils_test_is_real_type", "func_desc": "Check if a collection of matrices is of a real numeric type.\n\nThis function is used in pyRiemann preprocessing and validation steps where algorithms\nassume real-valued matrices (for example, covariance estimation and Riemannian\noperations on symmetric positive definite (SPD) matrices). It inspects the numeric\ndtype of the input matrix array to determine whether the data are stored as real\nvalued numbers (no imaginary component). This is a lightweight, dtype-level test:\nit does not check matrix properties such as symmetry, positive-definiteness, or\nshape validity beyond the array's dtype.", "tools": [{"function": {"description": "Check if a collection of matrices is of a real numeric type.\n\nThis function is used in pyRiemann preprocessing and validation steps where algorithms\nassume real-valued matrices (for example, covariance estimation and Riemannian\noperations on symmetric positive definite (SPD) matrices). It inspects the numeric\ndtype of the input matrix array to determine whether the data are stored as real\nvalued numbers (no imaginary component). This is a lightweight, dtype-level test:\nit does not check matrix properties such as symmetry, positive-definiteness, or\nshape validity beyond the array's dtype.", "name": "pyriemann_utils_test_is_real_type", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "The set of matrices to test, expected to be a NumPy ndarray\nwith shape (..., n, m). The leading dimensions (represented by ...)\nallow batching (e.g., multiple covariance matrices stacked as\nn_matrices x n_channels x n_channels). The function expects numeric\nentries and uses the array's dtype to decide whether values are real.\nBehavior is defined for ndarray inputs; passing non-ndarray objects is\nnot guaranteed by this docstring and may produce results consistent\nwith NumPy's isrealobj but is otherwise outside the stated contract.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m about to run pyRiemann’s covariance + SPD pipeline on a small EEG batch, but the upstream loader is messy: one export path sometimes writes complex-valued arrays even when the imaginary part is exactly zero (it tags the dtype as complex), and another path writes plain real floats. I initially thought I could just proceed if all numeric entries *look* real, but that’s unsafe because pyRiemann’s validation here is strictly dtype-level.\n\nSo: please run the real-dtype check on the batch below *as it would appear after NumPy array construction*, and tell me whether the collection is stored as a real numeric type. Note that exactly one matrix contains a single entry written with an explicit zero imaginary component to mimic the problematic exporter; everything else is standard floats.\n\nX = [\n  [[1.0, 0.2, 0.1], [0.2, 1.3, 0.0], [0.1, 0.0, 0.9]],\n  [[0.8, 0.1, 0.0], [0.1, 1.1, 0.3], [0.0, 0.3, 1.5]],\n  [[1.2, 0.0, 0.4], [0.0, 0.7, 0.2], [0.4, 0.2, 1.0+0.0j]]\n]\n\nOnly use the dtype-based criterion (don’t check SPD/symmetry); the question is strictly whether the batch is of a real numeric type.", "answers": "[{\"name\":\"pyriemann_utils_test_is_real_type\",\"arguments\":{\"X\":[[[1.0,0.2,0.1],[0.2,1.3,0.0],[0.1,0.0,0.9]],[[0.8,0.1,0.0],[0.1,1.1,0.3],[0.0,0.3,1.5]],[[1.2,0.0,0.4],[0.0,0.7,0.2],[0.4,0.2,{\"real\":1.0,\"imag\":0.0}]]]}}]"}
{"func_name": "pyriemann_utils_test_is_skew_sym", "func_desc": "Check if all matrices in X are skew-symmetric (X^T = -X) across the last two axes.\n\nThis utility is part of pyriemann.utils.test and is intended for use in unit tests, validation checks, and preprocessing/diagnostics inside the pyRiemann library. In the pyRiemann context (multivariate biosignal processing and Riemannian geometry of matrices), it helps verify that a candidate array of square matrices represents antisymmetric operators (for example, elements of the Lie algebra of SO(n) or antisymmetric residuals produced during intermediate computations). The check is performed element-wise with a numerical tolerance (see behavior below) and does not modify the input array.", "tools": [{"function": {"description": "Check if all matrices in X are skew-symmetric (X^T = -X) across the last two axes.\n\nThis utility is part of pyriemann.utils.test and is intended for use in unit tests, validation checks, and preprocessing/diagnostics inside the pyRiemann library. In the pyRiemann context (multivariate biosignal processing and Riemannian geometry of matrices), it helps verify that a candidate array of square matrices represents antisymmetric operators (for example, elements of the Lie algebra of SO(n) or antisymmetric residuals produced during intermediate computations). The check is performed element-wise with a numerical tolerance (see behavior below) and does not modify the input array.", "name": "pyriemann_utils_test_is_skew_sym", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "The input array containing one or more square matrices to test. Expected shape is (..., n, n), i.e. at least a 2-D array where the last two dimensions form n-by-n matrices. Each n-by-n matrix is tested independently for skew-symmetry. The array must be a NumPy ndarray; if another array-like object is supplied, NumPy operations inside the function may raise an exception.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m validating an intermediate step in a pyRiemann-based biosignal pipeline where I stack residual operators and then (later) map them onto SO(3). I was going to sanity-check the raw residual batch directly, but I just realized one of the matrices picked up a tiny numerical bleed from a prior whitening step (so it’s not *exactly* antisymmetric in floating point, but it should still pass a unit-test style antisymmetry check).\n\nPlease run the skew-symmetry validator (X^T = -X across the last two axes) on the following batch of two 3×3 matrices, using the library’s built-in numerical tolerance behavior (i.e., don’t hand-tune a tolerance; just use the default behavior expected for unit-test checks):\n\nMatrix A = [[0.0, 2.0, -1.0], [-2.0, 0.0, 4.0], [1.0, -4.0, 0.0]]\nMatrix B = [[0.0, 0.5, 0.0], [-0.5, 0.0, 1.2000000001], [0.0, -1.2, 0.0]]\n\nReturn whether the *batch* passes (i.e., all matrices satisfy skew-symmetry within tolerance).", "answers": "[{\"name\":\"pyriemann_utils_test_is_skew_sym\",\"arguments\":{\"X\":[[[0.0,2.0,-1.0],[-2.0,0.0,4.0],[1.0,-4.0,0.0]],[[0.0,0.5,0.0],[-0.5,0.0,1.2000000001],[0.0,-1.2,0.0]]]}}]"}
{"func_name": "pyriemann_utils_test_is_square", "func_desc": "pyriemann.utils.test.is_square checks whether the last two dimensions of a numpy array represent square matrices.\n\nThis function is typically used in pyRiemann preprocessing and validation code to ensure that inputs that are expected to be batches of square matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, which are symmetric positive definite and therefore square) have the correct shape before further Riemannian operations. It performs a lightweight shape check only and does not inspect matrix values or properties such as symmetry or positive definiteness.", "tools": [{"function": {"description": "pyriemann.utils.test.is_square checks whether the last two dimensions of a numpy array represent square matrices.\n\nThis function is typically used in pyRiemann preprocessing and validation code to ensure that inputs that are expected to be batches of square matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, which are symmetric positive definite and therefore square) have the correct shape before further Riemannian operations. It performs a lightweight shape check only and does not inspect matrix values or properties such as symmetry or positive definiteness.", "name": "pyriemann_utils_test_is_square", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "Array expected to contain one or more matrices arranged in its last two axes, with shape (..., n, n). The function requires X to be at least 2-dimensional (ndim >= 2). In typical pyRiemann usage, X would be an array of covariance matrices with shape (n_epochs, n_channels, n_channels) or a single matrix with shape (n_channels, n_channels).", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m about to run a Riemannian mean on a stack of EEG covariance estimates, and I only want a quick shape sanity-check (not SPD/symmetry). I initially planned to validate the entire batch, but I just realized the acquisition pipeline may have appended a non-matrix metadata row/column to one epoch, so I should *exclude the single epoch whose last two dimensions are not equal* and then run the square-matrix check on the remaining epochs (they should all share the same channel count). Using the data below, first drop the uniquely non-square epoch by inspecting shapes, then confirm the remaining batch is square in its last two dimensions.\n\nX = [\n  [[1.0, 0.2, 0.1, 0.0], [0.2, 1.0, 0.3, 0.1], [0.1, 0.3, 1.0, 0.4], [0.0, 0.1, 0.4, 1.0]],\n  [[1.1, 0.0, 0.2, 0.1], [0.0, 0.9, 0.1, 0.0], [0.2, 0.1, 1.2, 0.3], [0.1, 0.0, 0.3, 1.0], [0.0, 0.0, 0.0, 0.0]],\n  [[0.8, 0.1, 0.0, 0.2], [0.1, 1.0, 0.2, 0.0], [0.0, 0.2, 0.9, 0.1], [0.2, 0.0, 0.1, 1.1]]\n]", "answers": "[{\"name\": \"pyriemann_utils_test_is_square\", \"arguments\": {\"X\": [[[1.0, 0.2, 0.1, 0.0], [0.2, 1.0, 0.3, 0.1], [0.1, 0.3, 1.0, 0.4], [0.0, 0.1, 0.4, 1.0]], [[0.8, 0.1, 0.0, 0.2], [0.1, 1.0, 0.2, 0.0], [0.0, 0.2, 0.9, 0.1], [0.2, 0.0, 0.1, 1.1]]]}}]"}
{"func_name": "pyriemann_utils_viz_plot_biscatter", "func_desc": "Plot scatter of bi-class predictions.\n\nThis function visualizes two-dimensional predictions, distances, or probabilities for a binary\nclassification problem as a scatter plot. It is intended for use in pyRiemann workflows\nwhere classifiers or estimators produce two values per sample (for example, scores derived\nfrom covariance-matrix-based BCI pipelines such as MDM or tangent-space classifiers). The\nplot helps inspect separation between the two classes by grouping points according to the\nlabels provided in y, drawing a diagonal reference line, and enforcing equal axis limits so\nthat distances from the diagonal are comparable on both axes.", "tools": [{"function": {"description": "Plot scatter of bi-class predictions.\n\nThis function visualizes two-dimensional predictions, distances, or probabilities for a binary\nclassification problem as a scatter plot. It is intended for use in pyRiemann workflows\nwhere classifiers or estimators produce two values per sample (for example, scores derived\nfrom covariance-matrix-based BCI pipelines such as MDM or tangent-space classifiers). The\nplot helps inspect separation between the two classes by grouping points according to the\nlabels provided in y, drawing a diagonal reference line, and enforcing equal axis limits so\nthat distances from the diagonal are comparable on both axes.", "name": "pyriemann_utils_viz_plot_biscatter", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "2D array with one row per sample (n_matrices).\nEach row contains two numeric values produced by a classifier, distance metric,\nor probability estimator. In BCI and covariance-matrix analysis contexts (see\nthe package README), X typically contains scores computed from SPD/HPD matrix\nrepresentations of multichannel biosignals. The function requires X to be a\ntwo-dimensional array with exactly two columns; otherwise a ValueError is raised.", "default": ""}, "y": {"type": "array", "items": {"type": "any"}, "description": "Array of labels with one entry per row of X.\nLabels indicate class membership for each sample and are used to split X into\ntwo point sets to be plotted with different transparencies. The function expects\nexactly two unique labels in y (binary classification). Labels can be numeric\nor string-like as supported by numpy.unique. If the number of unique labels is\nnot exactly two, a ValueError is raised. y must have the same first-dimension\nlength as X (n_matrices); mismatch will cause indexing errors.", "default": ""}}, "required": ["X", "y"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a pyRiemann-style binary BCI pipeline where each trial yields two distance-like scores (first coordinate is the score w.r.t. class 0, second coordinate w.r.t. class 1). I want a bi-scatter plot to assess class separation.\n\nInitially I was going to plot all 10 trials as-is, but I just realized one trial looks like a logging glitch: it’s the only one where the two coordinates are identical (that would imply equal distance to both class prototypes, which is extremely unlikely in this run given the rest of the batch). Please exclude that single suspicious trial and also drop its corresponding label so X and y stay aligned, then plot the bi-scatter.\n\nUse:\nX = [[0.12,0.85],[0.20,0.78],[0.15,0.80],[0.18,0.74],[0.10,0.90],[0.82,0.22],[0.75,0.30],[0.88,0.15],[0.79,0.25],[0.84,0.18]]\ny = [0,0,0,0,0,1,1,1,1,1]\n\n(After filtering, plot the remaining trials, grouped by the remaining labels, with the usual diagonal reference and equal axis limits.)", "answers": "[{\"name\":\"pyriemann_utils_viz_plot_biscatter\",\"arguments\":{\"X\":[[0.12,0.85],[0.2,0.78],[0.15,0.8],[0.18,0.74],[0.1,0.9],[0.82,0.22],[0.75,0.3],[0.88,0.15],[0.79,0.25],[0.84,0.18]],\"y\":[0,0,0,0,0,1,1,1,1,1]}}]"}
{"func_name": "pyscf_agf2__agf2_build_mats_dfragf2_incore", "func_desc": "build_mats_dfragf2_incore(qxi, qja, e_occ, e_vir, os_factor=1.0, ss_factor=1.0)\n    Build nmo-by-nmo intermediate matrices for the density-fitted fragment AGF2 procedure using the in-core C driver AGF2df_vv_vev_islice and MPI slicing over occupied orbitals.\n    \n    This function is a thin Python wrapper around the compiled libagf2 routine AGF2df_vv_vev_islice. It prepares inputs, determines array sizes (number of auxiliary functions, occupied and virtual orbitals, and molecular orbitals), slices the occupied-orbital index range for the current MPI rank, calls the C driver with C-contiguous buffers via ctypes, then performs an MPI all-reduce to sum the per-rank contributions. The outputs vv and vev are the two nmo-by-nmo intermediate matrices produced by the C routine; these matrices are used inside the AGF2 (second-order Green's function) implementation in PySCF for assembling correlation/self-energy contributions in the density-fitted fragment variant (dfragf2).", "tools": [{"function": {"description": "build_mats_dfragf2_incore(qxi, qja, e_occ, e_vir, os_factor=1.0, ss_factor=1.0)\nBuild nmo-by-nmo intermediate matrices for the density-fitted fragment AGF2 procedure using the in-core C driver AGF2df_vv_vev_islice and MPI slicing over occupied orbitals.\n\nThis function is a thin Python wrapper around the compiled libagf2 routine AGF2df_vv_vev_islice. It prepares inputs, determines array sizes (number of auxiliary functions, occupied and virtual orbitals, and molecular orbitals), slices the occupied-orbital index range for the current MPI rank, calls the C driver with C-contiguous buffers via ctypes, then performs an MPI all-reduce to sum the per-rank contributions. The outputs vv and vev are the two nmo-by-nmo intermediate matrices produced by the C routine; these matrices are used inside the AGF2 (second-order Green's function) implementation in PySCF for assembling correlation/self-energy contributions in the density-fitted fragment variant (dfragf2).", "name": "pyscf_agf2__agf2_build_mats_dfragf2_incore", "parameters": {"properties": {"qxi": {"type": "array", "items": {"type": "any"}, "description": "Array holding density-fitted integrals or intermediate tensors arranged so that qxi.size == naux * nmo * nocc where naux is the number of auxiliary basis functions, nmo is the number of molecular orbitals, and nocc is the number of occupied orbitals (nocc == e_occ.size). The code treats qxi as C-contiguous memory and calls np.asarray(qxi, order='C') internally; therefore qxi must be convertible to a C-order contiguous numpy array. The practical role of qxi is to supply the C driver with the auxiliary-occupied-molecular orbital data needed to build the vv and vev intermediates for AGF2 density-fitted fragment calculations.", "default": ""}, "qja": {"type": "array", "items": {"type": "any"}, "description": "Array holding density-fitted integrals or intermediate tensors arranged so that qja.size == naux * nocc * nvir where nvir is the number of virtual orbitals (nvir == e_vir.size). Like qxi, qja is converted to C-order inside the function. qja provides the auxiliary-occupied-virtual data consumed by the C driver to construct the same intermediates used by the AGF2 algorithm.", "default": ""}, "e_occ": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of occupied orbital energies with length nocc. The function uses e_occ to form denominators or energy-dependent factors inside the C driver; it is converted to a C-order numpy array named e_i internally. The practical significance is that these orbital energies define the occupied-space spectral positions used by AGF2 correlation integrals.", "default": ""}, "e_vir": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of virtual orbital energies with length nvir. The function uses e_vir as the virtual-space energies (converted internally to a C-order numpy array named e_a) required by the C driver to evaluate energy-dependent contributions to the vv and vev intermediates in AGF2.", "default": ""}, "os_factor": {"type": "float", "description": "Scalar multiplicative factor applied to opposite-spin contributions inside the AGF2df_vv_vev_islice driver. Defaults to 1.0. In practice this parameter lets callers scale the opposite-spin part of the two-body contributions (for example, to implement spin-scaling variants of correlation treatments) before the intermediates are accumulated.", "default": 1.0}, "ss_factor": {"type": "float", "description": "Scalar multiplicative factor applied to same-spin contributions inside the AGF2df_vv_vev_islice driver. Defaults to 1.0. This parameter lets callers scale the same-spin part of the two-body contributions analogously to os_factor.", "default": 1.0}}, "required": ["qxi", "qja", "e_occ", "e_vir", "os_factor", "ss_factor"], "type": "any"}}, "type": "function"}], "query": "I’m still prototyping a tiny density-fitted fragment AGF2 test case to build the in-core nmo×nmo intermediates vv and vev, but I realized I don’t want to hard-code the spin-scaling numbers because I’m trying to mimic our “light” SCS sweep logic from the pipeline. Please use the same toy DF tensors (2 auxiliary functions; 2 occupied MOs; 3 virtual MOs) and the same orbital energies as below. Initially I was going to apply mild spin-scaling, but then I remembered our convention: keep the opposite-spin channel slightly enhanced while the same-spin channel is reduced by the complementary amount so that their average stays exactly at unity. Use that rule to choose the two scaling factors and then call the in-core builder (no MPI behavior needed).\n\nToy inputs:\n- qxi = [[[0.1, -0.05], [0.2, 0.03], [0.0, 0.07], [-0.02, 0.04], [0.05, -0.01]], [[0.12, 0.02], [-0.08, 0.09], [0.06, -0.03], [0.01, 0.05], [-0.04, 0.11]]]\n- qja = [[[0.3, -0.1, 0.05], [-0.2, 0.15, 0.08]], [[0.07, 0.12, -0.04], [0.09, -0.06, 0.02]]]\n- e_occ = [-0.55, -0.41]\n- e_vir = [0.12, 0.27, 0.44]\n\n(As a reminder: “slightly enhanced”/“reduced” here refers to the same magnitude deviation from 1.0 in opposite directions, consistent with the mild setting I was about to use.)", "answers": "[{\"name\":\"pyscf_agf2__agf2_build_mats_dfragf2_incore\",\"arguments\":{\"qxi\":[[[0.1,-0.05],[0.2,0.03],[0.0,0.07],[-0.02,0.04],[0.05,-0.01]],[[0.12,0.02],[-0.08,0.09],[0.06,-0.03],[0.01,0.05],[-0.04,0.11]]],\"qja\":[[[0.3,-0.1,0.05],[-0.2,0.15,0.08]],[[0.07,0.12,-0.04],[0.09,-0.06,0.02]]],\"e_occ\":[-0.55,-0.41],\"e_vir\":[0.12,0.27,0.44],\"os_factor\":1.2,\"ss_factor\":0.8}}]"}
{"func_name": "pyscf_agf2__agf2_build_mats_ragf2_incore", "func_desc": "build_mats_ragf2_incore builds two frequency-independent matrices (vv and vev) used by the restricted AGF2 (rAGF2) implementation by wrapping the compiled driver libagf2.AG F2ee_vv_vev_islice. This function is part of the PySCF agf2 module (Python-based Simulations of Chemistry Framework) and is used in electronic-structure workflows to convert a four-index two-electron integral tensor and orbital energy arrays into condensed matrices that enter the AGF2 self-energy construction and subsequent algebraic Green's function procedures.", "tools": [{"function": {"description": "build_mats_ragf2_incore builds two frequency-independent matrices (vv and vev) used by the restricted AGF2 (rAGF2) implementation by wrapping the compiled driver libagf2.AG F2ee_vv_vev_islice. This function is part of the PySCF agf2 module (Python-based Simulations of Chemistry Framework) and is used in electronic-structure workflows to convert a four-index two-electron integral tensor and orbital energy arrays into condensed matrices that enter the AGF2 self-energy construction and subsequent algebraic Green's function procedures.\n", "name": "pyscf_agf2__agf2_build_mats_ragf2_incore", "parameters": {"properties": {"qeri": {"type": "array", "items": {"type": "any"}, "description": "A contiguous, C-ordered 4-dimensional array of two-electron repulsion integrals in the molecular orbital basis. The code asserts qeri.ndim == 4 and uses qeri.shape[0] as the number of molecular orbitals nmo. In practical AGF2/rAGF2 usage this tensor supplies the electron-electron interaction integrals needed by the C driver to form the vv and vev contributions; providing an array with incorrect dimensionality or non-C-ordering may raise an AssertionError or cause the underlying C driver to fail.", "default": ""}, "e_occ": {"type": "array", "items": {"type": "any"}, "description": "A 1-dimensional, C-ordered array of occupied orbital energies. The length of this array is taken as nocc (number of occupied orbitals) and determines the portion of occupied-space indices processed on each MPI rank. These orbital energies are used by the native C routine to evaluate energy denominators and weighting factors when constructing the matrices; supplying arrays of incorrect length or dtype may lead to incorrect results or a segfault in the C driver.", "default": ""}, "e_vir": {"type": "array", "items": {"type": "any"}, "description": "A 1-dimensional, C-ordered array of virtual (unoccupied) orbital energies. The length of this array is taken as nvir (number of virtual orbitals) and is required by the C driver to compute denominators and intermediate quantities for the vv and vev matrices. As with e_occ, wrong sizing or non-contiguous memory can cause runtime failures in the wrapped routine.", "default": ""}, "os_factor": {"type": "float", "description": "Opposite-spin scaling factor (default 1.0). This scalar multiplies the opposite-spin contribution inside the AGF2 matrix construction performed by the C driver and is provided to allow spin-component-scaled variants of AGF2. Typical values are 1.0 (no scaling) or empirically tuned values used in SCS/OS schemes; passing a non-finite value will propagate to the C driver and may produce NaNs or raise errors.", "default": 1.0}, "ss_factor": {"type": "float", "description": "Same-spin scaling factor (default 1.0). This scalar multiplies the same-spin contribution inside the AGF2 matrix construction performed by the C driver. As with os_factor, use 1.0 for the unscaled method or other positive floats for spin-scaled variants. Non-finite or extreme values can produce invalid numerical results or failures.", "default": 1.0}}, "required": ["qeri", "e_occ", "e_vir", "os_factor", "ss_factor"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny rAGF2 regression test (nmo=3, nocc=1, nvir=2) to validate the vv/vev builders against a hand-assembled MO-basis ERI tensor. Please call build_mats_ragf2_incore to construct the frequency-independent vv and vev matrices from the qeri tensor and the occupied/virtual orbital energies given below.\n\nHowever, I just realized the spin-component scaling inputs I wrote down were copied from a UAGF2 notebook: the two SCS factors are swapped relative to the convention used in the restricted AGF2 wrapper I’m using here. So do NOT use the values as-is; instead, use the pair of scaling factors that results from exchanging the roles of the two numbers I initially provided.\n\nData:\n- qeri (3x3x3x3) = [[[[0.70,0.02,0.01],[0.02,0.60,0.03],[0.01,0.03,0.50]],[[0.02,0.10,0.00],[0.10,0.40,0.02],[0.00,0.02,0.30]],[[0.01,0.00,0.05],[0.00,0.02,0.20],[0.05,0.20,0.25]]],[[[0.02,0.10,0.00],[0.10,0.40,0.02],[0.00,0.02,0.30]],[[0.60,0.01,0.02],[0.01,0.65,0.04],[0.02,0.04,0.55]],[[0.03,0.02,0.01],[0.02,0.15,0.05],[0.01,0.05,0.22]]],[[[0.01,0.00,0.05],[0.00,0.02,0.20],[0.05,0.20,0.25]],[[0.03,0.02,0.01],[0.02,0.15,0.05],[0.01,0.05,0.22]],[[0.50,0.02,0.03],[0.02,0.55,0.01],[0.03,0.01,0.58]]]]\n- e_occ = [-0.62]\n- e_vir = [0.15, 0.38]\n- Initial (possibly mislabeled) SCS pair: os_factor=1.2 and ss_factor=0.8; apply the corrected convention by swapping them before calling the function.", "answers": "[{\"name\": \"pyscf_agf2__agf2_build_mats_ragf2_incore\", \"arguments\": {\"qeri\": [[[[0.7, 0.02, 0.01], [0.02, 0.6, 0.03], [0.01, 0.03, 0.5]], [[0.02, 0.1, 0.0], [0.1, 0.4, 0.02], [0.0, 0.02, 0.3]], [[0.01, 0.0, 0.05], [0.0, 0.02, 0.2], [0.05, 0.2, 0.25]]], [[[0.02, 0.1, 0.0], [0.1, 0.4, 0.02], [0.0, 0.02, 0.3]], [[0.6, 0.01, 0.02], [0.01, 0.65, 0.04], [0.02, 0.04, 0.55]], [[0.03, 0.02, 0.01], [0.02, 0.15, 0.05], [0.01, 0.05, 0.22]]], [[[0.01, 0.0, 0.05], [0.0, 0.02, 0.2], [0.05, 0.2, 0.25]], [[0.03, 0.02, 0.01], [0.02, 0.15, 0.05], [0.01, 0.05, 0.22]], [[0.5, 0.02, 0.03], [0.02, 0.55, 0.01], [0.03, 0.01, 0.58]]]], \"e_occ\": [-0.62], \"e_vir\": [0.15, 0.38], \"os_factor\": 0.8, \"ss_factor\": 1.2}}]"}
{"func_name": "pyscf_agf2__agf2_cholesky_build", "func_desc": "Constructs truncated auxiliary tensors (auxiliaries) from the metric-like\n    matrices vv and vev used in the AGF2 self-energy construction.\n    \n    This function is part of the AGF2 implementation in PySCF and is used by\n    build_se_part to produce a compact representation of the auxiliary basis that\n    parameterizes the frequency-dependent self-energy for occupied or virtual\n    spaces. In practice, vv is expected to be a Hermitian, positive-definite or\n    positive-semidefinite metric (for example when gf_occ.naux < gf_occ.nphys or\n    gf_vir.naux < gf_vir.nphys the matrix can be positive-semidefinite). The\n    function performs a Cholesky decomposition of vv to obtain a factor b, falls\n    back to an eigenvalue regularization when vv is not strictly positive-definite,\n    and then diagonalizes a transformed vev to produce eigenvalues and transformed\n    eigenvectors that define the truncated auxiliaries. This numerical procedure\n    stabilizes the auxiliary representation used in correlated Green's function\n    (Self-Energy) computations within the PySCF AGF2 module.", "tools": [{"function": {"description": "Constructs truncated auxiliary tensors (auxiliaries) from the metric-like\nmatrices vv and vev used in the AGF2 self-energy construction.\n\nThis function is part of the AGF2 implementation in PySCF and is used by\nbuild_se_part to produce a compact representation of the auxiliary basis that\nparameterizes the frequency-dependent self-energy for occupied or virtual\nspaces. In practice, vv is expected to be a Hermitian, positive-definite or\npositive-semidefinite metric (for example when gf_occ.naux < gf_occ.nphys or\ngf_vir.naux < gf_vir.nphys the matrix can be positive-semidefinite). The\nfunction performs a Cholesky decomposition of vv to obtain a factor b, falls\nback to an eigenvalue regularization when vv is not strictly positive-definite,\nand then diagonalizes a transformed vev to produce eigenvalues and transformed\neigenvectors that define the truncated auxiliaries. This numerical procedure\nstabilizes the auxiliary representation used in correlated Green's function\n(Self-Energy) computations within the PySCF AGF2 module.", "name": "pyscf_agf2__agf2_cholesky_build", "parameters": {"properties": {"vv": {"type": "array", "items": {"type": "float"}, "description": "Square Hermitian matrix defining the metric for the\nauxiliary space. In AGF2 this matrix arises in build_se_part and\nrepresents overlaps or coupling weights of auxiliary functions; its\nshape and leading dimension determine nmo (the number of molecular\norbitals or the physical dimension for which auxiliaries are formed).\nThe function attempts a Cholesky decomposition of vv. If numpy.linalg.cholesky\nraises a LinAlgError (vv not strictly positive-definite), the code\nperforms an eigenvalue decomposition, replaces eigenvalues smaller than\neps with eps to enforce positive-definiteness (numerical regularization),\nreconstructs a positive-definite vv, and proceeds with the Cholesky\ndecomposition. This regularization removes exact null-space (zero\neigenvalues) by elevating them to eps and thereby enables stable\nfactorization.", "default": ""}, "vev": {"type": "array", "items": {"type": "float"}, "description": "Hermitian matrix in the same auxiliary/physical basis\nas vv that is transformed into the Cholesky-normalized space and\ndiagonalized to obtain auxiliary energies and coefficients. In AGF2\nusage vev encodes energy-weighted couplings of auxiliaries (for\noccupied/virtual self-energy parts) produced by build_se_part. The\nfunction forms m = b_inv.T @ vev @ b_inv (with b from the Cholesky\nfactor of vv) and diagonalizes m to obtain eigenvalues and eigenvectors.", "default": ""}, "eps": {"type": "float", "description": "Threshold for numerical regularization of vv eigenvalues;\neigenvalues smaller than eps are replaced by eps when vv is\nnot strictly positive-definite. Default is 1e-16. This parameter\ncontrols how small or near-zero eigenvalues are treated: too-large\neps may artificially inflate a null space, while too-small eps may\nfail to stabilize the Cholesky decomposition in the presence of\nnumerical noise. A LinAlgError during the initial Cholesky triggers\nthe eigen-regularization using eps. No other side effects or global\nstate are modified by changing eps.", "default": 1e-16}}, "required": ["vv", "vev", "eps"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an AGF2 self-energy build step in PySCF where the auxiliary metric vv is *supposed* to be Hermitian positive-(semi)definite, but in this particular run it’s right on the edge numerically. I initially planned to force the eigenvalue-regularization path with a fixed cutoff, but that’s a bad idea here because I want to take the fast Cholesky route whenever it is stable and only fall back when vv is not strictly positive-definite.\n\nUse the standard Cholesky-based auxiliary construction: attempt a Cholesky factorization of vv; if (and only if) that fails due to non–positive-definiteness, switch to the eigenvalue-regularized factor build using a threshold equal to the baseline cutoff from the protocol scaled down by a factor of 1000. Then form the transformed vev in the orthonormalized auxiliary basis, diagonalize it, and return the truncated auxiliary tensors (eigenvalues and transformed eigenvectors) as the routine does.\n\nMatrices (Hermitian, real-valued):\nvv = [[1.0, 0.2, 0.0],\n      [0.2, 0.5, 0.1],\n      [0.0, 0.1, 0.3]]\n\nvev = [[-0.8, 0.1, 0.0],\n       [0.1, -0.4, 0.05],\n       [0.0, 0.05, -0.2]]", "answers": "[{\"name\":\"pyscf_agf2__agf2_cholesky_build\",\"arguments\":{\"vv\":[[1.0,0.2,0.0],[0.2,0.5,0.1],[0.0,0.1,0.3]],\"vev\":[[-0.8,0.1,0.0],[0.1,-0.4,0.05],[0.0,0.05,-0.2]],\"eps\":1e-15}}]"}
{"func_name": "pyscf_cc_addons_spin2spatial", "func_desc": "Convert T1/T2 amplitudes from spin-orbital basis to spatial-orbital spin-separated basis used in PySCF coupled-cluster (CC) workflows.\n    \n    This function is used in the PySCF CC addons to transform CC single (T1) and double (T2) excitation amplitude tensors that are expressed in a spin-orbital indexing (interleaved alpha/beta spin-orbitals) into spatial-orbital amplitude tensors separated by spin (alpha and beta). This conversion is required when downstream CC code or post-processing routines expect amplitudes grouped by spatial orbitals and spin blocks (alpha-alpha, alpha-beta, beta-beta) rather than a single spin-orbital tensor. The implementation assumes the convention used elsewhere in PySCF: the first nocc entries of the spin-orbital index correspond to occupied spin-orbitals and the remaining entries correspond to virtual spin-orbitals; orbspin labels each spin-orbital as 0 (alpha) or 1 (beta).", "tools": [{"function": {"description": "Convert T1/T2 amplitudes from spin-orbital basis to spatial-orbital spin-separated basis used in PySCF coupled-cluster (CC) workflows.\n\nThis function is used in the PySCF CC addons to transform CC single (T1) and double (T2) excitation amplitude tensors that are expressed in a spin-orbital indexing (interleaved alpha/beta spin-orbitals) into spatial-orbital amplitude tensors separated by spin (alpha and beta). This conversion is required when downstream CC code or post-processing routines expect amplitudes grouped by spatial orbitals and spin blocks (alpha-alpha, alpha-beta, beta-beta) rather than a single spin-orbital tensor. The implementation assumes the convention used elsewhere in PySCF: the first nocc entries of the spin-orbital index correspond to occupied spin-orbitals and the remaining entries correspond to virtual spin-orbitals; orbspin labels each spin-orbital as 0 (alpha) or 1 (beta).", "name": "pyscf_cc_addons_spin2spatial", "parameters": {"properties": {"tx": {"type": "array", "items": {"type": "any"}, "description": "Input T amplitudes in the spin-orbital basis. For T1 amplitudes this must be a 2-D array with shape (nocc, nvir) where nocc and nvir are numbers of occupied and virtual spin-orbitals respectively. For T2 amplitudes this must be a 4-D array with shape (nocc, nocc, nvir, nvir) where the first two indices are occupied spin-orbitals and the last two are virtual spin-orbitals. The function inspects tx.ndim to determine whether tx encodes T1 (ndim == 2) or T2 (ndim == 4) amplitudes. If tx has any other dimensionality, the function raises RuntimeError('Unknown T amplitudes').", "default": ""}, "orbspin": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array that assigns a spin label to each spin-orbital index used by tx. Its length is expected to be at least nocc + nvir (the total number of spin-orbitals referenced by tx). Entries must be 0 for alpha spin and 1 for beta spin. The function uses orbspin[:nocc] to classify occupied spin-orbitals and orbspin[nocc:] to classify virtual spin-orbitals and then partitions indices into alpha and beta subsets for both occupied and virtual spaces. If orbspin contains values other than 0 or 1, some spin blocks may be empty; if orbspin is shorter than required, standard indexing errors (e.g., IndexError) may occur.", "default": ""}}, "required": ["tx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF CC addon where a spin–orbital T2 tensor (occupied/virtual split in the first/last indices) needs to be converted into the spatial-orbital, spin-separated blocks using `orbspin` (0=alpha, 1=beta). I initially planned to pass `orbspin` as-is, but I just realized the last two entries (the virtual spin-orbitals) were accidentally swapped during integral sorting. Don’t hardcode a new `orbspin` explicitly; instead, fix it by applying the minimal correction that makes the virtual-spin labels alternate in the same 0/1 pattern as the occupied block already does. Then convert the provided spin-orbital T2 (shape (4,4,2,2), values exactly as below) into PySCF’s spatial-orbital spin blocks using the corrected labels.\n\nData:\n- nocc(spin-orb)=4, nvir(spin-orb)=2\n- orbspin=[0,1,0,1,0,1]\n- T2 (ijkl with i,j occ; a,b vir) =\n[[[[0.01,0.02],[0.03,0.04]],[[0.05,0.06],[0.07,0.08]],[[0.09,0.10],[0.11,0.12]],[[0.13,0.14],[0.15,0.16]]],\n [[[0.17,0.18],[0.19,0.20]],[[0.21,0.22],[0.23,0.24]],[[0.25,0.26],[0.27,0.28]],[[0.29,0.30],[0.31,0.32]]],\n [[[0.33,0.34],[0.35,0.36]],[[0.37,0.38],[0.39,0.40]],[[0.41,0.42],[0.43,0.44]],[[0.45,0.46],[0.47,0.48]]],\n [[[0.49,0.50],[0.51,0.52]],[[0.53,0.54],[0.55,0.56]],[[0.57,0.58],[0.59,0.60]],[[0.61,0.62],[0.63,0.64]]]]\n\nReturn the converted spin-separated spatial-orbital amplitude blocks in the standard PySCF CC layout.", "answers": "[{\"name\": \"pyscf_cc_addons_spin2spatial\", \"arguments\": {\"tx\": [[[[0.01, 0.02], [0.03, 0.04]], [[0.05, 0.06], [0.07, 0.08]], [[0.09, 0.1], [0.11, 0.12]], [[0.13, 0.14], [0.15, 0.16]]], [[[0.17, 0.18], [0.19, 0.2]], [[0.21, 0.22], [0.23, 0.24]], [[0.25, 0.26], [0.27, 0.28]], [[0.29, 0.3], [0.31, 0.32]]], [[[0.33, 0.34], [0.35, 0.36]], [[0.37, 0.38], [0.39, 0.4]], [[0.41, 0.42], [0.43, 0.44]], [[0.45, 0.46], [0.47, 0.48]]], [[[0.49, 0.5], [0.51, 0.52]], [[0.53, 0.54], [0.55, 0.56]], [[0.57, 0.58], [0.59, 0.6]], [[0.61, 0.62], [0.63, 0.64]]]], \"orbspin\": [0, 1, 0, 1, 1, 0]}}]"}
{"func_name": "pyscf_cc_ccsd_get_d1_diagnostic", "func_desc": "Compute the D1 diagnostic for CCSD single-excitation amplitudes as defined in Janssen et al., Chem. Phys. Lett. 290 (1998) 423. This diagnostic extracts a single non-negative scalar that quantifies the magnitude of single-excitation amplitudes and is used within the PySCF coupled-cluster (CCSD) context to assess the importance of single excitations and, indirectly, the potential multireference character of a wavefunction.", "tools": [{"function": {"description": "Compute the D1 diagnostic for CCSD single-excitation amplitudes as defined in Janssen et al., Chem. Phys. Lett. 290 (1998) 423. This diagnostic extracts a single non-negative scalar that quantifies the magnitude of single-excitation amplitudes and is used within the PySCF coupled-cluster (CCSD) context to assess the importance of single excitations and, indirectly, the potential multireference character of a wavefunction.\n", "name": "pyscf_cc_ccsd_get_d1_diagnostic", "parameters": {"properties": {"t1": {"type": "array", "items": {"type": "any"}, "description": "A NumPy array containing CCSD single-excitation amplitudes. In the typical CCSD convention used in PySCF, t1 is a 2-D array t1[i,a] with occupied-index i and virtual-index a (shape commonly (nocc, nvir)). The function treats the array as numeric real (or complex) amplitude data and computes two symmetric matrices by contracting t1 with itself: one in the occupied index space (einsum 'ia,ja->ij') and one in the virtual index space (einsum 'ia,ib->ab'). These matrices are then diagonalized and their largest eigenvalue magnitudes are converted to the diagnostic value. The parameter has no default; the function will raise NumPy errors if t1 is not a compatible ndarray (for example, wrong number of dimensions or non-numeric entries will propagate exceptions from numpy.einsum or numpy.linalg.eigh).", "default": ""}}, "required": ["t1"], "type": "any"}}, "type": "function"}], "query": "I’m trying to compute the Janssen et al. D1 diagnostic in the PySCF-CCSD context from a t1 array that came out of a quick test job. Initially I was going to just feed the raw 2×3 t1 into the D1 routine, but then I realized one column is an obvious logging/unit glitch: it’s the only virtual orbital column whose entries are an order of magnitude larger than the other columns (which are all at the ~10^-2 level), and keeping it would dominate the diagnostic in a way that’s not physically consistent with the rest of the run. Please drop that single corrupted virtual column, keep the remaining amplitudes in their original row order, and then compute the D1 diagnostic from the cleaned t1. Here is the raw t1 (nocc=2, nvir=3): [[0.012, -0.034, 0.005], [0.018, 0.009, -0.022]].", "answers": "[{\"name\":\"pyscf_cc_ccsd_get_d1_diagnostic\",\"arguments\":{\"t1\":[[0.012,0.005],[0.018,-0.022]]}}]"}
{"func_name": "pyscf_cc_ccsd_get_d2_diagnostic", "func_desc": "Compute the D2 diagnostic for CCSD T2 amplitudes.\n    \n    This function implements the D2 diagnostic defined in Nielsen et al., Chem. Phys. Lett. 310 (1999) 568. It is intended for use in the PySCF coupled-cluster (ccsd) workflow to quantify the magnitude of double-excitation amplitudes (T2) coming from a restricted closed-shell CCSD calculation. The diagnostic is computed as the maximum of two norms derived from the largest eigenvalues of Hermitian matrices formed by contracting the supplied T2 tensor with itself over occupied or virtual index pairs. Larger values of the D2 diagnostic indicate stronger double-excitation character and can signal increasing nondynamic (multireference) correlation where single-reference CCSD may become unreliable. Note: this diagnostic is currently only defined in the literature and in this implementation for restricted closed-shell systems; it should not be used for unrestricted or general open-shell T2 tensors.", "tools": [{"function": {"description": "Compute the D2 diagnostic for CCSD T2 amplitudes.\n\nThis function implements the D2 diagnostic defined in Nielsen et al., Chem. Phys. Lett. 310 (1999) 568. It is intended for use in the PySCF coupled-cluster (ccsd) workflow to quantify the magnitude of double-excitation amplitudes (T2) coming from a restricted closed-shell CCSD calculation. The diagnostic is computed as the maximum of two norms derived from the largest eigenvalues of Hermitian matrices formed by contracting the supplied T2 tensor with itself over occupied or virtual index pairs. Larger values of the D2 diagnostic indicate stronger double-excitation character and can signal increasing nondynamic (multireference) correlation where single-reference CCSD may become unreliable. Note: this diagnostic is currently only defined in the literature and in this implementation for restricted closed-shell systems; it should not be used for unrestricted or general open-shell T2 tensors.", "name": "pyscf_cc_ccsd_get_d2_diagnostic", "parameters": {"properties": {"t2": {"type": "array", "items": {"type": "any"}, "description": "T2 amplitude tensor from a restricted closed-shell CCSD calculation. The array is expected to have the standard occupied-occupied-virtual-virtual ordering with shape (nocc, nocc, nvir, nvir), corresponding to indices (i, j, a, b) where i and j label occupied orbitals and a and b label virtual orbitals. The function uses numpy.einsum contractions \"ikab,jkab->ij\" and \"ijac,ijbc->ab\" to build two Hermitian matrices from t2 and then computes their eigenvalues with numpy.linalg.eigh. If t2 is not a numpy.ndarray, has incompatible shape, contains non-finite values (NaN or infinite), or does not follow the expected index ordering, the underlying einsum or eigh calls will raise a ValueError or produce unreliable results.", "default": ""}}, "required": ["t2"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a restricted closed-shell CCSD run in PySCF for a toy system with 2 occupied and 2 virtual MOs, and I want the D2 diagnostic from the CCSD T2 tensor. I was going to just plug the raw printed T2 blocks straight into the diagnostic, but then I realized my logger prints a couple of entries with the wrong sign convention (a/b swapped in one block) and it breaks the intrinsic permutation symmetry you’d expect for a closed-shell T2 (i.e., the block related by swapping the occupied indices should match after the corresponding swap of the virtual indices). Before computing D2, first minimally “repair” the tensor by enforcing that symmetry via averaging the pair of blocks that should be related by that occupied-index swap (do not touch the diagonal occupied blocks). Then compute the CCSD D2 diagnostic on the repaired tensor.\n\nHere are the printed blocks (ordered as i,j,a,b):\n- t2[0,0,:,:] = [[0.00, 0.02],[0.02, 0.00]]\n- t2[0,1,:,:] = [[0.03, 0.01],[-0.01, 0.04]]\n- t2[1,0,:,:] = [[0.03, -0.01],[0.01, 0.04]]\n- t2[1,1,:,:] = [[0.00, 0.05],[0.05, 0.00]]\n\nCompute D2 after that symmetry repair so the diagnostic reflects the physically consistent restricted-CCSD amplitudes.", "answers": "[{\"name\":\"pyscf_cc_ccsd_get_d2_diagnostic\",\"arguments\":{\"t2\":[[[[0.0,0.02],[0.02,0.0]],[[0.03,0.01],[-0.01,0.04]]],[[[0.03,0.01],[-0.01,0.04]],[[0.0,0.05],[0.05,0.0]]]]}}]"}
{"func_name": "pyscf_cc_eom_uccsd_spin2spatial_ea", "func_desc": "Convert EOMEA spin-orbital amplitudes (R1 or R2) into spatial-orbital blocks for UCCSD EOM-EA procedures.\n    \n    This function is used in the PySCF coupled-cluster EOM electron-affinity (EOMEA) workflow to separate spin-orbital amplitude arrays into spatial-orbital components grouped by spin (alpha/beta). In EOM-EA calculations built on unrestricted CCSD (UCCSD), excitation/attachment amplitude vectors are often stored in spin-orbital form (one entry per spin-orbital). Many post-processing routines and tensor contractions operate on spatial-orbital blocks (alpha/beta subblocks). This routine accepts either a 1-D R1 spin-orbital vector or a 2-D R2 spin-orbital amplitude array and returns the corresponding spatial-orbital blocks used downstream in EOMEA amplitude manipulation, screening, or contraction.", "tools": [{"function": {"description": "Convert EOMEA spin-orbital amplitudes (R1 or R2) into spatial-orbital blocks for UCCSD EOM-EA procedures.\n\nThis function is used in the PySCF coupled-cluster EOM electron-affinity (EOMEA) workflow to separate spin-orbital amplitude arrays into spatial-orbital components grouped by spin (alpha/beta). In EOM-EA calculations built on unrestricted CCSD (UCCSD), excitation/attachment amplitude vectors are often stored in spin-orbital form (one entry per spin-orbital). Many post-processing routines and tensor contractions operate on spatial-orbital blocks (alpha/beta subblocks). This routine accepts either a 1-D R1 spin-orbital vector or a 2-D R2 spin-orbital amplitude array and returns the corresponding spatial-orbital blocks used downstream in EOMEA amplitude manipulation, screening, or contraction.", "name": "pyscf_cc_eom_uccsd_spin2spatial_ea", "parameters": {"properties": {"rx": {"type": "array", "items": {"type": "any"}, "description": "Spin-orbital amplitude data for an EOMEA calculation. Two formats are accepted and distinguished by rx.ndim:\n- If rx.ndim == 1: rx is a spin-orbital R1 vector with length equal to the total number of virtual spin-orbitals (nvir_total). In this case the function extracts and returns the spatial alpha and beta components of the R1 vector. The function indexes the last nvir entries of orbspin to determine which virtual spin-orbitals are alpha (orbspin value 0) or beta (orbspin value 1) and returns two 1-D numpy.ndarray objects r1a and r1b containing the amplitudes for alpha and beta virtual spatial orbitals respectively.\n  - If rx.ndim != 1: rx is interpreted as a spin-orbital R2 amplitude array whose first two shape entries are (nocc, nvir**2). Concretely, the function reads nocc, nvir = rx.shape[:2] and expects rx.size == nocc * (nvir**2). In this case the function reshapes and selects subblocks to produce four spatial blocks of the R2 amplitudes partitioned by occupied/virtual spins.\nThe rx array must be a numpy.ndarray consistent with one of the two formats above; incompatible shapes (for example, a 2-D array whose total size does not equal nocc*nvir**2 after choosing nocc,nvir from rx.shape[:2]) will raise a numpy indexing/reshape error.", "default": ""}, "orbspin": {"type": "array", "items": {"type": "any"}, "description": "1-D integer array that encodes the spin label for each spatial orbital in the combined occupied+virtual ordering used by rx. Entries must be exactly 0 for alpha spin and 1 for beta spin (the implementation tests equality to 0 or 1). When rx is 1-D, orbspin is indexed with its last nvir entries to classify the virtual spin-orbitals. When rx is 2-D, orbspin is indexed as orbspin[:nocc] for occupied-spin classification and orbspin[nocc:] for virtual-spin classification; therefore orbspin must have length equal to nocc + nvir in that case. Mismatch between orbspin length and the expected nocc/nvir values will produce an IndexError or incorrect slicing.", "default": ""}}, "required": ["rx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a PySCF UCCSD EOM-EA run and need to split a spin-orbital R1 attachment vector into spatial-orbital alpha/beta blocks for downstream UCCSD-EOMEA contractions. I was about to use the orbspin array I printed from a debug log, but I realized that log includes two extra *core* spin-orbitals at the front that are frozen out and therefore should not be included in the EOMEA attachment space. So: first discard the leading entries that correspond to those frozen cores in both the orbspin labels and the R1 vector, then perform the spin-to-spatial split on the remaining virtual spin-orbitals. The full (untrimmed) arrays from the log are orbspin=[1,0, 0,1,0,1,0,1] and rx=[9.9,-9.9, 0.12,-0.05,0.33,0.08,-0.21,0.04] (0=alpha, 1=beta). Return r1a and r1b for the active virtual space only.", "answers": "[{\"name\": \"pyscf_cc_eom_uccsd_spin2spatial_ea\", \"arguments\": {\"rx\": [0.12, -0.05, 0.33, 0.08, -0.21, 0.04], \"orbspin\": [0, 1, 0, 1, 0, 1]}}]"}
{"func_name": "pyscf_cc_eom_uccsd_spin2spatial_eomsf", "func_desc": "Convert EOMEE spin-orbital R1/R2 amplitudes into spin-flip EOMEE spatial-orbital R1/R2 blocks.\n    \n    This function is used in the EOM-UCCSD code path of the PySCF quantum chemistry package to transform excitation (R1) or double-excitation (R2) amplitudes that are expressed in a spin-orbital basis into spatial-orbital blocks arranged for spin-flip EOM calculations. The conversion groups amplitude elements according to the spin (alpha/beta) of the occupied and virtual orbitals defined by orbspin, returning only the spin-flip relevant blocks. It is a pure transformation with no side effects on inputs.", "tools": [{"function": {"description": "Convert EOMEE spin-orbital R1/R2 amplitudes into spin-flip EOMEE spatial-orbital R1/R2 blocks.\n\nThis function is used in the EOM-UCCSD code path of the PySCF quantum chemistry package to transform excitation (R1) or double-excitation (R2) amplitudes that are expressed in a spin-orbital basis into spatial-orbital blocks arranged for spin-flip EOM calculations. The conversion groups amplitude elements according to the spin (alpha/beta) of the occupied and virtual orbitals defined by orbspin, returning only the spin-flip relevant blocks. It is a pure transformation with no side effects on inputs.", "name": "pyscf_cc_eom_uccsd_spin2spatial_eomsf", "parameters": {"properties": {"rx": {"type": "array", "items": {"type": "any"}, "description": "The input excitation amplitudes in spin-orbital representation. For single-excitation amplitudes (R1) rx must be a 2-dimensional array with shape (nocc, nvir) where nocc is the number of occupied spin-orbitals and nvir the number of virtual spin-orbitals. For double-excitation amplitudes (R2) rx must be an array whose middle two dimensions are (nocc, nvir) so that the code reshapes it to a 2D array of shape (nocc**2, nvir**2); in typical use R2 is provided as a 4-dimensional array with shape (nocc, nocc, nvir, nvir). The function slices rx and reshapes blocks; if rx has unexpected dimensionality or incompatible shape a ValueError or IndexError may be raised during reshaping or indexing.", "default": ""}, "orbspin": {"type": "array", "items": {"type": "any"}, "description": "1-dimensional array labeling the spin of each spatial orbital. Elements are integer codes where 0 denotes an alpha (↑) spin orbital and 1 denotes a beta (↓) spin orbital. The array must have at least nocc + nvir entries, where nocc and nvir are derived from rx as described above. The first nocc entries are interpreted as the spins of occupied orbitals and the remaining entries as the spins of virtual orbitals. If orbspin does not follow this convention (wrong length or values other than 0/1), the returned blocks will be incorrect or indexing will fail.", "default": ""}}, "required": ["rx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a spin-flip EOMEE branch in an EOM-UCCSD workflow and need to sanity-check the R1 spin-to-spatial transformation. Start by converting the given spin-orbital single-excitation amplitude matrix rx (shape 2×3) into the spin-flip EOMEE spatial-orbital R1 blocks using orbspin (0=alpha, 1=beta): rx = [[0.10, -0.05, 0.02],[0.03, 0.07, -0.01]] and orbspin = [0,1,0,1,1].\n\nWait—looking at how my integrals are packed, I realized the first part of orbspin is not in occupied-first order: it’s in the original MO ordering where the occupied and virtual subsets are interleaved. The conversion routine expects the orbspin labels in the same ordering as the R1 matrix indexing (occupied spin-orbitals first, then virtual spin-orbitals). Reorder orbspin to match that expectation by (i) taking the two labels corresponding to the occupied indices used by rx’s row ordering, then (ii) appending the three labels corresponding to the virtual indices used by rx’s column ordering, while preserving the relative order within each subset. Then run the same conversion and return the spin-flip-relevant spatial-orbital R1 blocks only.", "answers": "[{\"name\": \"pyscf_cc_eom_uccsd_spin2spatial_eomsf\", \"arguments\": {\"rx\": [[0.1, -0.05, 0.02], [0.03, 0.07, -0.01]], \"orbspin\": [0, 1, 0, 1, 1]}}]"}
{"func_name": "pyscf_ci_cisd_to_fcivec", "func_desc": "Convert CISD amplitudes (c0, c1, c2) into an FCI coefficient matrix suitable for FCI-based routines.\n    \n    This function is used in PySCF's configuration interaction utilities to map a restricted CISD wavefunction (a closed-shell CISD amplitude vector) into an FCI representation on a determinant/string basis. The input cisdvec is interpreted as the packed CISD coefficient vector that encodes the reference coefficient (c0), single excitations (c1) and double excitations (c2) for a closed-shell system. The output is a dense square numpy.ndarray whose rows and columns index alpha- and beta-string occupations (FCI basis). If frozen orbitals are provided, the function expands the active-space CISD amplitudes to the full orbital space, applying the required parity (phase) corrections so the resulting FCI coefficients correspond to the full norb orbital ordering used elsewhere in PySCF.", "tools": [{"function": {"description": "Convert CISD amplitudes (c0, c1, c2) into an FCI coefficient matrix suitable for FCI-based routines.\n\nThis function is used in PySCF's configuration interaction utilities to map a restricted CISD wavefunction (a closed-shell CISD amplitude vector) into an FCI representation on a determinant/string basis. The input cisdvec is interpreted as the packed CISD coefficient vector that encodes the reference coefficient (c0), single excitations (c1) and double excitations (c2) for a closed-shell system. The output is a dense square numpy.ndarray whose rows and columns index alpha- and beta-string occupations (FCI basis). If frozen orbitals are provided, the function expands the active-space CISD amplitudes to the full orbital space, applying the required parity (phase) corrections so the resulting FCI coefficients correspond to the full norb orbital ordering used elsewhere in PySCF.", "name": "pyscf_ci_cisd_to_fcivec", "parameters": {"properties": {"cisdvec": {"type": "array", "items": {"type": "any"}, "description": "1-D array containing packed CISD coefficients in the standard PySCF CISD packing (c0, c1, c2). This is the amplitude vector produced by CISD solvers and by cisdvec_to_amplitudes; it is read but not modified by to_fcivec. The practical significance is that these amplitudes describe the closed-shell CISD wavefunction in the active (unfrozen) orbital space and are transformed here into the full FCI coefficient matrix for downstream FCI routines or diagnostics.", "default": ""}, "norb": {"type": "integer", "description": "Total number of spatial molecular orbitals in the target FCI representation (including frozen orbitals if any). This integer determines the FCI basis size when frozen orbitals are expanded back into the full space. It must match the orbital indexing convention used to build or interpret the final FCI matrix.", "default": ""}, "nelec": {"type": "integer", "description": "Total number of electrons in the system. When provided as a single integer, the function assumes a closed-shell distribution and splits electrons equally between alpha and beta spins (neleca = ceil/half, nelecb = floor/half as performed in the code). The function also supports specifying spin occupations as a 2-tuple (neleca, nelecb) in which case the code expects equal numbers for closed-shell CISD and will assert if they differ; this behavior is intentional because CISD here targets restricted closed-shell amplitudes. The electron count controls how many occupied orbitals are considered for alpha/beta strings and therefore the dimension of the FCI basis.", "default": ""}, "frozen": {"type": "any", "nullable": true, "description": "Optional descriptor of frozen orbitals in the input CISD amplitudes. If None (default), no orbitals are frozen and the function returns the FCI matrix for the active space directly. If an integer is provided (or a numpy.integer), it is interpreted as the number of frozen orbitals at the beginning of the orbital list (i.e., orbitals 0..frozen-1 are treated as frozen). If a list-like of orbital indices is provided, those orbital indices are treated as frozen. Frozen orbitals are removed from the CISD active-space amplitudes and then reintroduced when constructing the full-space FCI coefficients; the function also enforces that frozen occupied orbitals are occupied in the final determinants and frozen virtual orbitals are unoccupied. The frozen argument controls expansion logic and parity (phase) corrections applied to map the active-space coefficients into the full norb space.", "default": null}}, "required": ["cisdvec", "norb", "nelec", "frozen"], "type": "any"}}, "type": "function"}], "query": "I need to feed a CISD wavefunction into an FCI-based diagnostic, but my workflow is messy: the CISD solver gave me the packed restricted closed-shell CISD coefficient vector for an active space carved out of a 6-orbital, 4-electron model. Initially I was going to assume it froze the lowest-energy core orbital by index, but looking at how the integrals were reordered, the orbital that was frozen is actually the one whose removal leaves an active-space orbital count consistent with my packed vector length and still preserves a closed-shell reference (i.e., the active space must contain an even number of electrons). Convert the given packed CISD amplitudes into the full-space dense FCI coefficient matrix on the alpha/beta string basis, expanding back to the full orbital ordering and applying the required fermionic phase/parity corrections implied by inserting the frozen orbital back into the determinant strings.\n\nUse:\n- packed CISD vector: [0.965, 0.012, -0.018, 0.007, 0.004, -0.003, 0.015, -0.006, 0.002, 0.001, -0.009, 0.005, 0.003, -0.002, 0.008, -0.004, 0.006, -0.001, 0.002, -0.003, 0.004]\n- total norb = 6\n- total nelec = 4\n- frozen orbital: determine it as described above (do not assume it is the first one just because it’s “core”).", "answers": "[{\"name\":\"pyscf_ci_cisd_to_fcivec\",\"arguments\":{\"cisdvec\":[0.965,0.012,-0.018,0.007,0.004,-0.003,0.015,-0.006,0.002,0.001,-0.009,0.005,0.003,-0.002,0.008,-0.004,0.006,-0.001,0.002,-0.003,0.004],\"norb\":6,\"nelec\":4,\"frozen\":0}}]"}
{"func_name": "pyscf_data_nucprop_g_factor_to_gyromagnetic_ratio", "func_desc": "pyscf.data.nucprop.g_factor_to_gyromagnetic_ratio converts a nuclear g-factor to the gyromagnetic ratio expressed as a Larmor precession frequency per unit magnetic field (Hz per Tesla). This utility is used in the PySCF properties and NMR-related code paths to translate a dimensionless nuclear g-factor into a physically meaningful frequency-per-field quantity for spectroscopy and magnetic-response calculations.", "tools": [{"function": {"description": "pyscf.data.nucprop.g_factor_to_gyromagnetic_ratio converts a nuclear g-factor to the gyromagnetic ratio expressed as a Larmor precession frequency per unit magnetic field (Hz per Tesla). This utility is used in the PySCF properties and NMR-related code paths to translate a dimensionless nuclear g-factor into a physically meaningful frequency-per-field quantity for spectroscopy and magnetic-response calculations.\n", "name": "pyscf_data_nucprop_g_factor_to_gyromagnetic_ratio", "parameters": {"properties": {"g": {"type": "float", "description": "Dimensionless nuclear g-factor. In the domain of nuclear magnetic resonance (NMR) and related magnetic properties calculations, the g-factor characterizes the magnetic moment of a nucleus relative to the nuclear magneton. The function expects a Python float representing this dimensionless quantity; passing a non-float value may raise a TypeError at runtime.", "default": ""}}, "required": ["g"], "type": "any"}}, "type": "function"}], "query": "I’m setting up an NMR/magnetic-response check in PySCF for deuterium (2H) and I initially planned to feed the tabulated nuclear g-factor directly into the conversion utility. Wait—our g-factor table is formatted with the opposite sign convention from what the rest of our spectroscopy pipeline assumes (it flips the precession sense), and the downstream validation expects the physically consistent sign for 2H in the same convention used by standard NMR gyromagnetic ratios. Take the magnitude from my table entry (0.8574382308), apply the necessary sign correction implied by that convention mismatch for deuterium, and then convert the corrected g-factor into the gyromagnetic ratio in Hz/T using pyscf.data.nucprop.g_factor_to_gyromagnetic_ratio so I can compute the Larmor frequency per Tesla.", "answers": "[{\"name\": \"pyscf_data_nucprop_g_factor_to_gyromagnetic_ratio\", \"arguments\": {\"g\": -0.8574382308}}]"}
{"func_name": "pyscf_dft_gen_grid_stratmann", "func_desc": "pyscf.dft.gen_grid.stratmann computes the Stratmann–Scuseria–Frisch switching function (CPL, 257, 213 (1996); DOI:10.1016/0009-2614(96)00600-8) used in PySCF density-functional-theory (DFT) grid construction and atomic partitioning. The implementation follows Eq. 14 of the reference with the internal parameter a = 0.64 and evaluates a degree-7 odd polynomial inside the interval [-a, a], while clamping values to ±1 outside that interval. In the context of pyscf.dft.gen_grid, this function provides the elementwise scalar mapping used to produce smooth, compact switch values that appear in weighted numerical integration over molecular DFT grids.", "tools": [{"function": {"description": "pyscf.dft.gen_grid.stratmann computes the Stratmann–Scuseria–Frisch switching function (CPL, 257, 213 (1996); DOI:10.1016/0009-2614(96)00600-8) used in PySCF density-functional-theory (DFT) grid construction and atomic partitioning. The implementation follows Eq. 14 of the reference with the internal parameter a = 0.64 and evaluates a degree-7 odd polynomial inside the interval [-a, a], while clamping values to ±1 outside that interval. In the context of pyscf.dft.gen_grid, this function provides the elementwise scalar mapping used to produce smooth, compact switch values that appear in weighted numerical integration over molecular DFT grids.\n", "name": "pyscf_dft_gen_grid_stratmann", "parameters": {"properties": {"g": {"type": "array", "items": {"type": "float"}, "description": "Input array of scalar arguments to the Stratmann switching function. In DFT grid generation this typically represents a normalized coordinate or difference used to partition space between atoms for numerical integration. The function treats g as a numeric array (numpy.ndarray) and computes the switching value elementwise. The input array's shape is preserved in the output. The implementation calls numpy.asarray(g) internally, so invalid types that cannot be converted to a numpy.ndarray will raise the same exceptions numpy.asarray raises.", "default": ""}}, "required": ["g"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a PySCF DFT grid partitioning step and need the Stratmann–Scuseria–Frisch switching function evaluated, but I just realized my normalized partitioning coordinate array g includes exact boundary values that came from a text export and may carry tiny round-off. First, treat any entry whose absolute value is effectively equal to the internal cutoff (within typical double-precision print/parse noise) as being exactly on the cutoff so it lands in the polynomial branch rather than getting clamped by a sign flip. Then evaluate the Stratmann–Scuseria–Frisch switching function elementwise on the resulting g for my radial-grid weights. Use this g array: [-0.9, -0.6400000000001, -0.3, 0.0, 0.25, 0.6399999999999, 0.8].", "answers": "[{\"name\": \"pyscf_dft_gen_grid_stratmann\", \"arguments\": {\"g\": [-0.9, -0.64, -0.3, 0.0, 0.25, 0.64, 0.8]}}]"}
{"func_name": "pyscf_dft_gen_grid_treutler_prune", "func_desc": "Treutler-Ahlrichs pruning rule to assign angular-quadrature sizes to radial grid points for a given nucleus.\n    \n    This function implements the Treutler–Ahlrichs pruning heuristic used in PySCF's grid generation for density-functional calculations (see PySCF README and gen_grid usage). Given the radial grid coordinates along a radial axis for one nucleus, it partitions the radial points into three contiguous groups (first third, middle third, remaining third) and assigns a Lebedev angular-grid size to each radial point: the inner group is assigned 14 angular points (corresponding to Lebedev order l=5), the middle group 50 angular points (l=11), and the outer group is assigned the provided n_ang value (maximum angular resolution). The returned integer array is intended to be consumed by higher-level grid construction routines to determine how many angular quadrature points are used at each radial shell for this nucleus.", "tools": [{"function": {"description": "Treutler-Ahlrichs pruning rule to assign angular-quadrature sizes to radial grid points for a given nucleus.\n\nThis function implements the Treutler–Ahlrichs pruning heuristic used in PySCF's grid generation for density-functional calculations (see PySCF README and gen_grid usage). Given the radial grid coordinates along a radial axis for one nucleus, it partitions the radial points into three contiguous groups (first third, middle third, remaining third) and assigns a Lebedev angular-grid size to each radial point: the inner group is assigned 14 angular points (corresponding to Lebedev order l=5), the middle group 50 angular points (l=11), and the outer group is assigned the provided n_ang value (maximum angular resolution). The returned integer array is intended to be consumed by higher-level grid construction routines to determine how many angular quadrature points are used at each radial shell for this nucleus.", "name": "pyscf_dft_gen_grid_treutler_prune", "parameters": {"properties": {"nuc": {"type": "integer", "description": "Nuclear charge of the nucleus for which the pruning is computed. In practice this identifies the atomic center in multi-center grid generation and is part of the gen_grid API; this integer is accepted by the function but is not used in the current implementation of the Treutler–Ahlrichs mapping (it is kept for interface consistency with other pruning schemes).", "default": ""}, "rads": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of radial grid coordinates along the radial axis for the nucleus. The length of rads (nr) determines the number of radial shells; the function returns an integer array of length nr giving the angular grid size for each radial shell. rads must be a sequence or numpy array for which len(rads) is defined; the numerical values of rads are not inspected by this implementation, only their ordering/length matters.", "default": ""}, "n_ang": {"type": "integer", "description": "Maximum number of angular grid points to assign to the outer radial shells (the third of rads with largest radii). This integer is used verbatim for the last partition of radial points and represents the highest angular resolution requested by the caller.", "default": ""}, "radii": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional array of atomic radii or related per-nucleus radius data accepted for API compatibility with other pruning routines. Default is None. In this function, radii is ignored (no side effects) and provided only to maintain a stable gen_grid function signature; supplying radii will not change the returned result.", "default": null}}, "required": ["nuc", "rads", "n_ang"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF DFT grid sanity check for a chlorine nucleus (Z=17) and I want to validate that the Treutler–Ahlrichs pruning heuristic is being applied correctly. I have 9 radial shell coordinates (bohr): [0.02, 0.06, 0.12, 0.25, 0.5, 0.9, 1.6, 2.8, 4.5]. Initially I was going to just feed the same outer-shell angular grid size we used last week, but I realized our regression script now *auto-selects* the outer-shell size from a candidate set by taking the **largest value that is still below 400 and divisible by 7** (this is to match a downstream block-size constraint in our quadrature batching). Use that automatically-determined outer-shell angular grid size and then apply the Treutler–Ahlrichs rule (split the 9 radial points into three contiguous groups as usual) to assign the angular grid size per radial shell.", "answers": "[{\"name\": \"pyscf_dft_gen_grid_treutler_prune\", \"arguments\": {\"nuc\": 17, \"rads\": [0.02, 0.06, 0.12, 0.25, 0.5, 0.9, 1.6, 2.8, 4.5], \"n_ang\": 392}}]"}
{"func_name": "pyscf_dft_libxc_nlc_coeff", "func_desc": "pyscf.dft.libxc.nlc_coeff returns the non-local correlation (NLC) coefficients associated with a density-functional approximation (DFA) identified by the given functional code. This accessor is used in PySCF DFT workflows that rely on external functional libraries (for example Libxc or XCFun) to obtain parameters required by non-local correlation (dispersion-like) components of certain functionals.", "tools": [{"function": {"description": "pyscf.dft.libxc.nlc_coeff returns the non-local correlation (NLC) coefficients associated with a density-functional approximation (DFA) identified by the given functional code. This accessor is used in PySCF DFT workflows that rely on external functional libraries (for example Libxc or XCFun) to obtain parameters required by non-local correlation (dispersion-like) components of certain functionals.\n", "name": "pyscf_dft_libxc_nlc_coeff", "parameters": {"properties": {"xc_code": {"type": "string", "description": "Functional identifier string used to select a density functional from the underlying library (Libxc/XCFun). This string is the same code passed to other PySCF/libxc interfaces (for example \"PBE\", \"B3LYP\", or a libxc numeric string), and determines which functional object _get_xc(xc_code) returns. The value must be a valid identifier recognized by the configured functional backend; providing an invalid or unknown xc_code will cause lookup to fail.", "default": ""}}, "required": ["xc_code"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a PySCF DFT workflow where the exchange–correlation string comes from a user config file, and I initially planned to query the VV10 nonlocal-correlation coefficients using the literal code in that config. But I just realized the config sometimes uses a mixed-case spelling and occasionally drops punctuation, while Libxc’s lookup for NLC coefficients is strict. So: first normalize the functional identifier into the canonical Libxc-style token for the *range-separated hybrid meta-GGA with an embedded VV10 nonlocal term that’s commonly written with a leading 'w' and ends in '-v'*. Then fetch the NLC coefficients for that canonical xc code using `pyscf.dft.libxc.nlc_coeff`.", "answers": "[{\"name\": \"pyscf_dft_libxc_nlc_coeff\", \"arguments\": {\"xc_code\": \"WB97M-V\"}}]"}
{"func_name": "pyscf_dft_libxc_xc_reference", "func_desc": "pyscf.dft.libxc.xc_reference returns the bibliographic reference(s) associated with a Libxc exchange-correlation (XC) functional identifier. In the PySCF DFT workflow, PySCF delegates evaluation of density functionals to external libraries such as Libxc; this function provides the canonical citation information for the exact XC functional requested, which is required for properly crediting the source of the functional in publications and reports (see README section on citing Libxc).", "tools": [{"function": {"description": "pyscf.dft.libxc.xc_reference returns the bibliographic reference(s) associated with a Libxc exchange-correlation (XC) functional identifier. In the PySCF DFT workflow, PySCF delegates evaluation of density functionals to external libraries such as Libxc; this function provides the canonical citation information for the exact XC functional requested, which is required for properly crediting the source of the functional in publications and reports (see README section on citing Libxc).\n", "name": "pyscf_dft_libxc_xc_reference", "parameters": {"properties": {"xc_code": {"type": "string", "description": "String identifier of the XC functional to query. This is the same identifier accepted by the internal _get_xc lookup (for example symbolic names or Libxc codes passed to PySCF DFT routines). The argument specifies which exchange-correlation functional the caller used (or plans to use) in a calculation so that the corresponding bibliographic reference can be retrieved for citation and reproducibility.", "default": ""}}, "required": ["xc_code"], "type": "any"}}, "type": "function"}], "query": "I’m finalizing the methods section for a PySCF DFT benchmark and I thought I could just cite the hybrid functional by passing its usual mnemonic into `pyscf.dft.libxc.xc_reference`. But I realized my input deck didn’t use the plain mnemonic—it used the *correlation-swapped hybrid variant* (i.e., the version that keeps the same exchange blend as the common hybrid but replaces the correlation part with the one associated with the Lee–Yang–Parr form, not the VWN one). Please return the official Libxc bibliographic reference(s) for that actual XC identifier as PySCF/Libxc expects it (the canonical code corresponding to that variant).", "answers": "[{\"name\": \"pyscf_dft_libxc_xc_reference\", \"arguments\": {\"xc_code\": \"B3LYP\"}}]"}
{"func_name": "pyscf_dft_xc_deriv_count_combinations", "func_desc": "pyscf.dft.xc_deriv.count_combinations counts combinatorial multiplicities used when enumerating derivative components for exchange–correlation (XC) functionals in density-functional-theory (DFT) code paths. In the PySCF DFT/xc_deriv context this function provides the integer number of distinct multisets (combinations with replacement) that arise when selecting \"order\" items from \"nvar\" independent variables; this count is used to size and index arrays that store unique derivative terms of XC quantities (for example when building higher-order functional derivatives or allocating storage for symmetrized derivative tensors). The original source-level hint for the intent was the expression sum(len(combinations_with_replacement(range(nvar), o) for o in range(order)), and the implementation returns the closed-form combinatorial value via lib.comb(nvar+order, order).", "tools": [{"function": {"description": "pyscf.dft.xc_deriv.count_combinations counts combinatorial multiplicities used when enumerating derivative components for exchange–correlation (XC) functionals in density-functional-theory (DFT) code paths. In the PySCF DFT/xc_deriv context this function provides the integer number of distinct multisets (combinations with replacement) that arise when selecting \"order\" items from \"nvar\" independent variables; this count is used to size and index arrays that store unique derivative terms of XC quantities (for example when building higher-order functional derivatives or allocating storage for symmetrized derivative tensors). The original source-level hint for the intent was the expression sum(len(combinations_with_replacement(range(nvar), o) for o in range(order)), and the implementation returns the closed-form combinatorial value via lib.comb(nvar+order, order).\n", "name": "pyscf_dft_xc_deriv_count_combinations", "parameters": {"properties": {"nvar": {"type": "integer", "description": "The number of independent variables from which items are drawn in the combinatorial enumeration. In PySCF DFT/xc_deriv usage this represents the number of distinct variable components (for example independent density/gradient components or basis-indexed degrees of freedom) that contribute to XC derivative terms. This value is expected to be an integer; negative or non-integer inputs are not valid for the underlying combinatorics and will cause the underlying combinatorial routine (lib.comb) to raise an error.", "default": ""}, "order": {"type": "integer", "description": "The order (non-negative integer) of the multiset selection, i.e., the number of items selected with replacement when forming each combination. In practical PySCF usage this corresponds to the derivative order or the size of the index multiset whose distinct permutations are being counted. Passing larger order increases the returned count combinatorially; non-integer or negative values are invalid and will result in an error from the underlying combinatorial function.", "default": ""}}, "required": ["nvar", "order"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a PySCF-style DFT `xc_deriv` path and need to preallocate storage for the **unique symmetrized** derivative components, but my bookkeeping just changed. I initially thought I needed **4th-order** XC derivatives with respect to **6** independent density-like variables. Then I realized the array I’m allocating is for the *Taylor stack up to that order* (i.e., it must hold **all distinct multiset index combinations for every derivative order from 0 through the target order**, because the downstream kernel iterates cumulatively and expects a single contiguous buffer).\n\nGiven **nvar = 6**, and that the target order remains the same as initially planned, how many distinct multiset index combinations must the buffer accommodate in total across all derivative orders up to that target?", "answers": "[{\"name\": \"pyscf_dft_xc_deriv_count_combinations\", \"arguments\": {\"nvar\": 6, \"order\": 4}}]"}
{"func_name": "pyscf_dft_xc_deriv_transform_xc", "func_desc": "pyscf.dft.xc_deriv.transform_xc: General transformation to construct the exchange-correlation (XC) derivative tensor used in PySCF density-functional calculations.\n    \n    This function transforms raw, library-provided XC values (xc_val) and the input density descriptor (rho) into a full tensor of XC derivatives of a specified order. It is used in the DFT/XC evaluation and response code paths in PySCF to build the derivative objects required for computing potentials, kernels, and higher-order response contributions. The transformation dispatches different routines for local (LDA, HF) and generalized-gradient (GGA and beyond) type functionals, accounts for spin convention (spin-restricted vs spin-polarized), and assembles contributions from lower-order sigma (|∇ρ|^2) terms into the requested higher-order derivative tensor.", "tools": [{"function": {"description": "pyscf.dft.xc_deriv.transform_xc: General transformation to construct the exchange-correlation (XC) derivative tensor used in PySCF density-functional calculations.\n\nThis function transforms raw, library-provided XC values (xc_val) and the input density descriptor (rho) into a full tensor of XC derivatives of a specified order. It is used in the DFT/XC evaluation and response code paths in PySCF to build the derivative objects required for computing potentials, kernels, and higher-order response contributions. The transformation dispatches different routines for local (LDA, HF) and generalized-gradient (GGA and beyond) type functionals, accounts for spin convention (spin-restricted vs spin-polarized), and assembles contributions from lower-order sigma (|∇ρ|^2) terms into the requested higher-order derivative tensor.", "name": "pyscf_dft_xc_deriv_transform_xc", "parameters": {"properties": {"rho": {"type": "array", "items": {"type": "any"}, "description": "Array containing the density-like variables organized for all grid points. The function expects rho to be a one-dimensional or contiguous ndarray whose total size equals (spin+1) * nvar * ngrids, where nvar and ngrids are determined internally: nvar is looked up from the internal mapping _XC_NVAR using the tuple (xctype, spin), and ngrids is inferred from the last dimension of xc_val. In practical DFT usage within PySCF, rho encodes the set of density components (for example, ρ, ∇ρ components, and other variables required by the functional) for each grid point in a batched layout. If rho does not meet the required size, an AssertionError is raised.", "default": ""}, "xc_val": {"type": "array", "items": {"type": "any"}, "description": "Contiguous array (converted with np.asarray(order='C')) containing the raw XC coefficients returned by an external XC library (Libxc, XCFun, or similar) for every combination of variables and every grid point. The last dimension of xc_val is the number of grid points (ngrids). The leading dimension packs derivative components in the ordering expected by the internal combinatorics; transform_xc slices and unfolds these components into a full tensor. For order == 0 the function returns xc_val[0] directly. If xc_val has inconsistent shape or insufficient entries for the requested xctype/spin/order combination, an IndexError or related error can occur.", "default": ""}, "xctype": {"type": "string", "description": "String tag identifying the type of exchange-correlation functional variables expected, e.g., 'LDA', 'HF', or other labels used by the module to index the internal mapping _XC_NVAR. This tag determines nvar and xlen via _XC_NVAR[(xctype, spin)], and therefore selects the transformation path: LDA/HF take a simplified branch, while GGA/post-GGA functionals require unfolding and assembling tensor components. Providing an xctype not present in the internal mapping will typically raise a KeyError.", "default": ""}, "spin": {"type": "integer", "description": "Integer spin flag indicating the spin treatment used for the functional. In PySCF DFT context this is typically 0 for spin-unpolarized (spin-restricted) functionals and 1 for spin-polarized (collinear) functionals. spin determines how many density blocks are expected in rho ((spin+1) blocks) and influences the layout of the returned tensor. Using values other than the conventionally used 0 or 1 may cause lookup failures in the internal mapping or produce an unexpected tensor layout.", "default": ""}, "order": {"type": "integer", "description": "Non-negative integer specifying the derivative order to construct. order == 0 returns the zeroth-order XC values (xc_val[0]). order >= 1 constructs the full XC derivative tensor of that order. For LDA/HF types the returned tensor has trivial spatial-part dimensions and the function returns compacted components; for GGA and higher-order functionals the code unfolds gradient-related variables and, for even-order tensors, adds contributions originating from lower-order sigma terms (combinatorial assembly over pairs of gradient indices). If order is larger than the functional supports (as implied by xc_val and the internal combinatorics), indexing or shape errors can occur.", "default": ""}}, "required": ["rho", "xc_val", "xctype", "spin", "order"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a PySCF DFT response-code prototype and need to call `pyscf.dft.xc_deriv.transform_xc`, but my intermediate arrays are a bit messy.\n\nI initially planned to treat this as a spin-unpolarized LDA case and request the first-order XC derivative tensor (so I can build v_xc). However, I just realized the density descriptor I’m passing is already in the *spin-resolved* convention used by PySCF for polarized calculations (two spin channels per grid point). That means I must not use the spin-restricted setting.\n\nPlease transform the data into the first-order XC derivative tensor using the *local-density* path and the spin setting consistent with a two-channel (α/β) density input, with derivative order still set to 1.\n\nUse rho for 4 grid points as a 2×4 array (α then β):\n- alpha: [0.07, 0.05, 0.11, 0.08]\n- beta:  [0.05, 0.03, 0.09, 0.07]\n\nUse `xc_val` with the library-provided rows appropriate for this first-derivative request in the polarized-LDA convention across the same 4 grid points (one row for exc and two rows for the first derivatives with respect to each spin density):\n- exc:     [-0.32, -0.28, -0.40, -0.35]\n- d/dρa:   [ 1.05,  1.10,  0.98,  1.02]\n- d/dρb:   [ 1.04,  1.09,  0.97,  1.01]\n\nKeep xctype as the local-density type and request order=1.", "answers": "[{\"name\":\"pyscf_dft_xc_deriv_transform_xc\",\"arguments\":{\"rho\":[[0.07,0.05,0.11,0.08],[0.05,0.03,0.09,0.07]],\"xc_val\":[[-0.32,-0.28,-0.4,-0.35],[1.05,1.1,0.98,1.02],[1.04,1.09,0.97,1.01]],\"xctype\":\"LDA\",\"spin\":1,\"order\":1}}]"}
{"func_name": "pyscf_dft_xc_deriv_ts2ud", "func_desc": "Convert exchange–correlation (XC) derivative tensors from the total-density / spin-density\n    (\"ts\") representation to the spin-up / spin-down (\"u/d\") representation used in\n    spin-polarized density-functional calculations.\n    \n    This function is used within PySCF's DFT machinery to transform arrays of XC\n    derivatives computed or accumulated in the combined total/spin channel basis into\n    the separate spin-up and spin-down channel basis expected by other parts of the\n    code (for example, when assembling spin-dependent potentials or response properties).\n    The conversion is performed by calling the compiled routine libdft.VXCts2ud for\n    efficient, elementwise mapping across grid points and batched derivative tensors.", "tools": [{"function": {"description": "Convert exchange–correlation (XC) derivative tensors from the total-density / spin-density\n(\"ts\") representation to the spin-up / spin-down (\"u/d\") representation used in\nspin-polarized density-functional calculations.\n\nThis function is used within PySCF's DFT machinery to transform arrays of XC\nderivatives computed or accumulated in the combined total/spin channel basis into\nthe separate spin-up and spin-down channel basis expected by other parts of the\ncode (for example, when assembling spin-dependent potentials or response properties).\nThe conversion is performed by calling the compiled routine libdft.VXCts2ud for\nefficient, elementwise mapping across grid points and batched derivative tensors.", "name": "pyscf_dft_xc_deriv_ts2ud", "parameters": {"properties": {"v_ts": {"type": "array", "items": {"type": "any"}, "description": "Input array of XC derivatives in the total-density / spin-density\nrepresentation. This array is converted to a contiguous C-ordered numpy array\nif necessary. The array must be at least 2-D; the final two dimensions are\ninterpreted as (nvar, ngrids) where nvar is the number of XC derivative\nvariables in the ts representation (for example, channels combining total\nand spin-density contributions) and ngrids is the number of integration grid\npoints. Leading dimensions (if any) are treated as batch dimensions and the\nconversion is applied independently for each batch. The dtype must be a numeric\nfloating-point type compatible with the underlying libdft routine (typically\nfloat64); if the dtype or memory layout is incompatible, a low-level error\n(from ctypes or the C routine) may be raised.", "default": ""}}, "required": ["v_ts"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a spin-polarized PySCF DFT grid run where the XC first-derivative tensor was accumulated in the total/spin (t/s) channel basis for 4 grid points (so the leading dimension corresponds to the two t/s channels). I initially thought I could feed this directly into the part of the code that builds spin-dependent v_xc, but I realized that downstream expects separate spin-up/spin-down channels and my current array is in the combined basis. Please convert the t/s derivative data into the u/d representation (spin-up and spin-down) so I can assemble v_xc^up and v_xc^down. Use the conventional mapping where the total channel is the sum of spin components and the spin channel is their difference.\n\nInput v_ts (shape 2×4, channels ordered as [total, spin]):\n- total: [0.25, 0.30, 0.28, 0.22]\n- spin:  [0.05, -0.02, 0.00, 0.03]\n\nReturn the transformed array in u/d channel order for the same 4 grid points.", "answers": "[{\"name\": \"pyscf_dft_xc_deriv_ts2ud\", \"arguments\": {\"v_ts\": [[0.25, 0.3, 0.28, 0.22], [0.05, -0.02, 0.0, 0.03]]}}]"}
{"func_name": "pyscf_dft_xc_deriv_ud2ts", "func_desc": "pyscf.dft.xc_deriv.ud2ts converts exchange–correlation (XC) derivative arrays from the spin-up/spin-down (\"u\",\"d\" or \"a\",\"b\") representation to the total-density/spin-density (\"rho\",\"s\") representation used in PySCF's DFT grid-based integrals and response calculations. This conversion is commonly needed when evaluating XC potentials and kernels coming from spin-resolved functional evaluations (external XC libraries) and when downstream routines expect total- and spin-density derivative components. The routine delegates the low-level memory transformation to the compiled helper libdft.VXCud2ts for performance.\n    \n    This function accepts a NumPy array containing XC derivatives in a spin-channel representation and returns a NumPy array of the same shape and dtype containing the corresponding derivatives in the total-density / spin-density representation. The conversion implemented corresponds to the linear map vrho = (va + vb)/2 and vs = (va - vb)/2 applied to the spin channels, applied across the array layout used by PySCF's grid-based XC derivative machinery.", "tools": [{"function": {"description": "pyscf.dft.xc_deriv.ud2ts converts exchange–correlation (XC) derivative arrays from the spin-up/spin-down (\"u\",\"d\" or \"a\",\"b\") representation to the total-density/spin-density (\"rho\",\"s\") representation used in PySCF's DFT grid-based integrals and response calculations. This conversion is commonly needed when evaluating XC potentials and kernels coming from spin-resolved functional evaluations (external XC libraries) and when downstream routines expect total- and spin-density derivative components. The routine delegates the low-level memory transformation to the compiled helper libdft.VXCud2ts for performance.\n\nThis function accepts a NumPy array containing XC derivatives in a spin-channel representation and returns a NumPy array of the same shape and dtype containing the corresponding derivatives in the total-density / spin-density representation. The conversion implemented corresponds to the linear map vrho = (va + vb)/2 and vs = (va - vb)/2 applied to the spin channels, applied across the array layout used by PySCF's grid-based XC derivative machinery.", "name": "pyscf_dft_xc_deriv_ud2ts", "parameters": {"properties": {"v_ud": {"type": "array", "items": {"type": "any"}, "description": "Input array of XC derivatives in the spin-up / spin-down representation. The array must be convertible to a C-contiguous numpy.ndarray (the function calls numpy.asarray with order='C'). The last two dimensions of v_ud are interpreted as (nvar, ngrids), where nvar is the number of variables per spin channel and ngrids is the number of grid points. Any leading dimensions are interpreted in paired groups that represent higher-order derivative structure; the function computes order = v_ud.ndim // 2 and therefore expects v_ud.ndim to be an even integer (for example ndims >= 2 and ndims % 2 == 0). Practically, v_ud is the array produced by PySCF/XC evaluation code that stores, for each grid point and variable, the contributions for the two spin channels; this function re-expresses those contributions as total-density and spin-density components. The dtype and shape of the returned array will match v_ud.", "default": ""}}, "required": ["v_ud"], "type": "any"}}, "type": "function"}], "query": "I’m wiring an external XC library into a PySCF LR-DFT prototype and I need to adapt its spin-resolved *potential-derivative* output to what PySCF’s grid routines expect.\n\nInitially I thought I could pass the library’s (va,vb) array straight through, but I realized the downstream response builder is using the *total-density / spin-density* convention, so I need to transform the spin channels accordingly.\n\nHere’s the derivative buffer for a tiny test grid (5 points). It is shaped (nvar=2, ngrids=5), where the first row is the “up/alpha” channel and the second row is the “down/beta” channel:\nva = [0.12, 0.15, 0.10, 0.08, 0.11]\nvb = [-0.03, -0.02, -0.04, -0.05, -0.01]\n\nConvert it into the representation where the two output channels are the symmetric and antisymmetric combinations of the input channels (the convention PySCF uses for total density and spin density in its DFT derivative arrays), preserving shape and dtype, so I can pass it into the response routine without further reshaping.", "answers": "[{\"name\": \"pyscf_dft_xc_deriv_ud2ts\", \"arguments\": {\"v_ud\": [[0.12, 0.15, 0.1, 0.08, 0.11], [-0.03, -0.02, -0.04, -0.05, -0.01]]}}]"}
{"func_name": "pyscf_dft_xc_utils_format_xc_code", "func_desc": "pyscf.dft.xc.utils.format_xc_code formats and normalizes a user-supplied density-functional exchange–correlation (XC) description string for use inside PySCF. In the PySCF DFT workflow (which delegates functional evaluation to external libraries such as Libxc or XCFun), this function is used to produce a compact, upper-cased, whitespace-free representation of an XC description and to translate range-separated-hybrid (RSH) parameter notation from the common RSH(omega, alpha, beta) form into the internal RSH(alpha; beta; omega) form expected by downstream PySCF parsers and code.\n    \n    This function performs the following concrete operations observed in the source code:\n    1. Removes all ASCII space characters and newline characters from description, then converts the entire string to upper case. This normalization makes functional names and tokens consistent with PySCF’s downstream parsing and with conventions used when citing or delegating to external XC libraries.\n    2. If the token 'RSH' is absent after normalization, returns the normalized string unchanged.\n    3. If one or more 'RSH' fragments are present, splits the string on the substring 'RSH' and for each fragment of the form RSH(omega,alpha,beta) rewrites it as RSH(alpha;beta;omega) by moving the first numeric parameter (omega) to the final position and replacing commas between the three parameters with semicolons in the internal notation. Non-RSH parts of the description are preserved in their normalized form and rejoined with the literal substring 'RSH'.", "tools": [{"function": {"description": "pyscf.dft.xc.utils.format_xc_code formats and normalizes a user-supplied density-functional exchange–correlation (XC) description string for use inside PySCF. In the PySCF DFT workflow (which delegates functional evaluation to external libraries such as Libxc or XCFun), this function is used to produce a compact, upper-cased, whitespace-free representation of an XC description and to translate range-separated-hybrid (RSH) parameter notation from the common RSH(omega, alpha, beta) form into the internal RSH(alpha; beta; omega) form expected by downstream PySCF parsers and code.\n\nThis function performs the following concrete operations observed in the source code:\n1. Removes all ASCII space characters and newline characters from description, then converts the entire string to upper case. This normalization makes functional names and tokens consistent with PySCF’s downstream parsing and with conventions used when citing or delegating to external XC libraries.\n2. If the token 'RSH' is absent after normalization, returns the normalized string unchanged.\n3. If one or more 'RSH' fragments are present, splits the string on the substring 'RSH' and for each fragment of the form RSH(omega,alpha,beta) rewrites it as RSH(alpha;beta;omega) by moving the first numeric parameter (omega) to the final position and replacing commas between the three parameters with semicolons in the internal notation. Non-RSH parts of the description are preserved in their normalized form and rejoined with the literal substring 'RSH'.", "name": "pyscf_dft_xc_utils_format_xc_code", "parameters": {"properties": {"description": {"type": "string", "description": "The original XC description string provided by the caller. In PySCF this typically identifies a density-functional or a compound specification that may include tokens such as 'RSH(...)' for range-separated-hybrid functionals. The function expects a Python str; the function will remove spaces and newlines and convert to upper case before performing any RSH-specific reordering. Supplying strings that are already normalized is allowed and will be returned (possibly with only the RSH reordering applied).", "default": ""}}, "required": ["description"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a PySCF DFT input and thought I could just normalize this XC string as-is, but then I realized downstream parsing will choke if the range-separated-hybrid part isn’t converted into PySCF’s internal parameter ordering (the first numeric parameter in the parentheses needs to be moved to the end, and the separators between the three numbers must be the internal ones). Also, the string came from a templating step that injected random spaces and a newline, and the functional names should be canonicalized for Libxc/XCFun dispatch.\n\nPlease run the standard PySCF XC normalization on this exact description (including the whitespace/newline) and return the internal, compact XC code:\n\"  rsh(0.33, 0.25, 0.75) +  b3lyp \\n\"", "answers": "[{\"name\":\"pyscf_dft_xc_utils_format_xc_code\",\"arguments\":{\"description\":\"  rsh(0.33, 0.25, 0.75) +  b3lyp \\n\"}}]"}
{"func_name": "pyscf_dft_xcfun_eval_xc1", "func_desc": "pyscf.dft.xcfun.eval_xc1 evaluates an exchange-correlation (XC) functional and its derivatives by calling the XCFun backend via PySCF's xcfun wrapper. This function is used in PySCF density-functional calculations to obtain the XC energy density and higher-order derivatives on a set of real-space integration grid points; the output ordering of derivative components follows the XCFun convention used by the underlying C library. The implementation accepts local, semi-local, and hybrid functionals (types 'HF', 'LDA', 'GGA', 'MGGA') and supports up to MAX_DERIV_ORDER derivatives as asserted at runtime.", "tools": [{"function": {"description": "pyscf.dft.xcfun.eval_xc1 evaluates an exchange-correlation (XC) functional and its derivatives by calling the XCFun backend via PySCF's xcfun wrapper. This function is used in PySCF density-functional calculations to obtain the XC energy density and higher-order derivatives on a set of real-space integration grid points; the output ordering of derivative components follows the XCFun convention used by the underlying C library. The implementation accepts local, semi-local, and hybrid functionals (types 'HF', 'LDA', 'GGA', 'MGGA') and supports up to MAX_DERIV_ORDER derivatives as asserted at runtime.\n", "name": "pyscf_dft_xcfun_eval_xc1", "parameters": {"properties": {"xc_code": {"type": "string", "description": "Functional specification string understood by PySCF/ XCFun (for example, names parsed by parse_xc). This string determines which XC functional(s) are evaluated, how hybrid mixing factors and range-separation parameters are interpreted, and which XCFun internal function identifiers (fn_ids) and factors (facs) are passed to the XCFun C routine. The function will call parse_xc(xc_code) internally to extract hybrid parameters and component function identifiers.", "default": ""}, "rho": {"type": "array", "items": {"type": "any"}, "description": "Input density array provided on integration grid points. This array is coerced to numpy.asarray(..., order='C', dtype=numpy.double) before use. For non-spin-polarized functionals (spin=0) rho is expected to contain the density-related variables arranged so that the final axis indexes grid points; for spin-polarized cases (spin=1) rho must contain spin-resolved variables accordingly. For meta-GGA (xctype == 'MGGA') if rho.shape[-2] == 6 the code will internally select components rho[..., [0,1,2,3,5], :] to match the MGGA variable ordering expected by XCFun. The last axis of rho is interpreted as the number of grid points (ngrids).", "default": ""}, "spin": {"type": "integer", "description": "Spin flag used to select variable count and ordering from xc_deriv._XC_NVAR. Typical values are 0 for spin-unpolarized (restricted) and 1 for spin-polarized (unrestricted) evaluations. The chosen spin value affects nvar and xlen (internal variable counts) and therefore the reshape/mapping of rho into shape (spin+1, nvar, ngrids) before calling XCFun.", "default": 0}, "deriv": {"type": "integer", "description": "Highest derivative order to evaluate (an integer). The function asserts deriv <= MAX_DERIV_ORDER and will raise an AssertionError if the requested derivative order exceeds the compiled/allowed maximum. The number of derivative components returned per grid point is computed as comb(xlen + deriv, deriv) where xlen depends on xctype and spin via xc_deriv._XC_NVAR.", "default": 1}, "omega": {"type": "float", "nullable": true, "description": "Optional global range-separation parameter. If provided (not None), the single float value overrides the per-functional omega returned by parse_xc and is used for all component functionals. If hyb[2] (the parsed omega) is non-zero but multiple RSH components exist, the implementation currently duplicates the same omega value for each component (i.e., different omegas per component are not supported). If omega is None, the per-component omegas determined by parse_xc are used (or zeroed when no range separation applies).", "default": null}}, "required": ["xc_code", "rho", "omega", "spin", "deriv"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a tiny real-space integration batch for an RKS (spin-unpolarized) DFT run, and I need the XC energy density and its first derivatives on the grid. Initially I was going to keep the range-separation at the wrapper default, but then I realized this job is part of a reproducibility suite where we require that the *range-separation parameter be explicitly passed through without changing the physics* (i.e., keep it behaving exactly like the default). So: evaluate the PBE exchange–correlation functional with deriv=1 on a 3-point grid using rho laid out as 4 rows (rho, |∇rho|^2, laplacian, tau): [[0.12, 0.08, 0.20], [0.001, 0.002, 0.0005], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]. Return the XC energy density and first-derivative components following the XCFun ordering.", "answers": "[{\"name\": \"pyscf_dft_xcfun_eval_xc1\", \"arguments\": {\"xc_code\": \"PBE\", \"rho\": [[0.12, 0.08, 0.2], [0.001, 0.002, 0.0005], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], \"spin\": 0, \"deriv\": 1, \"omega\": null}}]"}
{"func_name": "pyscf_fci_addons_cre_b", "func_desc": "pyscf.fci.addons.cre_b constructs an (N+1)-electron configuration-interaction (CI) wavefunction by applying a beta-spin electron creation operator to an input N-electron CI wavefunction. This routine is part of the PySCF FCI addons used to manipulate CI coefficient arrays expressed in the occupation-string basis (rows index alpha strings, columns index beta strings). It maps each input beta string to the newly created beta string indices using cistring.gen_cre_str_index and applies the fermionic sign factors; an additional sign flip is applied when the number of alpha electrons is odd to account for operator interchange between alpha and beta spins.", "tools": [{"function": {"description": "pyscf.fci.addons.cre_b constructs an (N+1)-electron configuration-interaction (CI) wavefunction by applying a beta-spin electron creation operator to an input N-electron CI wavefunction. This routine is part of the PySCF FCI addons used to manipulate CI coefficient arrays expressed in the occupation-string basis (rows index alpha strings, columns index beta strings). It maps each input beta string to the newly created beta string indices using cistring.gen_cre_str_index and applies the fermionic sign factors; an additional sign flip is applied when the number of alpha electrons is odd to account for operator interchange between alpha and beta spins.\n", "name": "pyscf_fci_addons_cre_b", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "any"}, "description": "CI coefficients of the input N-electron wavefunction. Expected to be a 2D array with rows for alpha occupation strings and columns for beta occupation strings. If a 1D array is provided, the function will attempt to reshape it to shape (num_strings(norb, neleca), num_strings(norb, nelecb)) where num_strings is cistring.num_strings. The function does not modify the logical contents of the input array (it binds a local reshaped view/copy when needed), but an incompatible 1D shape will raise the same exceptions as numpy.reshape.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (norb). This is the size of the single-particle orbital basis used to enumerate occupation strings. It determines the domain of valid orbital indices for the creation operator and is forwarded to cistring helper routines.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "Tuple (neleca, nelecb) of two integers specifying the number of alpha (neleca) and beta (nelecb) electrons in the input CI function. These values are used to (1) determine expected string counts via cistring.num_strings and (2) select the appropriate creation-index table for adding a beta electron (nelecb -> nelecb+1).", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) at which the beta creation operator acts. This is an integer in the range of orbital labels [0, norb-1]. If ap_id does not match any created-orbital entry in the generated creation-index table (for example, if ap_id is outside 0..norb-1), the function produces a zero wavefunction for the attempted creation (see Returns). ap_id is interpreted exactly as an orbital label; no implicit wrapping or modulo is applied.", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a spin-adapted excitation pipeline in a 4-spatial-orbital FCI toy problem. I started by planning to apply a beta-spin creation operator on the third spatial orbital (0-based), but I just realized that in this specific test the beta string corresponding to that orbital is already occupied in some determinants, and I need the *first* orbital (scanning upward from 0) whose beta-occupation is guaranteed empty for every nonzero column in my CI matrix, so the operator never annihilates any component. Using the same CI coefficient array (alpha strings as rows, beta strings as columns) and (neleca, nelecb) = (1, 1), apply `pyscf.fci.addons.cre_b` at that orbital to construct the (N+1)-electron CI wavefunction, including the correct fermionic sign and the extra alpha-parity sign flip rule described in the function docs.\n\nCI (4x4):\n[[0.12, 0.0, -0.03, 0.05],\n [0.01, 0.2, 0.0, -0.04],\n [-0.06, 0.07, 0.15, 0.0],\n [0.0, -0.02, 0.09, 0.11]]", "answers": "[{\"name\":\"pyscf_fci_addons_cre_b\",\"arguments\":{\"ci0\":[[0.12,0.0,-0.03,0.05],[0.01,0.2,0.0,-0.04],[-0.06,0.07,0.15,0.0],[0.0,-0.02,0.09,0.11]],\"norb\":4,\"neleca_nelecb\":[1,1],\"ap_id\":0}}]"}
{"func_name": "pyscf_fci_addons_des_a", "func_desc": "Construct (N-1)-electron wavefunction by removing an alpha electron from the N-electron wavefunction in the PySCF FCI addon. This function implements the second-quantized annihilation operator a_p acting on the alpha-spin part of a full-configuration-interaction (FCI) CI vector, returning a new CI array that represents the state |N-1> = a_p |N>. It is used in PySCF for tasks that require explicit electron removal on the alpha spin manifold (for example, forming Dyson orbitals, computing ionization amplitudes, or intermediate steps in spin-adapted FCI manipulations).", "tools": [{"function": {"description": "Construct (N-1)-electron wavefunction by removing an alpha electron from the N-electron wavefunction in the PySCF FCI addon. This function implements the second-quantized annihilation operator a_p acting on the alpha-spin part of a full-configuration-interaction (FCI) CI vector, returning a new CI array that represents the state |N-1> = a_p |N>. It is used in PySCF for tasks that require explicit electron removal on the alpha spin manifold (for example, forming Dyson orbitals, computing ionization amplitudes, or intermediate steps in spin-adapted FCI manipulations).\n", "name": "pyscf_fci_addons_des_a", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "any"}, "description": "CI coefficients for the input N-electron wavefunction. The array is organized with rows corresponding to alpha occupation strings and columns corresponding to beta occupation strings. Each element is the CI amplitude for the corresponding alpha/beta string product. If ci0 is provided as a 1D array, the function will reshape it into a 2D array assuming the row and column dimensions equal cistring.num_strings(norb, neleca) and cistring.num_strings(norb, nelecb) respectively (see behavior below).", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the system. This value is used to enumerate occupation strings and to generate the mapping for single-alpha-electron removal. It determines the length of bitstrings used by the cistring utilities and must match the orbital indexing used in ap_id.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "Tuple of two integers (neleca, nelecb) where neleca is the number of alpha electrons and nelecb is the number of beta electrons in the input CI function. The function removes one alpha electron, so the output corresponds to (neleca-1, nelecb). If neleca <= 0, no removal is possible and the function returns an array of zeros shaped like the input (see behavior/failure modes).", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) specifying which alpha orbital the annihilation operator acts on. This index must refer to one of the norb spatial orbitals (conventionally 0 .. norb-1). The function locates contributions where that orbital is occupied in the alpha strings and maps them to the corresponding (neleca-1)-electron alpha strings, applying the appropriate fermionic sign from the string ordering.", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "I’m forming ionization intermediates from a 4-orbital FCI CI vector stored as a 2D array (alpha strings × beta strings) for an (neleca, nelecb) = (2, 1) calculation. Initially I was going to annihilate an alpha electron from orbital index 1, but I realized the downstream Dyson-orbital routine is written in the convention where orbitals are numbered starting at 1 in the lab notebook while PySCF’s operator routines expect the internal, zero-based orbital numbering. So: apply the alpha annihilation operator to remove an alpha electron from the orbital whose internal index is exactly one less than the notebook label “1”, using this CI coefficient array:\n[[0.12, -0.03, 0.00, 0.05],\n [0.01, 0.00, -0.02, 0.04],\n [0.00, 0.06, 0.03, -0.01],\n [0.02, -0.05, 0.07, 0.00],\n [0.00, 0.01, -0.04, 0.02],\n [-0.03, 0.00, 0.02, 0.01]]. Return the resulting (N−1)-electron CI array.", "answers": "[{\"name\": \"pyscf_fci_addons_des_a\", \"arguments\": {\"ci0\": [[0.12, -0.03, 0.0, 0.05], [0.01, 0.0, -0.02, 0.04], [0.0, 0.06, 0.03, -0.01], [0.02, -0.05, 0.07, 0.0], [0.0, 0.01, -0.04, 0.02], [-0.03, 0.0, 0.02, 0.01]], \"norb\": 4, \"neleca_nelecb\": [2, 1], \"ap_id\": 0}}]"}
{"func_name": "pyscf_fci_addons_des_b", "func_desc": "pyscf.fci.addons.des_b\n    Construct an (N-1)-electron configuration interaction (CI) wavefunction by applying a beta-spin electron annihilation operator to an N-electron CI wavefunction. This function is used in the PySCF FCI addons to obtain the CI vector that results from removing one beta electron from a specified spatial orbital; it is commonly used when building reduced-density matrices, computing transition amplitudes that involve electron removal, or when constructing intermediate states in FCI-based post-processing.", "tools": [{"function": {"description": "pyscf.fci.addons.des_b\nConstruct an (N-1)-electron configuration interaction (CI) wavefunction by applying a beta-spin electron annihilation operator to an N-electron CI wavefunction. This function is used in the PySCF FCI addons to obtain the CI vector that results from removing one beta electron from a specified spatial orbital; it is commonly used when building reduced-density matrices, computing transition amplitudes that involve electron removal, or when constructing intermediate states in FCI-based post-processing.", "name": "pyscf_fci_addons_des_b", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "any"}, "description": "CI coefficients of the input N-electron wavefunction. This is a 2D array with rows indexed by alpha-spin occupation strings and columns indexed by beta-spin occupation strings. Each element is the CI amplitude for the corresponding (alpha, beta) occupation pair. If a 1D array is passed, the function reshapes it internally to (num_alpha_strings, num_beta_strings) using cistring.num_strings(norb, neleca) and cistring.num_strings(norb, nelecb). The array is treated as read-only by this function; the input ci0 is not modified.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the system. This determines the length of occupation bitstrings used by underlying cistring utilities (cistring.gen_des_str_index and cistring.num_strings) and bounds valid orbital indices for the annihilation operator. Values must match the orbital indexing used to construct ci0.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "A two-element tuple (neleca, nelecb) giving the number of alpha and beta electrons in the input CI wavefunction ci0. These integers determine the dimensions of the input CI array (number of alpha strings = cistring.num_strings(norb, neleca); number of beta strings = cistring.num_strings(norb, nelecb)) and the parity correction applied to the sign prefactor when removing a beta electron.", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) specifying which spatial orbital the beta annihilation operator acts on. ap_id must be in the range [0, norb-1] for a physically meaningful annihilation; if ap_id does not match any occupied beta orbital patterns in the input ci0, the returned (N-1)-electron CI vector will be zero in the corresponding columns (see failure modes below).", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a tiny PySCF FCI wavefunction to build an electron-removal intermediate for a toy Dyson-amplitude check. We have 3 spatial orbitals with (neleca, nelecb) = (2, 1) and the N-electron CI coefficient matrix (alpha strings × beta strings)\nci0 = [[0.80, -0.10, 0.00],\n       [0.05,  0.20, -0.30],\n       [0.00,  0.15,  0.40]].\n\nInitially I was going to remove a beta electron from the spatial orbital with index 1 (0-based). Wait—given this particular (2,1) occupation sector, removing from that orbital can trivially annihilate whole beta-string components if that orbital isn’t occupied in the contributing determinants, which makes my downstream normalization test ambiguous. Instead, remove the beta electron from the *other* nonzero index (still using 0-based indexing) so that the resulting (N−1)-electron CI is guaranteed to have surviving contributions. Please construct the (N−1)-electron CI wavefunction using pyscf.fci.addons.des_b with the appropriate annihilation orbital index.", "answers": "[{\"name\": \"pyscf_fci_addons_des_b\", \"arguments\": {\"ci0\": [[0.8, -0.1, 0.0], [0.05, 0.2, -0.3], [0.0, 0.15, 0.4]], \"norb\": 3, \"neleca_nelecb\": [2, 1], \"ap_id\": 2}}]"}
{"func_name": "pyscf_fci_addons_guess_wfnsym", "func_desc": "pyscf.fci.addons.guess_wfnsym: Guess the wavefunction symmetry (irreducible representation ID) for a CI wavefunction from its CI coefficient vector(s). This utility is part of PySCF's full configuration interaction (FCI) addons and is used to infer the total spatial symmetry label of a CI wavefunction by combining the orbital symmetry labels (orbsym) of occupied orbitals in the occupied alpha/beta occupation strings. In the PySCF quantum-chemistry framework this inferred irrep ID is used for symmetry labeling, symmetry-based checks, and for choosing symmetry-adapted post-processing or reporting.", "tools": [{"function": {"description": "pyscf.fci.addons.guess_wfnsym: Guess the wavefunction symmetry (irreducible representation ID) for a CI wavefunction from its CI coefficient vector(s). This utility is part of PySCF's full configuration interaction (FCI) addons and is used to infer the total spatial symmetry label of a CI wavefunction by combining the orbital symmetry labels (orbsym) of occupied orbitals in the occupied alpha/beta occupation strings. In the PySCF quantum-chemistry framework this inferred irrep ID is used for symmetry labeling, symmetry-based checks, and for choosing symmetry-adapted post-processing or reporting.\n", "name": "pyscf_fci_addons_guess_wfnsym", "parameters": {"properties": {"ci": {"type": "array", "items": {"type": "any"}, "description": "CI coefficients. For a single CI wavefunction this is a 2D array where rows correspond to alpha occupation strings and columns correspond to beta occupation strings (the same convention used by PySCF FCI routines). If a sequence or higher-rank numpy.ndarray of CI vectors is provided, the function will evaluate the guessed symmetry for each CI vector and compare them; if they differ a warning is emitted and the symmetry from the first vector is returned. The practical role of this argument is to provide the expansion coefficients from which occupied determinant/string occupations are identified and their orbital irreps combined to produce the total wavefunction irrep.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals. This integer is used with range(norb) to generate occupation strings for alpha and beta electrons (via cistring.make_strings in the FCI addon). The value must match the number of orbital symmetry labels supplied in orbsym and the orbital indexing convention used to construct ci.", "default": ""}, "nelec": {"type": "array", "items": {"type": "any"}, "description": "Number of electrons. Either a single integer for the total electron count or a two-item list/tuple specifying (alpha, beta) electron counts. Internally this is unpacked into (neleca, nelecb) to generate the appropriate alpha and beta occupation strings. Providing explicit (alpha, beta) counts controls how the CI coefficients are interpreted with respect to spin occupations and therefore affects the computed wavefunction symmetry.", "default": ""}, "orbsym": {"type": "array", "items": {"type": "any"}, "description": "Orbital symmetry labels: a list of integers where each integer is the irrep ID assigned to the corresponding orbital (length should be equal to norb). These orbital irreps are combined (by summing/mapping according to the FCI addon symmetry conventions) over occupied orbitals in each determinant/string to deduce the total irreducible representation of the CI wavefunction. The values and mapping follow the irrep indexing conventions used elsewhere in PySCF.", "default": ""}}, "required": ["ci", "norb", "nelec", "orbsym"], "type": "any"}}, "type": "function"}], "query": "I’m trying to symmetry-label an FCI singlet CI vector in PySCF, but the data I got back from the pipeline is a bit messy.\n\nWe have 4 spatial orbitals and (alpha,beta) electrons = (2,2). The CI coefficient matrix is stored as (alpha strings × beta strings):\n[[ 0.80,  0.10],\n [ 0.05, -0.58],\n [ 0.00,  0.02]]\n\nI initially planned to pass the orbital irrep IDs as [0, 1, 1, 2]. But then I realized the symmetry labels were exported from a code that uses **one-based** irrep numbering, while PySCF expects **zero-based** IDs. So before calling the symmetry-guessing utility, adjust the irrep labels to match PySCF’s convention (without changing their relative pattern), and then infer the wavefunction’s irreducible representation ID from the CI vector.\n\nCan you run the symmetry guess with the corrected orbital symmetry IDs and report the resulting irrep ID?", "answers": "[{\"name\":\"pyscf_fci_addons_guess_wfnsym\",\"arguments\":{\"ci\":[[0.8,0.1],[0.05,-0.58],[0.0,0.02]],\"norb\":4,\"nelec\":[2,2],\"orbsym\":[-1,0,0,1]}}]"}
{"func_name": "pyscf_fci_addons_overlap", "func_desc": "pyscf.fci.addons.overlap\n    Compute the overlap (inner product) between two FCI (full configuration interaction) CI wavefunction coefficient arrays used in PySCF. This routine is used in FCI-related post-processing and analysis within the PySCF framework to evaluate <bra|ket> where bra and ket are CI coefficient tensors defined for a given number of orbitals and electron occupation. When the one-particle basis used to represent the CI wavefunctions is non-orthogonal, an optional overlap/rotation matrix or list of matrices can be provided to transform the bra coefficients into the ket orbital basis before taking the inner product.", "tools": [{"function": {"description": "pyscf.fci.addons.overlap\nCompute the overlap (inner product) between two FCI (full configuration interaction) CI wavefunction coefficient arrays used in PySCF. This routine is used in FCI-related post-processing and analysis within the PySCF framework to evaluate <bra|ket> where bra and ket are CI coefficient tensors defined for a given number of orbitals and electron occupation. When the one-particle basis used to represent the CI wavefunctions is non-orthogonal, an optional overlap/rotation matrix or list of matrices can be provided to transform the bra coefficients into the ket orbital basis before taking the inner product.", "name": "pyscf_fci_addons_overlap", "parameters": {"properties": {"bra": {"type": "array", "items": {"type": "float"}, "description": "CI coefficient array representing the \"bra\" wavefunction. In the PySCF FCI context this is the array of expansion coefficients for determinants/configurations corresponding to norb orbitals and nelec electron occupation (typically a tuple specifying numbers of alpha and beta electrons). The array may be complex-valued; the function uses the complex conjugate of bra when computing the overlap. If s is provided, bra is passed to transform_ci_for_orbital_rotation with arguments (bra, norb, nelec, s) to convert bra to the ket orbital basis prior to contraction. Note that this function assigns the transformed coefficients to a local variable named bra; the original object passed by the caller is not overwritten by this assignment unless the auxiliary transform function modifies the array in-place.", "default": ""}, "ket": {"type": "array", "items": {"type": "float"}, "description": "CI coefficient array representing the \"ket\" wavefunction in the target orbital basis. ket must be consistent with norb and nelec in shape and ordering so that a direct elementwise inner product is meaningful after any optional transformation of bra. ket is not modified by this function.", "default": ""}, "norb": {"type": "integer", "description": "Number of orbitals used in the CI expansion. In PySCF FCI routines this integer determines how transform_ci_for_orbital_rotation interprets orbital overlap/rotation matrices s and how CI coefficients map to occupation configurations; norb must match the orbital dimension implied by ket, bra, and any matrices supplied in s.", "default": ""}, "nelec": {"type": "any", "description": "Electron occupation specification used by the FCI routines, typically a two-element tuple (nalpha, nbeta) giving the number of alpha and beta electrons. This tuple is forwarded to transform_ci_for_orbital_rotation so the CI transformation is performed with the correct configuration space mapping.", "default": ""}, "s": {"type": "any", "nullable": true, "description": "The overlap matrix of a non-orthogonal one-particle basis, supplied either as a single 2D array or as a list of 2D arrays (the original source allows a single 2D array or a list of them). When s is not None, the function calls transform_ci_for_orbital_rotation(bra, norb, nelec, s) to transform the bra CI coefficients from their native orbital basis into the ket orbital basis before computing the overlap. If s is None (default), no orbital transformation is applied and the overlap is computed directly assuming the two CI coefficient arrays are expressed in the same orthonormal one-particle basis.", "default": null}}, "required": ["bra", "ket", "norb", "nelec", "s"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing two FCI wavefunctions in PySCF for a toy active space (4 spatial orbitals, nelec = (2,2)). Initially I was going to call `pyscf.fci.addons.overlap` directly on the raw CI coefficient tensors, but I realized the bra state was saved in a slightly *rotated* (yet still orthonormal) MO basis relative to the ket. So before taking ⟨bra|ket⟩, transform the bra into the ket basis using the single-particle rotation matrix that is closest to identity but swaps a small amount of weight between the first two spatial orbitals (i.e., a real 2×2 rotation embedded in the upper-left block, leaving the remaining orbitals unchanged). Use a mixing angle of 0.1 radians for that rotation, apply it consistently to both spin sectors as PySCF expects, and then compute the overlap.\n\nUse these CI coefficient arrays (as saved):\n- bra = [[0.6, 0.0, 0.3, 0.1], [0.0, 0.5, 0.2, 0.3]]\n- ket = [[0.55, 0.05, 0.25, 0.15], [0.02, 0.48, 0.22, 0.28]]\n\nAssume norb=4 and nelec=(2,2).", "answers": "[{\"name\": \"pyscf_fci_addons_overlap\", \"arguments\": {\"bra\": [[0.6, 0.0, 0.3, 0.1], [0.0, 0.5, 0.2, 0.3]], \"ket\": [[0.55, 0.05, 0.25, 0.15], [0.02, 0.48, 0.22, 0.28]], \"norb\": 4, \"nelec\": [2, 2], \"s\": [[0.9950041652780258, -0.09983341664682815, 0.0, 0.0], [0.09983341664682815, 0.9950041652780258, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]}}]"}
{"func_name": "pyscf_fci_addons_symmetrize_wfn", "func_desc": "Symmetrize the full configuration interaction (FCI) CI wavefunction by zeroing out determinant coefficients whose overall spatial symmetry does not match the requested target irreducible representation (irrep).\n    \n    This function is part of the pyscf.fci.addons utilities and is used in FCI calculations to enforce spatial symmetry on the many-electron wavefunction. It constructs alpha and beta occupation strings for the given number of orbitals and electrons, computes the symmetry of each determinant from the per-orbital irreps (orbsym), and returns a new CI coefficient array where coefficients of determinants that do not transform as the specified wfnsym are set to zero. The routine calls internal helpers (_unpack_nelec to interpret nelec, cistring.make_strings to enumerate determinants, and _symmetrize_wfn to perform the masking), and is typically used to reduce the CI space and to target states of a particular spatial symmetry in post-Hartree–Fock calculations.", "tools": [{"function": {"description": "Symmetrize the full configuration interaction (FCI) CI wavefunction by zeroing out determinant coefficients whose overall spatial symmetry does not match the requested target irreducible representation (irrep).\n\nThis function is part of the pyscf.fci.addons utilities and is used in FCI calculations to enforce spatial symmetry on the many-electron wavefunction. It constructs alpha and beta occupation strings for the given number of orbitals and electrons, computes the symmetry of each determinant from the per-orbital irreps (orbsym), and returns a new CI coefficient array where coefficients of determinants that do not transform as the specified wfnsym are set to zero. The routine calls internal helpers (_unpack_nelec to interpret nelec, cistring.make_strings to enumerate determinants, and _symmetrize_wfn to perform the masking), and is typically used to reduce the CI space and to target states of a particular spatial symmetry in post-Hartree–Fock calculations.", "name": "pyscf_fci_addons_symmetrize_wfn", "parameters": {"properties": {"ci": {"type": "array", "items": {"type": "any"}, "description": "CI coefficients arranged with rows corresponding to alpha-electron occupation strings and columns corresponding to beta-electron occupation strings. This array contains the amplitude of each Slater determinant product (alpha string × beta string) prior to symmetry filtering.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals. This integer determines the length of occupation strings generated for alpha and beta electrons and must match the length of orbsym.", "default": ""}, "nelec": {"type": "array", "items": {"type": "any"}, "description": "Total number of electrons as a single integer, or a two-item list [nalpha, nbeta] specifying the number of alpha and beta electrons respectively. The function internally unpacks this to (neleca, nelecb) to generate the appropriate alpha and beta occupation strings.", "default": ""}, "orbsym": {"type": "array", "items": {"type": "any"}, "description": "Per-orbital irrep identifiers (integer ids) that label the spatial symmetry (irreducible representation) of each orbital. The list length must equal norb. These irreps are used to determine the overall determinant symmetry by combining the occupied orbital irreps for a given occupation string.", "default": ""}, "wfnsym": {"type": "integer", "description": "The target wavefunction irrep identifier. Determinants whose combined orbital irrep does not equal this integer will be zeroed in the returned CI array. Defaults to 0 (the caller should interpret irrep id 0 according to the molecular point-group labeling used in the surrounding codebase).", "default": 0}}, "required": ["ci", "norb", "nelec", "orbsym", "wfnsym"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a tiny PySCF FCI wavefunction for a 4-orbital active space (2 alpha, 1 beta). I initially thought I should project onto the irrep labeled “1”, but I just realized the code path I’m using labels the totally symmetric representation as the one whose label equals the XOR (parity-sum) of the active-space orbital irreps for the closed-shell reference determinant (i.e., occupy the two lowest-index orbitals with alpha, and the lowest-index orbital with beta, using the same occupied set for the reference). So instead of hard-coding the target label, please compute that reference-determinant symmetry from the given orbital irreps and use it as the requested wavefunction symmetry.\n\nThen symmetrize my CI coefficient matrix by zeroing out coefficients of determinants whose overall spatial symmetry does not match that computed target. Use orbital irreps [0,1,1,0]. The CI array is (rows = alpha strings, cols = beta strings): [[0.12,-0.05,0.03,0.00],[0.07,0.20,-0.01,0.04],[0.00,-0.02,0.15,-0.06],[0.09,0.00,0.02,0.11],[0.01,-0.08,0.00,0.05],[0.00,0.06,-0.04,0.02]]. Return the symmetrized CI array.", "answers": "[{\"name\":\"pyscf_fci_addons_symmetrize_wfn\",\"arguments\":{\"ci\":[[0.12,-0.05,0.03,0.0],[0.07,0.2,-0.01,0.04],[0.0,-0.02,0.15,-0.06],[0.09,0.0,0.02,0.11],[0.01,-0.08,0.0,0.05],[0.0,0.06,-0.04,0.02]],\"norb\":4,\"nelec\":[2,1],\"orbsym\":[0,1,1,0],\"wfnsym\":1}}]"}
{"func_name": "pyscf_fci_cistring_addrs2str", "func_desc": "Convert a list of CI determinant addresses to occupation \"strings\" used by PySCF's FCI routines.", "tools": [{"function": {"description": "Convert a list of CI determinant addresses to occupation \"strings\" used by PySCF's FCI routines.\n", "name": "pyscf_fci_cistring_addrs2str", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of spatial orbitals in the FCI problem. This determines the width\nof each occupation string (number of orbitals whose occupation is encoded).\nIn the PySCF FCI domain, norb controls the bit-width of the integer-encoded\noccupation patterns. The implementation requires norb < 64 and will raise\nNotImplementedError if norb >= 64 because the routine uses 64-bit integer\nencodings and a low-level C routine that does not support wider encodings.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons (total) for the determinants. This parameter\ndefines which sector of the combinatorial list of determinants (strings)\nis addressed: num_strings(norb, nelec) is the total number of distinct\ndeterminants with norb orbitals and nelec electrons. The function relies on\nthis value to validate input addresses and to call the underlying C routine\nthat reconstructs occupation patterns for that electron count.", "default": ""}, "addrs": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of integer addresses (indices)\ninto the ordered list of CI determinant strings for the given (norb, nelec)\nsector. The signature documents this as a numpy.ndarray; the function\nconverts the supplied array to dtype numpy.int32 internally before use.\nEach element must satisfy 0 <= addrs[i] < num_strings(norb, nelec). If any\nelement is outside this range, an AssertionError is raised. The array length\ndetermines how many strings are produced and the order of returned strings\ncorresponds one-to-one to the input order.", "default": ""}}, "required": ["norb", "nelec", "addrs"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an FCI interface to PySCF and I want to sanity-check my determinant indexing. I initially planned to convert the CI determinant addresses [0, 4, 9, 12] for a sector with 6 spatial orbitals and 3 electrons into PySCF’s occupation strings. However, I just realized these addresses were logged by a script that *accidentally included the reference (all-zero) determinant as a header entry*, so the real CI addresses are the same list but with that header removed. Convert the corrected list of CI determinant addresses into PySCF occupation strings for the (norb, nelec) sector described above.", "answers": "[{\"name\": \"pyscf_fci_cistring_addrs2str\", \"arguments\": {\"norb\": 6, \"nelec\": 3, \"addrs\": [4, 9, 12]}}]"}
{"func_name": "pyscf_fci_cistring_gen_des_str_index", "func_desc": "Generate an index mapping that relates each N-electron occupation string (bitstring) defined by orb_list to the N-1-electron occupation strings produced by applying a fermionic annihilation operator to that N-electron string. This function is used in the PySCF full configuration interaction (FCI) cistring utilities to accelerate application of annihilation operators when assembling matrix elements, computing determinants, or performing excitation/de-excitation operations in FCI routines.\n    \n    The function builds the list of N-electron strings by calling make_strings(orb_list, nelec) and then calls the compiled helper libfci.FCIdes_str_index to fill a dense index table. The returned index table lets FCI code look up, for any N-electron string and any choice of an occupied orbital to annihilate, the address of the resulting (N-1)-electron string together with the fermionic sign (parity) resulting from the annihilation.", "tools": [{"function": {"description": "Generate an index mapping that relates each N-electron occupation string (bitstring) defined by orb_list to the N-1-electron occupation strings produced by applying a fermionic annihilation operator to that N-electron string. This function is used in the PySCF full configuration interaction (FCI) cistring utilities to accelerate application of annihilation operators when assembling matrix elements, computing determinants, or performing excitation/de-excitation operations in FCI routines.\n\nThe function builds the list of N-electron strings by calling make_strings(orb_list, nelec) and then calls the compiled helper libfci.FCIdes_str_index to fill a dense index table. The returned index table lets FCI code look up, for any N-electron string and any choice of an occupied orbital to annihilate, the address of the resulting (N-1)-electron string together with the fermionic sign (parity) resulting from the annihilation.", "name": "pyscf_fci_cistring_gen_des_str_index", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "any"}, "description": "List of orbital identifiers used to construct the occupation bitstrings. This list is passed unchanged to make_strings(orb_list, nelec). In the domain of PySCF FCI, orb_list defines the set and order of spatial spin-orbitals (or orbital labels) from which the N-electron occupation strings are drawn; the mapping produced by this function uses the same ordering and indexing convention as make_strings.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the source occupation strings (N). This function requires nelec > 0 and will assert if nelec is not positive. The function constructs mappings from each N-electron string to N-1-electron strings obtained by annihilating one electron.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny PySCF FCI sanity check for fermionic signs, starting from four spin-orbitals labeled [0, 1, 2, 3]. I initially thought I should stay in the 2-electron sector for building the annihilation (destruction) string index table, but I realized the downstream contraction I’m debugging actually annihilates once from the *highest occupied* reference and lands me in the sector with exactly one fewer electron than that. So instead of using the original electron count, build the dense des-string index table for the sector that is reduced by one relative to the 2-electron case, still using the same orbital list, so I can map each determinant in that sector to the corresponding determinant with one fewer electron and its fermionic parity when removing an electron from any occupied orbital.", "answers": "[{\"name\": \"pyscf_fci_cistring_gen_des_str_index\", \"arguments\": {\"orb_list\": [0, 1, 2, 3], \"nelec\": 1}}]"}
{"func_name": "pyscf_fci_cistring_gen_linkstr_index", "func_desc": "pyscf.fci.cistring.gen_linkstr_index generates a lookup table (link index) that describes how occupation-number strings (Slater determinant bit-strings used in Full Configuration Interaction (FCI) within PySCF) are transformed by a single fermionic annihilation-creation operator pair. This lookup table is used by FCI routines to efficiently form Hamiltonian and excitation matrix elements by mapping an initial string and an excitation (annihilate an occupied orbital, create in a target orbital) to the index of the resulting string and the accompanying fermionic phase factor.", "tools": [{"function": {"description": "pyscf.fci.cistring.gen_linkstr_index generates a lookup table (link index) that describes how occupation-number strings (Slater determinant bit-strings used in Full Configuration Interaction (FCI) within PySCF) are transformed by a single fermionic annihilation-creation operator pair. This lookup table is used by FCI routines to efficiently form Hamiltonian and excitation matrix elements by mapping an initial string and an excitation (annihilate an occupied orbital, create in a target orbital) to the index of the resulting string and the accompanying fermionic phase factor.\n", "name": "pyscf_fci_cistring_gen_linkstr_index", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "any"}, "description": "Ordered list of orbital indices (the spatial or spin-orbital labels) that define the basis for the bit-strings. The length of this list determines norb (the total number of orbitals) used to compute nvir = norb - nocc. In the PySCF FCI context, orb_list selects which orbitals are considered when constructing occupation strings via make_strings and when enumerating possible excitations.", "default": ""}, "nocc": {"type": "integer", "description": "Number of occupied orbitals in each string (the occupation count for the determinants). In the FCI domain this is the number of electrons (per spin sector when used in spin-specific routines) that are occupied in each occupation-number string; it sets the number of annihilation indices and therefore controls the layout and size (nocc and nocc*nvir blocks) of the returned link index.", "default": ""}, "strs": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional one-dimensional array of integer-encoded occupation strings (string indices) sorted in strictly increasing order. If None (the default), make_strings(orb_list, nocc) is called to generate the full ordered list of strings for the given orb_list and nocc. The array is converted to numpy.int64 internally. If strs is an instance of OIndexList, the routine delegates to gen_linkstr_index_o1(orb_list, nocc, strs, tril) and returns that result instead. This parameter allows restricting the lookup table to a precomputed subset or custom ordering of strings used elsewhere in PySCF FCI workflows.", "default": null}, "tril": {"type": "boolean", "description": "Flag controlling whether the underlying C routine should compute the link index in a triangular/half-storage mode. The boolean value is forwarded unchanged to the low-level libfci.FCIlinkstr_index routine. In practice, tril=True is used by algorithms that only need a triangular subset of links (e.g., exploiting symmetry or half-matrix storage) while tril=False computes the full link information. Default is False.", "default": false}}, "required": ["orb_list", "nocc", "tril", "strs"], "type": "any"}}, "type": "function"}], "query": "I’m building a PySCF FCI regression test for a spin sector that *should* correspond to 2 electrons distributed over 6 spin-orbitals (labels 0–5). Initially I was going to let the link-index generator enumerate determinants internally, but I realized the upstream determinant list I’m feeding in is messy: it contains duplicates and a few invalid entries (wrong Hamming weight / orbitals outside 0–5). I need you to generate the **full (non-triangular) link-index lookup table** for the **cleaned, unique, valid** determinant list only.\n\nHere’s the raw determinant list (occupation-number bit-strings as Python ints):\n[3, 5, 6, 9, 10, 12, 17, 18, 20, 24, 33, 34, 36, 40, 48, 3, 7, 0, 64, -1]\n\nRules/constraints:\n1) Keep only determinants that represent exactly the target electron count for this sector (i.e., bit-strings with the correct Hamming weight).\n2) Discard anything that references orbitals outside 0–5 (i.e., any set bit at or above the first orbital index that is *not* in the active list), and discard nonsensical values (negative).\n3) Remove duplicates while preserving the ascending order of the remaining integers.\n4) Use the active orbital list [0,1,2,3,4,5].\n5) Build the link table for single annihilation–creation moves (occupied -> unoccupied) with the **full storage** variant.\n\nReturn the link-index table mapping (string, excitation) -> (resulting string index, fermionic phase) for this cleaned determinant list.", "answers": "[{\"name\": \"pyscf_fci_cistring_gen_linkstr_index\", \"arguments\": {\"orb_list\": [0, 1, 2, 3, 4, 5], \"nocc\": 2, \"strs\": [3, 5, 6, 9, 10, 12, 17, 18, 20, 24, 33, 34, 36, 40, 48], \"tril\": false}}]"}
{"func_name": "pyscf_fci_cistring_gen_linkstr_index_o1", "func_desc": "gen_linkstr_index_o1 generates a lookup table that encodes how Slater determinants\n    (occupation strings) in the PySCF FCI cistring representation are related by a\n    single one-electron operator a_p^\\dagger a_q (a creation at orbital p and\n    annihilation at orbital q). This table is used in full configuration interaction\n    (FCI) routines inside PySCF to quickly enumerate nonzero connections between\n    determinants produced by single excitations or diagonal (p==q) one-electron\n    terms when assembling Hamiltonian or other one-body operator matrix elements.", "tools": [{"function": {"description": "gen_linkstr_index_o1 generates a lookup table that encodes how Slater determinants\n(occupation strings) in the PySCF FCI cistring representation are related by a\nsingle one-electron operator a_p^\\dagger a_q (a creation at orbital p and\nannihilation at orbital q). This table is used in full configuration interaction\n(FCI) routines inside PySCF to quickly enumerate nonzero connections between\ndeterminants produced by single excitations or diagonal (p==q) one-electron\nterms when assembling Hamiltonian or other one-body operator matrix elements.", "name": "pyscf_fci_cistring_gen_linkstr_index_o1", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "any"}, "description": "Ordered list of orbital indices that defines the orbital\nlabeling used to construct occupation strings. In the implementation the\nfunction requires orb_list to be equal to list(range(norb)) for some\nnorb (i.e., 0-based contiguous indexing). The list determines norb\n(number of orbitals) and is used to compute the number of virtual\norbitals nvir = norb - nelec. The practical significance in the FCI\ndomain is that orb_list fixes the mapping between orbital labels and\nbitstring/occupation-list representations used across the cistring\nroutines; an orb_list not equal to range(norb) will trigger an\nAssertionError.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the occupation strings (number of\noccupied orbitals per determinant). This integer determines the length\nof each occupation list in occslst (the number of ones in each string)\nand the layout/size of the returned link table. In FCI use, nelec is\nthe electron count for the configuration space being considered; nelec\n== 0 is a special-case handled by returning an empty-shaped index\narray.", "default": ""}, "strs": {"type": "any", "nullable": true, "description": "Optional list of occupation-index lists (OIndexList). Each\nelement of strs is an occupation string represented as an ordered list\n(or array/tuple) of nelec occupied orbital indices. This list serves as\noccslst (the full list of determinants) for which the link table is\nconstructed. If strs is None (the default), the function generates the\nstandard ordered list of occupation strings for the given orb_list and\nnelec by calling gen_occslst(orb_list, nelec). In practical FCI code you\npass a precomputed occslst (list of OIndexList) when you need the\nlink-table consistent with an externally constructed determinant order.\nIf strs does not contain all determinants reachable by single excitations\nfrom its members (i.e., is inconsistent with orb_list and nelec),\nlookups during construction will raise a KeyError.", "default": null}, "tril": {"type": "boolean", "description": "If False (default), return the full per-determinant link table\nin the raw format described below. If True, after building the table\nthe function calls reform_linkstr_index on the result and returns that\ntransformed representation. The tril flag is provided for callers that\nrequire the alternative (e.g., triangular or reduced) storage format\nproduced by reform_linkstr_index; the exact effect is the same as\napplying reform_linkstr_index(lidx) to the full index.", "default": false}}, "required": ["orb_list", "nelec", "tril", "strs"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny FCI debugging case in PySCF and need the one-electron (a_p^\\dagger a_q) excitation lookup table in the compact lower-triangular storage to speed up assembling a one-body operator. Start from the default PySCF cistring determinant ordering for a system with 6 spin-orbitals labeled 0–5.\n\nInitially I was going to treat this as a 3-electron problem, but I just realized I’m using it to validate a spin-adapted decomposition where the electrons are split into alpha and beta, and the beta sector is fixed to be exactly one electron. That means the sector I’m actually enumerating here is the alpha-string space only (so the electron count in this call should match the alpha electrons, not the total). Generate the lookup table accordingly, using the default determinant list (don’t pass an explicit `strs`). Return it in the reformed (tril) format.", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_linkstr_index_o1\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nelec\":2,\"strs\":null,\"tril\":true}}]"}
{"func_name": "pyscf_fci_cistring_gen_occslst", "func_desc": "pyscf.fci.cistring.gen_occslst — Generate occupied-orbital index lists for every occupation string used in FCI string representations.", "tools": [{"function": {"description": "pyscf.fci.cistring.gen_occslst — Generate occupied-orbital index lists for every occupation string used in FCI string representations.\n", "name": "pyscf_fci_cistring_gen_occslst", "parameters": {"properties": {"orb_list": {"type": "any", "description": "A sequence (typically a tuple) of orbital indices that defines the available single-particle orbitals from which occupations are formed. In the PySCF FCI context, these integers identify orbital positions that can be occupied in a bitstring representation; the function converts this input to a Python list internally (orb_list = list(orb_list)). If orb_list is not iterable, a TypeError will be raised by the conversion.", "default": ""}, "nelec": {"type": "integer", "description": "The number of electrons (occupied orbitals) per string. This integer must be non-negative; the function asserts nelec >= 0 and will raise an AssertionError for negative values. In the FCI domain, nelec controls the length of each returned occupation list and determines the combinatorial generation of occupation patterns.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m building a tiny FCI sanity-check for a radical fragment and I need the occupied-orbital index lists for *only* the physically meaningful 3-electron occupation strings. I started from 6 active spin-orbitals labeled 0–5, but then I realized two of those indices are actually ghost spin-orbitals that came from a bad AO-to-MO slicing step (they’re the two highest labels in that list). So don’t generate strings that ever occupy those; restrict to the remaining valid labels and then generate the occupied-orbital index list for every 3-electron occupation string used by PySCF’s CI-string representation.", "answers": "[{\"name\": \"pyscf_fci_cistring_gen_occslst\", \"arguments\": {\"orb_list\": [0, 1, 2, 3], \"nelec\": 3}}]"}
{"func_name": "pyscf_fci_cistring_gen_strings4orblist", "func_desc": "pyscf.fci.cistring.gen_strings4orblist: Generate occupation bitstrings for a specified list of orbitals used by the FCI routines in PySCF.\n    \n    This function enumerates all distinct occupation bitstrings with a fixed number of electrons (nelec) distributed over the orbitals listed in orb_list. It is used in the pyscf.fci module to represent Slater determinants / occupation patterns compactly as integer bitmasks when constructing and manipulating Full Configuration Interaction (FCI) wavefunctions and Hamiltonians. Each returned integer is a 64-bit signed integer (numpy.int64) whose binary representation encodes orbital occupancy: a bit value of 1 indicates the corresponding orbital is occupied, and 0 indicates it is unoccupied. The lowest (right-most) bit corresponds to the lowest orbital index in orb_list, so the integer value can be interpreted directly as a binary occupation string consistent with the examples and conventions in the FCI code paths.", "tools": [{"function": {"description": "pyscf.fci.cistring.gen_strings4orblist: Generate occupation bitstrings for a specified list of orbitals used by the FCI routines in PySCF.\n\nThis function enumerates all distinct occupation bitstrings with a fixed number of electrons (nelec) distributed over the orbitals listed in orb_list. It is used in the pyscf.fci module to represent Slater determinants / occupation patterns compactly as integer bitmasks when constructing and manipulating Full Configuration Interaction (FCI) wavefunctions and Hamiltonians. Each returned integer is a 64-bit signed integer (numpy.int64) whose binary representation encodes orbital occupancy: a bit value of 1 indicates the corresponding orbital is occupied, and 0 indicates it is unoccupied. The lowest (right-most) bit corresponds to the lowest orbital index in orb_list, so the integer value can be interpreted directly as a binary occupation string consistent with the examples and conventions in the FCI code paths.", "name": "pyscf_fci_cistring_gen_strings4orblist", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "any"}, "description": "List of orbital indices to consider when generating occupation strings. Each element is interpreted as an integer orbital index and the position of that orbital in orb_list determines which bit in the returned integer corresponds to that orbital (the element at orb_list[0] maps to the least-significant bit). The function converts the input to a list internally, so the caller should provide the indices in the desired order. Elements must be integers suitable for bit-shift operations; negative or non-integer entries will raise exceptions or cause incorrect results. The function switches to an alternative implementation (gen_occslst) when len(orb_list) >= 64 to avoid constructing bitstrings larger than a 64-bit word.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons (occupied orbitals) to place among the orbitals in orb_list. This is interpreted as a non-negative integer count. If nelec == 0 the function returns an array containing a single zero-valued bitstring (no occupied orbitals). If nelec is greater than len(orb_list) the function returns an empty array. If nelec is negative an AssertionError is raised (the function asserts nelec >= 0). Typical usage in the FCI domain is to call this with the number of spin-up or spin-down electrons to enumerate all occupation patterns for that spin sector.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny FCI benchmark in PySCF and initially I was going to enumerate all 2‑electron determinants over my active orbital list ordered as [0, 2, 5, 7], with the first entry mapped to the least‑significant bit. But I just realized I’m mixing spatial-orbital bookkeeping with spin-orbital determinants: the downstream FCI code path I’m testing actually assumes the active list contains both spin components explicitly, i.e., it’s the same set as before but with the corresponding partner orbital for each entry included as well, in the standard even/odd indexing convention. Keep the ordering as ‘original orbital then its partner’ for each item, and still map the first element of that expanded list to the least-significant bit. Now generate all distinct occupation bitstrings for the same electron count as before.", "answers": "[{\"name\": \"pyscf_fci_cistring_gen_strings4orblist\", \"arguments\": {\"orb_list\": [0, 1, 2, 3, 5, 6, 7, 8], \"nelec\": 2}}]"}
{"func_name": "pyscf_fci_cistring_reform_linkstr_index", "func_desc": "pyscf.fci.cistring.reform_linkstr_index compresses the (a, i) pair index stored in a link-string index array into a single lower-triangular integer index. The compression produces indices that are consistent with the 4-fold permutation symmetry commonly exploited for two-electron integrals in FCI (full configuration interaction) routines, allowing downstream code to identify equivalent excitation pairs and reduce redundant integral lookups or storage.", "tools": [{"function": {"description": "pyscf.fci.cistring.reform_linkstr_index compresses the (a, i) pair index stored in a link-string index array into a single lower-triangular integer index. The compression produces indices that are consistent with the 4-fold permutation symmetry commonly exploited for two-electron integrals in FCI (full configuration interaction) routines, allowing downstream code to identify equivalent excitation pairs and reduce redundant integral lookups or storage.\n", "name": "pyscf_fci_cistring_reform_linkstr_index", "parameters": {"properties": {"link_index": {"type": "array", "items": {"type": "any"}, "description": "A 3-dimensional numpy array that encodes link-string information produced and consumed by the pyscf.fci.cistring routines. The function expects link_index to have the shape (nlink, nrow, 4) or equivalent, where the last axis holds the fields (a, i, str1, sign) for each link entry. In this convention, column 0 (a) and column 1 (i) are integer orbital indices identifying the creation/annihilation orbital pair for the link; column 2 (str1) is an integer string index used by FCI determinant string tables; column 3 (sign) is an integer sign (+1 or -1) associated with the excitation. The array must be a numpy.ndarray; passing another type may raise a TypeError. Supplying an array with fewer than two indexed columns or with insufficient trailing dimension length will raise an IndexError.", "default": ""}}, "required": ["link_index"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a tiny FCI cache for two-electron integrals and I want to canonicalize excitation-pair keys using the same lower-triangular compression that respects the usual 4-fold permutation symmetry. I was going to compress every (a,i) entry in my link-string index table as-is, but I just realized my table is coming from a noisy intermediate dump: a few rows are duplicates up to swapping the two entries of the pair, and for symmetry-caching I must keep only one representative per unordered pair. \n\nSo: take the link_index array below (shape: 2 blocks × 3 rows × 4 fields, with columns [a, i, str1, sign]). First, for each row form an unordered pair key from (a,i) such that (a,i) and (i,a) are treated as the same physical excitation-pair for my cache; if the same unordered pair appears more than once anywhere in the array, keep only the first occurrence in the original traversal order (block-major, then row-major) and discard subsequent duplicates. Then run pyscf.fci.cistring.reform_linkstr_index on the de-duplicated table to compress the remaining (a,i) into the symmetry-consistent lower-triangular integer index, leaving the other fields untouched.\n\nUse this link_index array:\n[[[4,1,12,1],[3,0,7,-1],[5,2,19,1]],\n [[2,0,6,1],[4,3,15,-1],[1,0,3,1]]].", "answers": "[{\"name\": \"pyscf_fci_cistring_reform_linkstr_index\", \"arguments\": {\"link_index\": [[[4, 1, 12, 1], [3, 0, 7, -1], [5, 2, 19, 1]], [[2, 0, 6, 1], [4, 3, 15, -1], [1, 0, 3, 1]]]}}]"}
{"func_name": "pyscf_fci_cistring_strs2addr", "func_desc": "Convert a list of occupation bitstrings to CI determinant addresses used by the PySCF FCI routines.\n    \n    This function maps each occupation string (an integer encoding an electron occupation pattern using bit positions) to its corresponding integer address in the CI (configuration interaction) determinant ordering implemented by pyscf.fci.cistring and the underlying C helper libfci. It is intended for use when assembling or indexing FCI vectors and matrices: given raw bitstring representations of determinants, strs2addr returns the zero-based index into the CI space used throughout PySCF FCI code.", "tools": [{"function": {"description": "Convert a list of occupation bitstrings to CI determinant addresses used by the PySCF FCI routines.\n\nThis function maps each occupation string (an integer encoding an electron occupation pattern using bit positions) to its corresponding integer address in the CI (configuration interaction) determinant ordering implemented by pyscf.fci.cistring and the underlying C helper libfci. It is intended for use when assembling or indexing FCI vectors and matrices: given raw bitstring representations of determinants, strs2addr returns the zero-based index into the CI space used throughout PySCF FCI code.", "name": "pyscf_fci_cistring_strs2addr", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of orbitals whose occupations are encoded in each bitstring. Practically, each input integer in strings must use only the lowest norb bits to represent occupations. This function requires norb < 64 because bitstrings are converted to 64-bit integers (numpy.int64) for the mapping; if norb >= 64 the function raises NotImplementedError.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in each determinant. This value is used to determine the size and ordering of the CI space (num_strings(norb, nelec)) and to validate the address range that can be represented. If the total number of determinants num_strings(norb, nelec) is >= 2**31 the function raises NotImplementedError because addresses are returned as 32-bit integers.", "default": ""}, "strings": {"type": "array", "items": {"type": "any"}, "description": "Sequence of integer occupation strings to convert. Each element should be an integer (or convertible to numpy.int64) encoding the occupation pattern with bits corresponding to orbital occupations; typically the integer has exactly nelec bits set. The input is converted to a numpy.int64 array internally. Invalid bit patterns (for example using bits at positions >= norb or not matching nelec when such validation is expected downstream) may produce undefined results or errors from the underlying C routine.", "default": ""}}, "required": ["norb", "nelec", "strings"], "type": "any"}}, "type": "function"}], "query": "I’m assembling an FCI vector in PySCF for a small active space (6 spatial orbitals, 3 electrons). I initially planned to convert *all* the raw determinant bitstrings I collected from a screening script into CI addresses, but I just realized that script was emitting a mix of valid determinants and artifacts: some strings have the wrong electron count for this electron number. Please convert **only the determinants that actually correspond to the requested electron number** into **zero-based** CI determinant addresses using PySCF’s determinant ordering (pyscf.fci.cistring / libfci). Here are the candidate occupation bitstrings (integers): 0b000111 (7), 0b001011 (11), 0b010101 (21), 0b100011 (35), 0b001111 (15), 0b111111 (63).", "answers": "[{\"name\": \"pyscf_fci_cistring_strs2addr\", \"arguments\": {\"norb\": 6, \"nelec\": 3, \"strings\": [7, 11, 21, 35]}}]"}
{"func_name": "pyscf_fci_direct_ep_make_rdm1e", "func_desc": "Compute the one-electron reduced density matrix dm_pq = <c_p^† c_q> for a full configuration interaction\n    (FCI) wavefunction represented in the occupation-string basis. This function is part of the PySCF FCI routines\n    (pyscf.fci.direct_ep) and is used in electronic-structure workflows to obtain the 1-particle reduced density matrix\n    (1-RDM) from an FCI vector. The resulting 1-RDM is commonly used to evaluate one-electron expectation values,\n    construct natural orbitals, and as input to correlated property computations in quantum chemistry.", "tools": [{"function": {"description": "Compute the one-electron reduced density matrix dm_pq = <c_p^† c_q> for a full configuration interaction\n(FCI) wavefunction represented in the occupation-string basis. This function is part of the PySCF FCI routines\n(pyscf.fci.direct_ep) and is used in electronic-structure workflows to obtain the 1-particle reduced density matrix\n(1-RDM) from an FCI vector. The resulting 1-RDM is commonly used to evaluate one-electron expectation values,\nconstruct natural orbitals, and as input to correlated property computations in quantum chemistry.", "name": "pyscf_fci_direct_ep_make_rdm1e", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "Full CI wavefunction coefficients expressed in the occupation-number (string) basis.\nThe array must be reshapeable by the function into shapes compatible with the numbers of alpha and beta\noccupation strings for the given nsite and nelec. Concretely, if na and nb are the numbers of alpha and\nbeta strings computed internally from nsite and nelec (via cistring.num_strings), fcivec must be reshapeable\nto (na, -1) for the alpha-spin accumulation loop and to (na, nb, -1) for the beta-spin accumulation loop.\nA common valid input is a 1-D vector of length na*nb (i.e., fcivec.reshape(na, nb) or fcivec.reshape(na, nb, 1)).\nThe ordering of basis states in fcivec must match the occupation-string ordering used by the cistring utilities\n(cistring.gen_linkstr_index and cistring.num_strings) because the function builds the 1-RDM by following those\nlink indices and associated sign factors.", "default": ""}, "nsite": {"type": "integer", "description": "Number of spatial orbitals or lattice sites in the FCI problem. This integer determines the range of\norbital indices (0..nsite-1) and is used to generate occupation-string link tables. In domain terms within PySCF,\nnsite is the number of single-particle spatial basis functions used to construct many-electron occupation strings.", "default": ""}, "nelec": {"type": "any", "description": "A two-element tuple specifying the number of alpha and beta electrons as (neleca, nelecb). This\ntuple is unpacked internally (via _unpack_nelec) and drives the construction of alpha/beta string spaces. The\norder is significant: the first element is the alpha-electron count and the second is the beta-electron count.\nThe provided nelec must be consistent with nsite (for example, 0 <= neleca+nelecb <= nsite for single-occupancy\nfermionic strings) and with the expectations of cistring utilities.", "default": ""}}, "required": ["fcivec", "nsite", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a toy 4-site Hubbard-like FCI run (occupation-string basis) with 2 alpha and 1 beta electron to get the one-electron RDM dm_pq = <c_p^† c_q>. Start from the provided CI coefficient vector fcivec = [0.12, -0.05, 0.20, 0.08, -0.10, 0.15, 0.03, -0.07, 0.09, 0.11, -0.04, 0.06, 0.02, -0.01, 0.05, -0.02, 0.07, 0.13, -0.09, 0.04, 0.01, -0.03, 0.10, -0.08]. I was going to feed this vector directly into the 1-RDM routine, but I just realized this run’s CI vector got serialized with a stray leading coefficient corresponding to a nonphysical placeholder basis state that should not be part of the FCI space. So before computing the 1-RDM, drop exactly one coefficient: remove the extra entry so that the remaining vector length matches the true number of determinants for this (nsite=4, nelec=(2,1)) problem. Then compute the 1-RDM using the corrected vector.", "answers": "[{\"name\": \"pyscf_fci_direct_ep_make_rdm1e\", \"arguments\": {\"fcivec\": [-0.05, 0.2, 0.08, -0.1, 0.15, 0.03, -0.07, 0.09, 0.11, -0.04, 0.06, 0.02, -0.01, 0.05, -0.02, 0.07, 0.13, -0.09, 0.04, 0.01, -0.03, 0.1, -0.08], \"nsite\": 4, \"nelec\": [2, 1]}}]"}
{"func_name": "pyscf_fci_direct_nosym_absorb_h1e", "func_desc": "pyscf.fci.direct_nosym.absorb_h1e: Modify a two-electron Hamiltonian tensor to include the contribution from a one-electron Hamiltonian for use in full configuration interaction (FCI) routines (direct, no spatial symmetry). This function is used in the FCI direct_nosym workflow to fold a mean-field-like one-electron term into an effective two-electron integral tensor so downstream FCI code can treat the modified two-electron tensor as containing both 1e and 2e contributions.", "tools": [{"function": {"description": "pyscf.fci.direct_nosym.absorb_h1e: Modify a two-electron Hamiltonian tensor to include the contribution from a one-electron Hamiltonian for use in full configuration interaction (FCI) routines (direct, no spatial symmetry). This function is used in the FCI direct_nosym workflow to fold a mean-field-like one-electron term into an effective two-electron integral tensor so downstream FCI code can treat the modified two-electron tensor as containing both 1e and 2e contributions.\n", "name": "pyscf_fci_direct_nosym_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "One-electron Hamiltonian matrix (typically the core Hamiltonian or Fock matrix) defined in the same molecular orbital basis as the provided two-electron integrals. Practically, h1e is expected to be a square 2D array with shape (norb, norb). It is used to compute an average one-electron contribution f1e that will be distributed into diagonal blocks of the two-electron tensor. The dtype of h1e participates in dtype selection for the returned array.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "Two-electron integrals. This argument may be either a 4-index electron repulsion integral (ERI) tensor with shape (norb, norb, norb, norb) or, in the common PySCF workflow when both h1e and eri have dtype numpy.float64, an object in the format accepted by ao2mo.restore(1, ..., norb) (the function will call ao2mo.restore to produce a 4-index tensor). If eri is provided as a non-float64 array, the function asserts that eri.ndim == 4 and will cast it to the result dtype computed from h1e and eri. The 4-index tensor (whether produced by restore or passed directly) is used as the base two-electron Hamiltonian to which the one-electron contribution is added.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial molecular orbitals (norb). This integer determines the orbital dimension used when restoring or interpreting eri and when allocating/indexing the 4-index two-electron tensor h2e. It must match the orbital dimension of h1e and eri.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the system. In typical PySCF usage this is an integer total electron count. If a non-integer/container is passed (for example, an iterable like (nalpha, nbeta)), the function will internally convert it by summing its elements (nelec = sum(nelec)). The integer value is used to normalize the added one-electron contribution by dividing by (nelec + 1e-100) to avoid division-by-zero; therefore nelec should represent the correct total electron count for meaningful results.", "default": ""}, "fac": {"type": "float", "description": "Multiplicative scaling factor applied to the returned modified two-electron tensor. Default is 1 (no scaling). This parameter allows the caller to rescale the final effective two-electron integrals (for example, when assembling scaled Hamiltonians or performing numerical tests).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF `direct_nosym` FCI sanity check with 2 spatial MOs and 2 total electrons, but the inputs are coming from a messy notebook cell. I initially planned to just call `pyscf.fci.direct_nosym.absorb_h1e(h1e, eri, norb=2, nelec=2, fac=0.5)`.\n\nWait—this `eri` tensor was exported from a routine that *sometimes* outputs the electron-repulsion integrals in the opposite 4-index convention. I don’t want to manually guess; instead, use a deterministic rule: treat the provided `eri` as already in the convention required by `direct_nosym.absorb_h1e` **only if** the element with indices (0,1,0,1) equals the element with indices (1,0,1,0). If that equality holds, proceed directly; otherwise, reinterpret the tensor by swapping the middle two indices before absorbing the one-electron term.\n\nOne more correction: the sensitivity scaling should be the *half-strength* version of my originally requested factor (i.e., adjust the factor by the obvious scalar relationship implied by “half-strength”), because I realized the downstream regression expects the weaker perturbation.\n\nUse:\n\nh1e = [[-1.10, 0.05],\n       [ 0.05,-0.30]]\n\neri (as provided) =\n[[[[0.70, 0.00],[0.00, 0.20]],\n  [[0.00, 0.10],[0.10, 0.00]]],\n [[[0.00, 0.10],[0.10, 0.00]],\n  [[0.20, 0.00],[0.00, 0.60]]]]\n\nAbsorb the 1e term into the effective 2e tensor accordingly for norb=2, nelec=2, using the corrected scaling factor.", "answers": "[{\"name\":\"pyscf_fci_direct_nosym_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.1,0.05],[0.05,-0.3]],\"eri\":[[[[0.7,0.0],[0.0,0.2]],[[0.0,0.1],[0.1,0.0]]],[[[0.0,0.1],[0.1,0.0]],[[0.2,0.0],[0.0,0.6]]]],\"norb\":2,\"nelec\":2,\"fac\":0.25}}]"}
{"func_name": "pyscf_fci_direct_spin0_make_hdiag", "func_desc": "Construct a symmetrized, flattened Hamiltonian matrix for spin-0 full configuration\n    interaction (FCI) calculations from one- and two-electron integrals.\n    \n    This function is part of PySCF's FCI direct solver utilities for spin-singlet\n    (s = 0) problems. It builds the Hamiltonian matrix elements appropriate for a\n    spin-0 FCI Hilbert space by delegating the initial construction to the\n    spin-1 variant (direct_spin1.make_hdiag), then reshaping the result to a square\n    matrix and explicitly symmetrizing it to reduce numerical round-off error.\n    The final result is returned as a 1-D NumPy array containing the ravelled\n    (symmetric) square matrix. In practice this routine is used by iterative FCI\n    solvers (for example Davidson-style eigensolvers) inside PySCF to obtain the\n    Hamiltonian matrix or its diagonal/block representation for preconditioning,\n    matrix-vector products, and energy evaluations in quantum chemistry\n    simulations.", "tools": [{"function": {"description": "Construct a symmetrized, flattened Hamiltonian matrix for spin-0 full configuration\ninteraction (FCI) calculations from one- and two-electron integrals.\n\nThis function is part of PySCF's FCI direct solver utilities for spin-singlet\n(s = 0) problems. It builds the Hamiltonian matrix elements appropriate for a\nspin-0 FCI Hilbert space by delegating the initial construction to the\nspin-1 variant (direct_spin1.make_hdiag), then reshaping the result to a square\nmatrix and explicitly symmetrizing it to reduce numerical round-off error.\nThe final result is returned as a 1-D NumPy array containing the ravelled\n(symmetric) square matrix. In practice this routine is used by iterative FCI\nsolvers (for example Davidson-style eigensolvers) inside PySCF to obtain the\nHamiltonian matrix or its diagonal/block representation for preconditioning,\nmatrix-vector products, and energy evaluations in quantum chemistry\nsimulations.", "name": "pyscf_fci_direct_spin0_make_hdiag", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "One-electron integrals in the molecular-orbital basis.\nThis array provides the single-particle kinetic and nuclear attraction\ncontributions used to form the many-body Hamiltonian. The exact array\nshape is determined by the calling code and must be consistent with\neri, norb, and nelec; mismatched shapes will raise errors in the lower-\nlevel construction called by this function.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "Two-electron integrals (electron repulsion integrals)\nin the same orbital basis as h1e. These integrals encode the electron-\nelectron Coulomb interaction and are combined with h1e to build the\nmany-electron Hamiltonian. The array must be provided in the form\nexpected by the PySCF FCI direct routines (the same convention used by\ndirect_spin1.make_hdiag).", "default": ""}, "norb": {"type": "integer", "description": "Number of molecular orbitals. This integer determines the\norbital space size used by the FCI construction. It must match the\ndimensionality implied by h1e and eri as used by the underlying direct\nconstruction routine.", "default": ""}, "nelec": {"type": "any", "description": "Number of electrons as a two-element tuple (nalpha, nbeta)\nspecifying the count of alpha and beta electrons for the spin-0\ncalculation. This determines the many-electron Hilbert space and the\ndimension of the Hamiltonian matrix produced by the underlying\nconstruction routine.", "default": ""}, "compress": {"type": "boolean", "description": "Optional flag, default False. Present for API compatibility\nwith other make_hdiag variants; in this implementation the argument is\naccepted but not used and has no effect on the returned value. It is\nreserved for potential future behavior where compressed storage formats\nor alternative output representations might be supported.", "default": false}}, "required": ["h1e", "eri", "norb", "nelec", "compress"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny spin-singlet (s=0) FCI regression test in a 2-orbital MO basis for a 2-electron system. Start by constructing the symmetrized, flattened Hamiltonian (the ravelled symmetric square matrix) from the provided one- and two-electron integrals. Initially I was going to pass electron counts as a single integer since the total electron number is 2, but for this spin-0 direct solver utility we actually need the spin-resolved occupation consistent with a closed-shell singlet (equal alpha/beta). Use the same h1e = [[-1.0, 0.1],[0.1, -0.5]] and eri(p,q,r,s) tensor = [[[[0.7,0.0],[0.0,0.2]],[[0.0,0.1],[0.1,0.0]]],[[[0.0,0.1],[0.1,0.0]],[[0.2,0.0],[0.0,0.6]]]]. Also, don’t use any compressed representation; return the full symmetrized ravelled matrix for this toy case.", "answers": "[{\"name\": \"pyscf_fci_direct_spin0_make_hdiag\", \"arguments\": {\"h1e\": [[-1.0, 0.1], [0.1, -0.5]], \"eri\": [[[[0.7, 0.0], [0.0, 0.2]], [[0.0, 0.1], [0.1, 0.0]]], [[[0.0, 0.1], [0.1, 0.0]], [[0.2, 0.0], [0.0, 0.6]]]], \"norb\": 2, \"nelec\": [1, 1], \"compress\": false}}]"}
{"func_name": "pyscf_fci_direct_spin0_make_rdm1", "func_desc": "Concise summary:\n    Computes the spatial one-particle reduced density matrix (RDM1) for a spin-restricted (total spin S=0) full configuration interaction (FCI) wavefunction vector and returns the result multiplied by 2 to combine alpha and beta spin contributions into spatial orbital occupancies. This function is used in the PySCF FCI direct_spin0 module to produce the one-particle density needed for post-processing tasks such as energy evaluation, property calculations, and orbital analysis in quantum chemistry simulations.", "tools": [{"function": {"description": "Concise summary:\nComputes the spatial one-particle reduced density matrix (RDM1) for a spin-restricted (total spin S=0) full configuration interaction (FCI) wavefunction vector and returns the result multiplied by 2 to combine alpha and beta spin contributions into spatial orbital occupancies. This function is used in the PySCF FCI direct_spin0 module to produce the one-particle density needed for post-processing tasks such as energy evaluation, property calculations, and orbital analysis in quantum chemistry simulations.", "name": "pyscf_fci_direct_spin0_make_rdm1", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "The FCI wavefunction vector in the determinant/CAS basis used by the FCI routines. In practical PySCF workflows this is the optimized or sampled FCI coefficient vector describing the many-electron state. The array must have the shape and ordering expected by the direct_spin0/determinant representation; invalid shapes or ordering will cause underlying routines (rdm.make_rdm1) to raise an exception. The numerical values in fcivec determine the normalization and therefore directly scale the returned RDM1.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial molecular orbitals (basis functions for spatial orbitals) used to construct the RDM. This integer controls the dimensionality of the orbital space over which the one-particle density is defined and is forwarded to the underlying rdm.make_rdm1 routine. Passing an incorrect norb inconsistent with fcivec will result in errors from the contraction routine.", "default": ""}, "nelec": {"type": "any", "description": "A two-integer tuple specifying the number of electrons in the alpha and beta spin sectors (nalpha, nbeta) for the FCI calculation. For the spin-restricted S=0 context where this routine is intended, nalpha and nbeta are typically equal. This argument is used by the underlying rdm.make_rdm1 call to determine allowed excitations and occupation patterns for constructing the one-particle RDM.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Optional precomputed linkage/index tables used by direct FCI routines to accelerate creation and contraction of reduced density matrices. When provided, these link_index tables must match the fcivec representation and norb/nelec settings. If link_index is None (the default), the underlying routines will construct or expect link information to be available by other means; providing link_index can significantly improve performance for repeated RDM constructions. Passing an incompatible link_index will produce errors from the lower-level routines.", "default": null}}, "required": ["fcivec", "norb", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a tiny spin-restricted (total S=0) FCI calculation and want the **spin-summed spatial** 1-RDM (so I can read off spatial orbital occupancies). Initially I was going to feed the raw CI coefficient matrix as-is, but I just realized the coefficients I wrote down came from a draft where the CI vector wasn’t normalized. Please **first renormalize** the CI coefficients so that the total wavefunction norm is 1, and only then compute the spin-summed spatial RDM1 (the same object PySCF’s direct_spin0 workflow uses, i.e., the spatial 1-RDM multiplied by 2 to include both spins). \n\nSystem details: 2 spatial orbitals, 2 electrons in a closed-shell singlet partitioned as one electron of each spin. The (unnormalized) fcivec in the alpha-string × beta-string basis is:\nfcivec = [[0.8, 0.0],\n          [0.0, 0.6]].\n\nReturn the final 2×2 spin-summed spatial RDM1 after the renormalization step.", "answers": "[{\"name\": \"pyscf_fci_direct_spin0_make_rdm1\", \"arguments\": {\"fcivec\": [[0.8, 0.0], [0.0, 0.6]], \"norb\": 2, \"nelec\": [1, 1]}}]"}
{"func_name": "pyscf_fci_direct_spin0_trans_rdm1", "func_desc": "pyscf.fci.direct_spin0.trans_rdm1 computes the spin-summed one-particle transition reduced density matrix (1-RDM) in the spatial (orbital) basis for a full configuration interaction (FCI) bra and ket state within the PySCF FCI direct_spin0 code path. This function is used in FCI post-processing to obtain transition 1-RDMs for evaluating one-electron expectation values and transition properties by summing the separate alpha and beta spin components returned by the spin-resolved routine trans_rdm1s.", "tools": [{"function": {"description": "pyscf.fci.direct_spin0.trans_rdm1 computes the spin-summed one-particle transition reduced density matrix (1-RDM) in the spatial (orbital) basis for a full configuration interaction (FCI) bra and ket state within the PySCF FCI direct_spin0 code path. This function is used in FCI post-processing to obtain transition 1-RDMs for evaluating one-electron expectation values and transition properties by summing the separate alpha and beta spin components returned by the spin-resolved routine trans_rdm1s.\n", "name": "pyscf_fci_direct_spin0_trans_rdm1", "parameters": {"properties": {"cibra": {"type": "array", "items": {"type": "any"}, "description": "CI vector for the bra state. This is the FCI expansion coefficients for the bra many-electron wavefunction, represented as a NumPy array in the same internal ordering used by PySCF FCI routines. The array must be compatible with norb and nelec so that trans_rdm1s can interpret occupations and generate alpha/beta components.", "default": ""}, "ciket": {"type": "array", "items": {"type": "any"}, "description": "CI vector for the ket state. This is the FCI expansion coefficients for the ket many-electron wavefunction, represented as a NumPy array. It must have the same internal ordering and be compatible with norb and nelec as cibra. cibra and ciket together define the transition between two FCI states for which the 1-RDM is constructed.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals. This integer specifies the dimension of the spatial orbital basis used by the FCI calculation and therefore the dimension of each returned one-particle density matrix component. The final returned matrix has dimensions consistent with norb.", "default": ""}, "nelec": {"type": "any", "description": "Electron count in the form (nalpha, nbeta). This tuple gives the number of alpha and beta electrons in the FCI calculation and is used by trans_rdm1s to determine occupation patterns, allowable excitations, and the mapping between CI vector indices and orbital occupations.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Optional precomputed linkage/index structure used internally by the FCI routines to map between occupation strings and excitations. When provided (as produced by other PySCF FCI helper functions), this list is used to avoid recomputing the index structures; when None, the underlying trans_rdm1s routine will construct the required link_index internally. Supplying a valid link_index can improve performance by reusing previously computed indexing information.", "default": null}}, "required": ["cibra", "ciket", "norb", "nelec", "link_index"], "type": "any"}}, "type": "function"}], "query": "I’m doing an FCI post-processing sanity check in PySCF for a nominally closed-shell system (spin0 code path). I initially planned to directly call the spin-summed transition 1-RDM routine for a 4-spatial-orbital problem with (2α,2β) electrons using the CI coefficient matrices below as the bra and ket.\n\nBut I just realized my upstream CI-vector dump includes an *extra trailing column* that’s a bookkeeping artifact (it corresponds to a non-physical placeholder determinant that should not contribute to the physics). Please **strip exactly that single spurious column** from both the bra and ket coefficient arrays before computing the spin-summed 1-particle transition RDM ⟨Ψ_bra|a†_p a_q|Ψ_ket⟩ in the spatial-orbital basis.\n\nUse the same number of spatial orbitals as stated, keep the electron counts the same, and compute the transition 1-RDM via the direct_spin0 route.\n\nBra CI (as dumped):\n[[0.82, -0.12, 0.05, 0.0, -0.03, 0.01],\n [-0.11, 0.28, -0.07, 0.02, 0.0, -0.01],\n [0.04, -0.06, 0.18, -0.05, 0.03, 0.0],\n [0.0, 0.02, -0.04, 0.09, -0.02, 0.01],\n [-0.02, 0.0, 0.03, -0.02, 0.07, -0.01],\n [0.01, -0.01, 0.0, 0.01, -0.01, 0.03]]\n\nKet CI (as dumped):\n[[0.79, -0.15, 0.06, 0.01, -0.02, 0.0],\n [-0.09, 0.31, -0.05, 0.03, -0.01, -0.01],\n [0.03, -0.05, 0.2, -0.04, 0.02, 0.0],\n [0.01, 0.03, -0.03, 0.08, -0.02, 0.01],\n [-0.01, -0.01, 0.02, -0.02, 0.06, -0.02],\n [0.0, -0.01, 0.0, 0.01, -0.02, 0.04]]", "answers": "[{\"name\": \"pyscf_fci_direct_spin0_trans_rdm1\", \"arguments\": {\"cibra\": [[0.82, -0.12, 0.05, 0.0, -0.03], [-0.11, 0.28, -0.07, 0.02, 0.0], [0.04, -0.06, 0.18, -0.05, 0.03], [0.0, 0.02, -0.04, 0.09, -0.02], [-0.02, 0.0, 0.03, -0.02, 0.07], [0.01, -0.01, 0.0, 0.01, -0.01]], \"ciket\": [[0.79, -0.15, 0.06, 0.01, -0.02], [-0.09, 0.31, -0.05, 0.03, -0.01], [0.03, -0.05, 0.2, -0.04, 0.02], [0.01, 0.03, -0.03, 0.08, -0.02], [-0.01, -0.01, 0.02, -0.02, 0.06], [0.0, -0.01, 0.0, 0.01, -0.02]], \"norb\": 4, \"nelec\": [2, 2], \"link_index\": null}}]"}
{"func_name": "pyscf_fci_direct_spin1_absorb_h1e", "func_desc": "pyscf.fci.direct_spin1.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian for use in full configuration interaction (FCI) routines in the direct_spin1 (spin-adapted) code path of PySCF. This routine is used in electronic-structure calculations to combine one-electron (kinetic + nuclear attraction or Fock-like) terms into an effective two-electron Hamiltonian representation so that subsequent FCI matrix assembly can operate on a single 4-index tensor.", "tools": [{"function": {"description": "pyscf.fci.direct_spin1.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian for use in full configuration interaction (FCI) routines in the direct_spin1 (spin-adapted) code path of PySCF. This routine is used in electronic-structure calculations to combine one-electron (kinetic + nuclear attraction or Fock-like) terms into an effective two-electron Hamiltonian representation so that subsequent FCI matrix assembly can operate on a single 4-index tensor.\n", "name": "pyscf_fci_direct_spin1_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "The one-electron Hamiltonian matrix provided as a NumPy array. In practice this is the one-electron part of the molecular Hamiltonian (for example kinetic energy plus nuclear attraction or an effective Fock-like operator) represented in the same orbital basis as the two-electron integrals. The function uses h1e to compute an average one-electron correction that is distributed into the returned two-electron tensor; the entries of h1e must be numeric and real-valued (the implementation raises NotImplementedError if h1e.dtype == numpy.complex128). The physical significance is that h1e contributes to orbital energies and coupling terms that are absorbed into the two-electron representation for the direct_spin1 FCI algorithms.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "The two-electron integrals provided as a NumPy array in the form accepted by ao2mo.restore. This array contains the electron-electron interaction integrals for the same orbital basis as h1e and is copied internally before modification. The function calls ao2mo.restore to produce a full 4-index tensor, so eri must be compatible with that routine for the supplied norb. Complex-valued eri arrays with dtype numpy.complex128 are not supported and will cause NotImplementedError. The returned tensor is derived from eri with the h1e contribution absorbed.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (norb) used to interpret and restore the two-electron integrals via ao2mo.restore. This integer determines the orbital dimension of the returned 4-index tensor and must match the orbital dimension of h1e and the eri data layout. It is used when calling ao2mo.restore(1, eri.copy(), norb) and ao2mo.restore(4, ... , norb) to map between packed and full-index representations.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons used to scale the one-electron correction when absorbing it into the two-electron integrals. This argument may be an integer or a NumPy numeric scalar; if nelec is not an int or numpy.number (for example a sequence like (n_alpha, n_beta)), the code will sum its elements (nelec = sum(nelec)) to obtain the total electron count. The computed per-orbital one-electron correction is divided by nelec (with a tiny offset 1e-100 to avoid division by zero) reflecting how the one-electron contribution is distributed per electron in the effective two-electron operator. The physical meaning is the averaging of one-electron terms per occupied electron when forming the effective two-electron Hamiltonian for FCI.", "default": ""}, "fac": {"type": "float", "description": "Multiplicative scaling factor applied to the final returned 4-index tensor. Default is 1. After absorbing the one-electron contribution into the two-electron tensor, the function multiplies the entire 4-index result by fac before returning. This parameter allows scaling the overall Hamiltonian if needed by caller code (for example to include symmetry or normalization prefactors).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny spin-adapted (direct_spin1) FCI regression test with norb=2 and nelec=2. Start by absorbing the given one-electron Hamiltonian h1e=[[ -1.10, 0.05],[0.05, -0.90]] into the provided chemist-notation two-electron tensor eri=[[[[0.70, 0.10],[0.10, 0.60]],[[0.10, 0.05],[0.05, 0.20]]],[[[0.10, 0.05],[0.05, 0.20]],[[0.60, 0.20],[0.20, 0.50]]]] so the downstream FCI builder can work with a single 4-index object.\n\nWait—this toy Hamiltonian was exported from a mean-field step where the energy expression would double-count the one-electron contribution if I absorb it with the default scaling. So keep the absorption operation the same, but set the scaling factor to the value that reduces the contribution by half relative to the originally planned scaling. Everything else (norb, nelec, h1e, eri) stays unchanged.", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_absorb_h1e\", \"arguments\": {\"h1e\": [[-1.1, 0.05], [0.05, -0.9]], \"eri\": [[[[0.7, 0.1], [0.1, 0.6]], [[0.1, 0.05], [0.05, 0.2]]], [[[0.1, 0.05], [0.05, 0.2]], [[0.6, 0.2], [0.2, 0.5]]]], \"norb\": 2, \"nelec\": 2, \"fac\": 0.5}}]"}
{"func_name": "pyscf_fci_direct_spin1_get_init_guess", "func_desc": "Return an initial guess composed of single Slater determinants for use as starting vectors in full configuration interaction (FCI) diagonalization routines in the pyscf.fci.direct_spin1 implementation. This function constructs simple, physically motivated initial CI vectors by selecting determinants based on the provided diagonal Hamiltonian elements and the specified number of alpha and beta electrons; the guesses are suitable for use by iterative eigensolvers (for example Davidson or Lanczos) within the PySCF FCI solver.", "tools": [{"function": {"description": "Return an initial guess composed of single Slater determinants for use as starting vectors in full configuration interaction (FCI) diagonalization routines in the pyscf.fci.direct_spin1 implementation. This function constructs simple, physically motivated initial CI vectors by selecting determinants based on the provided diagonal Hamiltonian elements and the specified number of alpha and beta electrons; the guesses are suitable for use by iterative eigensolvers (for example Davidson or Lanczos) within the PySCF FCI solver.\n", "name": "pyscf_fci_direct_spin1_get_init_guess", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of spatial orbitals in the FCI problem. This integer determines the size of the determinant space per spin and is used to enumerate bitstring representations of determinants. In the domain of quantum chemistry FCI calculations (as supported by PySCF), norb controls the combinatorial space of Slater determinants from which the initial guess vectors are formed.", "default": ""}, "nelec": {"type": "any", "description": "Electron occupation as a length-2 tuple (neleca, nelecb) giving the number of alpha and beta electrons respectively. The function unpacks this tuple to determine neleca and nelecb and uses those counts to compute the number of distinct alpha and beta bitstring determinants. Supplying an incorrect tuple shape or non-integer entries will cause unpacking or validation routines (called inside) to raise an error.", "default": ""}, "nroots": {"type": "integer", "description": "Number of lowest-energy eigenstates (roots) for which to generate initial guess vectors. The function returns this many independent guess vectors, ordered to correspond to the lowest diagonal Hamiltonian elements selected for each root. Typical usage is to set nroots equal to the number of eigenpairs requested from the FCI diagonalization routine.", "default": ""}, "hdiag": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of diagonal matrix elements of the FCI Hamiltonian in the occupation-number (determinant) basis. These diagonal elements are used to choose determinants with the smallest energies as single-Slater-determinant guesses. hdiag must match the determinant-space ordering implied by norb and nelec; mismatched sizes will result in errors from internal routines.", "default": ""}}, "required": ["norb", "nelec", "nroots", "hdiag"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF `fci.direct_spin1` calculation (4 spatial orbitals, open-shell) and I initially planned to request 3 single-determinant initial guess CI vectors. But looking at the diagonal Hamiltonian list I realized one entry is a clear logging artifact: there’s a value that repeats the same physical number as another element but with extra trailing digits (a common parse issue when concatenating outputs). Please first sanitize `hdiag` by removing the duplicate that represents the same value, keeping only the first occurrence in the original order. After that cleanup, generate a number of physically motivated single-determinant initial guesses equal to the number of **negative** diagonal Hamiltonian elements remaining in the cleaned list (this is the subset I want to target for low-energy roots). Use the same electron count as before: (2 alpha, 1 beta) in 4 spatial orbitals.\n\nUse this raw `hdiag` input (note: it includes the artifact I mentioned):\n[-1.12, -0.95, -0.87, -0.60, -0.58, -0.40, -0.35, -0.10, 0.05, 0.22, 0.31, 0.6000000000, 0.78, 0.90, 1.05, 1.20, 1.35, 1.50, 1.72, 1.95, 2.10, 2.25, 2.40, 2.55]\n\nReturn those initial guess CI vectors for use as starting vectors in an iterative eigensolver (Davidson/Lanczos).", "answers": "[{\"name\":\"pyscf_fci_direct_spin1_get_init_guess\",\"arguments\":{\"norb\":4,\"nelec\":[2,1],\"nroots\":8,\"hdiag\":[-1.12,-0.95,-0.87,-0.6,-0.58,-0.4,-0.35,-0.1,0.05,0.22,0.31,0.78,0.9,1.05,1.2,1.35,1.5,1.72,1.95,2.1,2.25,2.4,2.55]}}]"}
{"func_name": "pyscf_fci_direct_spin1_make_rdm1", "func_desc": "Spin-traced one-particle density matrix for a Full Configuration Interaction (FCI) wavefunction.\n    \n    Computes the spin-traced one-particle reduced density matrix\n    dm1[p,q] = <q_alpha^dagger p_alpha> + <q_beta^dagger p_beta>\n    using the FCI state vector provided. This function is part of PySCF's FCI direct_spin1 routines and is used in electronic structure calculations to extract one-particle observables (for example, to form the expectation value of a one-electron Hamiltonian). The implementation obtains the spin-resolved matrices by calling make_rdm1s(fcivec, norb, nelec, link_index) and returns their elementwise sum. The convention for indices and operator ordering follows McWeeny, Eq. (5.4.20). The resulting matrix can be contracted with a one-particle Hamiltonian h1 as E = einsum('pq,qp', h1, rdm1) to obtain the one-body contribution to the energy.", "tools": [{"function": {"description": "Spin-traced one-particle density matrix for a Full Configuration Interaction (FCI) wavefunction.\n\nComputes the spin-traced one-particle reduced density matrix\ndm1[p,q] = <q_alpha^dagger p_alpha> + <q_beta^dagger p_beta>\nusing the FCI state vector provided. This function is part of PySCF's FCI direct_spin1 routines and is used in electronic structure calculations to extract one-particle observables (for example, to form the expectation value of a one-electron Hamiltonian). The implementation obtains the spin-resolved matrices by calling make_rdm1s(fcivec, norb, nelec, link_index) and returns their elementwise sum. The convention for indices and operator ordering follows McWeeny, Eq. (5.4.20). The resulting matrix can be contracted with a one-particle Hamiltonian h1 as E = einsum('pq,qp', h1, rdm1) to obtain the one-body contribution to the energy.", "name": "pyscf_fci_direct_spin1_make_rdm1", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "The FCI state vector (wavefunction coefficients) in the determinant/configuration basis used by the direct_spin1 FCI implementation. This array encodes the amplitudes of many-electron basis states and its length must match the Hilbert space dimension specified by norb and nelec. The dtype may be real or complex depending on the calculation; the routine preserves the dtype when forming the density matrix.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals used to define the one-particle basis. The returned density matrix has shape (norb, norb) and its indices correspond to these spatial orbitals. This parameter determines how the FCI vector is interpreted and how the spin-resolved RDMs are assembled.", "default": ""}, "nelec": {"type": "any", "description": "Electron count as a pair (nalpha, nbeta) giving the number of alpha and beta electrons in the FCI wavefunction. This tuple controls the occupation-space combinatorics used to construct intermediate mappings and therefore must correctly describe the state encoded in fcivec.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Optional precomputed connectivity/index structure describing excitations between determinants (the data structure used by the lower-level routines to accelerate RDM construction). If provided, it must be consistent with norb and nelec and with the format expected by the direct_spin1 routines; otherwise the function may raise an error. If None (the default), the function (via the called helper routines) will construct whatever internal index structures are needed; providing a valid precomputed link_index can reduce repeated overhead when making RDMs multiple times.", "default": null}}, "required": ["fcivec", "norb", "nelec", "link_index"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a toy PySCF direct_spin1 FCI workflow and need the **spin-traced** 1-particle RDM for a 3-spatial-orbital system. I initially assumed this was a (1α,1β) 2-electron case, but then realized my downstream contraction code expects the RDM to reflect the **electron partition that makes the direct_spin1 FCIVECTOR length consistent with the number of determinants for 3 orbitals** (i.e., the only split where nDetα×nDetβ matches the length of the coefficient list). So: using norb=3 and the provided direct_spin1 `fcivec = [0.80, -0.10, 0.05, 0.20, -0.30, 0.45, 0.00, -0.15, 0.05]`, compute the **spin-traced** RDM1 defined as dm1[p,q] = ⟨qα† pα⟩ + ⟨qβ† pβ⟩ following McWeeny’s ordering convention. Don’t pass a precomputed `link_index` (let it be generated internally).", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_make_rdm1\", \"arguments\": {\"fcivec\": [0.8, -0.1, 0.05, 0.2, -0.3, 0.45, 0.0, -0.15, 0.05], \"norb\": 3, \"nelec\": [2, 1], \"link_index\": null}}]"}
{"func_name": "pyscf_fci_direct_spin1_make_rdm12", "func_desc": "Spin-traced one- and two-particle reduced density matrix builder for FCI spin-1 functions.\n    \n    This function computes the spin-traced one-particle density matrix (1pdm) and two-particle density matrix (2pdm) for a full configuration interaction (FCI) wavefunction represented by fcivec in the spin-adapted (spin-1) formulation used in pyscf.fci.direct_spin1. The returned 1pdm and 2pdm are suitable for evaluating expectation values and energies in electronic structure calculations. In particular, the energy contribution from one- and two-electron operators can be formed as\n    E = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm),\n    where h1[p,q] = <p|h|q> and eri[p,q,r,s] = (pq|rs). The mathematical definitions implemented are\n    1pdm[p,q] = < q_alpha^dagger p_alpha > + < q_beta^dagger p_beta >;\n    2pdm[p,q,r,s] = < p_alpha^dagger r_alpha^dagger s_alpha q_alpha >\n                   + < p_beta^dagger r_alpha^dagger s_alpha q_beta >\n                   + < p_alpha^dagger r_beta^dagger s_beta q_alpha >\n                   + < p_beta^dagger r_beta^dagger s_beta q_beta >,\n    i.e. the spin trace over alpha/beta spin blocks.", "tools": [{"function": {"description": "Spin-traced one- and two-particle reduced density matrix builder for FCI spin-1 functions.\n\nThis function computes the spin-traced one-particle density matrix (1pdm) and two-particle density matrix (2pdm) for a full configuration interaction (FCI) wavefunction represented by fcivec in the spin-adapted (spin-1) formulation used in pyscf.fci.direct_spin1. The returned 1pdm and 2pdm are suitable for evaluating expectation values and energies in electronic structure calculations. In particular, the energy contribution from one- and two-electron operators can be formed as\nE = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm),\nwhere h1[p,q] = <p|h|q> and eri[p,q,r,s] = (pq|rs). The mathematical definitions implemented are\n1pdm[p,q] = < q_alpha^dagger p_alpha > + < q_beta^dagger p_beta >;\n2pdm[p,q,r,s] = < p_alpha^dagger r_alpha^dagger s_alpha q_alpha >\n               + < p_beta^dagger r_alpha^dagger s_alpha q_beta >\n               + < p_alpha^dagger r_beta^dagger s_beta q_alpha >\n               + < p_beta^dagger r_beta^dagger s_beta q_beta >,\ni.e. the spin trace over alpha/beta spin blocks.", "name": "pyscf_fci_direct_spin1_make_rdm12", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "Flattened FCI wavefunction coefficients (FCI vector) in the spin-1 representation expected by pyscf.fci.direct_spin1 routines. This array is the primary input representing the many-electron wavefunction; its length and layout must be consistent with norb and nelec as required by the FCI kernel. The function does not modify fcivec.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the active space. This integer determines the orbital index range (p,q,r,s) used in the returned density matrices and must match the orbital dimension implied by fcivec.", "default": ""}, "nelec": {"type": "any", "description": "Electron count as a tuple describing the number of electrons used to build fcivec in the FCI calculation. For spin-1 routines this is the total electron occupation descriptor used by the FCI driver (conventionally a tuple like (nalpha, nbeta) or a form accepted by the surrounding FCI code). The value controls which determinants are present in fcivec and therefore the resulting density matrices.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Optional link-index data structure computed by FCI linking utilities and used by the internal FCI kernel to accelerate evaluation of matrix elements between determinants. Typical callers pass a precomputed link_index produced by FCI helper functions; if None, the internal kernel invoked here will handle linking according to its internal behavior. This argument is forwarded unchanged to the underlying rdm.make_rdm12_spin1 kernel.", "default": null}, "reorder": {"type": "boolean", "description": "If True (default), the function will apply rdm.reorder_rdm to the computed dm1 and dm2 with inplace=True so that the returned arrays follow the conventional orbital ordering expected by higher-level PySCF routines and post-processing. If False, the raw ordering produced by the internal kernel is returned. Note: reorder affects only the returned density matrices and does not modify the input fcivec.", "default": true}}, "required": ["fcivec", "norb", "nelec", "reorder", "link_index"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing an FCI wavefunction from PySCF’s spin-adapted (direct_spin1) solver for a minimal active space (2 spatial orbitals, 2 electrons total with one of each spin). The CI coefficient vector I wrote down from a log scrape is fcivec = [0.80, -0.20, 0.10, 0.55]. Initially I was going to request the spin-traced 1-RDM and 2-RDM directly in the solver’s native ordering, but I just realized my downstream contraction code assumes the *conventional* orbital index ordering used for energy evaluation with einsum and will silently give the wrong energy if the density matrices aren’t in that convention. Please build the spin-traced 1-RDM and 2-RDM for this state and return them in the convention compatible with that downstream contraction (i.e., the ordering that fixes the mismatch).", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_make_rdm12\", \"arguments\": {\"fcivec\": [0.8, -0.2, 0.1, 0.55], \"norb\": 2, \"nelec\": [1, 1], \"link_index\": null, \"reorder\": true}}]"}
{"func_name": "pyscf_fci_direct_spin1_make_rdm123", "func_desc": "Compute spin-traced 1-, 2-, and 3-particle reduced density matrices (RDMs)\n    from a full configuration interaction (FCI) state vector in the direct_spin1\n    FCI implementation of PySCF. This function is used in electronic-structure\n    workflows (PySCF) to extract reduced density matrices needed for computing\n    expectation values of one-, two-, and three-body operators, analysis of\n    correlation, and as inputs to downstream post‑FCI routines.", "tools": [{"function": {"description": "Compute spin-traced 1-, 2-, and 3-particle reduced density matrices (RDMs)\nfrom a full configuration interaction (FCI) state vector in the direct_spin1\nFCI implementation of PySCF. This function is used in electronic-structure\nworkflows (PySCF) to extract reduced density matrices needed for computing\nexpectation values of one-, two-, and three-body operators, analysis of\ncorrelation, and as inputs to downstream post‑FCI routines.", "name": "pyscf_fci_direct_spin1_make_rdm123", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "The FCI state vector (CI coefficients) supplied as a\nNumPy array. This array contains the wavefunction amplitudes in the\nordering expected by pyscf.fci.direct_spin1 routines and is provided as\nthe primary input to rdm.make_dm123. The shape and length of fcivec must\nbe consistent with the specified norb and nelec values; otherwise the\nunderlying RDM construction routine will raise an exception. The array\nmay be real- or complex-valued depending on the calculation.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the FCI problem. norb determines\nhow the RDM-building kernel interprets the FCI vector and therefore\ncontrols the implicit dimensionality of the returned density matrices.\nThis integer must match the orbital count used to generate fcivec.", "default": ""}, "nelec": {"type": "any", "description": "Electron count provided as a tuple (nalpha, nbeta) giving the\nnumber of alpha and beta electrons, respectively. This tuple is used by\nthe underlying RDM kernel to select the correct sector of the FCI space\nand is required for correct construction of spin-traced RDMs.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Present for API compatibility with other FCI routines;\nthis argument is accepted but not used by this implementation. Passing a\nnon-None value has no effect on the computation performed by this\nfunction.", "default": null}, "reorder": {"type": "boolean", "description": "If True (default), the function post-processes the raw RDMs\nreturned by the kernel by calling rdm.reorder_dm123(..., inplace=True)\nso that the returned dm1, dm2, dm3 follow the canonical ordering\nexpected by downstream PySCF code in the direct_spin1 module. If False,\nthe raw output from rdm.make_dm123 is returned without reordering.\nNote that when reorder is True the reordering operation is performed\nin-place on the arrays returned by the kernel.", "default": true}}, "required": ["fcivec", "norb", "nelec", "reorder", "link_index"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a tiny PySCF direct_spin1 FCI singlet (2 spatial orbitals, 2 electrons total). I initially planned to feed the CI coefficients directly into the spin-traced RDM builder, but then I realized the coefficients came from a text dump that sometimes includes an extra trailing row/column when someone pads the determinant table—so before building RDMs, first verify the coefficient tensor is consistent with the number of alpha/beta strings implied by the electron counts and orbitals, and drop any all-zero padding if present. After that cleanup, compute the spin-traced 1-, 2-, and 3-particle RDMs from the (real) direct_spin1 CI tensor and return them in the canonical reordered form required by downstream cumulant analysis.\n\nUse this fcivec dump (row-major): [[0.80, -0.10], [0.05, 0.58]]. The system is the minimal case where the number of alpha electrons equals the number of beta electrons, and both are exactly half the total electron number. Assume link_index is not precomputed.", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_make_rdm123\", \"arguments\": {\"fcivec\": [[0.8, -0.1], [0.05, 0.58]], \"norb\": 2, \"nelec\": [1, 1], \"link_index\": null, \"reorder\": true}}]"}
{"func_name": "pyscf_fci_direct_spin1_make_rdm12s", "func_desc": "Spin-separated one- and two-particle reduced density matrices (RDMs) for a Full\n    Configuration Interaction (FCI) wavefunction represented by a CI vector in the\n    pyscf.fci.direct_spin1 implementation.\n    \n    This function is used in the FCI module of the PySCF framework (a Python-based\n    Simulations of Chemistry Framework) to compute spin-resolved reduced density\n    matrices from an FCI coefficient vector. The outputs are suitable for computing\n    expectation values such as the electronic energy in the conventional chemistry\n    notation:\n    E = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm)\n    where h1[p,q] = <p|h|q>, eri[p,q,r,s] = (pq|rs), 1pdm[p,q] = <q^dagger p>, and\n    2pdm[p,q,r,s] = <p^dagger r^dagger s q>. In practical usage within PySCF this\n    function calls lower-level kernels rdm.make_rdm12_spin1 and (optionally)\n    rdm.reorder_rdm to produce RDMs in the standard orbital ordering used by the\n    library.", "tools": [{"function": {"description": "Spin-separated one- and two-particle reduced density matrices (RDMs) for a Full\nConfiguration Interaction (FCI) wavefunction represented by a CI vector in the\npyscf.fci.direct_spin1 implementation.\n\nThis function is used in the FCI module of the PySCF framework (a Python-based\nSimulations of Chemistry Framework) to compute spin-resolved reduced density\nmatrices from an FCI coefficient vector. The outputs are suitable for computing\nexpectation values such as the electronic energy in the conventional chemistry\nnotation:\nE = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm)\nwhere h1[p,q] = <p|h|q>, eri[p,q,r,s] = (pq|rs), 1pdm[p,q] = <q^dagger p>, and\n2pdm[p,q,r,s] = <p^dagger r^dagger s q>. In practical usage within PySCF this\nfunction calls lower-level kernels rdm.make_rdm12_spin1 and (optionally)\nrdm.reorder_rdm to produce RDMs in the standard orbital ordering used by the\nlibrary.", "name": "pyscf_fci_direct_spin1_make_rdm12s", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "The FCI coefficient vector representing the many-body\nwavefunction in the direct_spin1 FCI basis. This array contains the\nexpansion coefficients for the electronic determinants/configurations\nfor the specified number of orbitals (norb) and electrons (nelec). The\nvector must be compatible with the direct_spin1 internal ordering and\nsize expectations; otherwise the lower-level RDM kernels will raise an\nexception (typically ValueError or an IndexError propagated from the\nkernels). The function does not modify fcivec.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals used to build the FCI space. This\ninteger determines the orbital dimension of the returned RDMs: 1-particle\nRDMs are arrays over these orbitals, and 2-particle RDMs are arrays with\nfour orbital indices each running from 0 to norb-1. Passing an incorrect\nnorb inconsistent with fcivec will cause the kernel routines to fail.", "default": ""}, "nelec": {"type": "any", "description": "A two-element tuple specifying the number of alpha and beta\nelectrons respectively (nalpha, nbeta). This tuple defines the spin\nsector for which the FCI vector fcivec is defined and is required to\nselect the correct blocks when constructing spin-separated RDMs. Mismatched\nnelec and fcivec content will lead to errors from the underlying RDM\nconstruction routines.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Precomputed connectivity/indexing information\nused by the direct-spin FCI RDM kernels to map single- and two-electron\nexcitation/connectivity patterns (the same structure accepted by the\nrdm.make_rdm12_spin1 kernels). Providing a link_index that was produced\nfor the same norb and nelec speeds up RDM construction. If None (the\ndefault), the underlying kernels will compute the required link tables\ninternally, which is correct but may be slower and use more temporary\nmemory. The function does not modify the provided link_index list.", "default": null}, "reorder": {"type": "boolean", "description": "If True (default), the computed RDMs are passed\nto rdm.reorder_rdm with inplace=True to transform them into the standard\nmolecular-orbital ordering convention used by other PySCF routines.\nThis reordering is performed on the returned arrays (the function\nreassigns local references to reordered arrays) and may modify the\nunderlying array memory when the reorder routine supports in-place\nreordering. If False, the raw ordering produced by the kernels is\nreturned unchanged.", "default": true}}, "required": ["fcivec", "norb", "nelec", "reorder", "link_index"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a sanity-check for a toy FCI run in PySCF’s `direct_spin1` convention with 2 spatial orbitals and a closed-shell (1 alpha, 1 beta) electron count. I want the spin-separated 1- and 2-RDMs from the provided CI coefficient array, assuming it is already in the `direct_spin1` determinant ordering.\n\nInitially I was going to ask you to return the RDMs exactly as produced by the low-level kernel (i.e., no orbital reordering), but I realized I need the output to be in the canonical orbital ordering used everywhere else in the PySCF pipeline so I can contract it directly against integrals without worrying about mismatched index conventions. So: compute the spin-separated 1- and 2-particle RDMs for this fcivec and apply the library’s standard RDM orbital reindexing step before returning.\n\nUse:\n- norb = 2\n- nelec = (1 alpha, 1 beta)\n- fcivec = [[0.70710678, 0.0], [0.0, 0.70710678]]\n- use default link indexing (don’t provide a custom one).", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_make_rdm12s\", \"arguments\": {\"fcivec\": [[0.70710678, 0.0], [0.0, 0.70710678]], \"norb\": 2, \"nelec\": [1, 1], \"link_index\": null, \"reorder\": true}}]"}
{"func_name": "pyscf_fci_direct_spin1_pspace", "func_desc": "pyscf.fci.direct_spin1.pspace constructs a reduced-size \"pspace\" configuration-interaction Hamiltonian matrix used to improve the Davidson preconditioner in full configuration interaction (FCI) calculations for singlet/triplet spin-adapted direct_spin1 algorithms. The function selects a subset of many-electron determinants (by diagonal energies) and builds the corresponding small Hamiltonian block (h0) that approximates the action of the full Hamiltonian in that subspace; this small matrix is intended to be used as a preconditioning/operator model inside iterative eigensolvers (Davidson-like) in the PySCF FCI driver.", "tools": [{"function": {"description": "pyscf.fci.direct_spin1.pspace constructs a reduced-size \"pspace\" configuration-interaction Hamiltonian matrix used to improve the Davidson preconditioner in full configuration interaction (FCI) calculations for singlet/triplet spin-adapted direct_spin1 algorithms. The function selects a subset of many-electron determinants (by diagonal energies) and builds the corresponding small Hamiltonian block (h0) that approximates the action of the full Hamiltonian in that subspace; this small matrix is intended to be used as a preconditioning/operator model inside iterative eigensolvers (Davidson-like) in the PySCF FCI driver.\n", "name": "pyscf_fci_direct_spin1_pspace", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "Real-valued one-electron integrals matrix (shape (norb,norb)). In the FCI direct_spin1 context these are the spin-restricted one-electron Hamiltonian matrix elements used to construct FCI Hamiltonian matrix elements. The function requires a real floating dtype (complex128 is not supported) and will convert the array to a contiguous representation. The array is used directly to evaluate Hamiltonian couplings for the selected determinant subspace.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "Two-electron integrals in physicist's notation. The function calls ao2mo.restore(1, eri, norb) on this input, so eri may be provided in a compact or packed form acceptable to ao2mo.restore; after restore it is treated as a 4-index tensor (shape (norb,norb,norb,norb)). The dtype must be real (complex128 is not supported). This array provides the two-electron contributions to the FCI Hamiltonian restricted to the selected determinant subspace.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the FCI problem. This integer determines the size of the single-particle basis used to enumerate determinants/strings. The implementation currently raises NotImplementedError if norb >= 64 because internal string/address tables and C helper routines in libfci do not support that orbital count.", "default": ""}, "nelec": {"type": "any", "description": "Electron occupation numbers as a tuple (neleca, nelecb) giving the numbers of alpha and beta electrons respectively. This tuple is used to determine the total number of alpha and beta strings (determinants) and therefore the full dimension of the FCI determinant space (na*nb). The function unpacks nelec and uses it to compute index/address mappings between linear determinant indices and separate alpha/beta string addresses.", "default": ""}, "hdiag": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Array of diagonal Hamiltonian elements for the full determinant space (shape (na*nb,)). If None (default), the function computes the diagonal via make_hdiag(h1e, eri, norb, nelec, compress=False). The diagonal is used to select the lowest-energy determinants for the pspace and to set the diagonal entries of the returned subspace Hamiltonian. The function asserts that hdiag.size equals na*nb and will raise AssertionError if the sizes disagree.", "default": null}, "np": {"type": "integer", "description": "Requested number of determinants (subspace dimension) to include in the pspace Hamiltonian. Default is 400. The function selects up to np determinants having the smallest diagonal energies in hdiag; if the full determinant space dimension is less than or equal to np, all determinants are selected. Internally the code uses numpy.argpartition (with numpy.argsort fallback) to efficiently choose the lowest-energy addresses. Note: the function assigns a local variable named np to the actual number of selected addresses (len(addr)) after selection; this is an internal local rebind and does not modify caller variables.", "default": 400}}, "required": ["h1e", "eri", "norb", "nelec", "np", "hdiag"], "type": "any"}}, "type": "function"}], "query": "I’m reproducing a minimal PySCF direct_spin1 FCI Davidson-preconditioner regression, but I realized my initial plan to “just pick 3 determinants” is brittle: in this 2-orbital, (1α,1β) problem there are only 4 determinants total, and our CI driver sometimes requests an oversized pspace; in that case I want the pspace builder to fall back to the **full determinant space** (i.e., include every determinant available for this electron count) so the small Hamiltonian block is the exact Hamiltonian in that space.\n\nUse these integrals (same as before) and build the pspace Hamiltonian block accordingly:\n- norb = 2\n- nelec = (1 alpha, 1 beta)\n- h1e = [[-1.0, 0.1], [0.1, -0.5]]\n- eri (physicist notation) = [[[[0.7, 0.0],[0.0, 0.2]],[[0.0, 0.1],[0.1, 0.0]]],[[[0.0, 0.1],[0.1, 0.0]],[[0.2, 0.0],[0.0, 0.6]]]]\n\nInitially I was going to set np=3, but given the above constraint, set np to the smallest value that guarantees the full determinant space is included for this system (so the returned h0 is the full 4×4 block). Don’t pass a precomputed hdiag; let the routine compute it.", "answers": "[{\"name\": \"pyscf_fci_direct_spin1_pspace\", \"arguments\": {\"h1e\": [[-1.0, 0.1], [0.1, -0.5]], \"eri\": [[[[0.7, 0.0], [0.0, 0.2]], [[0.0, 0.1], [0.1, 0.0]]], [[[0.0, 0.1], [0.1, 0.0]], [[0.2, 0.0], [0.0, 0.6]]]], \"norb\": 2, \"nelec\": [1, 1], \"hdiag\": null, \"np\": 4}}]"}
{"func_name": "pyscf_fci_fci_dhf_slow_absorb_h1e", "func_desc": "pyscf.fci.fci_dhf_slow.absorb_h1e modifies a two-electron Hamiltonian tensor so that it includes the contribution from a one-electron Hamiltonian, producing a 4-index two-electron Hamiltonian with the one-electron effects absorbed for use in Full Configuration Interaction (FCI) routines in PySCF.", "tools": [{"function": {"description": "pyscf.fci.fci_dhf_slow.absorb_h1e modifies a two-electron Hamiltonian tensor so that it includes the contribution from a one-electron Hamiltonian, producing a 4-index two-electron Hamiltonian with the one-electron effects absorbed for use in Full Configuration Interaction (FCI) routines in PySCF.\n", "name": "pyscf_fci_fci_dhf_slow_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "One-electron Hamiltonian matrix in the molecular orbital basis. Expected shape is (norb, norb). This array represents kinetic energy and nuclear attraction terms (or an effective one-electron operator) used by FCI routines. The function uses h1e to compute an effective diagonal correction to the two-electron tensor; h1e is not modified by this function.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "Electron repulsion integral tensor in chemist’s notation, expected shape (norb, norb, norb, norb). This 4-dimensional array supplies the raw two-electron integrals required by FCI. The routine transposes and negates eri to form the base two-electron Hamiltonian and does not modify the input eri array in-place (intermediate arrays are created).", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (norb). This integer selects diagonal indices and must be consistent with the first two dimensions of h1e and all four dimensions of eri. If norb does not match the actual dimensions of h1e or eri, NumPy indexing/broadcasting errors will occur.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the system. This integer is used to scale the one-electron contribution added to the diagonal blocks of the two-electron Hamiltonian by a factor 2.0/(nelec + 1e-100). The tiny constant 1e-100 prevents division-by-zero at the numerical level; physically, nelec is expected to be a non-negative integer electron count (nelec == 0 is allowed as input but produces a formally scaled result that may be meaningless for electronic structure).", "default": ""}, "fac": {"type": "float", "description": "Multiplicative scaling factor applied to the final modified two-electron Hamiltonian. Default is 1.0. This parameter allows rescaling the returned tensor for algorithmic uses (for example, unit conversions or prefactors required by downstream FCI code).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "I’m building a tiny relativistic (4c/DHF-style) FCI toy problem with 2 spatial orbitals and 2 electrons, and I want to pre-pack the one-electron terms into the two-electron tensor (chemist’s notation) before handing it to an FCI routine.\n\nInitially I was going to use the same scaling factor we used in the last test (the one that’s exactly half), but I just realized that choice would double-count because my ERIs were already generated with a prefactor applied at the integral transformation stage. So instead, use the scaling that is exactly twice the previous one.\n\nUse these inputs:\n\nh1e = [[-1.10, 0.05],[0.05, -0.30]]\n\neri[p,q,r,s] =\n[[[[0.70, 0.00],[0.00, 0.20]],\n  [[0.00, 0.10],[0.05, 0.00]]],\n [[[0.00, 0.05],[0.10, 0.00]],\n  [[0.20, 0.00],[0.00, 0.60]]]]\n\nPlease call the PySCF helper that absorbs h1e into eri for DHF FCI, with norb=2 and nelec=2, using that corrected scaling factor.", "answers": "[{\"name\": \"pyscf_fci_fci_dhf_slow_absorb_h1e\", \"arguments\": {\"h1e\": [[-1.1, 0.05], [0.05, -0.3]], \"eri\": [[[[0.7, 0.0], [0.0, 0.2]], [[0.0, 0.1], [0.05, 0.0]]], [[[0.0, 0.05], [0.1, 0.0]], [[0.2, 0.0], [0.0, 0.6]]]], \"norb\": 2, \"nelec\": 2, \"fac\": 1.0}}]"}
{"func_name": "pyscf_fci_fci_dhf_slow_get_init_guess", "func_desc": "Initializes a set of CI (configuration interaction) guess vectors composed of single Slater determinants for use in the PySCF FCI solver (module pyscf.fci.fci_dhf_slow). The function constructs a list of one-hot CI vectors (each vector has unit amplitude on a single determinant) chosen from the determinants with the smallest diagonal Hamiltonian elements, and applies a tiny deterministic perturbation to the first vector to break exact degeneracies and improve the stability of iterative eigensolvers (e.g., Davidson). This routine is used as the starting guess for FCI diagonalization in quantum chemistry calculations performed with the PySCF framework.", "tools": [{"function": {"description": "Initializes a set of CI (configuration interaction) guess vectors composed of single Slater determinants for use in the PySCF FCI solver (module pyscf.fci.fci_dhf_slow). The function constructs a list of one-hot CI vectors (each vector has unit amplitude on a single determinant) chosen from the determinants with the smallest diagonal Hamiltonian elements, and applies a tiny deterministic perturbation to the first vector to break exact degeneracies and improve the stability of iterative eigensolvers (e.g., Davidson). This routine is used as the starting guess for FCI diagonalization in quantum chemistry calculations performed with the PySCF framework.\n", "name": "pyscf_fci_fci_dhf_slow_get_init_guess", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of orbitals used to enumerate Slater determinant bitstrings. In this FCI context, norb is passed to cistring.num_strings(norb, nelec) to determine the total number of determinant basis states (na). norb must be a non-negative integer; invalid values may lead to exceptions when computing the number of strings.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons used to form the Slater determinants. This integer, together with norb, determines the dimension na of each CI vector (the length of the one-hot vectors). nelec must be consistent with norb (e.g., 0 <= nelec <= norb); inconsistent values can cause cistring.num_strings to raise errors.", "default": ""}, "nroots": {"type": "integer", "description": "Number of initial root vectors requested. The function returns min(nroots, hdiag.size) guess vectors. nroots is expected to be a positive integer; supplying nroots <= 0 is invalid and can produce undefined behavior or exceptions (numpy.argpartition is used internally with nroots-1).", "default": ""}, "hdiag": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional numpy array containing the diagonal elements of the Hamiltonian in the determinant basis (the diagonal energies for each Slater determinant). Its length (hdiag.size) should match the number of determinants na returned by cistring.num_strings(norb, nelec). The dtype of hdiag determines the dtype of the generated CI vectors (real or complex). If hdiag.ndim != 1 or its length does not match na, an IndexError or incorrect indexing may occur.", "default": ""}}, "required": ["norb", "nelec", "nroots", "hdiag"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny Dirac-HF FCI regression in PySCF (pyscf.fci.fci_dhf_slow) with 6 spin-orbitals and 3 electrons, and I’m seeding a Davidson solver from diagonal Hamiltonian estimates. I initially planned to request 4 one-hot CI guess vectors from the lowest hdiag determinants. But I just realized my hdiag list includes a bookkeeping artifact: the final entry is from a dummy ‘reference’ determinant that is not part of the 6-orbital/3-electron determinant space and must be excluded before ranking. So: generate the initial CI guess vectors using only the physically valid determinants (i.e., drop exactly the spurious trailing value), and keep the number of requested roots the same as before. Use these hdiag values (Hartree): [-1.90, -1.85, -1.88, -1.70, -1.60, -1.92, -1.75, -1.10, -1.55, -1.40, -1.30, -1.20, -1.00, -0.95, -0.80, -0.60, -0.50, -0.30, -0.10, 0.05].", "answers": "[{\"name\": \"pyscf_fci_fci_dhf_slow_get_init_guess\", \"arguments\": {\"norb\": 6, \"nelec\": 3, \"nroots\": 4, \"hdiag\": [-1.9, -1.85, -1.88, -1.7, -1.6, -1.92, -1.75, -1.1, -1.55, -1.4, -1.3, -1.2, -1.0, -0.95, -0.8, -0.6, -0.5, -0.3, -0.1]}}]"}
{"func_name": "pyscf_fci_fci_slow_absorb_h1e", "func_desc": "pyscf.fci.fci_slow.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian, producing an effective two-electron Hamiltonian used by Full Configuration Interaction (FCI) routines in PySCF. This operation is used in the pyscf.fci.fci_slow code path to combine one- and two-electron terms so downstream FCI code can work with a single 4-index operator while preserving the correct mean-field one-electron contribution.\n    \n    This function expects a one-electron Hamiltonian matrix and a two-electron integral array compatible with ao2mo.restore(1, eri, norb). It restores the full 4-index two-electron tensor from eri, computes an orbital-space mean-field contribution from the two-electron tensor, forms a one-electron correction f1e = h1e - 0.5 * trace_contraction(h2e), scales that correction by 1/nelec (with a tiny floor to avoid division by zero), and adds the scaled correction to the two-electron tensor on the appropriate index pairs. The returned tensor has the same floating dtype as h1e and shape (norb, norb, norb, norb). The original inputs are not modified because eri is copied before restoration; the function uses ao2mo.restore(1, eri.copy(), norb) internally.", "tools": [{"function": {"description": "pyscf.fci.fci_slow.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian, producing an effective two-electron Hamiltonian used by Full Configuration Interaction (FCI) routines in PySCF. This operation is used in the pyscf.fci.fci_slow code path to combine one- and two-electron terms so downstream FCI code can work with a single 4-index operator while preserving the correct mean-field one-electron contribution.\n\nThis function expects a one-electron Hamiltonian matrix and a two-electron integral array compatible with ao2mo.restore(1, eri, norb). It restores the full 4-index two-electron tensor from eri, computes an orbital-space mean-field contribution from the two-electron tensor, forms a one-electron correction f1e = h1e - 0.5 * trace_contraction(h2e), scales that correction by 1/nelec (with a tiny floor to avoid division by zero), and adds the scaled correction to the two-electron tensor on the appropriate index pairs. The returned tensor has the same floating dtype as h1e and shape (norb, norb, norb, norb). The original inputs are not modified because eri is copied before restoration; the function uses ao2mo.restore(1, eri.copy(), norb) internally.", "name": "pyscf_fci_fci_slow_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "any"}, "description": "One-electron Hamiltonian matrix in orbital basis. Typically a square matrix of size (norb, norb). This array provides the kinetic and nuclear attraction (and any other one-electron) terms that should be absorbed into the two-electron representation. The function preserves the dtype of h1e for the returned tensor.", "default": ""}, "eri": {"type": "array", "items": {"type": "any"}, "description": "Two-electron integrals provided in the format accepted by ao2mo.restore(1, eri, norb). The function calls eri.copy() and then ao2mo.restore to obtain a full 4-index tensor of shape (norb, norb, norb, norb); therefore eri may be a packed or transformed array as produced by PySCF integral routines. The original eri array passed by the caller is not modified.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals. This determines the dimensionality used when restoring eri into a full 4-index tensor; the restored tensor has shape (norb, norb, norb, norb). Must be consistent with the content/shape of eri for ao2mo.restore to succeed.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons used to scale the one-electron correction. If nelec is an integer type (int or numpy.integer), that value is used. If nelec is not an integer but is iterable (for example a tuple/list [nalpha, nbeta]), the function will replace nelec by sum(nelec) to obtain the total electron count. The function divides by (nelec + 1e-100) to avoid division-by-zero; if nelec is zero this results in a very large scaling and may produce numerically unstable values.", "default": ""}, "fac": {"type": "float", "description": "Global multiplicative factor applied to the returned two-electron tensor after the one-electron contribution has been absorbed. Default is 1. This parameter can be used to scale the effective two-electron Hamiltonian (for example, to apply uniform scaling to correlation terms).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny 2-orbital (norb=2) pedagogical FCI check with 2 electrons, but I realized my downstream test harness is using the slow FCI path that always calls the helper which *restores* ERIs internally—so I shouldn’t pre-expand or otherwise change the integral symmetry/packing myself.\n\nPlease take the same numerical inputs below, and run the one-electron-into-two-electron absorption step exactly as in the slow-path helper: it should internally restore the ERI as needed, compute the mean-field contraction from the restored 4-index tensor, form the corrected one-electron term, and then add it back into the two-electron tensor using the same scaling rule as the helper.\n\nHowever, there’s a catch: I initially thought I could set the electron count by the total electron number, but for this helper the scaling should be based on the electron number *per spin channel* for a closed-shell 2-electron case (so it’s half the total electron count). Use the default scaling factor otherwise.\n\nInputs:\n- h1e = [[-1.256, 0.102], [0.102, -0.475]]\n- eri (given as a full 2x2x2x2 tensor, but treat it as whatever the helper expects to restore): [[[[0.700,0.000],[0.000,0.120]],[[0.000,0.180],[0.050,0.000]]],[[[0.000,0.050],[0.180,0.000]],[[0.120,0.000],[0.000,0.650]]]]\n\nReturn the effective 4-index two-electron tensor produced by that absorption step.", "answers": "[{\"name\": \"pyscf_fci_fci_slow_absorb_h1e\", \"arguments\": {\"h1e\": [[-1.256, 0.102], [0.102, -0.475]], \"eri\": [[[[0.7, 0.0], [0.0, 0.12]], [[0.0, 0.18], [0.05, 0.0]]], [[[0.0, 0.05], [0.18, 0.0]], [[0.12, 0.0], [0.0, 0.65]]]], \"norb\": 2, \"nelec\": 1, \"fac\": 1.0}}]"}
{"func_name": "pyscf_fci_spin_op_contract_ss", "func_desc": "pyscf.fci.spin_op.contract_ss contracts the total-spin-squared operator S^2 with a Full Configuration Interaction\n    (FCI) CI wavefunction, returning the resulting CI vector S^2 |CI>. This routine is used in quantum-chemistry FCI\n    calculations within the PySCF framework to evaluate the action of the S^2 operator on a CI expansion (for example\n    to compute expected S^2 values or to form matrix-vector products involving S^2 in spin-analysis and spin-adaptation\n    workflows).", "tools": [{"function": {"description": "pyscf.fci.spin_op.contract_ss contracts the total-spin-squared operator S^2 with a Full Configuration Interaction\n(FCI) CI wavefunction, returning the resulting CI vector S^2 |CI>. This routine is used in quantum-chemistry FCI\ncalculations within the PySCF framework to evaluate the action of the S^2 operator on a CI expansion (for example\nto compute expected S^2 values or to form matrix-vector products involving S^2 in spin-analysis and spin-adaptation\nworkflows).", "name": "pyscf_fci_spin_op_contract_ss", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "any"}, "description": "FCI wavefunction expansion coefficients provided as a NumPy array. In practice this array\nmust contain the CI coefficients for all alpha/beta string pairs and its total size must equal\nna * nb where na = cistring.num_strings(norb, neleca) and nb = cistring.num_strings(norb, nelecb).\nThe function performs fcivec.reshape(na, nb) internally, so callers may pass a 1-D array of length na*nb\nor an already-shaped 2-D array with shape (na, nb). The elements are treated as floating-point CI amplitudes\nused to build the S^2|CI> result; the input array is read but not modified in-place by this function.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (spin-orbital pairs) in the basis used to construct the determinants/strings.\nThis integer is passed to cistring helper functions (cistring.num_strings, cistring.gen_des_str_index,\ncistring.gen_cre_str_index) to enumerate occupation strings and to build creation/destruction index maps.\nnorb must be a non-negative integer consistent with the electron counts in nelec.", "default": ""}, "nelec": {"type": "any", "description": "Two-element tuple (neleca, nelecb) giving the integer counts of alpha and beta electrons respectively.\nThese integers determine na and nb above and select the appropriate creation/destruction maps. The function\nexpects nelec to describe a physically valid occupation (0 <= neleca, nelecb <= norb). The difference\nneleca - nelecb enters the Sz^2 contribution added to the S^2 contraction.", "default": ""}}, "required": ["fcivec", "norb", "nelec"], "type": "any"}}, "type": "function"}], "query": "I’m doing a PySCF FCI spin-analysis sanity check for a toy active space with 4 spatial orbitals. I initially planned to contract S^2 using the electron counts from my earlier diagonalization, but I realized that run implicitly enforced an equal number of alpha and beta electrons, while the CI coefficients I’m holding now clearly correspond to the *one-unpaired-electron* sector (i.e., alpha exceeds beta by exactly one). So, instead of using the old electron tuple, contract the total-spin-squared operator S^2 with the CI vector using the electron-count consistent with that constraint. \n\nUse this fcivec (shape 6x4):\n[[0.12, -0.03, 0.00, 0.05],\n [0.08,  0.10, -0.02, 0.00],\n [-0.04, 0.06, 0.09, -0.01],\n [0.00, -0.07, 0.11, 0.02],\n [0.03, 0.00, -0.05, 0.04],\n [-0.02, 0.01, 0.00, 0.07]]\n\nReturn the CI vector S^2|CI> with the same shape as the input.", "answers": "[{\"name\": \"pyscf_fci_spin_op_contract_ss\", \"arguments\": {\"fcivec\": [[0.12, -0.03, 0.0, 0.05], [0.08, 0.1, -0.02, 0.0], [-0.04, 0.06, 0.09, -0.01], [0.0, -0.07, 0.11, 0.02], [0.03, 0.0, -0.05, 0.04], [-0.02, 0.01, 0.0, 0.07]], \"norb\": 4, \"nelec\": [2, 1]}}]"}
{"func_name": "pyscf_grad_rhf_make_rdm1e", "func_desc": "pyscf.grad.rhf.make_rdm1e constructs the energy-weighted one-particle (one-electron) density matrix used in restricted Hartree–Fock (RHF) gradient calculations. The matrix is assembled in the atomic-orbital (AO) basis by summing contributions from molecular orbitals (MOs) that are treated as occupied (mo_occ > 0), where each occupied MO contribution is weighted by the product of its orbital energy and occupation. In PySCF this energy-weighted density appears when forming response terms and energy derivatives with respect to nuclear coordinates.", "tools": [{"function": {"description": "pyscf.grad.rhf.make_rdm1e constructs the energy-weighted one-particle (one-electron) density matrix used in restricted Hartree–Fock (RHF) gradient calculations. The matrix is assembled in the atomic-orbital (AO) basis by summing contributions from molecular orbitals (MOs) that are treated as occupied (mo_occ > 0), where each occupied MO contribution is weighted by the product of its orbital energy and occupation. In PySCF this energy-weighted density appears when forming response terms and energy derivatives with respect to nuclear coordinates.\n", "name": "pyscf_grad_rhf_make_rdm1e", "parameters": {"properties": {"mo_energy": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of molecular orbital energies. Each entry corresponds to a molecular orbital (MO) and is used to weight that MO's contribution to the energy-weighted density as (mo_energy * mo_occ) for occupied orbitals. The function selects orbitals where the corresponding entry in mo_occ is greater than zero (mo_occ > 0). mo_energy must align with the second dimension of mo_coeff and with mo_occ so that indexing mo_energy[mo_occ>0] is valid; if lengths are incompatible an IndexError or ValueError may be raised.", "default": ""}, "mo_coeff": {"type": "array", "items": {"type": "any"}, "description": "Two-dimensional MO coefficient matrix that transforms MO amplitudes to the AO basis. The convention expected by this function is that columns of mo_coeff correspond to MOs (so mo_coeff[:, i] is the AO representation of MO i). Only the columns corresponding to occupied orbitals (mo_occ > 0) are used. The returned energy-weighted density is computed as the dot product of the occupied-coefficient columns weighted by (mo_energy * mo_occ) and the conjugate transpose of those columns, therefore mo_coeff may be real or complex and the result will follow accordingly. Incompatible shapes between mo_coeff and the boolean mask mo_occ>0 will produce standard NumPy errors.", "default": ""}, "mo_occ": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of occupations for each MO. Typical RHF usage supplies integer occupations (0 or 2), but fractional occupations are accepted provided they are greater than zero to be treated as occupied. The boolean mask mo_occ > 0 determines which orbitals contribute; occupations equal to or less than zero are excluded. mo_occ must have the same length as mo_energy and the number of columns in mo_coeff, otherwise indexing will fail.", "default": ""}}, "required": ["mo_energy", "mo_coeff", "mo_occ"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an RHF nuclear-gradient workflow for a tiny toy system (3 AOs, 4 MOs). I was going to build the energy-weighted one-electron density matrix by treating any MO with a positive occupation as occupied, but I realized my upstream SCF dump sometimes contains tiny negative occupations from numerical noise for formally virtual orbitals. So: first sanitize the occupations by setting any MO with occupation <= 0 to exactly 0, then build the AO-basis energy-weighted 1e density using only the orbitals that remain occupied after that sanitization (i.e., sum each contributing MO’s outer product weighted by its orbital energy times its occupation).\n\nUse these MO quantities:\n- mo_energy = [-0.72, -0.31, 0.15, 0.48]\n- mo_occ = [2, 2, -1e-12, 0]\n- mo_coeff (AO x MO) = [[0.80, 0.10, 0.30, 0.20],[0.50, -0.60, 0.40, 0.10],[0.30, 0.79, -0.10, 0.50]]\n\nPlease construct the resulting energy-weighted one-electron density matrix in the AO basis (the same object produced by pyscf.grad.rhf.make_rdm1e) for this sanitized-occupation case.", "answers": "[{\"name\": \"pyscf_grad_rhf_make_rdm1e\", \"arguments\": {\"mo_energy\": [-0.72, -0.31, 0.15, 0.48], \"mo_coeff\": [[0.8, 0.1, 0.3, 0.2], [0.5, -0.6, 0.4, 0.1], [0.3, 0.79, -0.1, 0.5]], \"mo_occ\": [2, 2, 0, 0]}}]"}
{"func_name": "pyscf_gto_basis_load", "func_desc": "Convert a basis-set identifier or file into the PySCF internal basis representation for a single atomic symbol.\n    \n    This function is used throughout PySCF to obtain the basis definition that can be assigned to mol.basis[symb] for electronic structure calculations (for example, assigning a standard STO-3G or user-supplied basis to an atom). It accepts either a case-insensitive basis set name (which may match built-in aliases, GTH aliases, user-provided basis sets, or common Pople names), a filesystem path to a file containing basis definitions, or a compound identifier that encodes a contraction scheme using the \"@<scheme>\" suffix. The returned value is the basis data for the requested atomic symbol in the internal format expected by PySCF routines that build atomic orbital integrals.", "tools": [{"function": {"description": "Convert a basis-set identifier or file into the PySCF internal basis representation for a single atomic symbol.\n\nThis function is used throughout PySCF to obtain the basis definition that can be assigned to mol.basis[symb] for electronic structure calculations (for example, assigning a standard STO-3G or user-supplied basis to an atom). It accepts either a case-insensitive basis set name (which may match built-in aliases, GTH aliases, user-provided basis sets, or common Pople names), a filesystem path to a file containing basis definitions, or a compound identifier that encodes a contraction scheme using the \"@<scheme>\" suffix. The returned value is the basis data for the requested atomic symbol in the internal format expected by PySCF routines that build atomic orbital integrals.", "name": "pyscf_gto_basis_load", "parameters": {"properties": {"filename_or_basisname": {"type": "string", "description": "Case-insensitive basis set name or path to a basis file. If a registry/name is given, special characters in the name are tolerated/normalized and the function will attempt several resolution strategies in this order: built-in alias lookup, user-basis lookup, GTH alias lookup (which switches to CP2K-format parsing), Pople-style parsing, direct parsing with the NWchem-style parser, direct parsing with the CP2K-style parser, and finally a lookup via the basis-set-exchange (BSE) API if that optional dependency is available. If the string contains an \"@<scheme>\" suffix (for example \"cc-pVDZ@seg\"), the part after \"@\" is interpreted as a contraction scheme and applied to the loaded basis (the scheme is converted internally by _convert_contraction); if the path refers to an existing file, the file is parsed (NWchem-style first, then CP2K-style) and the contraction scheme is applied afterward. Providing a filesystem path causes the function to read that file; providing a basis name results in no file IO other than reading bundled data files. If the BSE API is not installed and the basis is not found locally, the function will emit a warning recommending installation of the basis-set-exchange package.", "default": ""}, "symb": {"type": "string", "description": "Atomic symbol for which to extract the basis (e.g., \"C\", \"O\"). Non-letter characters are removed from this string before lookup (only alphabetic characters are retained). The symb argument determines which element's basis is returned from a multi-element basis file or database entry; it is the key used to select the returned basis definition and is the same symbol used in mol.basis assignments.", "default": ""}, "optimize": {"type": "boolean", "description": "Controls whether parsing should attempt contraction optimization when constructing the internal representation. When True, parsers may merge or optimize contraction coefficients according to PySCF's internal rules; when False (default) the basis is parsed without attempting those optimizations. The practical significance is that enabling optimization can reduce redundant contracted primitives and slightly change contraction representation while preserving numerical equivalence for most integrals. Default: False.", "default": false}}, "required": ["filename_or_basisname", "symb", "optimize"], "type": "any"}}, "type": "function"}], "query": "I’m building a PySCF water calculation and initially I was going to just do `mol.basis['O'] = 'def2-TZVP'`. Wait—my group’s pipeline sometimes swaps in a local basis-file when the basis name isn’t resolvable in the current environment, and those files occasionally include multiple elements. So please show me how to robustly load the oxygen basis using the same loader PySCF uses internally, but with contraction-pattern optimization enabled during parsing. Also, the identifier I’m given may include an \"@...\" suffix that requests an alternate contraction scheme; if that suffix is present, honor it rather than stripping it. What exact call should I make to obtain the internal basis representation for the oxygen atom so I can assign it to `mol.basis['O']`?", "answers": "[{\"name\": \"pyscf_gto_basis_load\", \"arguments\": {\"filename_or_basisname\": \"def2-TZVP\", \"symb\": \"O\", \"optimize\": true}}]"}
{"func_name": "pyscf_gto_mole_atom_types", "func_desc": "pyscf.gto.mole.atom_types identifies symmetry-inequivalent atom groups in a PySCF molecular atom list and optionally refines those groups by atomic basis equivalence and collinear magnetic moment. It is used in PySCF to determine which atom indices share the same chemical identity and basis representation (important for assigning basis sets, exploiting molecular symmetry, and handling ghost atoms), and to split those groups further when spin polarization (magnetic moment) distinguishes otherwise identical atomic sites.", "tools": [{"function": {"description": "pyscf.gto.mole.atom_types identifies symmetry-inequivalent atom groups in a PySCF molecular atom list and optionally refines those groups by atomic basis equivalence and collinear magnetic moment. It is used in PySCF to determine which atom indices share the same chemical identity and basis representation (important for assigning basis sets, exploiting molecular symmetry, and handling ghost atoms), and to split those groups further when spin polarization (magnetic moment) distinguishes otherwise identical atomic sites.\n", "name": "pyscf_gto_mole_atom_types", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "any"}, "description": "A list of atom descriptors in the same format used by PySCF molecular input. Each entry is expected to be an indexable sequence whose first element is the atomic label string (for example 'H', 'C', 'O', or a label beginning with 'GHOST'). The function uses the first element of each atom descriptor as the symbol to group by and returns indices into this input list. The index values in the returned groups are zero-based positions into this list.", "default": ""}, "basis": {"type": "any", "nullable": true, "description": "Optional mapping from atom labels (strings) to basis specification objects (the same basis identifiers used elsewhere in PySCF). When basis is None (the default), grouping is done by the raw atom label string (after handling GHOST renaming). When a basis dict is provided, the function consults a standardization routine (_std_symbol) to compare the provided atom label and its standardized symbol; atoms whose basis entries are identical (basis[a_label] == basis[std_symbol]) are merged under the standardized symbol key. This behavior is used to ensure that atoms with equivalent basis definitions (even if labeled differently) are treated as symmetry-equivalent for basis assignment. The basis argument must be a dict if provided; the function does not change basis itself.", "default": null}, "magmom": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional one-dimensional array of per-atom magnetic moment identifiers used to further split atom groups by collinear spin orientation. The array is converted internally with numpy.asarray; each element is compared against the group's atom indices. Allowed magmom values are exactly -1, 0, or 1, which the function maps to suffixes 'd', 'o', and 'u' respectively. If magmom is provided, groups that contain more than one unique magmom value are subdivided into new dictionary keys formed as \"ELEMENT_suffix\" (for example \"Fe_u\" for up, \"Fe_d\" for down, \"Fe_o\" for zero). The magmom array must have one entry for each atom index referenced in atoms; if its length or indexing is inconsistent with atoms, an IndexError may occur.", "default": null}}, "required": ["atoms", "magmom", "basis"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a tiny PySCF sanity-check for symmetry handling: a linear CO with a ghost site placed on the molecular axis. I initially thought I should split symmetry-inequivalent atom groups by supplying the per-site collinear magnetic moments. But I just realized one of the moments I wrote down was attached to the wrong site in my notebook: the *only* nonzero moment belongs to the atom that is *not* the ghost and is also the atom with the *larger z-coordinate among the real nuclei*. Keep the ghost site nonmagnetic.\n\nPlease use `pyscf.gto.mole.atom_types` to identify symmetry-inequivalent atom groups and refine them by (i) basis equivalence and (ii) collinear magnetic moments, for the following atom list and basis mapping:\n\natoms = [(\"C\", (0.0, 0.0, 0.0)), (\"O\", (0.0, 0.0, 1.128)), (\"GHOST-H\", (0.0, 0.0, 2.5))]\n\nbasis mapping = {\"C\": \"cc-pVDZ\", \"O\": \"cc-pVDZ\", \"H\": \"sto-3g\", \"GHOST-H\": \"sto-3g\"}\n\nSet the magmom vector consistently with the correction above (same length/order as the atom list) and then return the symmetry-inequivalent groups after refinement.", "answers": "[{\"name\": \"pyscf_gto_mole_atom_types\", \"arguments\": {\"atoms\": [[\"C\", [0.0, 0.0, 0.0]], [\"O\", [0.0, 0.0, 1.128]], [\"GHOST-H\", [0.0, 0.0, 2.5]]], \"basis\": {\"C\": \"cc-pVDZ\", \"O\": \"cc-pVDZ\", \"H\": \"sto-3g\", \"GHOST-H\": \"sto-3g\"}, \"magmom\": [0, 1, 0]}}]"}
{"func_name": "pyscf_gto_mole_cart2zmat", "func_desc": "pyscf.gto.mole.cart2zmat converts a list/array of Cartesian atomic coordinates into a z-matrix style multiline string used by PySCF for simple text representation of molecular geometry.", "tools": [{"function": {"description": "pyscf.gto.mole.cart2zmat converts a list/array of Cartesian atomic coordinates into a z-matrix style multiline string used by PySCF for simple text representation of molecular geometry.\n", "name": "pyscf_gto_mole_cart2zmat", "parameters": {"properties": {"coord": {"type": "array", "items": {"type": "any"}, "description": "Array of Cartesian coordinates for the atoms in the molecule. The function expects coord to be an array-like sequence of 3D position vectors (typically shape (N, 3) where N is the number of atoms) containing numeric values. Each row coord[i] is the Cartesian position of atom i in the same linear coordinate units used by the caller (the function does not perform unit conversion). In the PySCF context, this function is used to produce a compact z-matrix-style string that can be printed, logged, or used when constructing simple geometry input fragments for molecular calculations; the coordinates provided determine all bond lengths, bond angles, and dihedral angles computed by the function.", "default": ""}}, "required": ["coord"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a quick PySCF single-point on an *isolated* water molecule, and I want a z-matrix style multiline geometry string (PySCF text format) derived from Cartesian coordinates in Angstrom.\n\nInitially I was going to pass the coordinates in the order O, H, H as they came off my script. But I just realized my upstream generator sometimes outputs the two hydrogens in arbitrary order, and for a robust z-matrix I need the **two non-oxygen atoms ordered by increasing x-coordinate** (so the one with the more negative x should be listed first among the hydrogens). Oxygen stays first.\n\nUse these coordinates (Å):\n- O: (0.000000, 0.000000, 0.000000)\n- H: (0.758602, 0.000000, 0.504284)\n- H: (-0.758602, 0.000000, 0.504284)\n\nConvert them into the PySCF z-matrix style multiline string accordingly.", "answers": "[{\"name\": \"pyscf_gto_mole_cart2zmat\", \"arguments\": {\"coord\": [[0.0, 0.0, 0.0], [-0.758602, 0.0, 0.504284], [0.758602, 0.0, 0.504284]]}}]"}
{"func_name": "pyscf_gto_mole_conc_env", "func_desc": "pyscf.gto.mole.conc_env concatenates the integral environment arrays of two Mole-like objects to build a single combined environment suitable for computing cross-molecule integrals (for example, integrals between basis functions µ on mol1 and ν on mol2, <µ|ν>). This helper is used in PySCF to prepare the internal integer/pointer tables (atm, bas) and the floating-point environment vector (env) that the low-level integral engines (for example, gto.moleintor.getints) expect when evaluating integrals that span two separate Mole instances.\n    \n    The function returns new arrays that represent the concatenation of the per-molecule integral descriptors. It adjusts pointer and atom-index columns in the second molecule's atm and bas arrays so that their internal offsets refer correctly into the concatenated env and the combined atom list. The function copies and modifies the atm2 and bas2 inputs internally and does not mutate the original caller arrays.", "tools": [{"function": {"description": "pyscf.gto.mole.conc_env concatenates the integral environment arrays of two Mole-like objects to build a single combined environment suitable for computing cross-molecule integrals (for example, integrals between basis functions µ on mol1 and ν on mol2, <µ|ν>). This helper is used in PySCF to prepare the internal integer/pointer tables (atm, bas) and the floating-point environment vector (env) that the low-level integral engines (for example, gto.moleintor.getints) expect when evaluating integrals that span two separate Mole instances.\n\nThe function returns new arrays that represent the concatenation of the per-molecule integral descriptors. It adjusts pointer and atom-index columns in the second molecule's atm and bas arrays so that their internal offsets refer correctly into the concatenated env and the combined atom list. The function copies and modifies the atm2 and bas2 inputs internally and does not mutate the original caller arrays.", "name": "pyscf_gto_mole_conc_env", "parameters": {"properties": {"atm1": {"type": "array", "items": {"type": "any"}, "description": "Integer array encoding atom-related pointers and metadata for the first molecule (mol1). In PySCF this array stores integer fields used by integral routines (pointer indices to coordinates, Zetas, etc.). The array is treated as a 2-D integer table; its length (number of rows) is interpreted as the number of atoms in mol1 (natm_off). This argument is essential for building cross-integral environments because its atom indices and pointer fields become the lower-index part of the concatenated atom table.", "default": ""}, "bas1": {"type": "array", "items": {"type": "any"}, "description": "Integer array encoding basis-function descriptors for the first molecule (mol1). In PySCF this array stores integers such as the index of the atom each basis belongs to and pointer offsets into the env array for exponents and coefficients. It is treated as a 2-D integer table and becomes the first block of the concatenated basis table; its atom indices and pointer fields are used unchanged in the output.", "default": ""}, "env1": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional numeric array (environment vector) for the first molecule (mol1) that stores floating-point data required by integrals (coordinates, exponents, contraction coefficients, etc.). The length of this array (off) is used as an offset to relocate pointer fields from the second molecule so both molecules reference the same combined env buffer.", "default": ""}, "atm2": {"type": "array", "items": {"type": "any"}, "description": "Integer array encoding atom-related pointers and metadata for the second molecule (mol2). This array is copied internally and then modified: pointer-type columns (e.g., pointers to coordinate or Zeta entries) are incremented by off = len(env1) so they point into the concatenated env, and atom indices will be shifted by the number of atoms in atm1. The caller’s atm2 is not modified because the function operates on a copy, but atm2 must be a numpy.ndarray with the expected integer table layout used by PySCF integral routines.", "default": ""}, "bas2": {"type": "array", "items": {"type": "any"}, "description": "Integer array encoding basis-function descriptors for the second molecule (mol2). This array is copied internally and then modified: the ATOM_OF column (which designates which atom a basis belongs to) is incremented by natm_off = len(atm1) so that atom indices refer to the concatenated atom table, and pointer columns for exponent and coefficient entries are incremented by off to refer into the concatenated env. The original bas2 passed by the caller remains unchanged.", "default": ""}, "env2": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional numeric array (environment vector) for the second molecule (mol2). This array is concatenated after env1 to form the combined environment. Values in env2 are not modified; their indices are referenced by adjusted pointer fields written into the copied bas2/atm2.", "default": ""}}, "required": ["atm1", "bas1", "env1", "atm2", "bas2", "env2"], "type": "any"}}, "type": "function"}], "query": "I’m setting up cross-molecule overlap integrals in PySCF, but my two Mole objects were built by different parts of the pipeline (one is a He atom “fragment”, the other is an H atom “probe”), and I only have their raw (atm, bas, env) tables.\n\nInitially I was going to just concatenate them directly to compute <μ(He)|ν(H)>, but I just realized the second fragment’s pointers were generated assuming its env starts at 0 and its atom list starts at 0—so if I naively stack arrays, the low-level intor will read the wrong coordinates/exponents. \n\nSo: please use pyscf.gto.mole.conc_env to build a combined (atm, bas, env) suitable for cross-fragment integrals, making sure the second fragment’s internal indices are shifted consistently into the merged atom list and its env offsets are shifted by the length of the first fragment’s env. Also, double-check that the original input arrays remain unmodified (i.e., treat them as immutable).\n\nUse exactly the arrays below as inputs:\n- atm1 = [[2, 20, 1, 0, 0, 0], [1, 24, 1, 0, 0, 0]]\n- bas1 = [[0, 0, 3, 1, 0, 30, 33, 0], [1, 0, 3, 1, 0, 36, 39, 0]]\n- env1 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.36242139, 1.158923, 0.31364979, 0.15432897, 0.53532814, 0.44463454, 3.42525091, 0.62391373, 0.1688554, 0.15432897]\n- atm2 = [[1, 16, 1, 0, 0, 0]]\n- bas2 = [[0, 0, 3, 1, 0, 22, 25, 0]]\n- env2 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.4, 0.0, 0.0, 3.42525091, 0.62391373, 0.1688554, 0.15432897]\n\nReturn the concatenated (atm, bas, env) arrays.", "answers": "[{\"name\":\"pyscf_gto_mole_conc_env\",\"arguments\":{\"atm1\":[[2,20,1,0,0,0],[1,24,1,0,0,0]],\"bas1\":[[0,0,3,1,0,30,33,0],[1,0,3,1,0,36,39,0]],\"env1\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,6.36242139,1.158923,0.31364979,0.15432897,0.53532814,0.44463454,3.42525091,0.62391373,0.1688554,0.15432897],\"atm2\":[[1,16,1,0,0,0]],\"bas2\":[[0,0,3,1,0,22,25,0]],\"env2\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.4,0.0,0.0,3.42525091,0.62391373,0.1688554,0.15432897]}}]"}
{"func_name": "pyscf_gto_mole_fakemol_for_cgtf_charge", "func_desc": "Constructs a lightweight pyscf.gto.Mole that represents a Gaussian charge distribution centered at a single coordinate for use in CGTF charge-related integral routines.\n    \n    This function is used in the PySCF Gaussian integrals machinery when a localized Gaussian representation of an external point/compact charge is required (for example, in charge fitting or charge-interaction integrals used by CGTF-related code). It builds a minimal Mole object with internal arrays (_atm, _bas, _env) arranged so integrals can be evaluated as if there were an atom with one or a contracted set of s-type Gaussian primitives representing the charge. The returned Mole is marked as built (fakemol._built == True) and is intended only as a transient container for integral evaluation, not as a full chemical system description.", "tools": [{"function": {"description": "Constructs a lightweight pyscf.gto.Mole that represents a Gaussian charge distribution centered at a single coordinate for use in CGTF charge-related integral routines.\n\nThis function is used in the PySCF Gaussian integrals machinery when a localized Gaussian representation of an external point/compact charge is required (for example, in charge fitting or charge-interaction integrals used by CGTF-related code). It builds a minimal Mole object with internal arrays (_atm, _bas, _env) arranged so integrals can be evaluated as if there were an atom with one or a contracted set of s-type Gaussian primitives representing the charge. The returned Mole is marked as built (fakemol._built == True) and is intended only as a transient container for integral evaluation, not as a full chemical system description.", "name": "pyscf_gto_mole_fakemol_for_cgtf_charge", "parameters": {"properties": {"coord": {"type": "array", "items": {"type": "any"}, "description": "A 2D array with shape (1, 3) containing the Cartesian coordinates (x, y, z) of the center of the Gaussian charge distribution. The function asserts coord.shape[0] == 1 and will raise AssertionError if this is not satisfied. In the CGTF/charge context, this coordinate is the physical location of the point-like or compact Gaussian charge that the Mole will represent.", "default": ""}, "expnt": {"type": "array", "items": {"type": "any"}, "description": "Exponent(s) of the Gaussian primitive(s). Default is 1e+16. The function accepts scalar values or 1D arrays; inputs are converted via numpy.asarray(expnt).ravel(). If a single scalar is provided, a single primitive with that exponent is created. If an array is provided, its length must equal the length of contr_coeff and the function will create a contracted basis made from those exponents. Very large exponents (the default 1e+16) produce an extremely tightly localized Gaussian that approximates a point charge for integral purposes.", "default": 1e+16}, "contr_coeff": {"type": "array", "items": {"type": "any"}, "description": "Contraction coefficient(s) for the Gaussian primitive(s). Default is 1. The function accepts scalar values or 1D arrays; inputs are converted via numpy.asarray(contr_coeff).ravel(). For a single primitive, a single coefficient is used. For multiple primitives, the array size must match that of expnt; otherwise an AssertionError is raised. These coefficients are scaled internally to produce a normalized charge distribution for use in integrals.", "default": 1}}, "required": ["coord", "expnt", "contr_coeff"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF charge-fitting regression and need a fake Mole for a localized external Gaussian charge to feed into the CGTF charge-related integral routines. Start by placing the charge at (0.25, -0.10, 1.75) Å and using the same 3-primitive contracted s-type Gaussian idea as usual. However, I just realized the upstream geometry file stores coordinates in Å but the integral helper expects Bohr internally. So don’t use the raw numbers as-is: convert the given coordinate to atomic units first (using PySCF’s standard Å→Bohr factor), and then build the fake Mole with the same three exponents [50.0, 10.0, 2.0] and contraction coefficients [0.6, 0.3, 0.1].", "answers": "[{\"name\": \"pyscf_gto_mole_fakemol_for_cgtf_charge\", \"arguments\": {\"coord\": [[0.4724315311575, -0.188972612463, 3.3070207181025]], \"expnt\": [50.0, 10.0, 2.0], \"contr_coeff\": [0.6, 0.3, 0.1]}}]"}
{"func_name": "pyscf_gto_mole_format_atom", "func_desc": "pyscf.gto.mole.format_atom converts a user-provided molecular geometry (the same form accepted by Mole.atom in PySCF) into PySCF's internal atomic list format used throughout the library for building Mole objects, performing integrals, and running quantum-chemical simulations. The function normalizes nuclear-charge notations to element symbols, converts coordinates into atomic units (Bohr) according to the specified unit, applies an optional rotation (axes) and translation (origin), and accepts both inline geometry strings and Python lists or tuples as input. This conversion is a necessary preprocessing step in PySCF so downstream modules receive a consistent, unit-correct, and oriented geometry.", "tools": [{"function": {"description": "pyscf.gto.mole.format_atom converts a user-provided molecular geometry (the same form accepted by Mole.atom in PySCF) into PySCF's internal atomic list format used throughout the library for building Mole objects, performing integrals, and running quantum-chemical simulations. The function normalizes nuclear-charge notations to element symbols, converts coordinates into atomic units (Bohr) according to the specified unit, applies an optional rotation (axes) and translation (origin), and accepts both inline geometry strings and Python lists or tuples as input. This conversion is a necessary preprocessing step in PySCF so downstream modules receive a consistent, unit-correct, and oriented geometry.\n", "name": "pyscf_gto_mole_format_atom", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "any"}, "description": "Input geometry in the same forms accepted by Mole.atom. Practical examples from the source include a Python list of atom specifications (e.g. [['H', (0.0, 0.0, 0.0)], ['O', (0.0, 0.0, 1.0)]]) or a string containing atom lines separated by newlines, semicolons, or commas. If a string names an existing filesystem path, the file will be read and parsed. The parser accepts numeric coordinates, atomic-number or symbol notations (numeric nuclear charges are converted to element symbols), recognizes comment lines beginning with '#', ignores blank lines, and will call a Z-matrix parser (from_zmatrix) when the first non-comment token on the first line has fewer than four entries. Improperly formatted coordinate tokens raise ValueError. This argument is the primary user-facing way to supply molecular geometries to PySCF routines, so correct formatting here ensures accurate molecular integrals and electronic-structure results.", "default": ""}, "origin": {"type": "array", "items": {"type": "any"}, "description": "New axis origin used to translate coordinates before returning them. If origin is the default scalar 0, the input geometry is treated as already referenced to the global origin (0,0,0). In practice provide a length-3 array-like specifying the translation vector (in the same units as the input coordinates) to shift all atomic positions: the function computes (coords - origin) before applying axes and unit conversions. Supplying a non-3-vector will lead to broadcasting/subtraction errors or incorrect geometry; malformed origin values are a source of ValueError or NumPy errors at runtime.", "default": 0}, "axes": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional 3x3 matrix whose rows (new_x, new_y, new_z) define a new coordinate basis to rotate/scale the input coordinates. If axes is None (the common/default case), the function uses the 3x3 identity matrix so no rotation is applied. When provided, axes is multiplied by the coordinate vectors (after translating by origin and scaling by unit) to produce coordinates expressed in the new basis. Passing an array of incorrect shape or non-numeric entries will raise NumPy errors; axes is used to orient the molecule for symmetry alignment, fragment placement, or to match external coordinate conventions in downstream PySCF workflows.", "default": null}, "unit": {"type": "string", "description": "Descriptor of the units of the input coordinates. If unit is a string and refers to atomic units (common accepted strings include 'B', 'b', 'Bohr', 'bohr', 'AU', 'au' as recognized by the is_au helper), the coordinates are treated as already in Bohr and no length conversion is performed. If unit is a string referring to angstrom units (e.g. 'A', 'a', 'Angstrom', 'angstrom', 'Ang', 'ang') or any other non-AU string, the function converts Angstrom to Bohr by multiplying coordinates with (1 / param.BOHR). If unit is a numeric value, it is interpreted as the Bohr length in angstroms (for example, ~0.529177 for Bohr) and the code uses 1.0 / unit as the conversion factor; setting unit=1 preserves the numeric values of the input coordinates (no unit conversion). This parameter controls how the input geometry is converted to atomic units required by PySCF integrals; incorrect unit settings lead to physically incorrect interatomic distances and consequently wrong electronic-structure results.", "default": "angstrom"}}, "required": ["atoms", "axes", "unit", "origin"], "type": "any"}}, "type": "function"}], "query": "I’m preparing an NH3 geometry for a PySCF run and need to normalize it with `pyscf.gto.mole.format_atom`, but the coordinates I received are messy.\n\nUse this coordinate list (still in Angstrom):\n- N at (0.0000, 0.0000, 0.0000)\n- H at (0.0000, 0.9377, 0.3816)\n- H at (0.8121, -0.4689, 0.3816)\n- H at (-0.8121, -0.4689, 0.3816)\n\nInitially I was going to center at nitrogen by setting the origin to the N position and then do an axis swap. But wait—downstream I’m comparing to a reference that assumes the *post-centered* geometry is rotated so that what used to be the +y direction becomes the new +x, and what used to be the +x direction becomes the new +y, while keeping +z unchanged. So instead of the naive swap I first had in mind, please apply the handedness-preserving 90° in-plane reorientation consistent with that mapping.\n\nConvert this to PySCF’s internal atom list format using `unit=\"Angstrom\"`, with translation such that the nitrogen ends up exactly at the origin after preprocessing, and apply the rotation described above via `axes`.", "answers": "[{\"name\":\"pyscf_gto_mole_format_atom\",\"arguments\":{\"atoms\":[[\"N\",[0.0,0.0,0.0]],[\"H\",[0.0,0.9377,0.3816]],[\"H\",[0.8121,-0.4689,0.3816]],[\"H\",[-0.8121,-0.4689,0.3816]]],\"origin\":[0.0,0.0,0.0],\"axes\":[[0.0,1.0,0.0],[-1.0,0.0,0.0],[0.0,0.0,1.0]],\"unit\":\"Angstrom\"}}]"}
{"func_name": "pyscf_gto_mole_format_basis", "func_desc": "Convert a Mole.basis-like mapping into PySCF's internal basis-set data format used by integral formation and other electronic-structure routines.\n    \n    This function is used inside PySCF to prepare atom-centered basis descriptions provided by the user (for example via Mole.basis) into the canonical internal representation expected by downstream code such as integral builders and basis-manipulation utilities (for example decontract_basis). The function normalizes atom keys, converts each atom's basis specification using PySCF's basis converter pipeline, removes empty entries, and optionally sorts the resulting shell list by angular momentum so that functions which assume grouped angular-momentum blocks operate correctly.", "tools": [{"function": {"description": "Convert a Mole.basis-like mapping into PySCF's internal basis-set data format used by integral formation and other electronic-structure routines.\n\nThis function is used inside PySCF to prepare atom-centered basis descriptions provided by the user (for example via Mole.basis) into the canonical internal representation expected by downstream code such as integral builders and basis-manipulation utilities (for example decontract_basis). The function normalizes atom keys, converts each atom's basis specification using PySCF's basis converter pipeline, removes empty entries, and optionally sorts the resulting shell list by angular momentum so that functions which assume grouped angular-momentum blocks operate correctly.", "name": "pyscf_gto_mole_format_basis", "parameters": {"properties": {"basis_tab": {"type": "any", "description": "Mapping similar to Mole.basis that associates an atom identifier (for example 'H', 'C', or position-tagged labels such as 'H^2') to a basis specification. The mapping itself must be a dict object (it cannot be a str). Each value (atom basis specification) may be a basis name, a predefined basis description, or an explicit list/tuple describing shells; the converter produced by _generate_basis_converter is used to translate the provided specification into the internal form. The atom keys are normalized to element symbols by _atom_symbol before conversion.", "default": ""}, "sort_basis": {"type": "boolean", "description": "If True (default), the resulting list of shells for each atom is sorted by angular momentum (shell[0], ascending). Sorting groups basis functions of the same angular momentum together, which is important for routines such as decontract_basis that assume grouped angular-momentum blocks. If False, the original order produced by the converter is preserved (except that empty/false shells are filtered out).", "default": true}}, "required": ["basis_tab", "sort_basis"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF water calculation and I *thought* I could just reuse my Mole.basis override dict as-is. But I just realized I accidentally included an entry for a pseudo-atom label used in a preprocessing step (it has no real basis data and should be ignored), and I also remembered that one of my downstream routines assumes the **original shell order** (so do not reorder shells by angular momentum). \n\nPlease convert the following Mole.basis-like mapping into PySCF’s internal basis-set format used by integral formation, with these constraints:\n1) Normalize atom keys the way PySCF does.\n2) Drop any element keys whose basis specification is effectively empty / non-existent.\n3) Keep the shell order exactly as provided (no angular-momentum sorting).\n\nUse a minimal built-in basis for the heavy atom (the one in H2O that is not hydrogen), and for hydrogen use an explicit custom basis consisting of one contracted s shell followed by one p shell, with the primitive exponents/coefficients exactly as given.\n\nInput mapping:\n- heavy atom key: \"O\" -> built-in minimal basis\n- hydrogen key: \"H\" -> [[0, [3.42525091, 0.15432897], [0.62391373, 0.53532814], [0.1688554, 0.44463454]], [1, [1.0, 1.0]]]\n- artifact key: \"X\" -> []", "answers": "[{\"name\":\"pyscf_gto_mole_format_basis\",\"arguments\":{\"basis_tab\":{\"O\":\"sto-3g\",\"H\":[[0,[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]],[1,[1.0,1.0]]],\"X\":[]},\"sort_basis\":false}}]"}
{"func_name": "pyscf_gto_mole_format_ecp", "func_desc": "pyscf.gto.mole.format_ecp converts a user-supplied ECP (effective core potential) specification into the internal ECP data format used by PySCF's molecular integrals and basis handling routines. This function is used when constructing a Mole object or reading atomic basis/ECP input so that downstream code always sees a consistent mapping from element symbol to the ECP definition (including the number of core electrons and radial/ang-momentum projectors).", "tools": [{"function": {"description": "pyscf.gto.mole.format_ecp converts a user-supplied ECP (effective core potential) specification into the internal ECP data format used by PySCF's molecular integrals and basis handling routines. This function is used when constructing a Mole object or reading atomic basis/ECP input so that downstream code always sees a consistent mapping from element symbol to the ECP definition (including the number of core electrons and radial/ang-momentum projectors).\n", "name": "pyscf_gto_mole_format_ecp", "parameters": {"properties": {"ecp_tab": {"type": "any", "description": "Mapping of atom identifiers to ECP specifications supplied by the user. Each key is an atom identifier that _atom_symbol can accept (for example, an atomic index or an atomic label); keys are converted to standardized element symbols (ghost suffixes removed) via _atom_symbol and _std_symbol_without_ghost. Each value may be either:\n- a str: interpreted as the name or path of a standard ECP to load via basis.load_ecp(str_value, stdsymb). In this case the function attempts to load a predefined ECP dataset for the standardized element symbol; if basis.load_ecp returns None or an empty result, the function does not raise an exception but writes a diagnostic message to sys.stderr and does not add an entry for that atom to the returned mapping.\n  - a non-str (typically already in the internal structure): treated as an already-formatted ECP datum and inserted into the returned mapping unchanged.\nThe caller should provide a dict keyed by atom identifiers with values that are either a filename/label string or an internal-format ECP object. This parameter is required and must be of type dict; no other input types are accepted by the function.", "default": ""}}, "required": ["ecp_tab"], "type": "any"}}, "type": "function"}], "query": "I’m building a PySCF Mole for a Au–Cl complex and I want to run `pyscf.gto.mole.format_ecp` to normalize a mixed ECP table before it hits any integral code. Initially I was going to keep Au as the usual LANL-style string label, but I just realized my workflow needs the *same element symbol to appear in two fragments* (one uses lowercase/whitespace in the input) and PySCF will only treat them as identical if the key is the properly canonicalized element symbol. So: please first canonicalize the element keys (trim whitespace and fix case so both representations point to the same element), then convert the resulting ECP table into PySCF’s internal ECP mapping.\n\nHere is the messy user ECP table exactly as it comes from our parser (note the duplicate element written differently):\n{\n  ' au ': 'lanl2dz',\n  'Cl': {'nelec': 10, 'ecp': [[0, [2, 12.0, -6.0], [2, 3.0, -0.5]], [1, [2, 10.0, -5.0]]]},\n  'AU': 'lanl2dz'\n}\n\nAfter canonicalization, there should be only one gold entry (keep the label-based ECP for it) and the chlorine entry should remain as the explicit ECP object. Then run `pyscf.gto.mole.format_ecp` on that cleaned table and return the internal mapping it produces.", "answers": "[{\"name\":\"pyscf_gto_mole_format_ecp\",\"arguments\":{\"ecp_tab\":{\"Au\":\"lanl2dz\",\"Cl\":{\"nelec\":10,\"ecp\":[[0,[2,12.0,-6.0],[2,3.0,-0.5]],[1,[2,10.0,-5.0]]]}}}}]"}
{"func_name": "pyscf_gto_mole_format_pseudo", "func_desc": "Convert the input pseudopotential table (pseudo_tab) to the internal data format used by PySCF's GTO module for pseudopotentials.\n    \n    This function is used in the PySCF framework to prepare pseudopotential data so that downstream modules (basis set handling, integral evaluation, and quantum chemistry solvers) can consume a consistent, fully expanded representation. The input pseudo_tab is a mapping for atomic sites to pseudopotential specifications; values in pseudo_tab may be either:\n    - a string identifier naming a pseudopotential (in which case pyscf.pbc.gto.pseudo.load is called to obtain the internal representation for the standardized atomic symbol), or\n    - an already-formatted pseudopotential data structure (in which case the value is used verbatim).\n    \n    The function canonicalizes atom labels using the internal helper _symbol(atom) before using them as keys in the returned mapping. When a string identifier is provided for an atom, _std_symbol_without_ghost is used to compute the standardized symbol passed to pyscf.pbc.gto.pseudo.load. The returned structure matches the internal :attr:`pseudo` layout expected by PySCF's GTO routines.", "tools": [{"function": {"description": "Convert the input pseudopotential table (pseudo_tab) to the internal data format used by PySCF's GTO module for pseudopotentials.\n\nThis function is used in the PySCF framework to prepare pseudopotential data so that downstream modules (basis set handling, integral evaluation, and quantum chemistry solvers) can consume a consistent, fully expanded representation. The input pseudo_tab is a mapping for atomic sites to pseudopotential specifications; values in pseudo_tab may be either:\n- a string identifier naming a pseudopotential (in which case pyscf.pbc.gto.pseudo.load is called to obtain the internal representation for the standardized atomic symbol), or\n- an already-formatted pseudopotential data structure (in which case the value is used verbatim).\n\nThe function canonicalizes atom labels using the internal helper _symbol(atom) before using them as keys in the returned mapping. When a string identifier is provided for an atom, _std_symbol_without_ghost is used to compute the standardized symbol passed to pyscf.pbc.gto.pseudo.load. The returned structure matches the internal :attr:`pseudo` layout expected by PySCF's GTO routines.", "name": "pyscf_gto_mole_format_pseudo", "parameters": {"properties": {"pseudo_tab": {"type": "any", "description": "Mapping of atoms to pseudopotential specifications. Each key is an atom identifier accepted by the module (for example an element symbol or an atom specification used elsewhere in PySCF). Each value must be either a string pseudopotential identifier (for example a filename or a keyword recognized by pyscf.pbc.gto.pseudo.load) or an already-formatted pseudopotential data structure matching PySCF's internal format. The function does not accept a string for pseudo_tab itself; passing a string (instead of a dict-like mapping with an items() method) will raise an error because the implementation iterates pseudo_tab.items(). This parameter represents the user-provided pseudopotential assignment for a molecule or cell and is typically derived from user input or higher-level molecule construction routines.", "default": ""}}, "required": ["pseudo_tab"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a periodic PySCF Cell for a slightly messy SiC input where the atom labels aren’t consistent, and I need the pseudopotential mapping normalized into the internal GTO pseudo format before anything touches basis/integrals.\n\nInitially I was going to key everything by the literal species strings from my structure, but I just realized the structure builder will introduce a ghost site and also mixes case/spacing (e.g., one site is written like a ghost-tagged silicon label, and carbon appears with odd capitalization/whitespace). So don’t preserve the raw keys—please canonicalize the atom labels exactly the way PySCF does (i.e., the helper that turns an atom label into the canonical symbol for the returned dict), and when a pseudopotential is given by a string identifier, make sure the loader is called using the standardized symbol that *excludes* any ghost tagging.\n\nHere’s the pseudopotential table I want converted:\n\npseudo_tab = {\n  \"si\": \"gth-pbe\",\n  \" C  \": [\n    4,\n    [\n      [2, 1, [6.0, -4.20]],\n      [1, 0, [3.5, -1.10]]\n    ],\n    [\n      [2, [4.8, 12.0, -6.5]]\n    ]\n  ],\n  \"Si@GHOST\": \"gth-pbe\"\n}\n\nUse the string identifier for the silicon entries (so the pseudopotential must be loaded, not passed through). For carbon, the explicit data structure must be used verbatim (no normalization beyond what this conversion function does internally). Convert this pseudo_tab to the internal mapping used by PySCF’s GTO module.", "answers": "[{\"name\": \"pyscf_gto_mole_format_pseudo\", \"arguments\": {\"pseudo_tab\": {\"si\": \"gth-pbe\", \" C  \": [4, [[2, 1, [6.0, -4.2]], [1, 0, [3.5, -1.1]]], [[2, [4.8, 12.0, -6.5]]]], \"Si@GHOST\": \"gth-pbe\"}}}]"}
{"func_name": "pyscf_gto_mole_from_zmatrix", "func_desc": "Convert a Z-matrix string to Cartesian coordinates and atom symbols suitable for PySCF molecular input.", "tools": [{"function": {"description": "Convert a Z-matrix string to Cartesian coordinates and atom symbols suitable for PySCF molecular input.\n", "name": "pyscf_gto_mole_from_zmatrix", "parameters": {"properties": {"atomstr": {"type": "string", "description": "Multiline Z-matrix specification as a single string. This string is interpreted according to the Z-matrix parsing logic used in PySCF: semicolons (';') are treated as line separators, commas (',') are treated as token separators (they are replaced by spaces before parsing), blank lines and lines beginning with '#' are ignored as comments, and numeric fields may be Python expressions that are evaluated with eval() unless the module-level flag DISABLE_EVAL is set. Each non-comment line must begin with an atomic symbol token followed by additional tokens that specify how the atom is positioned relative to previously defined atoms:\n- A single token line (\"Symbol\") places the atom at the origin (first atom).\n- Three-token lines (\"Symbol X R\") treat the third token as a numeric expression or literal for a bond length R and place the atom on the +x axis at distance R relative to the origin or to the reference atom implied by prior entries (this corresponds to the second atom in a typical Z-matrix).\n- Five-token lines (\"Symbol ibond bond iangle angle\") supply four values (the code accepts the four values either as separate tokens or as an evaluable expression). The first value (ibond, 1-based index) selects the reference atom for the bond length; the second value (bond) is the bond length; the third value (iangle, 1-based index) selects the reference atom that defines the bond angle; and the fourth value (angle) is the bond angle in degrees. The function converts angles from degrees to radians and uses rotation matrices (pyscf.symm.rotation_mat) to place the atom in 3D.\n- Seven-or-more-token lines include dihedral information (\"Symbol ibond bond iangle angle idih dihedral\"), where idih (1-based index) selects the atom that defines the dihedral and dihedral is given in degrees. The code parses six numeric values (ibond, bond, iangle, angle, idih, dihedral) and computes the Cartesian placement using successive rotations.\nThe parser enforces increasing requirements for the minimum number of tokens per line as atoms are added and raises an error if a line does not meet the current minimum. Index arguments in the Z-matrix are 1-based in the input and converted internally to 0-based indices.", "default": ""}}, "required": ["atomstr"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a small benchmark in PySCF and I started from this water Z-matrix: `O; H 1 0.9584; H 1 0.9584 2 104.5`. Initially I was going to pass it through as-is, but I just realized the downstream comparison is against a reference geometry that assumes the *same* internal coordinates except the angle is in radians (not degrees). Keep the bond lengths unchanged, but convert only the angular term accordingly, then convert the resulting Z-matrix to Cartesian coordinates + atom symbols suitable for PySCF `Mole` input.", "answers": "[{\"name\": \"pyscf_gto_mole_from_zmatrix\", \"arguments\": {\"atomstr\": \"O; H 1 0.9584; H 1 0.9584 2 1.8238690683340744\"}}]"}
{"func_name": "pyscf_gto_mole_is_au", "func_desc": "Return whether the given unit string is recognized as atomic units (A.U.) by PySCF.\n    \n    This function is part of pyscf.gto.mole and is used within the PySCF molecular input and geometry handling code to decide whether user-supplied unit strings should be interpreted as atomic units (a.u.). In quantum chemistry and within PySCF, atomic units are a common internal convention for expressing distances, energies, and other physical quantities; correctly detecting that a user requested atomic units affects scaling and interpretation of numerical values passed to the library.", "tools": [{"function": {"description": "Return whether the given unit string is recognized as atomic units (A.U.) by PySCF.\n\nThis function is part of pyscf.gto.mole and is used within the PySCF molecular input and geometry handling code to decide whether user-supplied unit strings should be interpreted as atomic units (a.u.). In quantum chemistry and within PySCF, atomic units are a common internal convention for expressing distances, energies, and other physical quantities; correctly detecting that a user requested atomic units affects scaling and interpretation of numerical values passed to the library.", "name": "pyscf_gto_mole_is_au", "parameters": {"properties": {"unit": {"type": "string", "description": "A unit identifier provided by the caller (for example, from molecule input). The function expects a Python str and performs a case-insensitive check of its prefix. Practically, strings that start with 'AU' (e.g., 'au', 'A.U.', 'AU') or with 'B' (commonly used as a shorthand for Bohr, e.g., 'B', 'Bohr', 'BOHR') are treated as indicating atomic units. If unit is not a str (for example, None, a numeric type, or another object), the function will not attempt conversion and will return False.", "default": ""}}, "required": ["unit"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF geometry ingestion step where the upstream code writes units inconsistently. Initially I was going to special-case the label that explicitly names the atomic-unit length and, if PySCF treats it as A.U., skip coordinate conversion. But I realized downstream users sometimes paste the same label with messy formatting (mixed case plus trailing/leading whitespace), and my pre-check should mirror PySCF’s own unit recognition rather than my own string normalization rules. So: using the PySCF `gto.mole` unit-detection helper, determine whether the whitespace-padded, case-jumbled version of that same atomic-unit length label would be recognized as atomic units (A.U.) by PySCF, so I can decide whether to bypass conversion.", "answers": "[{\"name\": \"pyscf_gto_mole_is_au\", \"arguments\": {\"unit\": \"  bOhR  \"}}]"}
{"func_name": "pyscf_gto_mole_len_cart", "func_desc": "pyscf.gto.mole.len_cart computes the number of Cartesian Gaussian-type basis functions associated with a given angular momentum quantum number. This function is part of the PySCF gto.mole utilities and is used when building atomic orbital basis sets, allocating integrals and arrays, and mapping angular momentum labels (s, p, d, f, ...) to the corresponding number of Cartesian components required by Cartesian Gaussian basis representations.", "tools": [{"function": {"description": "pyscf.gto.mole.len_cart computes the number of Cartesian Gaussian-type basis functions associated with a given angular momentum quantum number. This function is part of the PySCF gto.mole utilities and is used when building atomic orbital basis sets, allocating integrals and arrays, and mapping angular momentum labels (s, p, d, f, ...) to the corresponding number of Cartesian components required by Cartesian Gaussian basis representations.\n", "name": "pyscf_gto_mole_len_cart", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number for the atomic orbital. This is expected to be a non-negative integer where l = 0 corresponds to an s-type function, l = 1 to p-type, l = 2 to d-type, l = 3 to f-type, and so on. The function computes the Cartesian count using the combinatorial formula (l + 1) * (l + 2) // 2, which yields common values: l=0 -> 1, l=1 -> 3, l=2 -> 6, l=3 -> 10. The argument has no default and must be supplied by the caller. Passing negative integers is not physically meaningful for angular momentum (the formula will produce a numeric result for some negative inputs but such results should not be used in basis construction). Passing non-integer types is not supported by the function signature (annotated as int) and may produce an incorrect or non-integer numeric result; callers should ensure l is an int before calling.", "default": ""}}, "required": ["l"], "type": "any"}}, "type": "function"}], "query": "I’m preallocating AO integral buffers in a PySCF workflow and initially assumed I needed an f-type shell, so I was about to call the Cartesian-component counter at the angular momentum corresponding to f. But I just realized the downstream module is doing a pure (spherical-harmonic) transform after reading the basis, so I should only allocate the *Cartesian* intermediates for the shell that is exactly one angular-momentum step lower than f (since that’s the last shell that stays Cartesian in this code path). Using `pyscf.gto.mole.len_cart`, how many Cartesian Gaussian basis functions does that shell have?", "answers": "[{\"name\": \"pyscf_gto_mole_len_cart\", \"arguments\": {\"l\": 2}}]"}
{"func_name": "pyscf_gto_mole_len_spinor", "func_desc": "pyscf.gto.mole.len_spinor returns the number of spinor components (spinor basis functions) associated with a given orbital angular momentum quantum number and kappa index. This helper is used in the PySCF molecular integrals and basis construction code to determine how many spinor functions should be allocated or iterated for a particular (l, kappa) channel when building relativistic or spinor-enabled basis sets in the pyscf.gto.mole module.", "tools": [{"function": {"description": "pyscf.gto.mole.len_spinor returns the number of spinor components (spinor basis functions) associated with a given orbital angular momentum quantum number and kappa index. This helper is used in the PySCF molecular integrals and basis construction code to determine how many spinor functions should be allocated or iterated for a particular (l, kappa) channel when building relativistic or spinor-enabled basis sets in the pyscf.gto.mole module.\n", "name": "pyscf_gto_mole_len_spinor", "parameters": {"properties": {"l": {"type": "integer", "description": "Orbital angular momentum quantum number for the shell. In the context of PySCF and atomic/molecular basis construction, l selects the spatial angular momentum (e.g., 0 for s, 1 for p). This parameter is expected to be an integer (typically non-negative in normal quantum-chemical use). The function uses l in simple linear formulas to compute the spinor count; if l is negative or not an integer the numeric result may be non-physical or a TypeError may be raised by Python arithmetic/compare operations.", "default": ""}, "kappa": {"type": "integer", "description": "Kappa index that distinguishes spinor coupling types for the given angular momentum. In PySCF this integer controls which formula is used to count spinor components: a kappa value of 0 triggers the special case for full four-component-like counting (4*l + 2), kappa < 0 selects the (2*l + 2) branch, and kappa > 0 selects the (2*l) branch. The value is compared to zero using integer comparisons; if kappa is not an int the comparison may raise a TypeError.", "default": ""}}, "required": ["l", "kappa"], "type": "any"}}, "type": "function"}], "query": "I’m preallocating buffers for a 4-component relativistic (spinor) basis build in PySCF and I initially planned to size the block for a d-shell using the κ channel with magnitude 3. But I just realized that in my generator, κ comes from the j = l − 1/2 branch (so I need the κ value consistent with that, not the other fine-structure partner). For l = 2, call the PySCF helper that returns the number of spinor components for the (l, κ) channel and tell me how many spinor basis functions get allocated for that block.", "answers": "[{\"name\": \"pyscf_gto_mole_len_spinor\", \"arguments\": {\"l\": 2, \"kappa\": 2}}]"}
{"func_name": "pyscf_gto_mole_uncontract", "func_desc": "pyscf.gto.mole.uncontract: Uncontract a PySCF internal-format contracted basis (_basis) into a flat list of primitive Gaussian basis entries suitable for routines that require primitive (uncontracted) functions, such as low-level integral generation, linear-dependency checks, or exporting primitives for analysis. This function implements the behavior used in PySCF to convert contracted basis descriptions (lists of shells with contracted primitives) into a list where each primitive has an explicit exponent and a unit contraction coefficient.", "tools": [{"function": {"description": "pyscf.gto.mole.uncontract: Uncontract a PySCF internal-format contracted basis (_basis) into a flat list of primitive Gaussian basis entries suitable for routines that require primitive (uncontracted) functions, such as low-level integral generation, linear-dependency checks, or exporting primitives for analysis. This function implements the behavior used in PySCF to convert contracted basis descriptions (lists of shells with contracted primitives) into a list where each primitive has an explicit exponent and a unit contraction coefficient.\n", "name": "pyscf_gto_mole_uncontract", "parameters": {"properties": {"_basis": {"type": "array", "items": {"type": "any"}, "description": "The contracted basis description in PySCF internal format. Each element of _basis is expected to be a list-like shell entry whose first item is the angular momentum (int). The second item may be an integer kappa (for relativistic/generalized-contraction information) or the first primitive tuple; when the second item is an integer (detected by isinstance(kappa, (int, np.integer))), the remaining items (from index 2 onward) are treated as primitive tuples; otherwise, primitives start from index 1. Each primitive tuple is expected to provide the exponent as its first element (the code uses p[0] to read the exponent). The function converts contracted primitives into entries with a contraction coefficient of 1. Practical significance: callers should pass the same internal basis representation produced or used elsewhere in PySCF (for example, output of gto.load or molecule._basis) so that uncontracted primitives match the conventions used across PySCF.", "default": ""}}, "required": ["_basis"], "type": "any"}}, "type": "function"}], "query": "I’m preparing primitives for a custom integral generator and started by calling `pyscf.gto.mole.uncontract` on a single-atom PySCF internal `_basis` with two shells (one s, one p). Initially I thought I should fully uncontract everything into unit-coefficient primitives. But I just realized the downstream routine has a bug: it silently drops shells whose primitives don’t come in *paired duplicates* (it expects a degenerate doublet for each primitive exponent so it can build two identical blocks).\n\nSo, instead of uncontracting the whole basis, please uncontract **only the shell that already has an even number of primitives**, leaving the other shell untouched (i.e., don’t expand it into primitives at all). Use the contracted `_basis` below as the input, and output the resulting primitive entries (unit coefficients) for just that qualifying shell.\n\nContracted `_basis`:\n[\n  [0, [33.8700000, 0.0254938], [5.0950000, 0.1903730], [1.1590000, 0.8521610]],\n  [1, [1.4070000, 0.3995130], [0.3880000, 0.7001150]]\n]", "answers": "[{\"name\": \"pyscf_gto_mole_uncontract\", \"arguments\": {\"_basis\": [[1, [1.407, 0.399513], [0.388, 0.700115]]]} }]"}
{"func_name": "pyscf_gto_moleintor_ascint3", "func_desc": "pyscf.gto.moleintor.ascint3 converts a legacy cint2-style integral routine name into the cint3-style name used by PySCF's molecular integral layer.\n    \n    This function is used in the molecular integrals subsystem of PySCF (Python-based Simulations of Chemistry Framework) to adapt older cint2 naming conventions to the cint3 naming conventions expected by the moleintor code paths that dispatch spinor, spherical, cartesian, or spin-specific integral implementations. In practice, callers supply the textual identifier of an integral routine (for example, a string used to look up a C-accelerated integral kernel) and ascint3 returns a normalized name suitable for selecting the corresponding cint3 kernel.\n    \n    Behavior and transformation rules:\n    - If the input string begins with the prefix 'cint', the leading 'c' is removed (e.g., 'cint2e' -> 'int2e'). This mirrors the transition from cint2-prefixed identifiers to cint3 identifiers within the PySCF integrals naming scheme.\n    - If the resulting name does not already end with one of the recognized suffixes '_sph', '_cart', '_spinor', or '_ssc', the function appends the suffix '_spinor'. This ensures the returned name refers to a spinor-capable cint3 kernel when the original name did not explicitly specify spherical/cartesian/spinor/ssc variants.\n    - No other parts of the name are modified.\n    \n    Side effects:\n    - This function has no side effects on program state; it returns a new string and does not mutate external objects.\n    \n    Failure modes and notes:\n    - The function expects intor_name to be a Python str as used throughout PySCF's integral name handling. If a non-string object is passed, attribute lookup for startswith/endswith will fail (raising an AttributeError), and such misuse should be avoided by callers.\n    - An empty string input will result in '_spinor' being returned per the transformation rules (empty does not start with 'cint' and does not end with any recognized suffix).\n    - The function does not validate that the returned name corresponds to an actually implemented kernel; it only performs syntactic normalization.", "tools": [{"function": {"description": "pyscf.gto.moleintor.ascint3 converts a legacy cint2-style integral routine name into the cint3-style name used by PySCF's molecular integral layer.\n\nThis function is used in the molecular integrals subsystem of PySCF (Python-based Simulations of Chemistry Framework) to adapt older cint2 naming conventions to the cint3 naming conventions expected by the moleintor code paths that dispatch spinor, spherical, cartesian, or spin-specific integral implementations. In practice, callers supply the textual identifier of an integral routine (for example, a string used to look up a C-accelerated integral kernel) and ascint3 returns a normalized name suitable for selecting the corresponding cint3 kernel.\n\nBehavior and transformation rules:\n- If the input string begins with the prefix 'cint', the leading 'c' is removed (e.g., 'cint2e' -> 'int2e'). This mirrors the transition from cint2-prefixed identifiers to cint3 identifiers within the PySCF integrals naming scheme.\n- If the resulting name does not already end with one of the recognized suffixes '_sph', '_cart', '_spinor', or '_ssc', the function appends the suffix '_spinor'. This ensures the returned name refers to a spinor-capable cint3 kernel when the original name did not explicitly specify spherical/cartesian/spinor/ssc variants.\n- No other parts of the name are modified.\n\nSide effects:\n- This function has no side effects on program state; it returns a new string and does not mutate external objects.\n\nFailure modes and notes:\n- The function expects intor_name to be a Python str as used throughout PySCF's integral name handling. If a non-string object is passed, attribute lookup for startswith/endswith will fail (raising an AttributeError), and such misuse should be avoided by callers.\n- An empty string input will result in '_spinor' being returned per the transformation rules (empty does not start with 'cint' and does not end with any recognized suffix).\n- The function does not validate that the returned name corresponds to an actually implemented kernel; it only performs syntactic normalization.", "name": "pyscf_gto_moleintor_ascint3", "parameters": {"properties": {"intor_name": {"type": "string", "description": "The original integral routine name string to convert. In the PySCF domain this is typically a cint2-style identifier used to reference C-integral implementations (for example 'cint2e_sph' or 'cint2e'). The function uses this value to produce a cint3-compatible identifier by removing a leading 'c' if present and ensuring a spinor/suffix is present.", "default": ""}}, "required": ["intor_name"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up a PySCF integral-dispatch shim that bridges a mixed bag of legacy cint2 kernel labels into the cint3 naming expected by `moleintor`. Initially I was going to convert the old overlap label from a previous script, but I just realized my downstream dispatcher already appends the spinor tag automatically whenever the kernel name lacks any of the explicit geometry/spin suffixes. That means if I feed it an already-suffixed name, it won’t double-append (good), but if I feed it a bare legacy name it will get tagged twice (bad).\n\nSo: take the *same* legacy overlap kernel identifier I mentioned before, run it through `pyscf.gto.moleintor.ascint3`, and return the cint3-style name **that will not trigger an extra suffix append in the downstream dispatcher** (i.e., ensure the output already ends in one of the recognized integral suffixes without me explicitly hardcoding which one in the script).", "answers": "[{\"name\": \"pyscf_gto_moleintor_ascint3\", \"arguments\": {\"intor_name\": \"cint1e_ovlp\"}}]"}
{"func_name": "pyscf_lib_linalg_helper_eigh_by_blocks", "func_desc": "Solve an ordinary or generalized eigenvalue problem by diagonalizing diagonal blocks\n    of a Hermitian (or real symmetric) matrix. This function is used in the PySCF\n    quantum chemistry framework to perform block-wise diagonalization when basis\n    functions are partitioned (for example by symmetry irreducible representations,\n    orbital symmetries, or other grouping labels). When labels are provided, the\n    rows and columns with the same label are grouped into a block and each block is\n    diagonalized independently; when labels is None, the full eigenproblem is\n    delegated to scipy.linalg.eigh.", "tools": [{"function": {"description": "Solve an ordinary or generalized eigenvalue problem by diagonalizing diagonal blocks\nof a Hermitian (or real symmetric) matrix. This function is used in the PySCF\nquantum chemistry framework to perform block-wise diagonalization when basis\nfunctions are partitioned (for example by symmetry irreducible representations,\norbital symmetries, or other grouping labels). When labels are provided, the\nrows and columns with the same label are grouped into a block and each block is\ndiagonalized independently; when labels is None, the full eigenproblem is\ndelegated to scipy.linalg.eigh.", "name": "pyscf_lib_linalg_helper_eigh_by_blocks", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "any"}, "description": "2D complex-Hermitian or real-symmetric matrix representing\nthe operator to diagonalize (for example a Fock, Hamiltonian, or other\nmolecular integral matrix). The function expects h.shape == (N, N).\nThe dtype of h determines the dtype of the returned eigenvector array.\nThis matrix is not modified in-place.", "default": ""}, "s": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional 2D overlap (metric) matrix for solving the\ngeneralized eigenvalue problem h x = s x e. If provided, s must be a\nsquare numpy.ndarray with shape (N, N) and should correspond to the\nsame basis as h (same ordering and size). If s is None (the default),\nan ordinary eigenvalue problem is solved. When labels is provided and s\nis given, each block uses the corresponding sub-block of s (s[idx][:,idx])\nto solve the generalized eigenproblem for that block.", "default": null}, "labels": {"type": "any", "nullable": true, "description": "Optional list providing a label for each basis function (each\nrow/column of h and s). labels must have length N (the matrix dimension)\nand define the partitioning into blocks: all indices with the same\nlabel are collected into a single diagonal block that is diagonalized\nindependently. Typical use in PySCF: labels are orbital symmetry names\nor irreducible representation identifiers (strings) returned by\nsymmetry utilities (e.g., symm.label_orb_symm). If labels is None\n(default), no block partitioning is performed and the full matrix is\ndiagonalized by scipy.linalg.eigh.", "default": null}}, "required": ["h", "labels", "s"], "type": "any"}}, "type": "function"}], "query": "I’m replicating a PySCF symmetry-adapted generalized eigen-solve, but my labeling file is messy. I first thought I could just pass the raw labels to the block-diagonalizer, but then I realized the labels come from a human-edited symmetry table and include inconsistent capitalization and trailing whitespace, which will incorrectly split blocks.\n\nPlease do the following workflow:\n1) Canonicalize the basis-function labels by trimming whitespace and normalizing case so that labels that differ only by those artifacts are treated as identical.\n2) Then solve the generalized Hermitian eigenproblem H x = S x e by diagonalizing each symmetry block independently using the canonicalized labels.\n\nUse the same 4x4 Hamiltonian H and overlap S as below, but use these *raw* labels (note the artifacts): [\"A1 \", \"a1\", \"B2\", \"b2  \"]. Return eigenvalues and eigenvectors per block.\n\nH = [[-1.0, 0.1, 0.0, 0.0],\n     [ 0.1,-0.5, 0.0, 0.0],\n     [ 0.0, 0.0, 0.2, 0.05],\n     [ 0.0, 0.0, 0.05, 0.3]]\nS = [[ 1.0, 0.02, 0.0, 0.0],\n     [0.02, 1.0, 0.0, 0.0],\n     [ 0.0, 0.0, 1.0, 0.01],\n     [ 0.0, 0.0, 0.01, 1.0]]", "answers": "[{\"name\":\"pyscf_lib_linalg_helper_eigh_by_blocks\",\"arguments\":{\"h\":[[-1.0,0.1,0.0,0.0],[0.1,-0.5,0.0,0.0],[0.0,0.0,0.2,0.05],[0.0,0.0,0.05,0.3]],\"s\":[[1.0,0.02,0.0,0.0],[0.02,1.0,0.0,0.0],[0.0,0.0,1.0,0.01],[0.0,0.0,0.01,1.0]],\"labels\":[\"A1\",\"A1\",\"B2\",\"B2\"]}}]"}
{"func_name": "pyscf_lib_linalg_helper_make_diag_precond", "func_desc": "Generate a diagonal preconditioner function for use in PySCF linear-algebra routines (e.g., iterative eigenvalue or subspace solvers). The returned function applies an elementwise division by a shifted diagonal to regularize the usual Jacobi-style preconditioner (Ax - x*e)/(diag(A) - e), breaking the correlation that can make basis vectors linearly dependent (see issue referenced in source). The implementation uses a fixed small cutoff (1e-8) to avoid exact division by zero after applying the level shift.", "tools": [{"function": {"description": "Generate a diagonal preconditioner function for use in PySCF linear-algebra routines (e.g., iterative eigenvalue or subspace solvers). The returned function applies an elementwise division by a shifted diagonal to regularize the usual Jacobi-style preconditioner (Ax - x*e)/(diag(A) - e), breaking the correlation that can make basis vectors linearly dependent (see issue referenced in source). The implementation uses a fixed small cutoff (1e-8) to avoid exact division by zero after applying the level shift.\n", "name": "pyscf_lib_linalg_helper_make_diag_precond", "parameters": {"properties": {"diag": {"type": "array", "items": {"type": "any"}, "description": "1-D array of the diagonal elements of the matrix A used by the preconditioner. This array supplies diag(A) in the expression diag - (e - level_shift). The array is captured by closure when make_diag_precond is called; the returned preconditioner will compute a temporary shifted-diagonal array (diagd) as diag - (e - level_shift) and will not modify the original diag object. In the PySCF context, diag typically comes from the diagonal of a Hamiltonian or other operator and is used to form a cheap, elementwise preconditioner for iterative linear-algebra routines.", "default": ""}, "level_shift": {"type": "float", "description": "Small positive scalar (default 0.001) added to the eigenvalue shift inside the denominator to break correlations between the numerator (Ax - x*e) and the denominator (diag(A) - e). Practically, this reduces the chance of very small denominators that would produce instabilities or nearly linearly dependent correction vectors. The value is applied as diag - (e - level_shift). Extremely small or zero values of level_shift may fail to prevent correlation; excessively large values will bias the preconditioning and may slow convergence.", "default": 0.001}}, "required": ["diag", "level_shift"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up PySCF’s iterative eigen-solver and initially planned to use a straightforward Jacobi diagonal preconditioner with the diagonal approximation diag(A) = [0.12, 0.55, 1.03, 1.58, 2.10, 3.05]. But I just realized our Davidson run is hitting near-degenerate roots, and using the same tiny stabilizing shift we used in last week’s well-separated spectrum is now making the preconditioned residuals too correlated (we’re seeing almost linearly dependent subspace vectors). So please generate the *shifted* diagonal preconditioner function, but tune the level shift to be the smallest value (on the same order as the original) that is still guaranteed to keep every denominator (diag(A) - e + shift) away from the hard zero-division cutoff used inside the helper, even in the worst case where the target eigenvalue e can land exactly on the smallest diagonal element. Use the same fixed cutoff behavior as the helper (don’t change it), and return the PySCF preconditioner callable built from that diagonal.", "answers": "[{\"name\":\"pyscf_lib_linalg_helper_make_diag_precond\",\"arguments\":{\"diag\":[0.12,0.55,1.03,1.58,2.1,3.05],\"level_shift\":1.0000000000000002e-08}}]"}
{"func_name": "pyscf_lib_misc_flatten", "func_desc": "Flatten a sequence of first-level iterables into a single list used by PySCF for assembling collections (e.g., lists of basis-function descriptors, integrals, or other per-component lists) when building larger data structures in the Python-based Simulations of Chemistry Framework.", "tools": [{"function": {"description": "Flatten a sequence of first-level iterables into a single list used by PySCF for assembling collections (e.g., lists of basis-function descriptors, integrals, or other per-component lists) when building larger data structures in the Python-based Simulations of Chemistry Framework.\n", "name": "pyscf_lib_misc_flatten", "parameters": {"properties": {"lst": {"type": "array", "items": {"type": "any"}, "description": "A list whose elements are themselves iterable containers (for example, lists or tuples) that should be concatenated in order. In the PySCF codebase this function is used to combine per-component lists produced by chemistry workflows into a single list. The function expects a Python list as the top-level container; each element of that list must be iterable. If an element is not iterable (for example, a bare int), a TypeError will be raised when attempting to iterate over that element. Note that iterable scalars such as strings will be iterated character-by-character (which is often undesirable), so callers should wrap such items in a non-iterable container or avoid passing raw strings.", "default": ""}}, "required": ["lst"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a PySCF run where I need a single ordered list of basis-function labels, but my upstream parser gave me three first-level iterables (core/valence/diffuse) and I initially planned to just concatenate them in that order. Wait—downstream I’m building a per-element block and the integrals builder expects the most chemically common center to come first; in this fragment that means the element that appears in both the valence and diffuse groups should lead the final ordering. So: take the three groups core=[\"H 1s\",\"He 1s\"], valence=(\"C 2s\",\"C 2p\",\"O 2s\",\"O 2p\"), diffuse=[\"O 3s\",\"O 3p\"), reorder the *groups* so the group(s) containing that repeated element come first (preserving the internal order within each group), then flatten the first-level iterables into a single list to pass downstream.", "answers": "[{\"name\": \"pyscf_lib_misc_flatten\", \"arguments\": {\"lst\": [[\"C 2s\", \"C 2p\", \"O 2s\", \"O 2p\"], [\"O 3s\", \"O 3p\"], [\"H 1s\", \"He 1s\"]]}}]"}
{"func_name": "pyscf_lib_misc_index_tril_to_pair", "func_desc": "pyscf.lib.misc.index_tril_to_pair maps a flattened lower-triangular (tril) linear index used in PySCF to the corresponding pair of row and column indices (i, j).\n    \n    Given a linear index or array of linear indices ij that encodes positions in a packed lower-triangular layout using the convention\n    ij = i * (i + 1) / 2 + j,\n    this function computes the pair indices i and j (zero-based, consistent with NumPy/Python) such that each returned pair satisfies the above relation. In PySCF and quantum chemistry codes, this mapping is commonly used when compactly storing symmetric matrices (for example overlap, density, or two-electron integrals) in a 1D array containing only the lower triangle; index_tril_to_pair recovers the original matrix coordinates used to reconstruct or address elements.", "tools": [{"function": {"description": "pyscf.lib.misc.index_tril_to_pair maps a flattened lower-triangular (tril) linear index used in PySCF to the corresponding pair of row and column indices (i, j).\n\nGiven a linear index or array of linear indices ij that encodes positions in a packed lower-triangular layout using the convention\nij = i * (i + 1) / 2 + j,\nthis function computes the pair indices i and j (zero-based, consistent with NumPy/Python) such that each returned pair satisfies the above relation. In PySCF and quantum chemistry codes, this mapping is commonly used when compactly storing symmetric matrices (for example overlap, density, or two-electron integrals) in a 1D array containing only the lower triangle; index_tril_to_pair recovers the original matrix coordinates used to reconstruct or address elements.", "name": "pyscf_lib_misc_index_tril_to_pair", "parameters": {"properties": {"ij": {"type": "array", "items": {"type": "any"}, "description": "A NumPy array of non-negative integer tril indices encoded with the convention ij = i*(i+1)/2 + j. ij must represent valid packed lower-triangular positions: i >= 0 and 0 <= j <= i for each element. The function expects a NumPy ndarray (typically integer dtype) and returns arrays of the same shape. Passing values that are negative, non-integer, or otherwise not representing valid tril indices will lead to undefined or incorrect results: negative values typically produce NaNs from the square-root step and then cause a conversion error when casting to integers.", "default": ""}}, "required": ["ij"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF integral pipeline where I packed a symmetric matrix (nominally 6x6) into a 1D lower-triangular buffer using ij = i*(i+1)/2 + j. I was going to decode these five packed indices directly: ij = [0, 3, 7, 14, 20]. Wait—one of these came from a 1-based index exported by a Fortran helper, while the rest are already 0-based (Python). The exported one is the only value that would be out-of-range for a 6x6 lower triangle if interpreted as 0-based, so first convert just that single bad index into the correct 0-based value, then decode all five into (i, j) pairs (0-based).", "answers": "[{\"name\":\"pyscf_lib_misc_index_tril_to_pair\",\"arguments\":{\"ij\":[0,3,7,14,19]}}]"}
{"func_name": "pyscf_lib_misc_invalid_method", "func_desc": "pyscf.lib.misc.invalid_method creates a placeholder callable that indicates a method is invalid or intentionally unimplemented within the PySCF framework. It is used when a class, module, or API wants to de-register or replace an existing method with a sentinel that consistently signals \"not implemented\" at runtime, aiding clear error reporting and API maintenance.", "tools": [{"function": {"description": "pyscf.lib.misc.invalid_method creates a placeholder callable that indicates a method is invalid or intentionally unimplemented within the PySCF framework. It is used when a class, module, or API wants to de-register or replace an existing method with a sentinel that consistently signals \"not implemented\" at runtime, aiding clear error reporting and API maintenance.\n", "name": "pyscf_lib_misc_invalid_method", "parameters": {"properties": {"name": {"type": "string", "description": "The canonical name to assign to the generated placeholder callable. This string is used in two practical ways within the PySCF codebase: it becomes the returned function's __name__ attribute (so tools, tracebacks, and introspection report the given name), and it is interpolated into the NotImplementedError message raised by the placeholder. The caller must provide a descriptive name for the method being de-registered; providing a non-str value is not supported and may produce a TypeError or unexpected diagnostics when the function is created or invoked.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "I’m refactoring a PySCF density-fitting (DF) plugin: I initially planned to keep the legacy `DF.get_jk` around and just warn, but I realized the new screening backend routes through that entry point in a way that would silently give wrong Coulomb/exchange results if it’s still callable. So I need to *hard-disable* the old hook by replacing it with PySCF’s standard invalid-method sentinel.\n\nCreate the placeholder callable using `pyscf.lib.misc.invalid_method`, but name the sentinel using the legacy method’s identifier after converting it to the project’s deprecation naming convention: take the dotted path `DF.get_jk`, replace the dot with an underscore, and then apply the convention of prefixing it with the two-letter tag used for deprecated APIs in this codebase (the same tag we use elsewhere for removed methods). Return the tool call that constructs that sentinel.", "answers": "[{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"DE_DF_get_jk\"}}]"}
{"func_name": "pyscf_lib_misc_repo_info", "func_desc": "pyscf.lib.misc.repo_info obtains repository metadata for a PySCF source tree: it resolves the supplied path to an absolute repository location and, when a Git repository is detected, assembles a one- or two-line Git status string containing original HEAD, branch name, and commit ID. This information is used by PySCF to report the source repository location and commit identifiers for reproducibility, debugging, logging, and citation purposes.\n    \n    The function inspects the filesystem for a .git directory either at the given path or in its parent directory. If a Git repository is found, repo_info calls the internal git_info routine and formats its output into human-readable lines such as \"GIT ORIG_HEAD <orig_head>\" and \"GIT HEAD (branch <branch>) <head>\" (or \"GIT HEAD      <head>\" when no branch name is present). If no Git repository is found, only the canonical absolute path is returned. The function does not modify files on disk; it only reads filesystem metadata and delegates to git_info for Git-specific details. The implementation currently does not collect information about external libraries (for example BLAS, libcint, libxc, libxcfun, tblis), as indicated by the in-source TODO.", "tools": [{"function": {"description": "pyscf.lib.misc.repo_info obtains repository metadata for a PySCF source tree: it resolves the supplied path to an absolute repository location and, when a Git repository is detected, assembles a one- or two-line Git status string containing original HEAD, branch name, and commit ID. This information is used by PySCF to report the source repository location and commit identifiers for reproducibility, debugging, logging, and citation purposes.\n\nThe function inspects the filesystem for a .git directory either at the given path or in its parent directory. If a Git repository is found, repo_info calls the internal git_info routine and formats its output into human-readable lines such as \"GIT ORIG_HEAD <orig_head>\" and \"GIT HEAD (branch <branch>) <head>\" (or \"GIT HEAD      <head>\" when no branch name is present). If no Git repository is found, only the canonical absolute path is returned. The function does not modify files on disk; it only reads filesystem metadata and delegates to git_info for Git-specific details. The implementation currently does not collect information about external libraries (for example BLAS, libcint, libxc, libxcfun, tblis), as indicated by the in-source TODO.", "name": "pyscf_lib_misc_repo_info", "parameters": {"properties": {"repo_path": {"type": "string", "description": "Path to a directory that is the repository root or a subdirectory of the repository.\nThe function converts this path to an absolute path (os.path.abspath) and, if a .git\ndirectory is not present at that location but is present in its parent directory,\nthe parent directory is treated as the repository root. This parameter must be a\nfilesystem path string; no validation is performed beyond os.path.isdir checks.", "default": ""}}, "required": ["repo_path"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a reproducibility appendix for a mixed QM/MM benchmark and need PySCF’s repo metadata in the log. Initially I was going to point `repo_info` directly at `./third_party/pyscf`, but I realized our vendoring step sometimes drops the `.git` directory inside that leaf folder while leaving it in the nearest ancestor (because we keep submodules as gitlinks in the parent checkout). So: call `pyscf.lib.misc.repo_info` on the directory that is the closest existing parent of `./third_party/pyscf` such that the function will still detect the same PySCF source tree location but can actually see the Git metadata if it exists. Use the result exactly as returned (absolute canonical path plus the one/two-line GIT status when detected) for my run log.", "answers": "[{\"name\": \"pyscf_lib_misc_repo_info\", \"arguments\": {\"repo_path\": \"./third_party\"}}]"}
{"func_name": "pyscf_lib_misc_square_mat_in_trilu_indices", "func_desc": "pyscf.lib.misc.square_mat_in_trilu_indices returns an n x n integer matrix that maps the unique elements of a flattened lower-triangular (tril) vector to positions in a full symmetric square matrix. This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to convert a compact storage of unique symmetric matrix elements (commonly used for symmetric operators, density matrices, and pairwise interaction tensors in quantum chemistry) back into a full symmetric index structure that can be used to reconstruct or address the full matrix.\n    \n    The function constructs the mapping by enumerating the lower-triangular indices in the same order as numpy.tril_indices(n), assigning each unique tril element a consecutive integer index from 0 to n*(n+1)//2 - 1, and mirroring those indices across the diagonal so that the output matrix is symmetric. The result is independent of any external state (no side effects) and uses O(n^2) time and memory to produce an (n, n) integer array.", "tools": [{"function": {"description": "pyscf.lib.misc.square_mat_in_trilu_indices returns an n x n integer matrix that maps the unique elements of a flattened lower-triangular (tril) vector to positions in a full symmetric square matrix. This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to convert a compact storage of unique symmetric matrix elements (commonly used for symmetric operators, density matrices, and pairwise interaction tensors in quantum chemistry) back into a full symmetric index structure that can be used to reconstruct or address the full matrix.\n\nThe function constructs the mapping by enumerating the lower-triangular indices in the same order as numpy.tril_indices(n), assigning each unique tril element a consecutive integer index from 0 to n*(n+1)//2 - 1, and mirroring those indices across the diagonal so that the output matrix is symmetric. The result is independent of any external state (no side effects) and uses O(n^2) time and memory to produce an (n, n) integer array.", "name": "pyscf_lib_misc_square_mat_in_trilu_indices", "parameters": {"properties": {"n": {"type": "integer", "description": "The dimension of the square matrix to construct. n must be a non-negative integer representing the number of rows and columns of the square matrix. In practical PySCF usage, n corresponds to the size of a one-particle basis or the dimension of any symmetric operator whose unique lower-triangular elements have been vectorized. Passing a non-integer type or a negative integer will result in errors from the underlying numpy calls (TypeError or ValueError) and is not supported.", "default": ""}}, "required": ["n"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF density-matrix post-processing script where I *thought* the AO density was 5×5, so I was about to build the symmetric index map for that. But I just realized the compact tril vector I saved includes two sentinel values at the end from a buggy writer (they’re not part of the density; the true data are exactly the unique symmetric elements). I can’t trust the dimension label anymore, so infer the intended matrix dimension from the tril-vector length after dropping those two trailing sentinels, then generate the n×n integer index map that mirrors lower-triangular indexing (same order as `numpy.tril_indices`) so each full-matrix (i,j) points to the correct position in the compact tril vector.", "answers": "[{\"name\": \"pyscf_lib_misc_square_mat_in_trilu_indices\", \"arguments\": {\"n\": 5}}]"}
{"func_name": "pyscf_lib_numpy_helper_broadcast_mul", "func_desc": "Broadcasted entrywise multiplication specialized for PySCF tensor operations.", "tools": [{"function": {"description": "Broadcasted entrywise multiplication specialized for PySCF tensor operations.\n", "name": "pyscf_lib_numpy_helper_broadcast_mul", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "3D input array in C order. In the PySCF context this is typically a stack of 2D blocks or tensor slices (for example, a collection of matrices indexed by a leading index used in electronic-structure routines). The function requires a.ndim == 3, a.shape[1:] == b.shape, and a.dtype to match b.dtype. The memory layout must satisfy the stride constraint checked in code (the last axis must be contiguous: a.strides[2] == a.itemsize). This array supplies the per-slice values that are multiplied by the 2D factor b for each leading index and participates in the elementwise multiplication out[:, :, :] += a[:, :, :] * b[None, :, :].", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "2D input array in C order. In PySCF usage this represents the 2D multiplier applied to every 2D slice of a (for example, a matrix of coefficients or scaling factors common across the leading dimension). The function requires b.ndim == 2, b.shape == a.shape[1:], and b.dtype to match a.dtype. The memory layout must satisfy b.strides[1] == b.itemsize (contiguous last axis). The first stride (b.strides[0]) is interpreted as the leading-dimension stride and is passed through to the low-level helper.", "default": ""}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional output array in C order with the same shape and dtype as a. If provided, out is modified in place and receives the accumulated result (out[:, :, :] is incremented by the broadcasted product). If out is None, the function allocates and returns a new zero-initialized ndarray with the same shape and dtype as a. When supplied, out must satisfy out.shape == a.shape, out.dtype == a.dtype, and the same last-axis contiguity constraint as a (out.strides[2] == out.itemsize).", "default": null}}, "required": ["a", "b", "out"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing three AO integral blocks from a PySCF post-HF run (shape (3,4,4)) and want to apply the same 4×4 screening/weight matrix (shape (4,4)) by broadcasted entrywise multiplication before I accumulate the contribution.\n\nInitially I was going to multiply the blocks by the coefficient matrix exactly as-is, but I just realized the coefficient matrix I pasted is the raw overlap-based weight and includes off-diagonal terms that must be symmetrized before applying it (otherwise the downstream contraction breaks the expected Hermitian symmetry). So: first replace the coefficient matrix with its symmetrized version formed by averaging it with its transpose, then perform the broadcasted elementwise multiplication between the (3,4,4) tensor and that symmetrized (4,4) matrix. Let the helper allocate the output (out=None).\n\nUse these arrays:\n\n- a (shape (3, 4, 4)) =\n  [\n    [\n      [1.0, 0.5, 0.0, -0.5],\n      [0.2, 0.3, 0.4, 0.5],\n      [0.0, -0.1, -0.2, -0.3],\n      [1.5, 1.0, 0.5, 0.0]\n    ],\n    [\n      [0.1, 0.2, 0.3, 0.4],\n      [0.0, 0.0, 0.1, 0.2],\n      [0.5, 0.4, 0.3, 0.2],\n      [-0.1, -0.2, -0.3, -0.4]\n    ],\n    [\n      [2.0, 1.0, 0.0, -1.0],\n      [0.3, 0.6, 0.9, 1.2],\n      [0.0, 0.2, 0.4, 0.6],\n      [1.0, 0.0, -1.0, -2.0]\n    ]\n  ]\n\n- b (shape (4, 4)) =\n  [\n    [1.0, 0.0, 0.5, 0.0],\n    [0.0, 1.0, 0.0, 0.5],\n    [0.5, 0.0, 1.0, 0.0],\n    [0.0, 0.5, 0.0, 1.0]\n  ]\n\nReturn the resulting (3,4,4) tensor.", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_broadcast_mul\", \"arguments\": {\"a\": [[[1.0, 0.5, 0.0, -0.5], [0.2, 0.3, 0.4, 0.5], [0.0, -0.1, -0.2, -0.3], [1.5, 1.0, 0.5, 0.0]], [[0.1, 0.2, 0.3, 0.4], [0.0, 0.0, 0.1, 0.2], [0.5, 0.4, 0.3, 0.2], [-0.1, -0.2, -0.3, -0.4]], [[2.0, 1.0, 0.0, -1.0], [0.3, 0.6, 0.9, 1.2], [0.0, 0.2, 0.4, 0.6], [1.0, 0.0, -1.0, -2.0]]], \"b\": [[1.0, 0.0, 0.5, 0.0], [0.0, 1.0, 0.0, 0.5], [0.5, 0.0, 1.0, 0.0], [0.0, 0.5, 0.0, 1.0]], \"out\": null}}]"}
{"func_name": "pyscf_lib_numpy_helper_condense", "func_desc": "Condense a 2D NumPy array into an array of block-wise reductions using a named reduction\n    operator. This function is part of PySCF's numpy_helper utilities and is used to\n    aggregate blocks of a 2-dimensional numerical array (for example, parts of a\n    density matrix or integral arrays) into a smaller 2D array of reduced values\n    (e.g., sums, maxima). The function accepts explicit block boundary indices along\n    rows and columns (loc_x and loc_y) and applies the specified operator to each\n    block, returning a condensed array with one entry per block pair.", "tools": [{"function": {"description": "Condense a 2D NumPy array into an array of block-wise reductions using a named reduction\noperator. This function is part of PySCF's numpy_helper utilities and is used to\naggregate blocks of a 2-dimensional numerical array (for example, parts of a\ndensity matrix or integral arrays) into a smaller 2D array of reduced values\n(e.g., sums, maxima). The function accepts explicit block boundary indices along\nrows and columns (loc_x and loc_y) and applies the specified operator to each\nblock, returning a condensed array with one entry per block pair.", "name": "pyscf_lib_numpy_helper_condense", "parameters": {"properties": {"opname": {"type": "string", "description": "Name of the reduction operator to apply to each block. The\nstring may be given in forms such as 'sum', 'numpy.sum', 'np.sum', or\nwith an 'NP_' prefix; leading 'numpy.' or 'np.' prefixes are removed and\na leading 'NP_' is ignored. For arrays with dtype numpy.double and for\nopname in ('sum', 'max', 'min', 'abssum', 'absmax', 'absmin', 'norm'),\nan optimized native implementation (_np_helper.NPcondense) is used.\nFor boolean or numpy.int8 arrays and opname in ('any', 'all'), a\nspecialized optimized implementation (_np_helper.NPbcondense) is used.\nFor other dtype/opname combinations, the function falls back to the\ncorresponding numpy.<opname> function. If opname is not available in\nthe optimized helper or as a numpy attribute, an AttributeError will be\nraised. This parameter determines the per-block aggregation semantics\n(e.g., sum over elements, logical any over elements) and therefore\ndirectly controls the meaning of the condensed output in PySCF data\nprocessing.", "default": ""}, "a": {"type": "array", "items": {"type": "any"}, "description": "A 2-dimensional NumPy array containing the data to be\ncondensed. The function asserts that a.ndim == 2 and will raise an\nAssertionError if this condition is not met. Typical uses in PySCF\ninclude condensing large matrices (for example, density matrices,\ntwo-dimensional intermediate arrays) into coarser block summaries. The\nfunction may reorder or copy this array internally: if a is Fortran\ncontiguous, it is transposed to facilitate a C-contiguous copy; then a\nC-order copy is created with numpy.asarray(a, order='C') before any\noptimized native routine is invoked. The dtype of a determines which\noptimized path (if any) is taken and influences the dtype of the\nreturned array.", "default": ""}, "loc_x": {"type": "array", "items": {"type": "any"}, "description": "Sequence of integer indices that define the row block\nboundaries. This should be a list-like object that can be converted to\na NumPy int32 array; internally loc_x is converted with\nnumpy.asarray(loc_x, numpy.int32). The number of row blocks is\nnloc_x = len(loc_x) - 1, so loc_x must contain at least two entries.\nThe i-th row block corresponds to rows loc_x[i]:loc_x[i+1] of a.\nIndices are passed through to optimized helpers or used for slicing in\nthe fallback path, so they must be valid for a (out-of-range or\nnon-monotonic indices can lead to IndexError, incorrect results, or\nundefined behavior in native helpers).", "default": ""}, "loc_y": {"type": "any", "nullable": true, "description": "Sequence of integer indices that define the column block\nboundaries, analogous to loc_x. If loc_y is None (the default), loc_y\nis taken equal to loc_x so squared tilings are produced. Internally it\nis converted to a NumPy int32 array. The number of column blocks is\nnloc_y = len(loc_y) - 1 and loc_y must contain at least two entries\nwhen provided explicitly. The j-th column block corresponds to\ncolumns loc_y[j]:loc_y[j+1] of a. As with loc_x, invalid indices may\nraise exceptions or cause undefined behavior when native helpers are\nused.", "default": null}}, "required": ["opname", "a", "loc_x", "loc_y"], "type": "any"}}, "type": "function"}], "query": "I’m condensing a 4×6 AO overlap-like matrix into a 2×3 block summary for a quick sanity check in a PySCF workflow. Initially I was going to use the plain blockwise sum, but I realized one AO shell in the first row-block has a sign convention flip relative to the rest of the pipeline, so for the reduction I actually need the **sum of magnitudes** within each block (i.e., treat all entries as nonnegative before aggregating) so cancellations don’t hide large couplings.\n\nPlease take the matrix A below, partition rows into two contiguous blocks that split the 4 rows exactly in half, and partition columns into three contiguous blocks of equal width that exactly tile the 6 columns. Then condense A into a 2×3 array by applying the named reduction corresponding to “sum of magnitudes” to each block.\n\nA =\n[[ 1.0, -1.0,  2.0,  0.0,  3.0,  1.0],\n [ 0.5,  2.0, -0.5,  1.5, -1.0,  0.0],\n [ 4.0,  1.0,  0.0, -2.0,  2.5, -0.5],\n [-1.5,  0.0,  1.0,  3.0,  0.0,  2.0]]", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_condense\", \"arguments\": {\"opname\": \"abssum\", \"a\": [[1.0, -1.0, 2.0, 0.0, 3.0, 1.0], [0.5, 2.0, -0.5, 1.5, -1.0, 0.0], [4.0, 1.0, 0.0, -2.0, 2.5, -0.5], [-1.5, 0.0, 1.0, 3.0, 0.0, 2.0]], \"loc_x\": [0, 2, 4], \"loc_y\": [0, 2, 4, 6]}}]"}
{"func_name": "pyscf_lib_numpy_helper_entrywise_mul", "func_desc": "pyscf.lib.numpy_helper.entrywise_mul performs entrywise (element-wise) multiplication of two 2-D NumPy arrays (matrices). It is a low-level numerical helper used in the PySCF (Python-based Simulations of Chemistry Framework) codebase for elementwise combination of matrix-valued data such as density matrices, intermediate arrays in integral transformations, or other 2-D numerical tensors that arise in quantum-chemistry algorithms. The implementation enforces strict shape, dimensionality and dtype compatibility and dispatches to optimized C routines for double-precision real and double-precision complex data to accelerate performance-critical paths in PySCF.", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.entrywise_mul performs entrywise (element-wise) multiplication of two 2-D NumPy arrays (matrices). It is a low-level numerical helper used in the PySCF (Python-based Simulations of Chemistry Framework) codebase for elementwise combination of matrix-valued data such as density matrices, intermediate arrays in integral transformations, or other 2-D numerical tensors that arise in quantum-chemistry algorithms. The implementation enforces strict shape, dimensionality and dtype compatibility and dispatches to optimized C routines for double-precision real and double-precision complex data to accelerate performance-critical paths in PySCF.\n", "name": "pyscf_lib_numpy_helper_entrywise_mul", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "Left operand matrix. Must be a 2-D NumPy array (a.ndim == 2). In the PySCF context this typically represents a matrix such as a density or intermediate tensor. The function requires that a and b have identical shape and identical dtype; otherwise an AssertionError is raised. The dtype is significant because float64 (numpy.double) and complex128 (numpy.complex128) trigger optimized native C implementations.", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "Right operand matrix. Must be a 2-D NumPy array (b.ndim == 2) with the same shape and dtype as a. The elementwise product a * b is computed. As with a, b is expected to have a memory layout compatible with the internal leading_dimension_order check (see behavior below) so that optimized paths can be used.", "default": ""}, "out": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Output matrix to store the result. If provided, it must be a NumPy ndarray whose canonical 2-D shape and leading-dimension information (as returned by leading_dimension_order) match those of a and b; otherwise an AssertionError is raised. If out is None (the default), a new array is allocated with numpy.empty_like(b) and returned. When provided, out is modified in-place and the same object is returned.", "default": null}}, "required": ["a", "b"], "type": "any"}}, "type": "function"}], "query": "I’m doing a PySCF post-processing cleanup where I apply an element-wise screening mask to an intermediate 3×3 matrix. I initially planned to multiply the intermediate matrix A by the mask matrix B directly, but then I realized the mask I got from an upstream step is actually an *attenuation mask* (it encodes “how much to suppress”), whereas what I need here is a *keep mask* (it encodes “how much to retain”). So before doing the entrywise multiplication, convert the attenuation mask into the keep mask by taking the complement with respect to unity (i.e., for each element use 1 minus that element). Then multiply A by this converted mask entrywise using `pyscf.lib.numpy_helper.entrywise_mul`. Both inputs are double-precision real and must remain 3×3.\n\nUse:\nA = [[0.25, -1.5, 2.0],\n     [3.75, 0.0, -0.5],\n     [1.2, 4.4, -2.2]]\n\nAttenuation mask (to be converted before multiplying):\nB = [[1.0, 0.0, 0.5],\n     [0.2, 1.0, 1.0],\n     [0.0, 0.75, 0.1]]", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_entrywise_mul\",\"arguments\":{\"a\":[[0.25,-1.5,2.0],[3.75,0.0,-0.5],[1.2,4.4,-2.2]],\"b\":[[0.0,1.0,0.5],[0.8,0.0,0.0],[1.0,0.25,0.9]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_hermi_sum", "func_desc": "pyscf.lib.numpy_helper.hermi_sum computes the elementwise Hermitian sum a + a.T.conj() with improved memory efficiency compared to making an explicit transpose copy. In the PySCF quantum-chemistry workflow this is used when forming Hermitian matrices from general (possibly non-Hermitian) blocks or slices (for example when constructing symmetric contributions to Fock or density matrices, or accumulating blockwise results from tensor contractions). The function supports a 2-D square matrix or a stack of square matrices stored as a 3-D array with axes layout (0,2,1) and attempts to use a C-accelerated path for C-contiguous arrays of dtype numpy.double or numpy.complex128; otherwise a Python fallback performs the blockwise computation. The result is written into and returned via the out buffer (which may be the input array if inplace=True), avoiding an additional full-array transpose copy.", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.hermi_sum computes the elementwise Hermitian sum a + a.T.conj() with improved memory efficiency compared to making an explicit transpose copy. In the PySCF quantum-chemistry workflow this is used when forming Hermitian matrices from general (possibly non-Hermitian) blocks or slices (for example when constructing symmetric contributions to Fock or density matrices, or accumulating blockwise results from tensor contractions). The function supports a 2-D square matrix or a stack of square matrices stored as a 3-D array with axes layout (0,2,1) and attempts to use a C-accelerated path for C-contiguous arrays of dtype numpy.double or numpy.complex128; otherwise a Python fallback performs the blockwise computation. The result is written into and returned via the out buffer (which may be the input array if inplace=True), avoiding an additional full-array transpose copy.\n", "name": "pyscf_lib_numpy_helper_hermi_sum", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Input array representing a single matrix (2-D, shape (N, N)) or a batch of matrices (3-D, shape (M, N, N)) when axes is (0,2,1). In PySCF this typically holds numeric linear-algebra objects such as Fock/density matrix blocks or intermediate arrays from integral transforms. The array must have the layout and shape described above: for 2-D the two dimensions must be equal; for 3-D the middle and last dimensions must be equal and axes must be (0,2,1). The function treats entries as complex when dtype is numpy.complex128; diagonal elements of the returned sum therefore have their imaginary parts canceled (they become twice the real part).", "default": ""}, "axes": {"type": "any", "nullable": true, "description": "Layout descriptor for 3-D inputs. Must be None for the 2-D case. For a batch of matrices provided as a 3-D array, set axes to (0,2,1) to indicate that the first axis indexes separate square matrices and the remaining two axes are the matrix rows and columns. The C-accelerated path only accepts axes == (0,2,1) for 3-D inputs; the Python fallback also requires this axes value for 3-D processing. If axes is None, only 2-D input is supported.", "default": null}, "hermi": {"type": "integer", "description": "Integer flag forwarded to the low-level C helper (passed as ctypes.c_int). This parameter is used by the internal optimized routine to indicate which triangular convention or Hermitian-handling mode the C backend should apply. The default value 1 corresponds to the library constant used in the PySCF helper to request the standard Hermitian-sum behavior. The function does not validate numeric values beyond forwarding the integer to the C helper.", "default": 1}, "inplace": {"type": "boolean", "description": "If True, the operation is performed in-place on the provided numpy.ndarray a and the input array is overwritten with the result. When inplace is True, a must be an instance of numpy.ndarray (an AssertionError is raised otherwise) and the out parameter is ignored. In-place use is convenient to reduce peak memory when the caller no longer needs the original array contents.", "default": false}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional preallocated numpy.ndarray serving as the buffer to receive the result. If inplace is False and out is provided, it is used as the backing buffer for the result allocation (via numpy.ndarray(..., buffer=out)), and must be compatible with the required shape and dtype; otherwise numpy will raise an error. If inplace is True, out is ignored because the input array a is used as the destination.", "default": null}}, "required": ["a", "hermi", "inplace", "out", "axes"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a real AO-basis Fock build where one intermediate 3×3 block got written in a scratch buffer that I *thought* was already Hermitian, so I was going to just symmetrize it in-place with the usual Hermitian sum. Wait—downstream I’m handing this to a routine that expects a fresh output buffer because the same scratch array is also reused later in the loop (so clobbering it would silently corrupt the next contraction). Keep the standard Hermitian behavior (i.e., form the elementwise a + a.T.conj() for a plain 2-D square matrix with no batched axes tricks), but do it in the memory-efficient way by writing into an explicitly provided output array instead of overwriting the input. The non-Hermitian 3×3 block is [[0.0, 1.2, -0.5], [0.8, 0.0, 2.1], [-0.3, 1.7, 0.0]]. What function call would perform this operation?", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_hermi_sum\", \"arguments\": {\"a\": [[0.0, 1.2, -0.5], [0.8, 0.0, 2.1], [-0.3, 1.7, 0.0]], \"axes\": null, \"hermi\": 1, \"inplace\": false, \"out\": [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_hermi_triu", "func_desc": "pyscf.lib.numpy_helper.hermi_triu produces a Hermitian or anti-Hermitian square matrix by copying the elements from the lower triangular part of a square NumPy array into the upper triangular part, using optimized native helpers from the PySCF C extension. This function is used in PySCF linear-algebra workflows (quantum chemistry matrix construction and symmetrization) to ensure the full matrix satisfies Hermitian or anti-Hermitian symmetry expected by downstream algorithms (e.g., diagonalization, integral transformations).", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.hermi_triu produces a Hermitian or anti-Hermitian square matrix by copying the elements from the lower triangular part of a square NumPy array into the upper triangular part, using optimized native helpers from the PySCF C extension. This function is used in PySCF linear-algebra workflows (quantum chemistry matrix construction and symmetrization) to ensure the full matrix satisfies Hermitian or anti-Hermitian symmetry expected by downstream algorithms (e.g., diagonalization, integral transformations).\n", "name": "pyscf_lib_numpy_helper_hermi_triu", "parameters": {"properties": {"mat": {"type": "array", "items": {"type": "any"}, "description": "A square NumPy array that will be symmetrized. The array must have shape (n, n) and dtype equal to numpy.double (float64) or numpy.complex128. The routine requires that mat be either C-contiguous (mat.flags.c_contiguous) or Fortran-contiguous (mat.flags.f_contiguous); if Fortran-contiguous the function operates on a C-contiguous transposed view internally. The array is modified in place when inplace is True (default); when inplace is False a copy of the input is made and the copy is modified and returned. The function asserts mat.size == n**2 which enforces squareness and will raise AssertionError if this condition is not met.", "default": ""}, "hermi": {"type": "integer", "description": "Flag selecting the type of symmetry to enforce. This integer must equal the module constants HERMITIAN (value 1) or ANTIHERMI (value 2). When hermi == HERMITIAN the returned matrix satisfies a_ij = conj(a_ji) for complex matrices (or a_ij = a_ji for real matrices), i.e., the matrix is Hermitian (symmetric for real dtype). When hermi == ANTIHERMI the returned matrix satisfies a_ij = -conj(a_ji) for complex matrices (or a_ij = -a_ji for real matrices), i.e., the matrix is anti-Hermitian (skew-symmetric for real dtype). The default (1) produces a Hermitian/symmetric matrix. If hermi is not one of the supported constants the function asserts and raises AssertionError.", "default": 1}, "inplace": {"type": "boolean", "description": "If True (default) the operation modifies the provided mat array (or the same underlying data if a transposed view is used for Fortran-contiguous inputs). If False the function first makes a copy of mat (mat.copy('A')) and then performs the symmetrization on that copy; the original input is left unchanged. In both cases the returned array has the same shape and dtype as the input.", "default": true}}, "required": ["mat", "inplace", "hermi"], "type": "any"}}, "type": "function"}], "query": "I’m building an AO-basis Fock-like intermediate in PySCF from a noisy integral dump, and I initially planned to symmetrize it as a standard Hermitian matrix from its reliable lower triangle. But I just realized this particular intermediate is supposed to represent a generator/commutator term, so the symmetry constraint is the one where the off-diagonal elements are negatives of each other’s complex conjugates (diagonal should end up purely imaginary if the input is consistent). I still only trust the lower triangle, and I need the symmetrized full matrix for a downstream eigen-solver that assumes that symmetry. Also: don’t mutate my original buffer because I reuse it for a diagnostic residual check. Use the same 3×3 data as before:\n[[1.2, 0.0, 0.0],\n [2.5, -0.7, 0.0],\n [-1.1, 3.3, 0.4]]", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_hermi_triu\", \"arguments\": {\"mat\": [[1.2, 0.0, 0.0], [2.5, -0.7, 0.0], [-1.1, 3.3, 0.4]], \"hermi\": 2, \"inplace\": false}}]"}
{"func_name": "pyscf_lib_numpy_helper_inv_base_repr_int", "func_desc": "pyscf.lib.numpy_helper.inv_base_repr_int: Convert a sequence of base-digits into the integer(s) they represent (inverse of base_repr_int). This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to map a positional-digit representation (for example, combinatorial indices, encoded occupation patterns, or multi-digit identifiers produced by lib.base_repr_int) back to standard integer indices; it accepts a numpy array of digits and a radix and returns the corresponding integer value(s).\n    \n    This function interprets elements of x as integer digits in the given base and computes the positional value by summing digit * base**position. For multi-dimensional inputs the last axis is treated as the digit axis (most-significant digit first along the last axis), and the function returns an array of integers shaped as the input shape with the last axis removed. For one-dimensional sequences it returns a scalar integer. The input is coerced to integer dtype using numpy.asarray(x, dtype=int) and the computation uses Python/numpy integer arithmetic; the function does not modify the caller's original object.", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.inv_base_repr_int: Convert a sequence of base-digits into the integer(s) they represent (inverse of base_repr_int). This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to map a positional-digit representation (for example, combinatorial indices, encoded occupation patterns, or multi-digit identifiers produced by lib.base_repr_int) back to standard integer indices; it accepts a numpy array of digits and a radix and returns the corresponding integer value(s).\n\nThis function interprets elements of x as integer digits in the given base and computes the positional value by summing digit * base**position. For multi-dimensional inputs the last axis is treated as the digit axis (most-significant digit first along the last axis), and the function returns an array of integers shaped as the input shape with the last axis removed. For one-dimensional sequences it returns a scalar integer. The input is coerced to integer dtype using numpy.asarray(x, dtype=int) and the computation uses Python/numpy integer arithmetic; the function does not modify the caller's original object.", "name": "pyscf_lib_numpy_helper_inv_base_repr_int", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Array of integer digits representing numbers in the specified radix. For a 1-D array, x is interpreted as a single sequence of digits ordered most-significant-digit first (for example, [d_n-1, ..., d_1, d_0] corresponds to sum d_i * base**(n-1-i)). For an N-D array, the last axis is the digit axis (length n) and each entry along the leading axes is converted to an integer; the function coerces x to dtype int via numpy.asarray(x, dtype=int). Note that 0-D (scalar) numpy inputs are not supported by the digit-slicing logic and will typically raise an exception; pass a 1-D sequence for single numbers or an N-D array with last axis as digits for batch conversion.", "default": ""}, "base": {"type": "integer", "description": "Radix (base) used for the positional representation. This integer is used as the exponent base for positional weights (base**k). The function expects an integer-valued base (as per the signature); non-integer types for base are not supported by the implementation and will raise a TypeError or produce incorrect results if coerced implicitly.", "default": ""}}, "required": ["x", "base"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing determinant indices in a PySCF CI bookkeeping step. I *thought* all three occupation-pattern IDs were clean base-4 digit vectors (MSD first, digit axis is the last axis), so I was going to invert them directly. But looking closer, one row has a single digit equal to the radix, which can only be a logging artifact and would produce an invalid base-digit encoding. Please (i) drop the corrupted row entirely, then (ii) run `pyscf.lib.numpy_helper.inv_base_repr_int` on the remaining valid digit vectors using the same radix as before, returning the integer index for each remaining ID.\n\nDigit vectors observed (MSD→LSD): [[0,3,1,2],[1,0,4,3],[2,2,2,2]].", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_inv_base_repr_int\",\"arguments\":{\"x\":[[0,3,1,2],[2,2,2,2]],\"base\":4}}]"}
{"func_name": "pyscf_lib_numpy_helper_isin_1d", "func_desc": "Check whether a 1-D target vector v appears exactly as a row in a collection of vectors vs.\n    This utility is used in the PySCF codebase to test membership of a specific flattened vector\n    (e.g., a molecular orbital coefficient vector, a determinant bitstring represented as an\n    integer vector, or any fixed-length parameter vector used in electronic-structure workflows)\n    within a list/array of candidate vectors. The function performs an elementwise exact\n    comparison (using absolute difference and integer/float equality) after flattening v and\n    reshaping vs to have rows of the same length; it does not perform approximate or\n    tolerance-based comparisons.", "tools": [{"function": {"description": "Check whether a 1-D target vector v appears exactly as a row in a collection of vectors vs.\nThis utility is used in the PySCF codebase to test membership of a specific flattened vector\n(e.g., a molecular orbital coefficient vector, a determinant bitstring represented as an\ninteger vector, or any fixed-length parameter vector used in electronic-structure workflows)\nwithin a list/array of candidate vectors. The function performs an elementwise exact\ncomparison (using absolute difference and integer/float equality) after flattening v and\nreshaping vs to have rows of the same length; it does not perform approximate or\ntolerance-based comparisons.", "name": "pyscf_lib_numpy_helper_isin_1d", "parameters": {"properties": {"v": {"type": "array", "items": {"type": "any"}, "description": "The target vector to search for. The function flattens v with\nnumpy.asarray(v).flatten() so any shape is accepted as long as the total number\nof elements equals the last dimension of vs after reshaping. In practice v\nrepresents a single configuration vector used in PySCF algorithms (for example,\na coefficient vector). Flattening means that multi-dimensional input will be\nviewed as a one-dimensional sequence of elements in row-major order.", "default": ""}, "vs": {"type": "array", "items": {"type": "any"}, "description": "A collection of vectors to search within. The implementation\nuses numpy.asarray(vs).reshape(-1, n) where n = len(v). Therefore the last\ndimension of vs must equal the length of the flattened v; otherwise numpy.reshape\nwill raise a ValueError. vs is interpreted as a 2-D array with each row being a\ncandidate vector; rows are compared to the flattened v by summing absolute\ndifferences across elements and checking for exact zero.", "default": ""}, "return_index": {"type": "boolean", "description": "If False (the default), the function returns only a boolean\nindicating whether v is present in vs. If True, the function returns a tuple\n(present, idx) where present is the boolean membership result and idx provides\nthe location(s) of matching rows in vs. The index behavior mirrors numpy.where:\nwhen there is exactly one matching row idx is returned as a Python int; when there\nare multiple matches idx is returned as a numpy.ndarray of integers; when there\nare no matches idx is returned as an empty numpy.ndarray. Default value is False.", "default": false}}, "required": ["v", "vs", "return_index"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a CI failure in an electronic-structure workflow where determinants are stored as fixed-length integer bitstrings. I want to check whether my *intended* determinant shows up **exactly** as a row in the candidate pool, and if so return all matching row index/indices.\n\nInitially I was going to search for the determinant I wrote down as `[1, 0, 1, 1, 0, 0]`, but I realized this list was transcribed from a log line that includes a leading metadata flag that is **not part of the determinant itself**. So instead of using the full 6-entry vector, drop the **first** entry and only use the remaining entries as the target. (The candidates are already clean and should be used as-is.)\n\nCandidates (rows): `[[0, 1, 1, 0, 0, 1], [1, 0, 1, 1, 0, 0], [1, 1, 0, 0, 1, 0], [1, 0, 1, 1, 0, 0]]`.\n\nUse an exact, elementwise comparison after flattening/reshaping (no tolerances), and return the matching row index/indices if any.", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_isin_1d\", \"arguments\": {\"v\": [0, 1, 1, 0, 0], \"vs\": [[0, 1, 1, 0, 0, 1], [1, 0, 1, 1, 0, 0], [1, 1, 0, 0, 1, 0], [1, 0, 1, 1, 0, 0]], \"return_index\": true}}]"}
{"func_name": "pyscf_lib_numpy_helper_solve_lineq_by_SVD", "func_desc": "Solves the linear system a * x = b using a truncated singular value decomposition (SVD).\n    This function is used in PySCF (a Python-based simulations of chemistry framework) to obtain\n    stable solutions for linear systems that arise in quantum-chemistry computations when the\n    coefficient matrix a may be singular or nearly singular. The implementation computes the\n    SVD of a, discards singular values smaller than the module-level constant LINEAR_DEP_THRESHOLD,\n    and forms a solution x from the retained singular vectors and singular values. This produces\n    a regularized least-squares solution (minimizing ||a x - b||_2 over the retained subspace)\n    and avoids numerical instability caused by tiny singular values.", "tools": [{"function": {"description": "Solves the linear system a * x = b using a truncated singular value decomposition (SVD).\nThis function is used in PySCF (a Python-based simulations of chemistry framework) to obtain\nstable solutions for linear systems that arise in quantum-chemistry computations when the\ncoefficient matrix a may be singular or nearly singular. The implementation computes the\nSVD of a, discards singular values smaller than the module-level constant LINEAR_DEP_THRESHOLD,\nand forms a solution x from the retained singular vectors and singular values. This produces\na regularized least-squares solution (minimizing ||a x - b||_2 over the retained subspace)\nand avoids numerical instability caused by tiny singular values.", "name": "pyscf_lib_numpy_helper_solve_lineq_by_SVD", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "The coefficient matrix in the linear system a * x = b. Expected to be\na two-dimensional NumPy array with shape (M, N), where M is the number of rows\n(equations) and N is the number of unknowns. The function calls numpy.linalg.svd(a)\nand therefore accepts real or complex dtypes; complex values are handled using conjugate\ntransposes where needed. In the PySCF domain, a typically represents matrices such as\noverlap or projection operators arising in electronic-structure routines, and ensuring\nnumerical stability when inverting or solving with these matrices is important for\ncorrect physical results.", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "The right-hand side array in the linear system. It must have a first\ndimension equal to M (the number of rows of a): shape (M,) for a single right-hand side\nor shape (M, K) for K right-hand sides solved simultaneously. The dtype should be\ncompatible with a (real or complex). In PySCF usage b often represents a vector or set\nof vectors of coefficients (for example, projections or residuals) that need to be\nmapped to solution coefficients x.", "default": ""}}, "required": ["a", "b"], "type": "any"}}, "type": "function"}], "query": "I’m trying to stabilize a linear solve that popped out of a four-equation electron-overlap constraint fit (think redundant constraints from near-collinear AO products). Start by solving the overdetermined system with truncated-SVD regularization using the coefficient matrix a = [[1.0, 0.99, 0.0], [0.99, 0.9801, 0.0], [0.0, 0.0, 1.0], [0.5, 0.5, 0.0]] and RHS b = [1.0, 0.99, 0.5, 0.8].\n\nWait—there’s a catch: in the QC pipeline, b was assembled from an integral file that includes a bookkeeping header line, so one of the four equations is not physical and must be dropped before solving. The non-physical row is the one whose coefficients do NOT couple to the third unknown at all and is also the only row whose RHS is inconsistent with the near-duplicate pair of the first two constraints. Remove that single spurious equation and then re-solve the remaining system with the same truncated-SVD procedure to get the regularized least-squares x.", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_solve_lineq_by_SVD\", \"arguments\": {\"a\": [[1.0, 0.99, 0.0], [0.99, 0.9801, 0.0], [0.0, 0.0, 1.0]], \"b\": [1.0, 0.99, 0.5]}}]"}
{"func_name": "pyscf_lib_numpy_helper_takebak_2d", "func_desc": "Reverse of take_2d: accumulate a small 2D block into a larger 2D array in-place.\n    \n    Performs the equivalent operation out[idx[:, None], idy] += a for a 2D block a, modifying out in-place and returning out. In the PySCF codebase this routine is used when assembling contributions computed on index subsets (for example, adding sub-blocks of integrals or density-matrix contributions back into a global 2D array). The function prefers C-optimized code paths for double and complex128 dtypes and falls back to NumPy advanced-index accumulation for other dtypes when thread_safe is True.", "tools": [{"function": {"description": "Reverse of take_2d: accumulate a small 2D block into a larger 2D array in-place.\n\nPerforms the equivalent operation out[idx[:, None], idy] += a for a 2D block a, modifying out in-place and returning out. In the PySCF codebase this routine is used when assembling contributions computed on index subsets (for example, adding sub-blocks of integrals or density-matrix contributions back into a global 2D array). The function prefers C-optimized code paths for double and complex128 dtypes and falls back to NumPy advanced-index accumulation for other dtypes when thread_safe is True.", "name": "pyscf_lib_numpy_helper_takebak_2d", "parameters": {"properties": {"out": {"type": "array", "items": {"type": "float"}, "description": "Destination 2D array to be modified in-place. Must be C-contiguous (out.flags.c_contiguous is required). out holds the global quantity (for example, a global matrix of integrals or a density-like object) into which the block a will be accumulated. The function will assert that out is C-contiguous and will raise an AssertionError if this is not satisfied. Indices in idx and idy must be valid for the dimensions of out; otherwise an IndexError may be raised.", "default": ""}, "a": {"type": "array", "items": {"type": "float"}, "description": "2D source array whose values will be added into out at the positions specified by idx and idy. Practically, a represents a sub-block computed for the rows indexed by idx and columns indexed by idy (as in many PySCF routines that compute blocks of matrices). a is converted to a C-ordered numpy array internally (order='C'); if a.dtype differs from out.dtype it will be cast to out.dtype before accumulation.", "default": ""}, "idx": {"type": "array", "items": {"type": "float"}, "description": "1D integer index array (dtype numpy.ndarray) containing row indices into out. The length of idx determines the number of rows of the block to be added. Internally idx is converted to dtype numpy.int32 before calling the C helper. Values must be valid row indices for out; out-of-range values will produce an IndexError.", "default": ""}, "idy": {"type": "array", "items": {"type": "float"}, "description": "1D integer index array (dtype numpy.ndarray) containing column indices into out. The length of idy determines the number of columns of the block to be added. Internally idy is converted to dtype numpy.int32 before calling the C helper. Values must be valid column indices for out; out-of-range values will produce an IndexError.", "default": ""}, "thread_safe": {"type": "boolean", "description": "Controls the accumulation implementation. Default True. When out.dtype is numpy.double or numpy.complex128, the function dispatches to a C-optimized helper (NPdtakebak_2d or NPztakebak_2d) and passes thread_safe as a flag to that helper. For other dtypes, if thread_safe is True the routine falls back to the NumPy advanced-index accumulation out[idx[:, None], idy] += a; if thread_safe is False and a C helper for the dtype is not available, NotImplementedError is raised. Use thread_safe=True when calling from multithreaded contexts where the fallback NumPy accumulation is acceptable; set thread_safe=False only when a C helper exists for the dtype and the caller specifically requires the non-thread-safe C path.", "default": true}}, "required": ["out", "a", "idx", "idy", "thread_safe"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a 6×6 AO-basis ERI-like intermediate in PySCF and need to accumulate a freshly computed 2×3 contribution block back into the global matrix in-place.\n\nInitially I was going to scatter-add it into rows [1, 3] and columns [0, 2, 5]. But I just realized the column index list came from a 1-based Fortran routine while the row indices were already converted to Python 0-based. That means only the columns are off by one and must be converted to the correct Python indices before accumulation (rows stay as-is). \n\nUse a thread-safe accumulation path.\n\nGlobal matrix (out):\n[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [2.0, 2.1, 2.2, 2.3, 2.4, 2.5]]\n\nBlock to accumulate (a), aligned with the intended (row, column) pairs in the same order as the provided index lists:\n[[0.01, 0.02, 0.03],\n [0.10, 0.20, 0.30]]", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_takebak_2d\", \"arguments\": {\"out\": [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.1, 1.2, 1.3, 1.4, 1.5], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.0, 2.1, 2.2, 2.3, 2.4, 2.5]], \"a\": [[0.01, 0.02, 0.03], [0.1, 0.2, 0.3]], \"idx\": [1, 3], \"idy\": [0, 1, 4], \"thread_safe\": true}}]"}
{"func_name": "pyscf_lib_numpy_helper_transpose_sum", "func_desc": "Compute the element-wise sum a + a.T (matrix plus its transpose) with improved memory\n    efficiency compared to constructing the transpose and summing separately. This helper\n    is provided in the PySCF library (Python-based Simulations of Chemistry Framework)\n    to build symmetrized matrices commonly needed in quantum-chemistry workflows\n    (e.g., symmetrizing density, overlap, Fock, or integral intermediate matrices)\n    while reducing temporary memory allocations. The implementation forwards the call\n    to hermi_sum(a, inplace=inplace, out=out) to perform the computation.", "tools": [{"function": {"description": "Compute the element-wise sum a + a.T (matrix plus its transpose) with improved memory\nefficiency compared to constructing the transpose and summing separately. This helper\nis provided in the PySCF library (Python-based Simulations of Chemistry Framework)\nto build symmetrized matrices commonly needed in quantum-chemistry workflows\n(e.g., symmetrizing density, overlap, Fock, or integral intermediate matrices)\nwhile reducing temporary memory allocations. The implementation forwards the call\nto hermi_sum(a, inplace=inplace, out=out) to perform the computation.", "name": "pyscf_lib_numpy_helper_transpose_sum", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Input array representing the matrix to be symmetrized.\nPractically, callers pass a 2-D square matrix (for example, a density or\nFock matrix in PySCF) whose element-wise sum with its transpose is desired.\nThe function preserves the numeric dtype of a in the returned result.\nThis argument is required and must be a NumPy ndarray; if it is not a\nvalid array type or has incompatible shape for a transpose-based sum,\nthe underlying hermi_sum implementation will raise an exception that\npropagates to the caller.", "default": ""}, "inplace": {"type": "boolean", "description": "If False (default), the function will produce the sum without\nguaranteeing modification of the input array a. If True, the function\nmay perform the operation in-place to save memory, modifying the contents\nof a to contain the result a + a.T. Using inplace=True reduces peak memory\nusage but has the side effect of mutating the provided input array, which\nmay be undesirable if a is needed unchanged later. If a is not writable\nor not suitable for in-place updates, the underlying hermi_sum call may\nraise an exception (for example, ValueError or TypeError), which is\npropagated to the caller.", "default": false}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional output array into which the result will be\nplaced. If provided, out must be a NumPy ndarray with shape and dtype\ncompatible with the result a + a.T so it can store the summed values.\nWhen out is given, the function writes the result into out and returns\nthat object. Supplying out can avoid an additional allocation; if out is\nthe same object as a and writable, the operation may be performed in-place.\nIf out is not provided (None, the default), the function returns a newly\nallocated array unless an in-place update of a is performed (in which\ncase a itself may be returned).", "default": null}}, "required": ["a", "inplace", "out"], "type": "any"}}, "type": "function"}], "query": "I’m in the middle of an SCF micro-iteration and I need to symmetrize a real 4×4 Fock-like matrix using the PySCF helper that computes a + a.T with minimal temporaries. Initially I was going to do this in-place to save memory, but I just realized the *unsymmetrized* matrix is still needed later in the same step for a diagnostics printout, so do **not** overwrite the input.\n\nAlso, to keep allocations flat inside the loop, write the result into a preallocated output buffer. However, my buffer was mistakenly allocated with the same shape but the wrong dtype (it’s complex even though the matrix is strictly real). Don’t allocate a new array—reuse the existing buffer, but make sure the final stored values correspond to the correct real symmetrized matrix.\n\nMatrix (row-major):\n[[0.75, 0.10, -0.05, 0.00],\n [0.20, 0.60,  0.08, 0.01],\n [-0.02, 0.04, 0.55, 0.03],\n [0.00, 0.02, 0.01, 0.50]].\n\nCompute the symmetrized result and place it into the provided output array while preserving the original input matrix.", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_transpose_sum\", \"arguments\": {\"a\": [[0.75, 0.1, -0.05, 0.0], [0.2, 0.6, 0.08, 0.01], [-0.02, 0.04, 0.55, 0.03], [0.0, 0.02, 0.01, 0.5]], \"inplace\": false, \"out\": [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_zdotNN", "func_desc": "zdotNN computes the non-conjugated complex matrix product c = alpha * (a * b) + beta * c where a and b are provided as separate real and imaginary parts. This routine is part of PySCF's low-level numpy_helper linear-algebra utilities and is used in quantum-chemistry workflows inside PySCF to assemble complex-valued matrices (for example intermediate products of density matrices, Fock/Kohn–Sham matrices, or complex integrals) while avoiding explicit complex dtype arithmetic by operating on real and imaginary components.", "tools": [{"function": {"description": "zdotNN computes the non-conjugated complex matrix product c = alpha * (a * b) + beta * c where a and b are provided as separate real and imaginary parts. This routine is part of PySCF's low-level numpy_helper linear-algebra utilities and is used in quantum-chemistry workflows inside PySCF to assemble complex-valued matrices (for example intermediate products of density matrices, Fock/Kohn–Sham matrices, or complex integrals) while avoiding explicit complex dtype arithmetic by operating on real and imaginary components.\n", "name": "pyscf_lib_numpy_helper_zdotNN", "parameters": {"properties": {"aR": {"type": "array", "items": {"type": "float"}, "description": "Real part of the left operand a. This array supplies the real component of the complex matrix a used in the product a*b. In PySCF linear algebra contexts aR typically represents the real part of a block or full matrix (e.g., a density or transform matrix). The array must be a numpy.ndarray; shape and exact dtype are validated implicitly by the underlying ddot operations and must be compatible with bR and bI for matrix multiplication.", "default": ""}, "aI": {"type": "array", "items": {"type": "float"}, "description": "Imaginary part of the left operand a. Supplies the imaginary component of a. aI must be a numpy.ndarray with shapes compatible with aR and the b* arrays; when a is purely real the caller should pass an array of zeros of matching shape.", "default": ""}, "bR": {"type": "array", "items": {"type": "float"}, "description": "Real part of the right operand b. Supplies the real component of the complex matrix b used in the product a*b. In PySCF, bR often represents the real part of transformation or integral blocks. Must be a numpy.ndarray compatible for multiplication with aR/aI.", "default": ""}, "bI": {"type": "array", "items": {"type": "float"}, "description": "Imaginary part of the right operand b. Supplies the imaginary component of b. If b is real, pass an array of zeros of matching shape.", "default": ""}, "alpha": {"type": "float", "description": "Scalar multiplier applied to the product a*b. The function computes alpha * (a*b) where a and b are reconstructed from their real/imaginary parts. Default is 1. This parameter is commonly used in linear-algebra kernels within PySCF to scale contributions (for example, when accumulating scaled intermediates).", "default": 1}, "cR": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional real part of an existing accumulator matrix c that will be updated. If provided, the final cR returned equals alpha*Re(a*b) + beta*cR_initial (subject to shape compatibility). If None, the routine relies on the underlying ddot implementation to allocate and return a newly created numpy.ndarray for the real part of the result. Passing an existing array lets callers avoid allocations and reuse memory for performance in iterative chemistry algorithms.", "default": null}, "cI": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional imaginary part of an existing accumulator matrix c that will be updated analogously to cR. If provided, the final cI equals alpha*Im(a*b) + beta*cI_initial. If None, a new numpy.ndarray will be allocated by the underlying ddot calls. Use this to control memory reuse in performance-critical PySCF code paths.", "default": null}, "beta": {"type": "float", "description": "Scalar multiplier applied to the initial accumulator c (cR and cI). The final returned c equals alpha*(a*b) + beta*c_initial when cR and/or cI are provided. Default is 0, which ignores any initial accumulator content. This is consistent with common BLAS-style scaling semantics used across PySCF's numeric kernels.", "default": 0}}, "required": ["aR", "aI", "bR", "bI", "alpha", "cI", "beta", "cR"], "type": "any"}}, "type": "function"}], "query": "I’m doing a PySCF post-processing build of a complex intermediate using the low-level numpy_helper `zdotNN` routine (non-conjugated complex GEMM on split real/imag parts) to avoid complex dtype. I have a 2×3 complex coefficient block A and a 3×2 complex integral block B, each given as separate real/imag numpy arrays:\n\nA(real) = [[1.0, -0.5, 0.0], [0.2, 0.3, -0.1]]\nA(imag) = [[0.0, 0.1, -0.2], [0.3, 0.0, 0.4]]\n\nB(real) = [[0.9, -0.3], [0.0, 0.5], [-0.4, 0.2]]\nB(imag) = [[0.1, 0.0], [-0.2, 0.3], [0.0, -0.1]]\n\nI also have an existing accumulator C0 (2×2) split into real/imag:\nC0(real) = [[0.5, -0.2], [0.1, 0.4]]\nC0(imag) = [[-0.3, 0.0], [0.2, -0.1]].\n\nInitially I was going to *add* half of C0 into the new product, i.e. C = (three-quarters)*(A@B) + (one-half)*C0. But I just realized my accumulator buffer is actually stored with the opposite sign convention in the imaginary channel (it came from a -i phase in an earlier AO→MO transform), so if I add it as-is I’ll flip the sign of the imaginary contribution. Keep everything else the same (same A, B, and scaling for the A@B term), but update C by combining the accumulator in the way that compensates for that sign mismatch. Use `zdotNN` and return the resulting updated real and imaginary parts of C.", "answers": "[{\"name\": \"pyscf_lib_numpy_helper_zdotNN\", \"arguments\": {\"aR\": [[1.0, -0.5, 0.0], [0.2, 0.3, -0.1]], \"aI\": [[0.0, 0.1, -0.2], [0.3, 0.0, 0.4]], \"bR\": [[0.9, -0.3], [0.0, 0.5], [-0.4, 0.2]], \"bI\": [[0.1, 0.0], [-0.2, 0.3], [0.0, -0.1]], \"alpha\": 0.75, \"cR\": [[0.5, -0.2], [0.1, 0.4]], \"cI\": [[0.3, 0.0], [-0.2, 0.1]], \"beta\": 0.5}}]"}
{"func_name": "pyscf_lo_ibo_shell_str", "func_desc": "pyscf.lo.ibo.shell_str constructs a space-separated label string that defines which atomic shells of a given angular momentum l are treated as core (bracketed) and which are treated as valence (unbracketed). This helper is used in the PySCF lo.ibo (localized / intrinsic bond orbital) context to build textual shell specifications for partitioning atomic orbitals into core and valence shells when preparing localization, orbital analysis, or shell-based selections in quantum-chemistry workflows within the PySCF framework.\n    \n    Detailed behavior: The function uses an internal pattern table (cor_shell) that maps angular momentum l to a template string (for example, l=0 -> \"[{n}s]\", l=1 -> \"[{n}px] [{n}py] [{n}pz]\", l=2 -> d-shell templates, l=3 -> f-shell templates). Core shell labels are formatted using these templates and remain enclosed in square brackets to mark them as \"core\" in downstream parsers; valence shell labels are the same templates but with the brackets removed. Principal quantum numbers are assigned sequentially beginning at n = l + 1; the function first emits n_cor core shells (in increasing principal quantum number order) and then n_val valence shells, joining all formatted labels with single spaces. The function does not modify external state and returns the constructed string.", "tools": [{"function": {"description": "pyscf.lo.ibo.shell_str constructs a space-separated label string that defines which atomic shells of a given angular momentum l are treated as core (bracketed) and which are treated as valence (unbracketed). This helper is used in the PySCF lo.ibo (localized / intrinsic bond orbital) context to build textual shell specifications for partitioning atomic orbitals into core and valence shells when preparing localization, orbital analysis, or shell-based selections in quantum-chemistry workflows within the PySCF framework.\n\nDetailed behavior: The function uses an internal pattern table (cor_shell) that maps angular momentum l to a template string (for example, l=0 -> \"[{n}s]\", l=1 -> \"[{n}px] [{n}py] [{n}pz]\", l=2 -> d-shell templates, l=3 -> f-shell templates). Core shell labels are formatted using these templates and remain enclosed in square brackets to mark them as \"core\" in downstream parsers; valence shell labels are the same templates but with the brackets removed. Principal quantum numbers are assigned sequentially beginning at n = l + 1; the function first emits n_cor core shells (in increasing principal quantum number order) and then n_val valence shells, joining all formatted labels with single spaces. The function does not modify external state and returns the constructed string.", "name": "pyscf_lo_ibo_shell_str", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum index of the shell type to label. This integer selects which template in the internal pattern table to use (0 => s, 1 => p, 2 => d, 3 => f, etc., as available in the hard-coded table). The index must be a valid index into the internal pattern table; providing an l outside the table's range will raise an IndexError.", "default": ""}, "n_cor": {"type": "integer", "description": "Number of core shells to produce for the selected angular momentum. Core shells are produced with principal quantum numbers n = l+1, l+2, ..., l+n_cor and are formatted using the core templates (kept in square brackets, e.g. \"[4s]\"). Supplying n_cor <= 0 yields no core entries (the function will simply omit core labels).", "default": ""}, "n_val": {"type": "integer", "description": "Number of valence shells to produce for the selected angular momentum. Valence shells follow immediately after the core shells in principal quantum number, i.e. n = l+1+n_cor, ..., l+n_cor+n_val, and are formatted using the same templates with square brackets removed (e.g. \"5s\"). Supplying n_val <= 0 yields no valence entries.", "default": ""}}, "required": ["l", "n_cor", "n_val"], "type": "any"}}, "type": "function"}], "query": "I’m setting up an IBO localization for a 3d transition-metal complex in PySCF and need the shell-selection label string for the d-manifold (l=2). Initially I was going to freeze the first d shell as core and keep the next two as valence, but I realized our basis/ECP combo already absorbs what would have been that lowest d shell into the effective core, so I should *not* bracket that lowest d shell here. Instead, treat as core only the d shell whose principal quantum number is one higher than the lowest allowed for l=2, and then include exactly two additional d shells after that as valence (unbracketed). What shell label string should I generate?", "answers": "[{\"name\": \"pyscf_lo_ibo_shell_str\", \"arguments\": {\"l\": 2, \"n_cor\": 1, \"n_val\": 2}}]"}
{"func_name": "pyscf_lo_orth_lowdin", "func_desc": "Compute the Löwdin (symmetric) orthonormalization matrix for a basis with overlap matrix s.\n    \n    This function implements the Löwdin (symmetric) orthogonalization commonly used in quantum chemistry and in the PySCF framework to transform a non-orthogonal atomic-orbital (AO) basis into an orthonormal basis. Given the overlap matrix s (S_mu,nu = <mu|nu>), the function diagonalizes s, discards eigencomponents with eigenvalues at or below a numerical threshold (1e-15) to avoid division by (near) zero, and constructs the matrix c_lowdin = V_diag(1/sqrt(e)) V^H restricted to the retained subspace. The returned matrix c_lowdin can be used as the column matrix of new basis vectors so that the new basis |i> = sum_mu |mu> c_lowdin[mu,i] satisfies c_lowdin^H s c_lowdin = I on the retained subspace. In PySCF this is used during basis orthonormalization steps such as constructing orthonormal molecular orbitals from AO coefficients.", "tools": [{"function": {"description": "Compute the Löwdin (symmetric) orthonormalization matrix for a basis with overlap matrix s.\n\nThis function implements the Löwdin (symmetric) orthogonalization commonly used in quantum chemistry and in the PySCF framework to transform a non-orthogonal atomic-orbital (AO) basis into an orthonormal basis. Given the overlap matrix s (S_mu,nu = <mu|nu>), the function diagonalizes s, discards eigencomponents with eigenvalues at or below a numerical threshold (1e-15) to avoid division by (near) zero, and constructs the matrix c_lowdin = V_diag(1/sqrt(e)) V^H restricted to the retained subspace. The returned matrix c_lowdin can be used as the column matrix of new basis vectors so that the new basis |i> = sum_mu |mu> c_lowdin[mu,i] satisfies c_lowdin^H s c_lowdin = I on the retained subspace. In PySCF this is used during basis orthonormalization steps such as constructing orthonormal molecular orbitals from AO coefficients.", "name": "pyscf_lo_orth_lowdin", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "float"}, "description": "Square overlap matrix S of the basis functions (S_mu,nu = <mu|nu>). This must be a numeric NumPy array; in practice it is the AO overlap matrix produced by PySCF routines. The function expects s to be Hermitian (s == s.conj().T) and positive semi-definite within numerical precision. The code diagonalizes s with scipy.linalg.eigh and therefore will raise an error if s is not square or if the underlying LAPACK call fails. Very small or zero eigenvalues (<= 1e-15) are treated as linear dependencies and are excluded from the returned transformation to avoid dividing by zero.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "I’m setting up an AO→orthonormal AO transformation (PySCF-style Löwdin/symmetric orthogonalization) for a toy water/STO-3G overlap matrix, but the matrix I got out of the integral dump is a bit “dirty”: it’s Hermitian in theory, yet the printed values aren’t perfectly symmetric due to formatting/roundoff. Start by symmetrizing the overlap as (S + S^T)/2 before doing anything else.\n\nThen compute the Löwdin orthonormalization matrix C such that C^T S C = I in the retained subspace, using the standard symmetric orthogonalization recipe (eigendecompose S, form V diag(e^{-1/2}) V^T). However, don’t use the default numerical cutoff: I initially planned to keep everything above the usual ~1e-15, but this dump comes from a single-precision intermediate and the smallest eigenmode is unreliable. Instead, drop any eigencomponent whose eigenvalue is not safely larger than machine noise for single precision (i.e., the conventional float32-scale safety cutoff used to avoid blow-ups).\n\nUse this approximately-measured 4×4 matrix (note the intentional asymmetry in a couple entries):\n[[1.0, 0.2500000, 0.1000000, 0.0500000],\n [0.2499996, 1.0,      0.2000000, 0.1500000],\n [0.1000002, 0.1999997, 1.0,      0.3000000],\n [0.0500001, 0.1500004, 0.3000003, 1.0]]\n\nReturn the Löwdin transformation matrix C (real-valued) computed under these constraints.", "answers": "[{\"name\": \"pyscf_lo_orth_lowdin\", \"arguments\": {\"s\": [[1.0, 0.25, 0.1, 0.05], [0.2499996, 1.0, 0.2, 0.15], [0.1000002, 0.1999997, 1.0, 0.3], [0.0500001, 0.1500004, 0.3000003, 1.0]]}}]"}
{"func_name": "pyscf_lo_orth_weight_orth", "func_desc": "pyscf.lo.orth.weight_orth: Construct weight-scaled Lowdin-orthonormalization coefficients for a given overlap matrix.\n    \n    This function constructs a new set of basis-coefficient vectors c_{mu,i} that define orthonormal basis functions under the original atomic-orbital overlap matrix s, after applying a multiplicative weight per basis function. It implements the transformation\n    c = w * [ (w s w)^{-1/2} ],\n    where w is the diagonal weighting defined by the 1D array weight, s is the square overlap matrix, and (w s w)^{-1/2} is obtained by the Lowdin symmetric orthonormalization of the weighted overlap matrix s1 = w s w. In PySCF this routine is used in localized orbital and basis-transformation utilities to produce a new basis |mu> c_{mu i} whose columns are orthonormal with respect to the original quantum-chemical overlap s (i.e., c^T s c = I), enabling downstream procedures that require an orthonormal representation (for example, population analysis, projector construction, or subsequent orthogonal transformations in electronic structure workflows).", "tools": [{"function": {"description": "pyscf.lo.orth.weight_orth: Construct weight-scaled Lowdin-orthonormalization coefficients for a given overlap matrix.\n\nThis function constructs a new set of basis-coefficient vectors c_{mu,i} that define orthonormal basis functions under the original atomic-orbital overlap matrix s, after applying a multiplicative weight per basis function. It implements the transformation\nc = w * [ (w s w)^{-1/2} ],\nwhere w is the diagonal weighting defined by the 1D array weight, s is the square overlap matrix, and (w s w)^{-1/2} is obtained by the Lowdin symmetric orthonormalization of the weighted overlap matrix s1 = w s w. In PySCF this routine is used in localized orbital and basis-transformation utilities to produce a new basis |mu> c_{mu i} whose columns are orthonormal with respect to the original quantum-chemical overlap s (i.e., c^T s c = I), enabling downstream procedures that require an orthonormal representation (for example, population analysis, projector construction, or subsequent orthogonal transformations in electronic structure workflows).", "name": "pyscf_lo_orth_weight_orth", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "float"}, "description": "Square overlap matrix of the original non-orthogonal basis. This is the atomic-orbital overlap matrix used in electronic-structure calculations; it must be a two-dimensional square array with shape (n, n) where n is the number of basis functions. The routine treats s as the metric for inner products: the output coefficients satisfy c.T @ s @ c = I when Lowdin orthogonalization succeeds. If s is not symmetric positive-definite (for example, if it has zero or negative eigenvalues), the underlying Lowdin procedure may fail or produce non-physical results.", "default": ""}, "weight": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of multiplicative weights w applied to basis functions. This must have length n and real entries; it defines a diagonal scaling matrix W with W_{mu,mu} = weight[mu]. The function forms the weighted overlap s1 = W @ s @ W by elementwise multiplication weight[:,None] * s * weight, then orthonormalizes s1 and finally multiplies the orthonormalization matrix by W to return the final coefficients. The weights are typically used in PySCF to bias, scale, or normalize basis functions prior to orthogonalization (for example, to impose atomic-centered damping or population-derived scaling).", "default": ""}}, "required": ["s", "weight"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a localized-orbital workflow for H2O/STO-3G and need the weight-scaled Löwdin orthonormalization coefficients via `pyscf.lo.orth.weight_orth`, but my inputs are a bit messy.\n\nI have the (supposedly) symmetric AO overlap matrix below. However, I just noticed our parser sometimes breaks symmetry at the ~1e-3 level, so before orthonormalizing, **replace `s` by its symmetrized form** (average it with its transpose). Then construct the coefficient matrix\n\nc = w * (w s w)^(-1/2)\n\nusing the provided diagonal weights.\n\nOverlap matrix (as read from file):\n\ns = [[1.000, 0.235, 0.210, 0.000, 0.000, 0.045, 0.045],\n     [0.236, 1.000, 0.180, 0.020, 0.020, 0.030, 0.030],\n     [0.210, 0.181, 1.000, 0.015, 0.015, 0.025, 0.025],\n     [0.000, 0.020, 0.015, 1.000, 0.300, 0.010, 0.005],\n     [0.000, 0.020, 0.015, 0.299, 1.000, 0.005, 0.010],\n     [0.045, 0.030, 0.025, 0.010, 0.005, 1.000, 0.150],\n     [0.045, 0.030, 0.025, 0.005, 0.010, 0.149, 1.000]]\n\nWeights:\nweight = [1.5, 1.5, 1.5, 1.0, 1.0, 0.8, 0.8]\n\nCompute the weight-scaled Löwdin orthonormalization coefficients `c` for the **symmetrized** overlap matrix so that the resulting columns satisfy c^T s c = I under the original overlap metric (up to numerical precision).", "answers": "[{\"name\": \"pyscf_lo_orth_weight_orth\", \"arguments\": {\"s\": [[1.0, 0.2355, 0.21, 0.0, 0.0, 0.045, 0.045], [0.2355, 1.0, 0.1805, 0.02, 0.02, 0.03, 0.03], [0.21, 0.1805, 1.0, 0.015, 0.015, 0.025, 0.025], [0.0, 0.02, 0.015, 1.0, 0.2995, 0.01, 0.005], [0.0, 0.02, 0.015, 0.2995, 1.0, 0.005, 0.01], [0.045, 0.03, 0.025, 0.01, 0.005, 1.0, 0.1495], [0.045, 0.03, 0.025, 0.005, 0.01, 0.1495, 1.0]], \"weight\": [1.5, 1.5, 1.5, 1.0, 1.0, 0.8, 0.8]}}]"}
{"func_name": "pyscf_mcpdft__libxc_split_x_c_comma", "func_desc": "Split an xc code string into two separate identifiers: one for exchange and one for\n    correlation, used by the PySCF mcpdft._libxc plumbing that maps user-specified\n    XC identifiers to LibXC exchange/correlation components. The function locates a\n    comma in the input string or in any alias/lookup it resolves, upper-cases the\n    input for alias matching, consults module tables for numeric LibXC codes and\n    aliases (XC_ALIAS, XC_CODES, XC_KEYS, INTCODES_TYPES, INTCODES_HYB), and enforces\n    a recursion limit when expanding aliases. This is used when parsing an\n    xc specification passed to higher-level MCPDFT/DFT routines so that the exchange\n    and correlation parts can be handled separately by LibXC or other backends.", "tools": [{"function": {"description": "Split an xc code string into two separate identifiers: one for exchange and one for\ncorrelation, used by the PySCF mcpdft._libxc plumbing that maps user-specified\nXC identifiers to LibXC exchange/correlation components. The function locates a\ncomma in the input string or in any alias/lookup it resolves, upper-cases the\ninput for alias matching, consults module tables for numeric LibXC codes and\naliases (XC_ALIAS, XC_CODES, XC_KEYS, INTCODES_TYPES, INTCODES_HYB), and enforces\na recursion limit when expanding aliases. This is used when parsing an\nxc specification passed to higher-level MCPDFT/DFT routines so that the exchange\nand correlation parts can be handled separately by LibXC or other backends.", "name": "pyscf_mcpdft__libxc_split_x_c_comma", "parameters": {"properties": {"xc": {"type": "string", "description": "The user-supplied XC code string to split. In practical PySCF\nuse this is the textual identifier for a density-functional\napproximation (for example a LibXC name, an alias, or a numeric code\nmapped to a LibXC functional). The function treats an empty string\nspecially (returns two empty strings), converts the input to upper\ncase for alias/code lookup, and accepts strings that already contain a\ncomma separating exchange and correlation. The parameter is read-only;\nno other parameters are modified.", "default": ""}}, "required": ["xc"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up PySCF’s `mcpdft._libxc` plumbing and initially planned to just pass the obvious combined LibXC-style XC spec for “B88 exchange + LYP correlation” into the comma-splitting utility. But I just realized our user input sanitization step sometimes replaces the comma with a semicolon (and we don’t control that upstream), while the splitter only recognizes a comma either in the raw input string or after alias expansion. So instead of feeding the literal combined identifier, pass an XC string that will still deterministically resolve (via the alias/lookup tables and upper-casing) into exactly those two components *with a comma present after resolution*, staying within the recursion limit. Show the exact call into the splitter utility with the corrected `xc` argument.", "answers": "[{\"name\":\"pyscf_mcpdft__libxc_split_x_c_comma\",\"arguments\":{\"xc\":\"BLYP\"}}]"}
{"func_name": "pyscf_mcpdft_otfnal_make_hybrid_fnal", "func_desc": "make_hybrid_fnal generates a PySCF-style hybrid exchange–correlation (XC) functional specification by combining a base libxc functional code (xc_code) with a small set of \"hybridization\" parameters (hyb) according to one of several predefined combination rules (hyb_type). This convenience wrapper is used in the mcpdft.otfnal code path of PySCF to produce hybrid functionals suitable for multiconfigurational pair-density functional theory workflows; it produces a functional specification that is ultimately passed to make_scaled_fnal and then consumed by PySCF's DFT machinery (pyscf.dft.libxc) for evaluation of exchange and correlation contributions.", "tools": [{"function": {"description": "make_hybrid_fnal generates a PySCF-style hybrid exchange–correlation (XC) functional specification by combining a base libxc functional code (xc_code) with a small set of \"hybridization\" parameters (hyb) according to one of several predefined combination rules (hyb_type). This convenience wrapper is used in the mcpdft.otfnal code path of PySCF to produce hybrid functionals suitable for multiconfigurational pair-density functional theory workflows; it produces a functional specification that is ultimately passed to make_scaled_fnal and then consumed by PySCF's DFT machinery (pyscf.dft.libxc) for evaluation of exchange and correlation contributions.\n", "name": "pyscf_mcpdft_otfnal_make_hybrid_fnal", "parameters": {"properties": {"xc_code": {"type": "string", "description": "The base functional identifier as used by pyscf.dft.libxc. This string names the underlying semilocal functional (for example a libxc label) whose exchange and correlation parts will be combined with exact (Hartree–Fock) exchange according to the hybridization rule. The function will raise an exception if xc_code already denotes a hybrid-type functional or if it contains a kinetic-energy functional component (these checks are performed by downstream routines that validate the libxc specification).", "default": ""}, "hyb": {"type": "float", "description": "Parameter(s) defining the hybridization. By default this is documented as a float scalar value, and the implementation will accept a scalar or a sequence-like object. If a scalar is provided it is coerced to a single-element sequence internally. The entries of hyb control the weight of exact exchange (HF) and the scaling of the semilocal exchange and correlation components according to the chosen hyb_type. Different hyb_type values impose specific length requirements:\n- For hyb_type 0, 1, 2, or 3: len(hyb) must be 1 (a single float); the single value is denoted hyb[0] in the formulas below.\n- For hyb_type 4: len(hyb) must be 2; the first element hyb[0] is the primary mixing parameter a and hyb[1] is used to define an exponent for correlation scaling.", "default": ""}, "hyb_type": {"type": "integer", "description": "The hybrid-construction rule to apply. The default is 1. hyb_type may be supplied as an integer code or as one of the string names below (case-sensitive, matched against the internal mapping). The implementation accepts the following codes and applies the corresponding construction; assertion failures will occur if the supplied hyb sequence length does not match the requirement of the selected type, a KeyError will occur if a string name is not recognized, and a RuntimeError is raised for an undefined numeric type outside the documented set.\n- 0 or 'translation': \"translation\" hybrid. Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=1). Requires a single-element hyb.\n- 1 or 'average': \"average\" hybrid (default). Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + (1-hyb)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=(1-hyb[0])). Requires a single-element hyb. This choice is motivated by the idea that hyb = 1 should recover the underlying wave function energy.\n- 2 or 'diagram': \"diagram\" hybrid. Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = c_code (no HF mixing in correlation). Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=0, fnal_x=(1-hyb[0]), fnal_c=1). Requires a single-element hyb.\n- 3 or 'lambda': \"lambda\" hybrid (as used in arXiv:1911.11162v1 / double-hybrid style constructions). Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + (1-hyb^2)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=(1-(hyb[0]*hyb[0]))). Requires a single-element hyb.\n- 4 or 'scaling': \"scaling\" hybrid motivated by density-scaling inequalities of Levy and Perdew. Let a = hyb[0] and define b = a**(1 + hyb[1]). Construction used: exchange part = a*HF + (1-a)*x_code; correlation part = a*HF + (1-b)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=a, hyb_c=a, fnal_x=(1-a), fnal_c=(1-b)). Requires two-element hyb (hyb[0] and hyb[1]) and implements an empirical correlation exponent 1+hyb[1].", "default": 1}}, "required": ["xc_code", "hyb", "hyb_type"], "type": "any"}}, "type": "function"}], "query": "I’m setting up an MCPDFT run for a 3d transition‑metal complex and initially planned to use a PBE‑family hybrid with the *average* hybridization rule and the usual “quarter exact‑exchange” recipe. Wait—our downstream benchmarking script assumes the hybrid fraction is specified as the complement of the DFT exchange fraction, and it also silently drops one percentage point because it counts the 0% endpoint as a bin. So: still use the PBE exchange–correlation base and the *average* hybrid scheme, but set the exact‑exchange mixing to the value you get after converting “one quarter” into percent, applying that one‑point drop, then converting back to a fraction for the `hyb` parameter. Generate the PySCF‑style hybrid XC functional spec with `make_hybrid_fnal`.", "answers": "[{\"name\": \"pyscf_mcpdft_otfnal_make_hybrid_fnal\", \"arguments\": {\"xc_code\": \"PBE\", \"hyb\": 0.24, \"hyb_type\": 1}}]"}
{"func_name": "pyscf_mcpdft_otfnal_make_scaled_fnal", "func_desc": "make a new libxc-style exchange-correlation code string that scales and mixes\n    wavefunction and density-functional components for use in PySCF MCPDFT\n    (on-top functional) workflows.\n    \n    This function constructs an xc_code string that represents a functional of the\n    form\n    Exc = hyb_x * E_x[Psi] + fnal_x * E_x[rho] + hyb_c * E_c[Psi] + fnal_c * E_c[rho],\n    where E_x[Psi]/E_c[Psi] are exchange/correlation energy contributions obtained\n    from a wave function (Psi) and E_x[rho]/E_c[rho] are exchange/correlation\n    density functionals evaluated by libxc (rho). This is used in the PySCF\n    mcpdft.otfnal context to build mixed functionals that combine explicit\n    wavefunction exchange/correlation (e.g., HF exchange or multiconfigurational\n    wave function components) with density-functional contributions provided by\n    libxc. The decomposition of any wavefunction energy E[Psi] into exchange (E_x)\n    and correlation (E_c) parts is arbitrary and must be consistent with how the\n    caller interprets those components.", "tools": [{"function": {"description": "make a new libxc-style exchange-correlation code string that scales and mixes\nwavefunction and density-functional components for use in PySCF MCPDFT\n(on-top functional) workflows.\n\nThis function constructs an xc_code string that represents a functional of the\nform\nExc = hyb_x * E_x[Psi] + fnal_x * E_x[rho] + hyb_c * E_c[Psi] + fnal_c * E_c[rho],\nwhere E_x[Psi]/E_c[Psi] are exchange/correlation energy contributions obtained\nfrom a wave function (Psi) and E_x[rho]/E_c[rho] are exchange/correlation\ndensity functionals evaluated by libxc (rho). This is used in the PySCF\nmcpdft.otfnal context to build mixed functionals that combine explicit\nwavefunction exchange/correlation (e.g., HF exchange or multiconfigurational\nwave function components) with density-functional contributions provided by\nlibxc. The decomposition of any wavefunction energy E[Psi] into exchange (E_x)\nand correlation (E_c) parts is arbitrary and must be consistent with how the\ncaller interprets those components.", "name": "pyscf_mcpdft_otfnal_make_scaled_fnal", "parameters": {"properties": {"xc_code": {"type": "string", "description": "libxc-style xc identifier string as used by pyscf.dft.libxc.\nThis string must represent a non-hybrid density functional and must\nnot include a kinetic-energy functional component. The function will\nparse xc_code into separate exchange and correlation formulas using\nlibxc helpers (split_x_c_comma and parse_xc_formula). If xc_code is\nalready a hybrid according to libxc, a RuntimeError is raised.", "default": ""}, "hyb_x": {"type": "float", "description": "fraction of the exchange contribution taken from the\nwavefunction (E_x[Psi]). Typical values are in [0,1] but the function\ndoes not enforce numeric bounds beyond Python float semantics. Default\nis 0. A nonzero hyb_x causes an 'HF' term with coefficient hyb_x to be\nappended to the assembled exchange formula.", "default": 0}, "hyb_c": {"type": "float", "description": "fraction of the correlation contribution taken from the\nwavefunction (E_c[Psi]). Typical values are in [0,1] but the function\ndoes not enforce numeric bounds beyond Python float semantics. Default\nis 0. A nonzero hyb_c causes an 'HF' term with coefficient hyb_c to be\nappended to the assembled correlation formula.", "default": 0}, "fnal_x": {"type": "float", "nullable": true, "description": "fraction of the exchange contribution taken from\nthe density functional (E_x[rho]). If None (the default) it is set to\n1 - hyb_x. When fnal_x != 1, all parsed exchange factor coefficients\nfrom xc_code are multiplied by fnal_x before any HF term for hyb_x is\nappended. This scales the contribution of the libxc exchange functional\nrelative to the wavefunction exchange fraction hyb_x.", "default": null}, "fnal_c": {"type": "float", "nullable": true, "description": "fraction of the correlation contribution taken\nfrom the density functional (E_c[rho]). If None (the default) it is\nset to 1 - hyb_c. When fnal_c != 1, all parsed correlation factor\ncoefficients from xc_code are multiplied by fnal_c before any HF term\nfor hyb_c is appended. This scales the contribution of the libxc\ncorrelation functional relative to the wavefunction correlation\nfraction hyb_c.", "default": null}}, "required": ["xc_code", "fnal_x", "fnal_c", "hyb_x", "hyb_c"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF MCPDFT on-top functional and initially planned to hard-code the mix as 30% wavefunction exchange and 10% wavefunction correlation on top of a PBE-based DFT piece. But I just noticed our reference implementation assumes the **DFT exchange+correlation weights must sum to the complement of the wavefunction weights**, otherwise we double-count total XC (this is for a constrained-energy scan where the total XC prefactor must remain 1.0 within numerical noise). So: start from the non-hybrid base functional string 'PBE,PBE'. Keep the wavefunction exchange and correlation fractions as originally intended. For the DFT scaling factors, do **not** use the previously proposed downscales; instead set the DFT exchange scale to the complement of the wavefunction exchange fraction, and set the DFT correlation scale to the complement of the wavefunction correlation fraction. Please generate the resulting mixed libxc-style xc_code string via the appropriate PySCF MCPDFT on-top functional utility.\n\n(Clarification: the complements are taken separately for exchange and correlation channels, i.e., each channel’s wavefunction+DFT weights should add to unity.)", "answers": "[{\"name\": \"pyscf_mcpdft_otfnal_make_scaled_fnal\", \"arguments\": {\"xc_code\": \"PBE,PBE\", \"hyb_x\": 0.3, \"hyb_c\": 0.1, \"fnal_x\": 0.7, \"fnal_c\": 0.9}}]"}
{"func_name": "pyscf_pbc_cc_eom_kccsd_ghf_spin2spatial_ea_doublet", "func_desc": "Convert R1/R2 electron-attached (EA) amplitudes from a spin-orbital\n    representation to a spatial-orbital representation separated into spin\n    blocks for alpha/beta components, for use in k-point periodic boundary\n    condition (PBC) EOM-KCCSD with generalized Hartree–Fock (GHF).\n    \n    This function is used in the pyscf.pbc.cc.eom_kccsd_ghf module to transform\n    spin-orbital EA excitation amplitudes (R1, R2) into spatial-orbital blocks\n    used by downstream routines that expect separate alpha and beta virtual\n    subspaces per k-point and spin. The conversion observes momentum (k-point)\n    conservation via kconserv and selects the amplitudes for a particular\n    k-point shift given by kshift. The resulting arrays preserve the input\n    dtype and are arranged to match the blocked spin structure produced from\n    orbspin (0 for alpha, 1 for beta) described in the PBC orbital indexing\n    used throughout the EOM-KCCSD implementation.", "tools": [{"function": {"description": "Convert R1/R2 electron-attached (EA) amplitudes from a spin-orbital\nrepresentation to a spatial-orbital representation separated into spin\nblocks for alpha/beta components, for use in k-point periodic boundary\ncondition (PBC) EOM-KCCSD with generalized Hartree–Fock (GHF).\n\nThis function is used in the pyscf.pbc.cc.eom_kccsd_ghf module to transform\nspin-orbital EA excitation amplitudes (R1, R2) into spatial-orbital blocks\nused by downstream routines that expect separate alpha and beta virtual\nsubspaces per k-point and spin. The conversion observes momentum (k-point)\nconservation via kconserv and selects the amplitudes for a particular\nk-point shift given by kshift. The resulting arrays preserve the input\ndtype and are arranged to match the blocked spin structure produced from\norbspin (0 for alpha, 1 for beta) described in the PBC orbital indexing\nused throughout the EOM-KCCSD implementation.", "name": "pyscf_pbc_cc_eom_kccsd_ghf_spin2spatial_ea_doublet", "parameters": {"properties": {"r1": {"type": "array", "items": {"type": "float"}, "description": "Spin-orbital R1 electron-attachment amplitude vector\nfor the selected k-point block. This 1-D array supplies the EA\nsingle-excitation amplitudes expressed in spin-orbital indexing for\nthe virtual orbitals associated with the active k-point shift (kshift).\nThe function slices r1 into alpha and beta virtual components using\norbspin[kshift] (values 0 for alpha, 1 for beta). The dtype of r1 is\npreserved in the returned r1a/r1b arrays. Passing r1 with a length or\nindex ordering inconsistent with orbspin for the chosen kshift will\nraise IndexError or produce incorrect slicing.", "default": ""}, "r2": {"type": "array", "items": {"type": "float"}, "description": "Spin-orbital R2 electron-attachment double-excitation\namplitude tensor with shape (nkpts, nkpts, nocc, nvir, nvir). In the\ncode this shape is assumed and used to infer nkpts, nocc, and nvir.\nr2[kj, ka] is expected to contain the amplitudes coupling occupied\norbitals at k-point kj to two virtual spin orbitals distributed over\nk-point ka and the momentum-conserving partner. The function reshapes\nand partitions r2 into four spin-blocked spatial-orbital tensors:\nr2aaa, r2aba, r2bab, r2bbb (see Returns). The dtype and numeric values\nof r2 are preserved in the outputs. If r2 does not have the expected\ndimensionality, a ValueError or IndexError will be raised.", "default": ""}, "kconserv": {"type": "array", "items": {"type": "float"}, "description": "Integer ndarray used to enforce momentum\n(k-point) conservation in PBC calculations. kconserv is indexed as\nkconserv[kshift, ka, kj] and must provide the partner k-point index\nkb that, together with kshift, ka, and kj, satisfies crystal momentum\nconservation for the two-virtual-orbital block. The array must be\nconsistent with the nkpts inferred from r2; otherwise indexing errors\nwill occur.", "default": ""}, "kshift": {"type": "integer", "description": "Integer index of the target k-point shift (the active k-point\nwhere the single EA amplitude r1 is provided). This value selects the\nvirtual-orbital subsets (via orbspin) and the appropriate slice of r1\nto produce r1a (alpha) and r1b (beta). kshift must be in the range\n[0, nkpts-1] where nkpts is derived from r2; out-of-range values will\nraise IndexError.", "default": ""}, "orbspin": {"type": "array", "items": {"type": "float"}, "description": "Array-like object of length nkpts (or indexable\nby k-point) where orbspin[k] is an integer array labeling each spatial\norbital at k-point k by spin: 0 for alpha and 1 for beta. The first\nnocc entries of orbspin[k] correspond to occupied-spin labels and the\nremaining entries correspond to virtual-spin labels. The function uses\norbspin to build index lists (idxoa, idxob, idxva, idxvb) that define\nthe mapping from spin-orbital ordering to spatial-orbital alpha/beta\nsubspaces. orbspin must contain only 0 and 1 values and match the\norbital ordering assumed by r1 and r2; inconsistent labels will lead\nto incorrect results or IndexError.", "default": ""}}, "required": ["r1", "r2", "kconserv", "kshift", "orbspin"], "type": "any"}}, "type": "function"}], "query": "I’m trying to debug a periodic EOM-KCCSD(GHF) EA pipeline with 2 k-points, but my upstream code is messy. Please take the provided spin-orbital EA amplitudes (R1, R2) and convert them to the spatial-orbital alpha/beta spin-blocked form used by downstream k-point PBC routines, enforcing momentum conservation using kconserv and the supplied orbspin labeling.\n\nInitially I thought the transformation should be done for the *second* k-point shift, but I realized the downstream Dyson-orbital check is indexing shifts from zero, so I actually need the shift that is exactly one less than what I first intended.\n\nData (assume any unspecified entries are exactly zero):\n- nkpts = 2, nocc = 1, nvir(so) = 4 at the selected shift.\n- R1 (spin-orbital EA singles at the selected shift): [0.10, -0.05, 0.20, 0.00]\n- R2 (spin-orbital EA doubles): shape (2, 2, 1, 4, 4)\n  - R2[0,0,0,:,:] = [[0.01, 0.00, -0.02, 0.00],\n                     [0.00, 0.03,  0.00, 0.04],\n                     [-0.02,0.00,  0.05, 0.00],\n                     [0.00, 0.04,  0.00, 0.06]]\n  - R2[0,1,0,:,:] = [[-0.01, 0.02, 0.00, 0.00],\n                     [ 0.02, 0.00, 0.03, 0.00],\n                     [ 0.00, 0.03, 0.00, 0.01],\n                     [ 0.00, 0.00, 0.01, 0.00]]\n  - R2[1,0,0,:,:] = [[0.00, -0.02, 0.01, 0.00],\n                     [-0.02, 0.00, 0.00, 0.02],\n                     [0.01, 0.00, 0.00, 0.03],\n                     [0.00, 0.02, 0.03, 0.00]]\n  - R2[1,1,0,:,:] = [[0.04, 0.00, 0.00, -0.01],\n                     [0.00, 0.02, 0.01,  0.00],\n                     [0.00, 0.01, 0.03,  0.00],\n                     [-0.01,0.00, 0.00,  0.05]]\n- kconserv = [[[0,1],[1,0]],[[1,0],[0,1]]]\n- orbspin: for each k-point k, orbspin[k] = [0, 0, 1, 0, 1] (1 occupied + 4 virtual spin orbitals with mixed alpha/beta)\n\nReturn the converted, spin-blocked spatial-orbital EA amplitudes for the corrected k-point shift.", "answers": "[{\"name\": \"pyscf_pbc_cc_eom_kccsd_ghf_spin2spatial_ea_doublet\", \"arguments\": {\"r1\": [0.1, -0.05, 0.2, 0.0], \"r2\": [[[[[0.01, 0.0, -0.02, 0.0], [0.0, 0.03, 0.0, 0.04], [-0.02, 0.0, 0.05, 0.0], [0.0, 0.04, 0.0, 0.06]]], [[[-0.01, 0.02, 0.0, 0.0], [0.02, 0.0, 0.03, 0.0], [0.0, 0.03, 0.0, 0.01], [0.0, 0.0, 0.01, 0.0]]]], [[[[0.0, -0.02, 0.01, 0.0], [-0.02, 0.0, 0.0, 0.02], [0.01, 0.0, 0.0, 0.03], [0.0, 0.02, 0.03, 0.0]]], [[[0.04, 0.0, 0.0, -0.01], [0.0, 0.02, 0.01, 0.0], [0.0, 0.01, 0.03, 0.0], [-0.01, 0.0, 0.0, 0.05]]]]], \"kconserv\": [[[0, 1], [1, 0]], [[1, 0], [0, 1]]], \"kshift\": 0, \"orbspin\": [[0, 0, 1, 0, 1], [0, 0, 1, 0, 1]]}}]"}
{"func_name": "pyscf_pbc_df_rsdf_helper_remove_exp_basis", "func_desc": "Remove Gaussian primitive functions from a PySCF GTO basis whose exponents lie outside the open interval (amin, amax).\n    \n    This function operates on basis specifications used throughout PySCF (for molecular or periodic/ PBC calculations) in the GTO basis format (as stored in mol._basis or cell._basis). Each primitive exponent is a float that controls the radial extent of a Gaussian-type orbital (GTO) primitive; filtering primitives by exponent is useful in practice to remove very diffuse or very tight primitives that can cause numerical instability, near-linear dependence, or excessive computational cost in electronic structure calculations.", "tools": [{"function": {"description": "Remove Gaussian primitive functions from a PySCF GTO basis whose exponents lie outside the open interval (amin, amax).\n\nThis function operates on basis specifications used throughout PySCF (for molecular or periodic/ PBC calculations) in the GTO basis format (as stored in mol._basis or cell._basis). Each primitive exponent is a float that controls the radial extent of a Gaussian-type orbital (GTO) primitive; filtering primitives by exponent is useful in practice to remove very diffuse or very tight primitives that can cause numerical instability, near-linear dependence, or excessive computational cost in electronic structure calculations.", "name": "pyscf_pbc_df_rsdf_helper_remove_exp_basis", "parameters": {"properties": {"basis": {"type": "array", "items": {"type": "any"}, "description": "PySCF GTO basis format describing shells and primitives for one atom or for an entire system. This matches the format used in mol._basis or cell._basis. Examples of accepted shapes are a list of shells for a single atom, e.g. [[0, (1.5, 1.)], [1, (0.7, 1.)]], or a dictionary mapping atomic symbols to such lists, e.g. {\"C\": [[0, (1.5, 1.)], [1, (0.7, 1.)]], \"H\": [[0, (0.3, 1.)]]}. Each primitive exponent in these structures is a float and represents the Gaussian exponent of that primitive.", "default": ""}, "amin": {"type": "float", "nullable": true, "description": "Lower exponent threshold. Primitives with exponents <= amin will be removed. If amin is not provided (None), the code treats amin as 0.0 by default, meaning no removal on the lower side except for non-positive exponents. Documented default in the function signature is None.", "default": null}, "amax": {"type": "float", "nullable": true, "description": "Upper exponent threshold. Primitives with exponents >= amax will be removed. If amax is not provided (None), the code treats amax as +infinity by default, meaning no removal on the upper side. Documented default in the function signature is None.", "default": null}}, "required": ["basis", "amin", "amax"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a periodic PySCF run and need to sanitize a hand-edited GTO basis before it blows up the SCF with near-linear dependence. Initially I was going to remove primitives using inclusive cutoffs (i.e., throw away anything at or beyond the bounds), but I re-checked the helper’s contract and it keeps only exponents strictly inside the allowed window, so anything exactly on either boundary must be treated as invalid too. \n\nPlease filter the basis dict below for C and H by removing any primitive whose exponent is NOT strictly between the lower and upper bounds. Use the same numerical bounds as in my notes (lower bound: the diffuse threshold; upper bound: the tight threshold). Return the cleaned basis in PySCF GTO format.\n\nBasis:\n{\n  \"C\": [\n    [0, [71.616837, 0.15432897], [13.045096, 0.53532814], [3.5305122, 0.44463454], [0.06, 0.02], [0.08, 0.03]],\n    [1, [2.9412494, 0.15591627], [0.6834831, 0.60768372], [0.2222899, 0.39195739], [40.0, 0.02], [45.0, 0.01]]\n  ],\n  \"H\": [\n    [0, [13.010701, 0.019685], [1.9622572, 0.137977], [0.44453796, 0.478148], [0.12194962, 0.50124], [0.08, 0.04], [0.05, 0.05]]\n  ]\n}", "answers": "[{\"name\":\"pyscf_pbc_df_rsdf_helper_remove_exp_basis\",\"arguments\":{\"basis\":{\"C\":[[0,[71.616837,0.15432897],[13.045096,0.53532814],[3.5305122,0.44463454],[0.06,0.02],[0.08,0.03]],[1,[2.9412494,0.15591627],[0.6834831,0.60768372],[0.2222899,0.39195739],[40.0,0.02],[45.0,0.01]]],\"H\":[[0,[13.010701,0.019685],[1.9622572,0.137977],[0.44453796,0.478148],[0.12194962,0.50124],[0.08,0.04],[0.05,0.05]]]},\"amin\":0.08,\"amax\":40.0}}]"}
{"func_name": "pyscf_pbc_grad_kuhf_make_rdm1e", "func_desc": "Compute the energy-weighted one-electron reduced density matrix for k-point\n    unrestricted Hartree–Fock (KUHF) gradient calculations.\n    \n    This function is part of the PySCF periodic-boundary-condition gradient machinery (pyscf.pbc.grad.kuhf).\n    It constructs the energy-weighted one-particle reduced density matrix (1-RDM) for an unrestricted\n    (k-point) calculation by delegating the per-spin construction to the k-point restricted-HF\n    gradient helper pyscf.pbc.grad.krhf.make_rdm1e and stacking the two spin components.\n    The resulting array is used in analytic gradient evaluations to accumulate contributions\n    that depend on orbital energies, coefficients, and occupations for each spin channel.", "tools": [{"function": {"description": "Compute the energy-weighted one-electron reduced density matrix for k-point\nunrestricted Hartree–Fock (KUHF) gradient calculations.\n\nThis function is part of the PySCF periodic-boundary-condition gradient machinery (pyscf.pbc.grad.kuhf).\nIt constructs the energy-weighted one-particle reduced density matrix (1-RDM) for an unrestricted\n(k-point) calculation by delegating the per-spin construction to the k-point restricted-HF\ngradient helper pyscf.pbc.grad.krhf.make_rdm1e and stacking the two spin components.\nThe resulting array is used in analytic gradient evaluations to accumulate contributions\nthat depend on orbital energies, coefficients, and occupations for each spin channel.", "name": "pyscf_pbc_grad_kuhf_make_rdm1e", "parameters": {"properties": {"mo_energy": {"type": "array", "items": {"type": "any"}, "description": "Per-spin molecular orbital energies. This must be an indexable\narray-like object with two elements corresponding to the two spin channels\n(conventionally alpha at index 0 and beta at index 1). Each element is passed\nas the energy argument to pyscf.pbc.grad.krhf.make_rdm1e for that spin and therefore\nshould have the shape and dtype expected by that helper (typically an array of\norbital energies for the k-point/periodic calculation). The values determine the\nenergy weighting applied to the occupied orbitals when forming the energy-weighted 1-RDM.", "default": ""}, "mo_coeff": {"type": "array", "items": {"type": "any"}, "description": "Per-spin molecular orbital coefficient arrays. This must be an\nindexable array-like object with two elements (alpha at index 0, beta at index 1).\nEach element contains the MO coefficients for the corresponding spin and is forwarded\nto pyscf.pbc.grad.krhf.make_rdm1e. The coefficients define the transformation from\natomic/basis representation to the molecular-orbital basis and thus determine the\nmatrix representation of the energy-weighted 1-RDM in the chosen orbital basis.", "default": ""}, "mo_occ": {"type": "array", "items": {"type": "any"}, "description": "Per-spin molecular orbital occupation numbers. This must be an\nindexable array-like object with two elements (alpha at index 0, beta at index 1).\nEach element supplies occupation information (typically 0, 1, 2 or fractional occupations\nfor k-point sampling) to pyscf.pbc.grad.krhf.make_rdm1e so that only occupied MOs\ncontribute to the energy-weighted 1-RDM. Occupation values control which orbitals\nare included and with what weight in gradient-related integrals.", "default": ""}}, "required": ["mo_energy", "mo_coeff", "mo_occ"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a KUHF k-point gradient sanity check in PySCF (pyscf.pbc.grad.kuhf). Initially I was going to just feed the alpha/beta MO energies, coefficients, and occupations directly into the energy-weighted 1e-RDM builder, but I realized one of the provided occupation arrays is inconsistent with a basic aufbau filling for a closed-shell-like starting guess: for that spin channel, the number of occupied orbitals should match the count of *negative-energy* orbitals in that channel (treat strictly negative energies as occupied candidates; nonnegative as virtuals). Please compute the KUHF energy-weighted 1-electron RDM (rdm1e) using the same MO energies and MO coefficients as below, but first adjust only that inconsistent spin-channel occupation vector to satisfy the rule (keep the other spin channel’s occupations exactly as given).\n\nMO data:\n- Alpha orbital energies: [-0.72, -0.31, 0.15]\n- Beta orbital energies:  [-0.68, -0.28, 0.22]\n- Alpha MO coefficients: [[0.80, 0.10, 0.00],[0.20, 0.90, 0.30],[0.00, 0.40, 0.95]]\n- Beta MO coefficients:  [[0.78, 0.12, 0.02],[0.25, 0.88, 0.28],[0.01, 0.45, 0.93]]\n- Occupations (as provided): alpha [1,1,0], beta [1,0,0]\n\nThen call the KUHF helper to build the stacked per-spin energy-weighted 1-RDM for use in analytic gradients.", "answers": "[{\"name\": \"pyscf_pbc_grad_kuhf_make_rdm1e\", \"arguments\": {\"mo_energy\": [[-0.72, -0.31, 0.15], [-0.68, -0.28, 0.22]], \"mo_coeff\": [[[0.8, 0.1, 0.0], [0.2, 0.9, 0.3], [0.0, 0.4, 0.95]], [[0.78, 0.12, 0.02], [0.25, 0.88, 0.28], [0.01, 0.45, 0.93]]], \"mo_occ\": [[1, 1, 0], [1, 1, 0]]}}]"}
{"func_name": "pyscf_pbc_gw_kugw_ac_AC_twopole_diag", "func_desc": "Analytic continuation to the real frequency axis for orbital-dependent self-energy data using a two-pole model fitted independently for each orbital. This routine is used in the periodic-boundary-condition (PBC) GW code path of PySCF (pyscf.pbc.gw) to convert frequency-dependent self-energy samples (sigma) defined on a discrete frequency grid (omega) into a compact set of fit parameters that parametrize an analytic two-pole approximation per orbital. The fitted parameters are intended for downstream evaluation of the real-axis self-energy or spectral functions in GW calculations and match the parameter vector expected by the two_pole_fit residual function used in the module.", "tools": [{"function": {"description": "Analytic continuation to the real frequency axis for orbital-dependent self-energy data using a two-pole model fitted independently for each orbital. This routine is used in the periodic-boundary-condition (PBC) GW code path of PySCF (pyscf.pbc.gw) to convert frequency-dependent self-energy samples (sigma) defined on a discrete frequency grid (omega) into a compact set of fit parameters that parametrize an analytic two-pole approximation per orbital. The fitted parameters are intended for downstream evaluation of the real-axis self-energy or spectral functions in GW calculations and match the parameter vector expected by the two_pole_fit residual function used in the module.\n", "name": "pyscf_pbc_gw_kugw_ac_AC_twopole_diag", "parameters": {"properties": {"sigma": {"type": "array", "items": {"type": "any"}, "description": "A 2D array containing the sampled self-energy values to be fitted. The code expects sigma.shape == (norbs, nw) where norbs is the number of orbitals and nw is the number of frequency points per orbital. Each row sigma[p] supplies the values to be fitted for orbital index p. These values are typically computed on an imaginary or discrete Matsubara frequency grid in GW calculations and are the target data for the two-pole analytic-continuation fit.", "default": ""}, "omega": {"type": "array", "items": {"type": "any"}, "description": "A 2D array or array-like of frequency grid points that correspond to the samples in sigma. The implementation calls omega[p] together with sigma[p] when fitting orbital p, so omega must supply per-orbital frequency coordinates compatible with the shape and ordering of sigma (commonly omega.shape == sigma.shape or broadcastable in the same indexing scheme). omega provides the independent-variable locations (frequency points) for the two-pole fit.", "default": ""}, "orbs": {"type": "array", "items": {"type": "any"}, "description": "A 1D array of orbital identifiers or indices used to determine occupation status relative to nocc. The implementation tests orbs[p] < nocc to decide the sign of certain initial-guess parameters for orbital p (occupied vs virtual), so orbs must contain values comparable with nocc (for example integer orbital indices or orbital ordering identifiers). The choice of initial guess affects convergence and physically encodes whether the orbital is occupied, which changes the starting sign of selected pole parameters in the two-pole model.", "default": ""}, "nocc": {"type": "integer", "description": "The number of occupied orbitals (an integer) used as the occupation threshold. Orbitals with orbs[p] < nocc are treated as occupied and receive a different initial parameter sign than orbitals with orbs[p] >= nocc. This influences the optimizer initial guess and therefore the fitted solution for occupied versus virtual orbitals in GW analytic continuation.", "default": ""}}, "required": ["sigma", "omega", "orbs", "nocc"], "type": "any"}}, "type": "function"}], "query": "I’m trying to do a two-pole analytic continuation (per-orbital) of diagonal GW self-energy samples from a PBC run (pyscf.pbc.gw). Initially I was going to just pass the raw sigma/omega blocks straight into the two-pole fitter for orbs=[0,1,2] with nocc=2.\n\nBut I just noticed the data came from a messy merge of two log files: one of the frequency points is a duplicated measurement at the same |omega| (so it would overweight that point in the least-squares fit), and for one orbital the last entry is a clear sign/formatting glitch (it flips sign relative to the monotonic trend and breaks causality on the imaginary axis).\n\nPlease do the following, in this order:\n1) For each orbital independently, remove the duplicated frequency point (keep only one instance of the repeated value).\n2) For the orbital that contains the obvious sign glitch at the largest |omega|, discard only that single corrupted sample (do not discard the whole orbital).\n3) After this cleaning, perform the standard two-pole analytic continuation fit independently for each orbital, using the same occupied/virtual split as originally intended (2 occupied, the remaining one virtual), and keep the same orbital list as above.\n\nUse these arrays (rows correspond to orbitals 0,1,2; columns correspond to the provided omega samples):\nomega = [\n  [0.1, 0.3, 0.5, 0.7, 0.7, 0.9],\n  [0.1, 0.3, 0.5, 0.7, 0.7, 0.9],\n  [0.1, 0.3, 0.5, 0.7, 0.7, 0.9]\n]\n\nsigma = [\n  [-0.42, -0.39, -0.35, -0.32, -0.32, -0.30],\n  [-0.28, -0.26, -0.24, -0.23, -0.23, -0.22],\n  [ 0.15,  0.13,  0.11,  0.10,  0.10, -0.09]\n]\n\nReturn the fitted two-pole parameter vectors in the exact format expected by the two_pole_fit residual used in pyscf.pbc.gw.", "answers": "[{\"name\": \"pyscf_pbc_gw_kugw_ac_AC_twopole_diag\", \"arguments\": {\"sigma\": [[-0.42, -0.39, -0.35, -0.32, -0.3], [-0.28, -0.26, -0.24, -0.23, -0.22], [0.15, 0.13, 0.11, 0.1]], \"omega\": [[0.1, 0.3, 0.5, 0.7, 0.9], [0.1, 0.3, 0.5, 0.7, 0.9], [0.1, 0.3, 0.5, 0.7]], \"orbs\": [0, 1, 2], \"nocc\": 2}}]"}
{"func_name": "pyscf_pbc_lib_kpts_helper_intersection", "func_desc": "Return the indices of rows in kpts1 that match any row in kpts2 within the module tolerance KPT_DIFF_TOL.\n    \n    This function is used in the PySCF periodic-boundary-condition (pbc) workflows to identify common k-points between two k-point meshes or sets. In practice, it helps determine which sampling points in reciprocal space (k-points) from kpts1 also appear in kpts2 within a small floating-point tolerance, a common need when aligning k-point meshes for band-structure calculations, integral evaluations, or symmetry-related operations in PySCF.", "tools": [{"function": {"description": "Return the indices of rows in kpts1 that match any row in kpts2 within the module tolerance KPT_DIFF_TOL.\n\nThis function is used in the PySCF periodic-boundary-condition (pbc) workflows to identify common k-points between two k-point meshes or sets. In practice, it helps determine which sampling points in reciprocal space (k-points) from kpts1 also appear in kpts2 within a small floating-point tolerance, a common need when aligning k-point meshes for band-structure calculations, integral evaluations, or symmetry-related operations in PySCF.", "name": "pyscf_pbc_lib_kpts_helper_intersection", "parameters": {"properties": {"kpts1": {"type": "array", "items": {"type": "any"}, "description": "A 2-D array of k-points. Each row is a k-point vector (for example, a 3-component reciprocal-space coordinate) and the array must have ndim == 2. The function treats each row as a coordinate in the same coordinate system used by kpts2.", "default": ""}, "kpts2": {"type": "array", "items": {"type": "any"}, "description": "A 2-D array of k-points with the same dimensionality (ndim == 2) as kpts1. Each row is a k-point vector to be compared against rows of kpts1.", "default": ""}}, "required": ["kpts1", "kpts2"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF PBC band-structure workflow where I need to align a coarse Monkhorst–Pack mesh (kpts1) with a separately generated k-point path list (kpts2). Initially I was going to intersect them directly, but I realized the path generator sometimes emits k-points slightly outside the first Brillouin-zone convention (i.e., components may differ by an integer due to periodicity), and those should be considered the same physical k-point as long as they coincide after wrapping back into the canonical fractional range.\n\nSo: before identifying common points, conceptually reduce each k-point component by periodicity (treat values that differ by an integer as equivalent), then find the indices of rows in kpts1 that match any row in kpts2 within the module tolerance.\n\nUse these lists:\n- kpts1 = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.5], [0.25, 0.25, 0.0]]\n- kpts2 = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.5], [0.25, 0.25, 0.0], [1.75, 0.0, 0.0]]\n\nReturn the indices in kpts1 that have a match in kpts2 under this periodic equivalence + tolerance rule.", "answers": "[{\"name\":\"pyscf_pbc_lib_kpts_helper_intersection\",\"arguments\":{\"kpts1\":[[0.0,0.0,0.0],[0.5,0.0,0.0],[0.0,0.5,0.0],[0.0,0.0,0.5],[0.25,0.25,0.0]],\"kpts2\":[[0.0,0.0,0.0],[0.0,0.0,0.5],[0.25,0.25,0.0],[1.75,0.0,0.0]]}}]"}
{"func_name": "pyscf_pbc_lib_kpts_helper_round_to_fbz", "func_desc": "Round scaled k-points to the first Brillouin zone used in PySCF periodic-boundary-condition (PBC) workflows.", "tools": [{"function": {"description": "Round scaled k-points to the first Brillouin zone used in PySCF periodic-boundary-condition (PBC) workflows.\n", "name": "pyscf_pbc_lib_kpts_helper_round_to_fbz", "parameters": {"properties": {"kpts": {"type": "array", "items": {"type": "any"}, "description": "Scaled k-points with shape (..., 3). Each row of the trailing dimension is a 3-component fractional k-point expressed in units of the reciprocal lattice vectors (i.e., values are expected to be in the range of typical scaled coordinates). This function reshapes the input to (-1, 3) internally, so the last dimension of kpts must be 3; otherwise a ValueError or shape-related exception will occur. The input array is not modified in place; a new ndarray with the same shape is returned.", "default": ""}, "wrap_around": {"type": "boolean", "description": "If True, map k-points into the interval [-0.5, 0.5) for each component; if False (default), map into [0.0, 1.0). This controls the final representation of fractional k-points in the first Brillouin zone: use wrap_around=True when the convention centers the zone at the origin (common in many solid-state calculations), and use False when the convention is 0..1 periodic coordinates.", "default": false}, "tol": {"type": "float", "description": "Tolerance used to consider k-points identical and to determine the rounding precision. K-points that differ by less than tol are treated as the same and are \"cleaned\" using lib.cleanse to remove tiny numerical noise before rounding. The function computes an internal decimal precision from tol via -log10((tol+1e-16)/10.) and then rounds to that many decimal places. Default value is 1e-06. Passing non-finite or nonsensical tol values may lead to unexpected rounding behavior; if tol is non-numeric a TypeError will be raised.", "default": 1e-06}}, "required": ["kpts", "tol", "wrap_around"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing a PySCF PBC band-structure run and I need to canonicalize a handful of *fractional* (scaled) k-points before symmetry reduction. Initially I was going to just wrap everything into the first Brillouin zone centered at Γ, but I realized the downstream interpolation code assumes the *half-open* convention so that boundary points don’t get double-counted. So please map every component into the interval that is symmetric about zero and excludes the positive boundary (i.e., the usual Γ-centered half-open FBZ).\n\nAlso, I can’t trust the raw floats: any component that is only off from a Brillouin-zone boundary by a tiny amount should be snapped cleanly to the boundary value instead of leaving numerical fuzz. Use a noise-cleaning tolerance equal to the same order as the largest |deviation from an exact integer or half-integer| you see in this specific list (so you must infer it from the data), and then apply that tolerance consistently.\n\nk-points (scaled):\n1) [1.0000002, -0.5000001, 0.25]\n2) [-0.75, 0.50000009, 1.2]\n3) [0.49999999, 0.0, -1e-7]\n\nReturn the wrapped-and-cleaned k-points in the same shape/order.", "answers": "[{\"name\": \"pyscf_pbc_lib_kpts_helper_round_to_fbz\", \"arguments\": {\"kpts\": [[1.0000002, -0.5000001, 0.25], [-0.75, 0.50000009, 1.2], [0.49999999, 0.0, -1e-07]], \"wrap_around\": true, \"tol\": 2e-07}}]"}
{"func_name": "pyscf_pbc_lib_kpts_map_kpts_tuples", "func_desc": "Find symmetry-related k-point tuples in the Brillouin zone for periodic\n    boundary condition (PBC) electronic-structure calculations.\n    \n    This function is used in the PySCF periodic boundary-condition k-point\n    infrastructure (pyscf.pbc.lib.kpts) to identify how k-point tuples are\n    mapped onto one another under a set of rotation operators. It is typically\n    used when constructing symmetry-adapted data structures (for example, when\n    assembling integrals or mapping k-point-dependent quantities across the\n    first Brillouin zone) so that operations on k-points account for symmetry\n    and reciprocal-lattice translations.", "tools": [{"function": {"description": "Find symmetry-related k-point tuples in the Brillouin zone for periodic\nboundary condition (PBC) electronic-structure calculations.\n\nThis function is used in the PySCF periodic boundary-condition k-point\ninfrastructure (pyscf.pbc.lib.kpts) to identify how k-point tuples are\nmapped onto one another under a set of rotation operators. It is typically\nused when constructing symmetry-adapted data structures (for example, when\nassembling integrals or mapping k-point-dependent quantities across the\nfirst Brillouin zone) so that operations on k-points account for symmetry\nand reciprocal-lattice translations.", "name": "pyscf_pbc_lib_kpts_map_kpts_tuples", "parameters": {"properties": {"kpts_scaled": {"type": "array", "items": {"type": "any"}, "description": "Array of scaled k-point tuples with shape\n(nkpts, ntuple, 3). Each entry is a tuple of ntuple k-points expressed\nin fractional (scaled) coordinates with respect to the reciprocal\nlattice vectors. nkpts is the number of distinct tuples in the\nBrillouin-zone sampling. The second dimension equals the ntuple\nargument and groups k-points that are considered together (for\nexample ntuple=2 for pairs). The third dimension has size 3 for the\nthree Cartesian components of each scaled k-point. In practice this\ninput comes from the PBC k-point generation routines in PySCF and\nmust match the expected tuple layout used elsewhere in the library.", "default": ""}, "ops": {"type": "array", "items": {"type": "any"}, "description": "Array of rotation operators with shape (nop, 3, 3)\nand integer entries. Each ops[s] is a 3x3 rotation operator that acts\non scaled k-point coordinates (in the same scaled coordinate system\nused by kpts_scaled). These operators represent point-group symmetry\nrotations (without fractional translations); the function applies\nops[s] to each k-point in a tuple to find the symmetry-related tuple.\nThe number of operators nop is the first dimension of this array.", "default": ""}, "ntuple": {"type": "integer", "description": "Dimension of tuples (the second dimension of kpts_scaled).\nDefault is 2. This parameter indicates how many k-points are grouped\nin each tuple (for example 2 for pairs, 3 for triplets). It must match\nthe second dimension of kpts_scaled, otherwise shape or indexing\ninconsistencies will occur.", "default": 2}, "tol": {"type": "float", "description": "Numerical tolerance used to decide when two k-points are\nequivalent modulo a reciprocal-lattice vector. K-points whose\ncoordinates differ by less than tol (after reduction to the first\nBrillouin zone via round_to_fbz) are considered identical. Default is\n1e-06. This tolerance controls sensitivity to floating-point rounding\nand should be chosen consistent with how k-points were generated.", "default": 1e-06}}, "required": ["kpts_scaled", "ops", "tol", "ntuple"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a simple-cubic PBC workflow in PySCF where I need symmetry-aware bookkeeping of **k-point pairs** (ntuple=2) in **scaled coordinates**. I initially planned to use the same ultra-tight equivalence tolerance we use for high-accuracy integral screening, but I just realized these k-points came out of a text pipeline that rounds to ~1e-6 in the last digit—so using a tolerance tighter than that will falsely split symmetry-equivalent tuples. Please therefore use a tolerance that is **two orders of magnitude looser than 1e-8**, while still treating tuples as equivalent **modulo reciprocal lattice vectors**.\n\nUse only a minimal symmetry set: (i) the identity, and (ii) the proper 90° rotation about the z-axis that maps +x to +y in the xy-plane (right-handed convention). For the following 4 k-point pairs, find the symmetry-related mapping between tuples under those operations:\n\nkpts_scaled = [\n  [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0], [0.0, 0.5, 0.0]],\n  [[0.25, 0.25, 0.0], [0.75, 0.25, 0.0]],\n  [[0.25, 0.25, 0.0], [0.25, 0.75, 0.0]]\n]\n\nReturn the mapping output as produced by the PySCF k-point tuple mapping helper (same one used in pyscf.pbc.lib.kpts).", "answers": "[{\"name\": \"pyscf_pbc_lib_kpts_map_kpts_tuples\", \"arguments\": {\"kpts_scaled\": [[[0.0, 0.0, 0.0], [0.5, 0.0, 0.0]], [[0.0, 0.0, 0.0], [0.0, 0.5, 0.0]], [[0.25, 0.25, 0.0], [0.75, 0.25, 0.0]], [[0.25, 0.25, 0.0], [0.25, 0.75, 0.0]]], \"ops\": [[[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0, -1, 0], [1, 0, 0], [0, 0, 1]]], \"ntuple\": 2, \"tol\": 1e-06}}]"}
{"func_name": "pyscf_pbc_symm_space_group_transform_rot", "func_desc": "pyscf.pbc.symm.space_group.transform_rot transforms a 3x3 rotation operator expressed in one lattice basis (a) into the equivalent rotation operator expressed in another lattice basis (b). This function is used inside the PySCF periodic-boundary-condition (pbc) symmetry utilities to convert point-group rotation operators when changing crystallographic coordinate systems or lattice basis representations, ensuring consistency of space-group symmetry operations across different basis choices.", "tools": [{"function": {"description": "pyscf.pbc.symm.space_group.transform_rot transforms a 3x3 rotation operator expressed in one lattice basis (a) into the equivalent rotation operator expressed in another lattice basis (b). This function is used inside the PySCF periodic-boundary-condition (pbc) symmetry utilities to convert point-group rotation operators when changing crystallographic coordinate systems or lattice basis representations, ensuring consistency of space-group symmetry operations across different basis choices.\n", "name": "pyscf_pbc_symm_space_group_transform_rot", "parameters": {"properties": {"op": {"type": "array", "items": {"type": "any"}, "description": "A (3,3) array containing the rotation operator expressed in the source basis system a. In the PySCF PBC symmetry context, op represents a point-group rotation matrix (operator) that acts on Cartesian coordinates expressed relative to the basis vectors given by a. The function treats op as a linear operator to be similarity-transformed into the target basis b.", "default": ""}, "a": {"type": "array", "items": {"type": "any"}, "description": "A (3,3) array of basis vectors for the source basis system a, provided in row-major order. Each row is one lattice/basis vector in the coordinate system used to interpret op. These basis vectors define the coordinate representation in which op is originally given; they must be non-degenerate (so that b.T inversion below is valid in the normal case).", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "A (3,3) array of basis vectors for the destination basis system b, provided in row-major order. Each row is one lattice/basis vector defining the target coordinate system to which op will be transformed. The algorithm computes the similarity transform P = inv(b.T) @ a.T to change the basis from a to b.", "default": ""}, "allow_non_integer": {"type": "boolean", "description": "Whether to allow non-integer entries in the transformed rotation matrix R returned in the b basis. Default value is False. In typical crystallographic usage, point-group rotation matrices expressed in an integer lattice basis are integer-valued; when allow_non_integer is False the function enforces this and returns an integer array (dtype int) after checking that R is integer within the symmetry precision threshold (SYMPREC). If allow_non_integer is True the function returns the floating-point matrix without enforcing integer rounding.", "default": false}}, "required": ["op", "a", "b", "allow_non_integer"], "type": "any"}}, "type": "function"}], "query": "I’m porting a PySCF PBC space-group setup from a simple cubic conventional cell (basis a) to a hexagonal/triangular in-plane primitive choice (basis b), and I need to carry over a 90° rotation about z. I *thought* I should keep the rotation matrix strictly integer-valued in the new basis to match how my old symm table was stored — but that’s wrong here because this basis change is not unimodular in the xy plane, so forcing integrality will silently mangle the operator.\n\nSo: transform the rotation op from basis a into basis b, and make sure the output is allowed to contain fractional (non-integer) entries.\n\nRotation op (in basis a): [[0, -1, 0],[1, 0, 0],[0, 0, 1]]\nBasis a: [[1.0, 0.0, 0.0],[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]\nBasis b: [[1.0, 0.0, 0.0],[0.5, 0.8660254, 0.0],[0.0, 0.0, 1.0]]", "answers": "[{\"name\":\"pyscf_pbc_symm_space_group_transform_rot\",\"arguments\":{\"op\":[[0,-1,0],[1,0,0],[0,0,1]],\"a\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],\"b\":[[1.0,0.0,0.0],[0.5,0.8660254,0.0],[0.0,0.0,1.0]],\"allow_non_integer\":true}}]"}
{"func_name": "pyscf_pbc_symm_space_group_transform_trans", "func_desc": "pyscf.pbc.symm.space_group.transform_trans transforms a translation operator given in one lattice-basis representation into the equivalent translation operator expressed in another lattice-basis representation used in periodic boundary-condition symmetry operations in PySCF.", "tools": [{"function": {"description": "pyscf.pbc.symm.space_group.transform_trans transforms a translation operator given in one lattice-basis representation into the equivalent translation operator expressed in another lattice-basis representation used in periodic boundary-condition symmetry operations in PySCF.\n", "name": "pyscf_pbc_symm_space_group_transform_trans", "parameters": {"properties": {"op": {"type": "array", "items": {"type": "any"}, "description": "Translation operator components in the source basis system :math:`\\mathbf{a}`. Must be a one-dimensional array of length 3 (shape (3,)). In the PySCF PBC/space-group context, op contains the fractional or basis-coordinate components of a translation vector expressed with respect to the row-major basis vectors supplied in `a`. This function treats `op` as a row vector and returns a new array with components in the target basis `b`.", "default": ""}, "a": {"type": "array", "items": {"type": "any"}, "description": "Basis vectors of the source basis system :math:`\\mathbf{a}` (row-major). Must be a 3x3 array (shape (3,3)). Each row is a basis vector of the :math:`\\mathbf{a}` system expressed in a common Cartesian coordinate frame. In practice, `a` represents the lattice or unit-cell basis convention from which `op` is specified.", "default": ""}, "b": {"type": "array", "items": {"type": "any"}, "description": "Basis vectors of the target basis system :math:`\\mathbf{b}` (row-major). Must be a 3x3 array (shape (3,3)). Each row is a basis vector of the :math:`\\mathbf{b}` system expressed in the same Cartesian frame as `a`. In PySCF workflows this allows converting translation operators between different unit-cell conventions or coordinate representations.", "default": ""}}, "required": ["op", "a", "b"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF PBC space-group symmetry pipeline where translations are being inconsistently represented across two lattice bases. I initially planned to transform the fractional translation op = [0.25, 0.5, 0.0] from the primitive cubic basis a = [[3.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 3.0]] into the rotated basis b = [[0.0, 3.0, 0.0], [-3.0, 0.0, 0.0], [0.0, 0.0, 3.0]]. But then I realized the translation was exported from a symmetry table that stores translations in a “wrapped” convention: if any component is > 0.5, it should be shifted by one lattice period to keep it in the interval [-0.5, 0.5] before doing any basis transform. Apply that wrapping rule first (without changing components that already satisfy it), then use pyscf.pbc.symm.space_group.transform_trans to express the resulting translation in the rotated basis b.", "answers": "[{\"name\": \"pyscf_pbc_symm_space_group_transform_trans\", \"arguments\": {\"op\": [0.25, 0.5, 0.0], \"a\": [[3.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 3.0]], \"b\": [[0.0, 3.0, 0.0], [-3.0, 0.0, 0.0], [0.0, 0.0, 3.0]]}}]"}
{"func_name": "pyscf_pbc_symm_symmetry_get_Dmat", "func_desc": "pyscf.pbc.symm.symmetry.get_Dmat computes the Wigner D-matrix for a given 3×3 rotation operator and an angular momentum quantum number l. This D-matrix is the matrix representation of the rotation in the angular-momentum-l irreducible representation and is used in PySCF periodic-boundary-condition (PBC) symmetry routines to construct symmetry-adapted representations of rotational operations in electronic structure calculations.", "tools": [{"function": {"description": "pyscf.pbc.symm.symmetry.get_Dmat computes the Wigner D-matrix for a given 3×3 rotation operator and an angular momentum quantum number l. This D-matrix is the matrix representation of the rotation in the angular-momentum-l irreducible representation and is used in PySCF periodic-boundary-condition (PBC) symmetry routines to construct symmetry-adapted representations of rotational operations in electronic structure calculations.\n", "name": "pyscf_pbc_symm_symmetry_get_Dmat", "parameters": {"properties": {"op": {"type": "array", "items": {"type": "any"}, "description": "rotation operator in (x,y,z) system. This parameter must be a 3-by-3 NumPy ndarray representing a Cartesian rotation (or improper rotation) applied to Cartesian basis vectors. The function computes the determinant of op to detect improper rotations; if det(op) < 0 the code treats op as an improper rotation, multiplies op by -1 for Euler-angle extraction, and multiplies the resulting D-matrix by (-1)**l. If op does not have shape (3,3) or is not numeric, subsequent numerical operations (determinant, dot product, Euler-angle extraction) will raise NumPy or shape-related exceptions.", "default": ""}, "l": {"type": "integer", "description": "angular momentum. Integer angular-momentum quantum number used to choose the degree of the Wigner D-matrix (the angular-momentum representation). This value is passed to the underlying Dmatrix routine that builds the representation for angular momentum l; it is expected to be an integer consistent with the domain of the Dmatrix implementation.", "default": ""}}, "required": ["op", "l"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF PBC symmetry pipeline where I need the Wigner D-matrix for the d-manifold rotation corresponding to a nominal C4 operation about the crystallographic z-axis. I was going to just feed the 3×3 operator for a +90° right-handed rotation about +z into `get_Dmat`, but then I realized the space-group generator I’m using defines rotations in the *active* convention while PySCF’s symmetry adapter expects the *passive* one. So: take the same Cartesian rotation operator I would have used for the +90° about z, convert it into the operator appropriate for the passive convention (i.e., the one that undoes that active rotation), and compute the Wigner D-matrix for the l=2 irreducible representation. Use that corrected 3×3 operator as the input to `pyscf.pbc.symm.symmetry.get_Dmat`.", "answers": "[{\"name\": \"pyscf_pbc_symm_symmetry_get_Dmat\", \"arguments\": {\"op\": [[0, 1, 0], [-1, 0, 0], [0, 0, 1]], \"l\": 2}}]"}
{"func_name": "pyscf_pbc_tools_k2gamma_double_translation_indices", "func_desc": "Compute an index array that maps k-point pair subscripts (two-dimensional\n    indices for a matrix indexed by k-point labels) to one-dimensional indices\n    that exploit translation symmetry in periodic boundary condition (PBC)\n    calculations. In the PySCF PBC context, many two-index quantities D[M,N]\n    depend only on the relative translation between k-points (for example the\n    symmetry D[M,N] = D[N-M] noted in the original implementation). This function\n    builds the integer index array that allows reconstructing the full 2D matrix\n    from a compact 1D array of symmetry-unique translation values.\n    \n    The function uses translation_map for each Cartesian reciprocal-lattice\n    direction (kx, ky, kz) to obtain a representative index per k-point under\n    translation symmetry, then combines those representatives with\n    numpy.ravel_multi_index to produce a single flat index for each ordered pair\n    (M, N). The resulting index array is suitable for indexing a 1D array D1 of\n    symmetry-unique translation-dependent values to produce the full 2D matrix\n    D2 via D2 = D1[double_translation_indices(kmesh)].", "tools": [{"function": {"description": "Compute an index array that maps k-point pair subscripts (two-dimensional\nindices for a matrix indexed by k-point labels) to one-dimensional indices\nthat exploit translation symmetry in periodic boundary condition (PBC)\ncalculations. In the PySCF PBC context, many two-index quantities D[M,N]\ndepend only on the relative translation between k-points (for example the\nsymmetry D[M,N] = D[N-M] noted in the original implementation). This function\nbuilds the integer index array that allows reconstructing the full 2D matrix\nfrom a compact 1D array of symmetry-unique translation values.\n\nThe function uses translation_map for each Cartesian reciprocal-lattice\ndirection (kx, ky, kz) to obtain a representative index per k-point under\ntranslation symmetry, then combines those representatives with\nnumpy.ravel_multi_index to produce a single flat index for each ordered pair\n(M, N). The resulting index array is suitable for indexing a 1D array D1 of\nsymmetry-unique translation-dependent values to produce the full 2D matrix\nD2 via D2 = D1[double_translation_indices(kmesh)].", "name": "pyscf_pbc_tools_k2gamma_double_translation_indices", "parameters": {"properties": {"kmesh": {"type": "any", "description": "A length-3 tuple of integers giving the k-point sampling\ngrid size in each periodic direction (kx, ky, kz). Each element\nrepresents the number of k-points along that lattice direction and\nis used as the dims argument for numpy.ravel_multi_index. This argument\nis required and must contain three positive integers because the\nimplementation accesses kmesh[0], kmesh[1], and kmesh[2] and builds\nper-dimension translation maps. In the PySCF PBC domain, kmesh\ndescribes the discrete sampling of the Brillouin zone used when\nexploiting translation symmetry to reduce storage and computation.", "default": ""}}, "required": ["kmesh"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF PBC workflow where I need the k-point-pair → translation-class lookup so I can reconstruct a full two-index k-point matrix D[M,N] from a compact 1D array D1 that only stores symmetry-unique relative translations (i.e., depends only on N−M under PBC). I was going to generate the double-translation indices for a 4×3×2 Monkhorst–Pack mesh, but I just realized my input k-mesh file includes the Gamma point as an explicit first line (a header-like artifact) and one of the mesh dimensions in that file is therefore effectively ‘one too large’ compared to the physically intended sampling. Please generate the double-translation indices for the physically intended mesh, i.e., the one obtained by correcting only the dimension that would have been inflated by that single extra leading entry, while leaving the other two dimensions unchanged from 4×3×2.", "answers": "[{\"name\":\"pyscf_pbc_tools_k2gamma_double_translation_indices\",\"arguments\":{\"kmesh\":[3,3,2]}}]"}
{"func_name": "pyscf_pbc_tools_pbc_cutoff_to_mesh", "func_desc": "Convert kinetic-energy (KE) cutoff to a minimal FFT mesh suitable for periodic\n    (boundary) plane-wave / FFT operations in PySCF.\n    \n    This function is used in PySCF's periodic-boundary-condition (PBC) tools to\n    choose the number of grid points along each lattice direction so that plane-wave\n    reciprocal vectors with kinetic energy up to the given cutoff are representable\n    on the uniform FFT grid. The conversion assumes the kinetic energy KE = |k|^2/2\n    (atomic units) and the empirical relation between maximum wavevector and grid\n    spacing (k_max ~ pi / grid_spacing). The routine computes the reciprocal\n    lattice b = 2*pi * inv(a.T), determines the effective reciprocal-axis\n    lengths via QR decompositions, calculates the required maximum reciprocal\n    index Gmax = sqrt(2*cutoff)/|r| for each lattice direction, and returns an\n    odd integer mesh mesh = ceil(Gmax)*2 + 1 to ensure symmetric sampling of\n    positive and negative reciprocal vectors.", "tools": [{"function": {"description": "Convert kinetic-energy (KE) cutoff to a minimal FFT mesh suitable for periodic\n(boundary) plane-wave / FFT operations in PySCF.\n\nThis function is used in PySCF's periodic-boundary-condition (PBC) tools to\nchoose the number of grid points along each lattice direction so that plane-wave\nreciprocal vectors with kinetic energy up to the given cutoff are representable\non the uniform FFT grid. The conversion assumes the kinetic energy KE = |k|^2/2\n(atomic units) and the empirical relation between maximum wavevector and grid\nspacing (k_max ~ pi / grid_spacing). The routine computes the reciprocal\nlattice b = 2*pi * inv(a.T), determines the effective reciprocal-axis\nlengths via QR decompositions, calculates the required maximum reciprocal\nindex Gmax = sqrt(2*cutoff)/|r| for each lattice direction, and returns an\nodd integer mesh mesh = ceil(Gmax)*2 + 1 to ensure symmetric sampling of\npositive and negative reciprocal vectors.", "name": "pyscf_pbc_tools_pbc_cutoff_to_mesh", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "The real-space cell lattice vectors. Each row\nrepresents a lattice vector that defines the periodic cell used in PBC\ncalculations. In practical PySCF workflows this array specifies the\ncell geometry for which the FFT grid is being chosen; it must be a\nnumeric 3-by-3 array compatible with numpy linear-algebra operations.", "default": ""}, "cutoff": {"type": "float", "description": "KE energy cutoff in atomic units (a.u.). This scalar\nspecifies the maximum one-electron kinetic energy (in the relation\nKE = |k|^2/2) that must be represented by the FFT mesh. The function\nexpects a non-negative finite float; negative values will lead to\ninvalid results (square-root of a negative number) or runtime warnings.", "default": ""}}, "required": ["a", "cutoff"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a periodic DFT test in PySCF for a tetragonal cell with lattice vectors (Bohr) [[7.20, 0.00, 0.00],[0.00, 7.20, 0.00],[0.00, 0.00, 12.00]]. Initially I was going to use a plane-wave kinetic-energy cutoff of 35.0 a.u. to pick the minimal odd FFT mesh. Wait—our workflow sometimes passes cutoffs in Rydberg, and I just realized this particular input file is in that convention, even though the comment says “a.u.”. So: treat the number 35.0 as being in the unit system where values are exactly twice the Hartree value. Convert it to the Hartree-based cutoff expected by PySCF’s cutoff-to-mesh routine, then compute the minimal odd FFT mesh (nx, ny, nz) for this cell that can represent reciprocal vectors up to that converted cutoff.", "answers": "[{\"name\": \"pyscf_pbc_tools_pbc_cutoff_to_mesh\", \"arguments\": {\"a\": [[7.2, 0.0, 0.0], [0.0, 7.2, 0.0], [0.0, 0.0, 12.0]], \"cutoff\": 17.5}}]"}
{"func_name": "pyscf_pbc_tools_pbc_mesh_to_cutoff", "func_desc": "pyscf.pbc.tools.pbc.mesh_to_cutoff converts a real-space grid mesh (number of grid points along each periodic lattice vector) into an approximate kinetic-energy (KE) cutoff for each spatial direction used in plane-wave / reciprocal-space treatments within PySCF's periodic-boundary-condition (PBC) tools. The function is used in PBC workflows to map a discrete mesh specification (mesh) into the maximum reciprocal-space G-vector magnitude per direction and then to the corresponding KE cutoff via ke = |Gmax|^2 / 2, enabling consistent selection of plane-wave or G-vector truncation thresholds from a simple mesh description.", "tools": [{"function": {"description": "pyscf.pbc.tools.pbc.mesh_to_cutoff converts a real-space grid mesh (number of grid points along each periodic lattice vector) into an approximate kinetic-energy (KE) cutoff for each spatial direction used in plane-wave / reciprocal-space treatments within PySCF's periodic-boundary-condition (PBC) tools. The function is used in PBC workflows to map a discrete mesh specification (mesh) into the maximum reciprocal-space G-vector magnitude per direction and then to the corresponding KE cutoff via ke = |Gmax|^2 / 2, enabling consistent selection of plane-wave or G-vector truncation thresholds from a simple mesh description.\n", "name": "pyscf_pbc_tools_pbc_mesh_to_cutoff", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "any"}, "description": "Real-space lattice vectors used to define the periodic cell. This array is treated as a square matrix whose transpose is inverted to form the reciprocal-lattice matrix b = 2*pi * inv(a.T). In practice this corresponds to the 3x3 matrix of lattice vectors supplied to PySCF PBC routines; the matrix must be square and invertible (otherwise np.linalg.LinAlgError will be raised). The reciprocal lattice computed from a determines the physical scale of G-vectors used to derive the KE cutoff.", "default": ""}, "mesh": {"type": "any", "description": "Tuple of three integers giving the number of grid points along each lattice vector direction in the real-space mesh (one integer per periodic direction). The code converts this to an integer array and computes half-grid extents as (np.asarray(mesh) - 1) // 2 to obtain the maximal index magnitude in each direction. mesh must have length 3 and contain integer-like values; supplying a different length or non-integer-like values will produce an error or non-physical results.", "default": ""}}, "required": ["a", "mesh"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a PySCF PBC plane-wave truncation test and I *thought* I should keep the FFT mesh at (41, 41, 61) for a simple cubic cell with lattice vectors a = [[3.57, 0.0, 0.0], [0.0, 3.57, 0.0], [0.0, 0.0, 3.57]] (Bohr). But I just realized our downstream Ewald/Poisson routines assume an even-number FFT grid for the Nyquist frequency handling, so odd grid sizes will subtly mismatch the reciprocal cutoff bookkeeping.\n\nPlease: take that original mesh and adjust it to the **closest even-valued mesh in each direction** (minimum total change across the 3 components; if there’s a tie for a component, choose the one that makes that component smaller). Then compute the approximate per-direction kinetic-energy cutoffs corresponding to that adjusted mesh using `pyscf.pbc.tools.pbc.mesh_to_cutoff` for the same cubic lattice `a`.", "answers": "[{\"name\": \"pyscf_pbc_tools_pbc_mesh_to_cutoff\", \"arguments\": {\"a\": [[3.57, 0.0, 0.0], [0.0, 3.57, 0.0], [0.0, 0.0, 3.57]], \"mesh\": [40, 40, 60]}}]"}
{"func_name": "pyscf_pbc_tools_pbc_round_to_cell0", "func_desc": "pyscf.pbc.tools.pbc.round_to_cell0 rounds scaled (fractional) coordinates to a reference unit cell used in periodic-boundary-condition (PBC) calculations. This function is part of the PySCF PBC tools and is used to map coordinate values expressed in unit-cell (scaled/fractional) coordinates into a canonical reference cell by delegating to pyscf.pbc.lib.kpts_helper.round_to_fbz with wrap_around=False and the specified tolerance.", "tools": [{"function": {"description": "pyscf.pbc.tools.pbc.round_to_cell0 rounds scaled (fractional) coordinates to a reference unit cell used in periodic-boundary-condition (PBC) calculations. This function is part of the PySCF PBC tools and is used to map coordinate values expressed in unit-cell (scaled/fractional) coordinates into a canonical reference cell by delegating to pyscf.pbc.lib.kpts_helper.round_to_fbz with wrap_around=False and the specified tolerance.\n", "name": "pyscf_pbc_tools_pbc_round_to_cell0", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "any"}, "description": "Array of scaled (fractional) coordinates to be rounded. In the PySCF PBC context, each element of this numpy.ndarray represents coordinate components expressed in units of the lattice vectors (scaled coordinates). The array may be any shape compatible with kpts_helper.round_to_fbz; typical usage is an array of shape (N, 3) for N coordinates, but the function accepts any numpy.ndarray shape that the underlying helper supports. This parameter is the primary input and determines which coordinate values are examined and adjusted according to the tolerance.", "default": ""}, "tol": {"type": "float", "description": "Rounding tolerance used to decide when a scaled coordinate is considered equal to a cell boundary or integer displacement. The default is 1e-6. Practically, values whose distance to the nearest integer multiple of the lattice basis is less than or equal to tol are treated as exact and are rounded accordingly. This parameter controls the sensitivity of the rounding operation and can be adjusted to avoid numerical noise causing incorrect cell assignments.", "default": 1e-06}}, "required": ["r", "tol"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up fractional (scaled) coordinates before a PySCF PBC run. Initially I planned to round everything into the canonical reference cell using the same tolerance as our SCF k-point mesh generator. But then I realized one coordinate is a clear artifact from a broken symmetry reduction step: it lies farther outside the unit cell than the maximum drift we ever see from floating-point noise in this workflow. So: first, discard any atom whose *any* fractional component deviates from the nearest integer by more than the noise bound implied by the rounding tolerance used for the mesh (i.e., anything not consistent with mere numerical noise). Then, for the remaining atoms, round/map their scaled coordinates into the canonical reference cell (cell0) using that same tolerance.\n\nRaw fractional coordinates:\nA: [-1e-7, 0.5, 1.0000002]\nB: [0.99999999, -2e-6, 0.25]\nC: [1.000012, 0.3333333, -0.0000004]\n\nPlease apply the mapping exactly as PySCF does via round_to_cell0 (wrap_around disabled), after filtering as described.", "answers": "[{\"name\": \"pyscf_pbc_tools_pbc_round_to_cell0\", \"arguments\": {\"r\": [[-1e-07, 0.5, 1.0000002], [0.99999999, -2e-06, 0.25]], \"tol\": 1e-05}}]"}
{"func_name": "pyscf_qmmm_mm_mole_create_mm_mol", "func_desc": "Create an MM (molecular mechanics) Mole object from coordinates and optional\n    charges/radii for use in QM/MM simulations in PySCF. This function is used by\n    the qmmm module to build a lightweight representation of the classical (MM)\n    region: it accepts either raw Cartesian coordinates or a preformatted atom\n    specification, normalizes the atom entries via gto.format_atom, converts\n    Gaussian radii into the internal zeta parameter (1 / radius^2) in atomic\n    units when provided, and returns a pyscf.gto.mole.Mole object that carries\n    the MM atoms, their partial charges, and Gaussian width parameters for\n    electrostatic embedding or other QM/MM interactions.", "tools": [{"function": {"description": "Create an MM (molecular mechanics) Mole object from coordinates and optional\ncharges/radii for use in QM/MM simulations in PySCF. This function is used by\nthe qmmm module to build a lightweight representation of the classical (MM)\nregion: it accepts either raw Cartesian coordinates or a preformatted atom\nspecification, normalizes the atom entries via gto.format_atom, converts\nGaussian radii into the internal zeta parameter (1 / radius^2) in atomic\nunits when provided, and returns a pyscf.gto.mole.Mole object that carries\nthe MM atoms, their partial charges, and Gaussian width parameters for\nelectrostatic embedding or other QM/MM interactions.", "name": "pyscf_qmmm_mm_mole_create_mm_mol", "parameters": {"properties": {"atoms_or_coords": {"type": "array", "items": {"type": "any"}, "description": "Cartesian coordinates of MM\natoms or a preformatted atom list. When supplied as a numpy.ndarray of\nshape (N, 3) or as a sequence of coordinate tuples [(x1, y1, z1),\n(x2, y2, z2), ...], each coordinate tuple is converted to a \"ghost\"\natom entry by prefixing a zero atomic symbol (0) so the resulting atom\nlist becomes [(0, (x1, y1, z1)), ...]. If atoms_or_coords is already\na formatted atom list (for example, [(symbol_or_number, (x, y, z)),\n...]), the function leaves the entries as-is and passes them to\ngto.format_atom. The function then calls gto.format_atom(atoms,\nunit=unit) to normalize the coordinate and atom-formatting; any\nformat-related errors are raised by gto.format_atom. This parameter is\nthe principal geometric input used to define the MM region in QM/MM\ncalculations performed with PySCF.", "default": ""}, "charges": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "1D array of partial charges for the MM\natoms. These charges are passed directly to the returned Mole via the\ncharges keyword so they become the classical point charges associated\nwith each MM site. If None (the default), no explicit charges are set\non the Mole object. If the length of charges does not match the number\nof atoms, an exception may be raised by the Mole constructor or by\ndownstream code that assumes per-atom charges.", "default": null}, "radii": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "1D array of Gaussian charge-distribution\nradii for MM atoms. When provided, the function converts radii to a\n1D numpy.ndarray of dtype float and flattens it. If unit is not the\nspecial atomic-unit string recognized as \"au\" by is_au(unit), radii are\nconverted from the given unit into Bohr (atomic units) by dividing by\nparam.BOHR. The internal Gaussian width parameter zeta is then\ncomputed as 1.0 / radii**2 and passed to the returned Mole as the\nzeta keyword. If radii is None (default), zeta is left as None and no\nGaussian smearing is applied. Mismatched radii length relative to the\nnumber of atoms may raise errors from the Mole constructor or other\ndownstream checks.", "default": null}, "unit": {"type": "string", "description": "Unit string for the input coordinates and radii. Default is\n\"Angstrom\". The unit string is forwarded to gto.format_atom for\ncoordinate interpretation. For radii conversion, the function treats\nnon-atomic-unit values by dividing by param.BOHR to obtain Bohr units\nbefore computing zeta. If an unrecognized unit string is provided,\ngto.format_atom or is_au(unit) may produce behavior consistent with\ntheir own validation rules.", "default": "Angstrom"}}, "required": ["atoms_or_coords", "charges", "unit", "radii"], "type": "any"}}, "type": "function"}], "query": "I’m building a PySCF QM/MM electrostatic embedding for a small protein pocket, and I need the MM-region Mole created from three MM sites given in Angstrom: (12.345, -3.210, 7.890), (13.100, -2.950, 8.250), (11.900, -3.600, 7.500). Initially I was going to pass the Gaussian smearing radii as provided, but I just realized those radii came from a tool that reports *diameters* (still in Angstrom). Please correct for that before creating the MM Mole. Use the same partial charges [-0.40, 0.20, 0.20]. The three reported ‘radii’ numbers from the upstream tool are [0.60, 0.55, 0.65] (Angstrom, but actually diameters). Build the MM Mole with the corrected smearing radii and the coordinates in Angstrom.", "answers": "[{\"name\": \"pyscf_qmmm_mm_mole_create_mm_mol\", \"arguments\": {\"atoms_or_coords\": [[12.345, -3.21, 7.89], [13.1, -2.95, 8.25], [11.9, -3.6, 7.5]], \"charges\": [-0.4, 0.2, 0.2], \"radii\": [0.3, 0.275, 0.325], \"unit\": \"Angstrom\"}}]"}
{"func_name": "pyscf_scf_addons_canonical_orth_", "func_desc": "pyscf.scf.addons.canonical_orth_: Compute Löwdin's canonical (symmetric) orthogonalization matrix for an atomic-orbital overlap matrix.", "tools": [{"function": {"description": "pyscf.scf.addons.canonical_orth_: Compute Löwdin's canonical (symmetric) orthogonalization matrix for an atomic-orbital overlap matrix.\n", "name": "pyscf_scf_addons_canonical_orth_", "parameters": {"properties": {"S": {"type": "array", "items": {"type": "any"}, "description": "Real symmetric overlap matrix of the basis functions (commonly the atomic-orbital overlap matrix S_ij = <phi_i|phi_j>) used in quantum-chemistry mean-field calculations within PySCF. The function assumes S is a square numpy.ndarray representing the overlap between basis functions; the diagonal elements are used to form a diagonal normalization prefactor because some symmetry-adapted basis functions may not be individually normalized. This matrix is not modified by the function.", "default": ""}, "thr": {"type": "float", "description": "Eigenvalue threshold (default 1e-07). Eigenvalues of the normalized overlap matrix smaller than thr are treated as numerically zero and their corresponding eigenvectors are discarded to avoid instabilities from near-linear dependencies in the basis. Choosing a larger thr will reduce the number of retained orthonormalized basis vectors (reduces rank); choosing a smaller thr will retain more directions but can amplify numerical noise. The default value 1e-07 reflects a typical tolerance used in PySCF for rejecting near-zero overlap eigenvalues.", "default": 1e-07}}, "required": ["S", "thr"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a minimal-basis SCF preconditioning step in PySCF and need the Löwdin (symmetric) canonical orthogonalizer for the AO overlap matrix\nS = [[1.0, 0.12, 0.05],\n     [0.12, 1.0, 0.08],\n     [0.05, 0.08, 1.0]].\n\nInitially I was going to keep the usual near-linear-dependency cutoff used for “quick-and-dirty” screening, but then I realized this overlap comes from a geometry where two basis functions are almost redundant, so I need the *more stringent* cutoff that is smaller by exactly three orders of magnitude than that default screening value. Compute the canonical orthogonalization matrix using that stricter threshold (discarding eigenmodes below it).", "answers": "[{\"name\":\"pyscf_scf_addons_canonical_orth_\",\"arguments\":{\"S\":[[1.0,0.12,0.05],[0.12,1.0,0.08],[0.05,0.08,1.0]],\"thr\":1e-09}}]"}
{"func_name": "pyscf_scf_addons_get_ghf_orbspin", "func_desc": "Get the spin label for each generalized Hartree–Fock (GHF) spin-orbital produced\n    when converting closed-shell RHF or spin-unrestricted UHF molecular orbitals\n    into a GHF spin-orbital ordering. This helper is used in PySCF workflows that\n    expand spatial molecular orbitals into spin-orbitals (two spin states per\n    spatial orbital) and need a deterministic mapping from the original RHF/UHF\n    occupation and energy information to a per-GHF-orbital spin label.\n    \n    Behavior summary: For RHF input the function interprets a single spatial\n    orbital energy array and a single occupancy array and constructs an ordering\n    that places occupied GHF spin-orbitals before virtual ones, breaking\n    degeneracies and open-shell ordering according to the RHF convention used in\n    PySCF (doubly occupied spatial orbitals produce alpha then beta spin-orbitals,\n    open-shell singly occupied orbitals produce a final alpha spin-orbital in the\n    occupied block, etc.). For UHF input the function treats separate alpha and\n    beta energy/occupation arrays, sorts occupied and virtual orbitals by their\n    energies (using rounding to 6 decimal places and a stable sort to avoid\n    spurious reordering from numerical noise), and returns a combined sequence of\n    alpha/beta labels that matches the energy-based ordering. The returned array\n    is intended to accompany a 2*nmo-length GHF coefficient matrix that arises\n    from expanding nmo spatial orbitals into spin-orbitals.", "tools": [{"function": {"description": "Get the spin label for each generalized Hartree–Fock (GHF) spin-orbital produced\nwhen converting closed-shell RHF or spin-unrestricted UHF molecular orbitals\ninto a GHF spin-orbital ordering. This helper is used in PySCF workflows that\nexpand spatial molecular orbitals into spin-orbitals (two spin states per\nspatial orbital) and need a deterministic mapping from the original RHF/UHF\noccupation and energy information to a per-GHF-orbital spin label.\n\nBehavior summary: For RHF input the function interprets a single spatial\norbital energy array and a single occupancy array and constructs an ordering\nthat places occupied GHF spin-orbitals before virtual ones, breaking\ndegeneracies and open-shell ordering according to the RHF convention used in\nPySCF (doubly occupied spatial orbitals produce alpha then beta spin-orbitals,\nopen-shell singly occupied orbitals produce a final alpha spin-orbital in the\noccupied block, etc.). For UHF input the function treats separate alpha and\nbeta energy/occupation arrays, sorts occupied and virtual orbitals by their\nenergies (using rounding to 6 decimal places and a stable sort to avoid\nspurious reordering from numerical noise), and returns a combined sequence of\nalpha/beta labels that matches the energy-based ordering. The returned array\nis intended to accompany a 2*nmo-length GHF coefficient matrix that arises\nfrom expanding nmo spatial orbitals into spin-orbitals.", "name": "pyscf_scf_addons_get_ghf_orbspin", "parameters": {"properties": {"mo_energy": {"type": "array", "items": {"type": "any"}, "description": "Molecular orbital energies. For RHF input this\nis a one-dimensional numpy array of length nmo containing spatial\norbital energies. For UHF input this is a sequence/array-like where\nmo_energy[0] and mo_energy[1] are the alpha and beta orbital-energy\narrays respectively (each of length nmo). The function uses the\ndimensionality of mo_energy[0] to guess RHF vs UHF when is_rhf is None:\nfor RHF mo_energy[0] is a scalar (ndim == 0) and the function treats\nthe input as RHF; for UHF mo_energy[0] is an array and the function\ntreats the input as UHF. These energies determine ordering for\ndegeneracy resolution and splitting between occupied and virtual sets.", "default": ""}, "mo_occ": {"type": "array", "items": {"type": "any"}, "description": "Molecular orbital occupation numbers. For RHF\ninput this is a one-dimensional numpy array of length nmo with values\n0, 1, or 2 (mo_occ == 2 indicates doubly occupied spatial orbitals;\nmo_occ > 0 is treated as occupied). For UHF input this is a\ntwo-element sequence/array-like where mo_occ[0] and mo_occ[1] are the\nalpha and beta occupation arrays respectively (each length nmo). The\nfunction counts mo_occ > 0 to determine occupied orbitals and uses\nmo_occ == 2 for distinguishing doubly-occupied vs singly-occupied\norbitals in the RHF case. Correct interpretation relies on the\nPySCF occupation conventions for RHF/UHF wavefunctions.", "default": ""}, "is_rhf": {"type": "boolean", "nullable": true, "description": "Flag to force interpretation of the inputs as\nRHF (True) or UHF (False). Default None causes automatic detection:\nif mo_energy[0].ndim == 0 the function assumes RHF, otherwise UHF.\nProviding an explicit boolean overrides the automatic guess. Passing\nan incorrect flag inconsistent with the shapes/contents of mo_energy\nand mo_occ will produce incorrect labels or runtime errors.", "default": null}}, "required": ["mo_energy", "mo_occ", "is_rhf"], "type": "any"}}, "type": "function"}], "query": "I’m building a PySCF post-processing step to expand a spin-unrestricted SCF result into a 2*nmo generalized-HF spin-orbital list, and I need the per-spin-orbital (alpha/beta) label array that matches the deterministic PySCF-style ordering.\n\nUse these UHF-like inputs:\n- alpha MO energies: [-0.743219, -0.512340, -0.128901, 0.042315]\n- beta  MO energies: [-0.7432189, -0.500100, -0.120000, 0.055000]\n- alpha occupations: [1, 1, 1, 0]\n- beta  occupations: [1, 1, 0, 0]\n\nInitially I was going to just sort by the raw floating-point energies, but I realized the pipeline needs the same ordering across machines/BLAS builds, and the helper’s convention is to mitigate tiny alpha/beta numerical noise by comparing energies only after applying the fixed-precision rounding it uses internally, while still using a stable ordering inside ties.\n\nSo: interpret the input as spin-unrestricted (not RHF), construct the occupied block first and the virtual block second, and within each block order by the helper’s rounded-energy/stable-sort rule. Return the resulting 2*nmo-length array of spin labels (alpha/beta) accompanying that GHF spin-orbital ordering.", "answers": "[{\"name\": \"pyscf_scf_addons_get_ghf_orbspin\", \"arguments\": {\"mo_energy\": [[-0.743219, -0.51234, -0.128901, 0.042315], [-0.7432189, -0.5001, -0.12, 0.055]], \"mo_occ\": [[1, 1, 1, 0], [1, 1, 0, 0]], \"is_rhf\": false}}]"}
{"func_name": "pyscf_scf_addons_partial_cholesky_orth_", "func_desc": "Partial Cholesky orthogonalization for curing basis-set overcompleteness in atomic-orbital (AO) representations used in PySCF SCF workflows. This routine implements the procedure described by S. Lehtola to identify a numerically robust subset of AO basis functions via a pivoted Cholesky decomposition of a normalized overlap matrix and to orthogonalize that retained sub-basis using canonical orthogonalization. The resulting transformation is intended for use in self-consistent field (SCF) and related electronic-structure calculations where near-linear dependencies (overcompleteness) of the AO basis degrade numerical stability.", "tools": [{"function": {"description": "Partial Cholesky orthogonalization for curing basis-set overcompleteness in atomic-orbital (AO) representations used in PySCF SCF workflows. This routine implements the procedure described by S. Lehtola to identify a numerically robust subset of AO basis functions via a pivoted Cholesky decomposition of a normalized overlap matrix and to orthogonalize that retained sub-basis using canonical orthogonalization. The resulting transformation is intended for use in self-consistent field (SCF) and related electronic-structure calculations where near-linear dependencies (overcompleteness) of the AO basis degrade numerical stability.\n", "name": "pyscf_scf_addons_partial_cholesky_orth_", "parameters": {"properties": {"S": {"type": "array", "items": {"type": "any"}, "description": "Square AO overlap matrix S_{μν} in the atomic-orbital basis. In practice this is the molecular overlap matrix produced by PySCF basis integrals. The routine expects S to be a finite, real-valued square matrix with positive diagonal elements (S_{μμ} > 0) so that normalization by the diagonal is defined. S represents the metric in which linear dependencies arise; its role is central because the algorithm normalizes S, sorts basis functions by Gershgorin-circle magnitude to improve pivot selection, and performs a pivoted Cholesky on the normalized matrix to identify a well-conditioned subspace.", "default": ""}, "canthr": {"type": "float", "description": "Threshold used by the canonical orthogonalization step applied to the selected sub-basis (defaults to 1e-07). In domain terms, this tolerance determines which small generalized eigenvalues of the sub-block overlap are treated as numerical zero during canonical orthogonalization; it controls how aggressively near-linear dependencies inside the retained sub-basis are removed. Smaller values make the orthogonalization more permissive (retain more components), larger values remove more components. The canonical orthogonalization is applied to the un-normalized sub-block S[idx, idx] after pivot selection.", "default": 1e-07}, "cholthr": {"type": "float", "description": "Tolerance passed to the pivoted Cholesky decomposition on the normalized overlap matrix (defaults to 1e-09). This parameter controls the stopping criterion of the pivoted Cholesky: the decomposition truncates when remaining pivots fall below cholthr in magnitude. In quantum-chemistry practice, cholthr sets how strictly the algorithm prunes basis functions that contribute negligibly to the span of the overlap matrix; tighter tolerances (smaller cholthr) retain more pivots and thus a larger sub-basis, while larger tolerances produce a smaller retained set to cure overcompleteness.", "default": 1e-09}}, "required": ["S", "canthr", "cholthr"], "type": "any"}}, "type": "function"}], "query": "I’m stabilizing an RHF run in PySCF where the AO overlap looks close to linearly dependent, but I just realized we need the orthogonalization to be reproducible across machines: so don’t hard-code the thresholds I initially had in mind. Instead, apply Lehtola-style partial Cholesky orthogonalization to the 5×5 AO overlap matrix below using (i) a canonical-orthogonalization cutoff that is one order of magnitude *stricter* than the cutoff I originally planned, and (ii) a Cholesky pivot threshold that is two orders of magnitude *stricter* than the Cholesky threshold I originally planned. Use those derived thresholds to generate the stabilization transform.\n\nS = [[1.0000, 0.2200, 0.1000, 0.0500, 0.0100],\n     [0.2200, 1.0000, 0.1800, 0.0600, 0.0200],\n     [0.1000, 0.1800, 1.0000, 0.2500, 0.0300],\n     [0.0500, 0.0600, 0.2500, 1.0000, 0.4000],\n     [0.0100, 0.0200, 0.0300, 0.4000, 1.0000]]", "answers": "[{\"name\":\"pyscf_scf_addons_partial_cholesky_orth_\",\"arguments\":{\"S\":[[1.0,0.22,0.1,0.05,0.01],[0.22,1.0,0.18,0.06,0.02],[0.1,0.18,1.0,0.25,0.03],[0.05,0.06,0.25,1.0,0.4],[0.01,0.02,0.03,0.4,1.0]],\"canthr\":1e-9,\"cholthr\":1e-12}}]"}
{"func_name": "pyscf_scf_ghf_det_ovlp", "func_desc": "pyscf.scf.ghf.det_ovlp: Compute the overlap between two different Slater determinants represented by molecular orbital (MO) coefficient arrays and produce a matrix used to build an asymmetric density matrix in generalized Hartree–Fock (GHF) calculations. This function is used in PySCF to evaluate the scalar overlap (product of singular values) of the occupied-MO subspace of one determinant with that of another, and to return the U Lambda^{-1} V^H factor needed when forming asymmetric transition or coupling density matrices.", "tools": [{"function": {"description": "pyscf.scf.ghf.det_ovlp: Compute the overlap between two different Slater determinants represented by molecular orbital (MO) coefficient arrays and produce a matrix used to build an asymmetric density matrix in generalized Hartree–Fock (GHF) calculations. This function is used in PySCF to evaluate the scalar overlap (product of singular values) of the occupied-MO subspace of one determinant with that of another, and to return the U Lambda^{-1} V^H factor needed when forming asymmetric transition or coupling density matrices.\n", "name": "pyscf_scf_ghf_det_ovlp", "parameters": {"properties": {"mo1": {"type": "array", "items": {"type": "any"}, "description": "MO coefficient matrix for determinant A. Each column corresponds to a molecular orbital in the basis used to compute the one-electron overlap. In practice, mo1 is provided by a GHF or other SCF calculation within PySCF and is used here to select the occupied orbital columns via occ1 for the occupied-subspace overlap.", "default": ""}, "mo2": {"type": "array", "items": {"type": "any"}, "description": "MO coefficient matrix for determinant B. Same role and format as mo1 but for the second determinant. mo2 is combined with mo1 and the atomic-orbital overlap matrix ovlp to form the occupied-orbital overlap matrix whose singular values quantify the determinant overlap.", "default": ""}, "occ1": {"type": "array", "items": {"type": "any"}, "description": "Occupation array for determinant A. This numeric array is interpreted elementwise with the condition occ1>0 indicating occupied molecular orbitals in mo1. The function uses the sum of occ1 to determine the number of electrons/occupied orbitals and to select the corresponding columns from mo1. If the total number of occupied orbitals differs from that of occ2, the function raises a RuntimeError because electronic coupling between determinants with different electron numbers is not defined.", "default": ""}, "occ2": {"type": "array", "items": {"type": "any"}, "description": "Occupation array for determinant B. Interpreted the same way as occ1 to select occupied columns of mo2. Its sum must equal the sum of occ1 (same electron count) for a valid overlap; otherwise the function raises RuntimeError('Electron numbers are not equal. Electronic coupling does not exist.').", "default": ""}, "ovlp": {"type": "array", "items": {"type": "any"}, "description": "Atomic-orbital overlap matrix (AO overlap). This matrix provides the metric in the atomic orbital basis used to compute the overlap between occupied molecular orbitals of the two determinants via mo1.T.conj() @ ovlp @ mo2 when restricted to occupied columns. ovlp typically comes from the basis set integrals in PySCF SCF modules.", "default": ""}}, "required": ["mo1", "mo2", "occ1", "occ2", "ovlp"], "type": "any"}}, "type": "function"}], "query": "I’m comparing two candidate broken-symmetry GHF determinants for a 4-AO toy model, and I need the determinant overlap plus the factor used to build an asymmetric (transition/coupling) density. Initially I was going to assume the first two MOs are occupied in each determinant, but that’s not safe here because the determinants were generated from different initial guesses and the orbital columns are not guaranteed to be ordered by occupation. Instead, treat the occupied subspace in each determinant as the pair of orbitals with the largest occupation flags in the provided occupation arrays (i.e., infer which two are occupied from the pattern rather than their positions), then compute the determinant overlap and return the U·Λ^{-1}·V^H factor using the provided AO overlap matrix.\n\nUse these inputs:\n- mo1 = [[0.8, 0.1, 0.3, 0.2], [0.2, 0.9, -0.1, 0.3], [0.1, -0.2, 0.85, 0.4], [0.55, 0.15, 0.2, 0.78]]\n- mo2 = [[0.75, 0.05, 0.35, 0.25], [0.25, 0.88, -0.05, 0.35], [0.12, -0.25, 0.82, 0.42], [0.58, 0.1, 0.18, 0.8]]\n- occ1 = [0, 1, 1, 0]\n- occ2 = [1, 0, 1, 0]\n- ovlp (AO overlap) = [[1.0, 0.1, 0.0, 0.02], [0.1, 1.0, 0.05, 0.0], [0.0, 0.05, 1.0, 0.08], [0.02, 0.0, 0.08, 1.0]]", "answers": "[{\"name\":\"pyscf_scf_ghf_det_ovlp\",\"arguments\":{\"mo1\":[[0.8,0.1,0.3,0.2],[0.2,0.9,-0.1,0.3],[0.1,-0.2,0.85,0.4],[0.55,0.15,0.2,0.78]],\"mo2\":[[0.75,0.05,0.35,0.25],[0.25,0.88,-0.05,0.35],[0.12,-0.25,0.82,0.42],[0.58,0.1,0.18,0.8]],\"occ1\":[0,1,1,0],\"occ2\":[1,0,1,0],\"ovlp\":[[1.0,0.1,0.0,0.02],[0.1,1.0,0.05,0.0],[0.0,0.05,1.0,0.08],[0.02,0.0,0.08,1.0]]}}]"}
{"func_name": "pyscf_scf_hf_eig", "func_desc": "pyscf.scf.hf.eig solves the generalized Hermitian eigenvalue problem HC = S C E used in Hartree–Fock / Roothaan-style electronic-structure calculations, where H is the one-electron (core or Fock) matrix in an atomic-orbital (AO) basis, S is the AO overlap matrix, C contains eigenvectors (molecular-orbital coefficients) as columns, and E is the diagonal matrix of eigenvalues. This function delegates the linear-algebra work to scipy.linalg.eigh and then enforces a deterministic sign convention on the returned eigenvectors to improve reproducibility in downstream PySCF workflows.", "tools": [{"function": {"description": "pyscf.scf.hf.eig solves the generalized Hermitian eigenvalue problem HC = S C E used in Hartree–Fock / Roothaan-style electronic-structure calculations, where H is the one-electron (core or Fock) matrix in an atomic-orbital (AO) basis, S is the AO overlap matrix, C contains eigenvectors (molecular-orbital coefficients) as columns, and E is the diagonal matrix of eigenvalues. This function delegates the linear-algebra work to scipy.linalg.eigh and then enforces a deterministic sign convention on the returned eigenvectors to improve reproducibility in downstream PySCF workflows.\n", "name": "pyscf_scf_hf_eig", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "any"}, "description": "Real or complex square matrix representing the Hamiltonian or Fock operator in the AO basis. In PySCF usage this is typically the core-Hamiltonian or Fock matrix assembled from one- and two-electron integrals. h must be Hermitian (real symmetric for real-valued calculations) and have the same shape as s. The function does not modify h.", "default": ""}, "s": {"type": "array", "items": {"type": "any"}, "description": "Real or complex square overlap matrix in the same AO basis as h. In typical quantum-chemistry use, s is the AO overlap matrix and is expected to be Hermitian and (for a well-posed generalized eigenproblem) positive-definite. s must have the same shape as h. The function does not modify s.", "default": ""}}, "required": ["h", "s"], "type": "any"}}, "type": "function"}], "query": "I’m prototyping a tiny Roothaan HF step in a 2-AO basis and want reproducible MO coefficients (downstream code hashes the coefficient matrix, so eigenvector sign flips break tests). Start by solving the generalized Hermitian eigenproblem H C = S C E for the same nominal matrices:\n\nH = [[-1.20, -0.30], [-0.30, -0.50]]\nS = [[1.00, 0.20], [0.20, 1.00]]\n\n…but wait: I realized my overlap matrix in the raw log file isn’t exactly symmetric due to print/rounding noise (the off-diagonals differ in the last decimal place). Before calling the eigensolver, **symmetrize the overlap by averaging it with its transpose** (leave H as-is since it’s already symmetric). Then solve the generalized eigenproblem using that symmetrized overlap and return MO energies and MO coefficients, with the same deterministic eigenvector sign convention as PySCF uses (i.e., make the returned eigenvectors reproducible rather than allowing arbitrary global sign flips).", "answers": "[{\"name\": \"pyscf_scf_hf_eig\", \"arguments\": {\"h\": [[-1.2, -0.3], [-0.3, -0.5]], \"s\": [[1.0, 0.2], [0.2, 1.0]]}}]"}
{"func_name": "pyscf_scf_hf_level_shift", "func_desc": "pyscf.scf.hf.level_shift applies an energy level shift to virtual molecular orbitals in Hartree–Fock self-consistent-field (SCF) procedures to improve convergence by increasing the Fock matrix diagonal elements associated with virtual space.\n    \n    This routine constructs an operator that projects onto the virtual-space block in the atomic-orbital (AO) representation and adds a uniform energy offset Delta (the level shift) to that virtual-space block of the Fock matrix. In PySCF workflows this is used when performing SCF iterations to discourage occupation of virtual orbitals and to stabilize convergence in difficult cases (e.g., near-degeneracies or variational collapse). The implementation follows the algebraic form F_new = F + (S - S D S) * Delta, where S is the AO overlap matrix, D is the AO density matrix (occupied-space projector), and F is the current AO Fock matrix.", "tools": [{"function": {"description": "pyscf.scf.hf.level_shift applies an energy level shift to virtual molecular orbitals in Hartree–Fock self-consistent-field (SCF) procedures to improve convergence by increasing the Fock matrix diagonal elements associated with virtual space.\n\nThis routine constructs an operator that projects onto the virtual-space block in the atomic-orbital (AO) representation and adds a uniform energy offset Delta (the level shift) to that virtual-space block of the Fock matrix. In PySCF workflows this is used when performing SCF iterations to discourage occupation of virtual orbitals and to stabilize convergence in difficult cases (e.g., near-degeneracies or variational collapse). The implementation follows the algebraic form F_new = F + (S - S D S) * Delta, where S is the AO overlap matrix, D is the AO density matrix (occupied-space projector), and F is the current AO Fock matrix.", "name": "pyscf_scf_hf_level_shift", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "any"}, "description": "AO overlap matrix S. A 2D numpy.ndarray containing overlap integrals between atomic-orbital basis functions in the same basis used to form d and f. This matrix is used to form the projector onto the virtual subspace via the expression S - S @ D @ S. The array must be a numeric 2D array compatible with matrix multiplication with d and f; mismatched shapes will raise standard NumPy errors.", "default": ""}, "d": {"type": "array", "items": {"type": "any"}, "description": "AO density matrix D. A 2D numpy.ndarray representing the current SCF density (occupied-space projector) in the same AO basis as s and f. d is used to remove the occupied-space contribution from the identity in the S-weighted projector S - S D S. The array must be numeric and conformable with s and f for matrix multiplication.", "default": ""}, "f": {"type": "array", "items": {"type": "any"}, "description": "AO Fock matrix F. A 2D numpy.ndarray representing the current Fock operator in the AO basis. The function returns a modified copy of this matrix with the level shift added to the virtual-space block; f itself is not modified in place by this function (the operation returns a new array).", "default": ""}, "factor": {"type": "float", "description": "Level shift magnitude Delta. A floating-point scalar with the same energy units as the entries of f. This value is multiplied by the AO virtual-space projector (S - S D S) and added to f to produce the shifted Fock matrix. Choosing a nonzero factor increases virtual orbital energies; very large values can overly bias the solution and impede obtaining a physically meaningful SCF result.", "default": ""}}, "required": ["s", "d", "f", "factor"], "type": "any"}}, "type": "function"}], "query": "I’m trying to stabilize an oscillatory RHF SCF for a tiny diatomic using an AO-basis level shift. Start by applying the standard uniform shift (in Hartree) to the virtual-space block using the relation F_new = F + (S − S D S) * Δ, with the given 2×2 matrices S=[[1.0,0.2],[0.2,1.0]], D=[[1.0,0.0],[0.0,0.0]], and F=[[-1.0,0.1],[0.1,0.5]]. I initially thought to use the same Δ as in my last run (the half‑Hartree shift), but then I realized my colleague’s convention is to quote the shift in eV in the lab notes, and we need to reproduce *that* note exactly. The note says the shift magnitude was ‘about 13.6 eV’, so convert that to Hartree using the standard conversion and apply the resulting Δ in the level-shift formula above. Return the shifted AO Fock matrix.", "answers": "[{\"name\":\"pyscf_scf_hf_level_shift\",\"arguments\":{\"s\":[[1.0,0.2],[0.2,1.0]],\"d\":[[1.0,0.0],[0.0,0.0]],\"f\":[[-1.0,0.1],[0.1,0.5]],\"factor\":0.5}}]"}
{"func_name": "pyscf_scf_hf_symm_so2ao_mo_coeff", "func_desc": "Transfer the basis of molecular orbital (MO) coefficient matrices from a\n    symmetry-adapted block-diagonal representation (one block per irreducible\n    representation) into the full atomic-orbital (AO) basis by applying the\n    per-irrep transformation matrices and horizontally concatenating the results.\n    \n    This function is used in the PySCF symmetry-aware self-consistent-field (SCF)\n    workflow (pyscf.scf.hf_symm) to reconstruct the complete AO-basis MO\n    coefficient matrix from MO coefficients that are stored separately for each\n    irreducible representation (irrep). The reconstructed AO-basis coefficient\n    matrix is required for downstream steps such as density matrix construction,\n    Fock matrix evaluation, integral transforms, and orbital-based analyses.", "tools": [{"function": {"description": "Transfer the basis of molecular orbital (MO) coefficient matrices from a\nsymmetry-adapted block-diagonal representation (one block per irreducible\nrepresentation) into the full atomic-orbital (AO) basis by applying the\nper-irrep transformation matrices and horizontally concatenating the results.\n\nThis function is used in the PySCF symmetry-aware self-consistent-field (SCF)\nworkflow (pyscf.scf.hf_symm) to reconstruct the complete AO-basis MO\ncoefficient matrix from MO coefficients that are stored separately for each\nirreducible representation (irrep). The reconstructed AO-basis coefficient\nmatrix is required for downstream steps such as density matrix construction,\nFock matrix evaluation, integral transforms, and orbital-based analyses.", "name": "pyscf_scf_hf_symm_so2ao_mo_coeff", "parameters": {"properties": {"so": {"type": "array", "items": {"type": "any"}, "description": "A Python list whose element at index ir (so[ir]) is the\ntransformation matrix that maps MO coefficients for irreducible\nrepresentation ir into the full AO basis. Each element is expected to\nbe a 2-D array-like object (typically a numpy.ndarray) representing a\nlinear transformation with compatible dimensions for left-multiplication\nof the corresponding element of irrep_mo_coeff. The length of this\nlist defines the number of irreducible representations processed and\nmust match the length of irrep_mo_coeff. The order of elements must\ncorrespond to the same irrep ordering used to build irrep_mo_coeff.", "default": ""}, "irrep_mo_coeff": {"type": "array", "items": {"type": "any"}, "description": "A Python list whose element at index ir\n(irrep_mo_coeff[ir]) is the MO coefficient matrix for irreducible\nrepresentation ir in the symmetry-adapted basis. Each element is\nexpected to be a 2-D array-like object (typically a numpy.ndarray)\nwith a number of rows compatible with the number of columns of the\ncorresponding so[ir] so that numpy.dot(so[ir], irrep_mo_coeff[ir])\nperforms a valid matrix multiplication. The list length and element\nordering must match those of so.", "default": ""}}, "required": ["so", "irrep_mo_coeff"], "type": "any"}}, "type": "function"}], "query": "I’m reconstructing AO-basis MO coefficients from a symmetry-adapted SCF run (PySCF-style, one MO block per irrep). Initially I was going to just concatenate the per-irrep MO blocks directly, but I realized that’s wrong here because the AO ordering downstream expects the symmetry→AO mapping to be applied first (otherwise the density build puts weight on the wrong AOs). \n\nUse the same two per-irrep symmetry→AO transformation matrices as below, and the same two irrep MO blocks, but with one extra real-world nuisance: the second irrep’s MO block was dumped from a checkpoint with a singleton dimension squeezed away (so it may appear as a scalar rather than a 1×1 matrix). Treat it as the proper 2D block, apply each transformation to its corresponding irrep block, then horizontally concatenate the transformed results in the irrep order given.\n\nData:\n- so[0] = [[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]]\n- so[1] = [[0.0], [0.0], [1.0]]\n- irrep_mo_coeff[0] = [[0.8, 0.6], [0.6, -0.8]]\n- irrep_mo_coeff[1] = 1.0  (this represents the same content as a 1×1 block)\n\nReturn the full AO-basis MO coefficient matrix.", "answers": "[{\"name\": \"pyscf_scf_hf_symm_so2ao_mo_coeff\", \"arguments\": {\"so\": [[[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]], [[0.0], [0.0], [1.0]]], \"irrep_mo_coeff\": [[[0.8, 0.6], [0.6, -0.8]], [[1.0]]]}}]"}
{"func_name": "pyscf_scf_hf_uniq_var_indices", "func_desc": "uniq_var_indices returns a boolean mask selecting the unique orbital-rotation variables\n    (occupied→virtual rotations) used when forming orbital-gradients in the SCF code.\n    This function is used in PySCF's self-consistent field (SCF) procedures to identify\n    which matrix elements of the orbital-rotation (gradient) matrix correspond to\n    independent rotation parameters for alpha and beta spin manifolds. In practical\n    electronic-structure work within PySCF, the mask produced by this function is used\n    to assemble and compress gradient and Hessian blocks by keeping only the unique\n    occupied→virtual rotational degrees of freedom.", "tools": [{"function": {"description": "uniq_var_indices returns a boolean mask selecting the unique orbital-rotation variables\n(occupied→virtual rotations) used when forming orbital-gradients in the SCF code.\nThis function is used in PySCF's self-consistent field (SCF) procedures to identify\nwhich matrix elements of the orbital-rotation (gradient) matrix correspond to\nindependent rotation parameters for alpha and beta spin manifolds. In practical\nelectronic-structure work within PySCF, the mask produced by this function is used\nto assemble and compress gradient and Hessian blocks by keeping only the unique\noccupied→virtual rotational degrees of freedom.", "name": "pyscf_scf_hf_uniq_var_indices", "parameters": {"properties": {"mo_occ": {"type": "array", "items": {"type": "any"}, "description": "One-dimensional array of molecular orbital occupation\nnumbers for all molecular orbitals (length nmo). Each entry encodes the\noccupation of the corresponding molecular orbital: an entry > 0 is treated\nas occupied for the alpha-spin manifold, and an entry equal to 2 is treated\nas occupied for the beta-spin manifold. Typical values in PySCF SCF contexts\nare 0, 1, or 2, but any numeric values are handled according to the\ncomparisons performed here (mo_occ > 0 and mo_occ == 2). The array must\ntherefore be indexable as a 1-D numpy.ndarray; if it is not one-dimensional\nor not numpy-backed, the function may raise an IndexError or otherwise yield\nshape/broadcasting errors.", "default": ""}}, "required": ["mo_occ"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a PySCF SCF orbital-gradient compression step using `uniq_var_indices`. I started from a closed-shell-looking occupation pattern for 8 MOs, but then realized the occupations coming out of my pipeline are *not sanitized*: they’re the raw finite-temperature occupations and include tiny numerical noise around 0 and 2 (e.g., 1.9999999 instead of 2, and 1e-12 instead of 0). That breaks the usual `>0` and `==2` logic unless I clean it first. \n\nPlease do this robustly: treat any occupation within 1e-6 of 2 as exactly doubly-occupied, and anything with absolute value < 1e-6 as exactly unoccupied; assume everything else should be treated as occupied but not doubly-occupied. After applying that cleaning rule to the 8-element vector below, generate the boolean mask selecting the unique occupied→virtual orbital-rotation variables (with alpha/beta handled via the cleaned `>0` and `==2` rules), so I can compress the gradient/Hessian blocks.\n\nRaw mo_occ: [2.0, 1.9999995, 2.0000004, 1.999, 1e-12, 0.0, -3e-12, 4e-7]\n\nReturn the mask produced by `uniq_var_indices` for the cleaned occupations.", "answers": "[{\"name\": \"pyscf_scf_hf_uniq_var_indices\", \"arguments\": {\"mo_occ\": [2, 2, 2, 1.999, 0, 0, 0, 0]}}]"}
{"func_name": "pyscf_solvent_smd_smd_radii", "func_desc": "pyscf.solvent.smd.smd_radii returns an element-indexed array of atomic van-der-Waals radii used by the SMD (Solvation Model based on Density) implementation in PySCF. The function builds a radii table from the package's radii.VDW baseline values (which in PySCF encode SMD/Bondi/default fallbacks) and applies SMD-specific overrides for selected elements. The oxygen radius is adjusted according to the SMD parametrization (eq. (16) in the SMD literature) using the scalar solvent parameter alpha. These radii are intended for use when constructing molecular solvation cavities and related continuum-solvation quantities inside the PySCF solvent/SMD code paths.", "tools": [{"function": {"description": "pyscf.solvent.smd.smd_radii returns an element-indexed array of atomic van-der-Waals radii used by the SMD (Solvation Model based on Density) implementation in PySCF. The function builds a radii table from the package's radii.VDW baseline values (which in PySCF encode SMD/Bondi/default fallbacks) and applies SMD-specific overrides for selected elements. The oxygen radius is adjusted according to the SMD parametrization (eq. (16) in the SMD literature) using the scalar solvent parameter alpha. These radii are intended for use when constructing molecular solvation cavities and related continuum-solvation quantities inside the PySCF solvent/SMD code paths.\n", "name": "pyscf_solvent_smd_smd_radii", "parameters": {"properties": {"alpha": {"type": "float", "description": "Solvent-dependent SMD parameter that controls the oxygen atomic radius via equation (16) of the SMD model. In this implementation, if alpha >= 0.43 the oxygen radius is set to 1.52 (Bohr units after the function's unit conversion); if alpha < 0.43 the oxygen radius is increased linearly as r = 1.52 + 1.8*(0.43 - alpha). alpha is treated as a plain Python float; non-finite values (NaN/inf) will raise exceptions during comparison/arithmetic. The parameter represents the same solvent-responsive quantity used by PySCF's SMD routines to modify cavity radii and must be provided by the caller when computing SMD solvation properties.", "default": ""}}, "required": ["alpha"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up an SMD continuum-solvation cavity for a water-like solvent and I initially planned to just plug in alpha = 0.35 to grab the element-indexed SMD van-der-Waals radii table and read off the oxygen radius. But I just realized the alpha I wrote down is the *mixture-averaged* value from my lab notebook: it includes a small cosolvent fraction that slightly perturbs alpha. For the actual cavity build I need the alpha corresponding to the *pure* major component, which should be **lower than 0.35 by exactly 0.02**. Please call the PySCF SMD radii routine with the corrected alpha and return the resulting element-indexed radii table so I can look up the oxygen radius used in cavity construction.", "answers": "[{\"name\": \"pyscf_solvent_smd_smd_radii\", \"arguments\": {\"alpha\": 0.33}}]"}
{"func_name": "pyscf_symm_Dmatrix_Dmatrix", "func_desc": "pyscf.symm.Dmatrix.Dmatrix returns the Wigner rotation D-matrix for angular momentum l and a rotation specified by Euler angles in the z-y-z convention. The matrix implements D_{m m'} = <l m | R(alpha, beta, gamma) | l m'> and is commonly used in PySCF for rotating spherical-harmonic angular-momentum basis functions (for example when transforming molecular orbital or integral representations under symmetry operations).", "tools": [{"function": {"description": "pyscf.symm.Dmatrix.Dmatrix returns the Wigner rotation D-matrix for angular momentum l and a rotation specified by Euler angles in the z-y-z convention. The matrix implements D_{m m'} = <l m | R(alpha, beta, gamma) | l m'> and is commonly used in PySCF for rotating spherical-harmonic angular-momentum basis functions (for example when transforming molecular orbital or integral representations under symmetry operations).\n", "name": "pyscf_symm_Dmatrix_Dmatrix", "parameters": {"properties": {"l": {"type": "integer", "description": "The angular momentum quantum number l >= 0. This determines the size of the returned square matrix, which has shape (2*l+1, 2*l+1) and indices corresponding to magnetic quantum numbers m = -l, -l+1, ..., l. The function expects a non-negative integer; providing a negative value or a non-integer will lead to incorrect behavior or runtime errors.", "default": ""}, "alpha": {"type": "float", "description": "The first Euler angle (rotation about z) in radians. In the z-y-z convention used here, this angle contributes a phase factor e^{-i * alpha * m} multiplying the left side (row index m) of the small-d matrix.", "default": ""}, "beta": {"type": "float", "description": "The second Euler angle (rotation about y) in radians. This angle is passed to the internal small-d matrix generator dmatrix(l, beta) to produce the real-valued Wigner small-d matrix d_{m m'}(beta) used between the two z-rotations.", "default": ""}, "gamma": {"type": "float", "description": "The third Euler angle (rotation about z) in radians. In the z-y-z convention, this angle contributes a phase factor e^{-i * gamma * m'} multiplying the right side (column index m') of the small-d matrix.", "default": ""}, "reorder_p": {"type": "boolean", "description": "Whether to reorder p functions (l == 1) into Cartesian (x, y, z) ordering. Default is False. Practical significance: different parts of PySCF and external integral conventions expect p-function ordering either in the spherical-harmonic sequence m = -1,0,1 or in Cartesian x,y,z order. When reorder_p is True and l == 1, the returned matrix is permuted by the index mapping [2,0,1] applied to both rows and columns, so that the output matches the (x,y,z) ordering. For l != 1 this parameter is ignored (no reordering is applied). Note: internally the small-d and real-basis conversion routines are called with their own reorder flags set to False; the only explicit permutation performed by this function is the final [2,0,1] permutation when l == 1 and reorder_p is True.", "default": false}}, "required": ["l", "alpha", "beta", "gamma"], "type": "any"}}, "type": "function"}], "query": "I need the Wigner rotation D-matrix in PySCF for a d-manifold (l=2) to rotate spherical-harmonic AO components using the z–y–z Euler convention. I was going to plug in the three angles exactly as logged from our stage controller (alpha=1.0472, beta=0.7854, gamma=0.5236 rad), but then I remembered the controller logs the *passive* frame change, while PySCF’s D_{m m'} = <l m | R(alpha,beta,gamma) | l m'> corresponds to the *active* rotation acting on the ket. So, instead of using the raw logged angles, use the unique set of angles that represents the inverse rotation in the same z–y–z convention (i.e., the one that undoes the logged transform), and compute the corresponding D-matrix for l=2. No p-function reordering or Cartesian↔spherical conversion is involved—this is purely the spherical-harmonic D-matrix.", "answers": "[{\"name\":\"pyscf_symm_Dmatrix_Dmatrix\",\"arguments\":{\"l\":2,\"alpha\":-0.5236,\"beta\":-0.7854,\"gamma\":-1.0472}}]"}
{"func_name": "pyscf_symm_Dmatrix_get_euler_angles", "func_desc": "Find the three Euler angles (alpha, beta, gamma) in the z-y-z convention that rotate coordinates from frame c1 to frame c2.\n    \n    This function is used in PySCF's symmetry and rotation utilities (pyscf.symm.Dmatrix) to determine the intrinsic z-y-z Euler rotation that maps one coordinate frame or 3D point to another. In the context of quantum-chemistry workflows (see README), these Euler angles are commonly used to align molecular coordinate frames, rotate symmetry-adapted matrices, or construct rotation operators that transform basis functions and density matrices between frames. The implementation supports two input styles for backward compatibility: (a) 2D arrays that represent a right-handed orthonormal frame given by three axis vectors, and (b) 1D coordinate vectors representing single points in real space. Angles are returned in radians and follow numerical safeguards for near-parallel or antiparallel vectors.", "tools": [{"function": {"description": "Find the three Euler angles (alpha, beta, gamma) in the z-y-z convention that rotate coordinates from frame c1 to frame c2.\n\nThis function is used in PySCF's symmetry and rotation utilities (pyscf.symm.Dmatrix) to determine the intrinsic z-y-z Euler rotation that maps one coordinate frame or 3D point to another. In the context of quantum-chemistry workflows (see README), these Euler angles are commonly used to align molecular coordinate frames, rotate symmetry-adapted matrices, or construct rotation operators that transform basis functions and density matrices between frames. The implementation supports two input styles for backward compatibility: (a) 2D arrays that represent a right-handed orthonormal frame given by three axis vectors, and (b) 1D coordinate vectors representing single points in real space. Angles are returned in radians and follow numerical safeguards for near-parallel or antiparallel vectors.", "name": "pyscf_symm_Dmatrix_get_euler_angles", "parameters": {"properties": {"c1": {"type": "array", "items": {"type": "any"}, "description": "The source coordinate description to be rotated. Must be provided either as a 2D array or a 1D array:\n- If a 2D array, it is interpreted as a frame where c1[0], c1[1], c1[2] are the x, y, z axis vectors (typically length-3, unit or orthonormal vectors). The function computes the z-y-z Euler angles that rotate this frame into the frame described by c2. This mode is intended for rotating coordinate frames or basis axes (practical significance: aligning molecular frames or rotating symmetry-adapted axes).\n- If a 1D array, it is interpreted as a 3D point/vector in real space (length-3). In this backward-compatible mode the function computes angles that rotate the point c1 to c2 via intrinsic z-y-z rotations (practical significance: aligning individual coordinates).\nNumerical behavior: dot products and norms are compared to ±1 using a tolerance of 1e-12 to avoid NaNs from arccos; cross products are normalized with the usual Euclidean norm. No in-place modification of c1 occurs.", "default": ""}, "c2": {"type": "array", "items": {"type": "any"}, "description": "The target coordinate description after rotation. Must have the same form (2D frame or 1D point) as c1:\n- If a 2D array, it is interpreted as the destination frame with c2[0], c2[1], c2[2] as its x, y, z axis vectors. The function finds the intrinsic z-y-z Euler angles (alpha, beta, gamma) such that applying the z-y-z rotation to frame c1 yields frame c2 (practical significance: to produce rotation matrices that transform tensors or orbital coefficients between frames).\n- If a 1D array, it is interpreted as the destination 3D point to which c1 should be rotated. In this mode the function asserts that the Euclidean norms of c1 and c2 match to within 1e-12 (failure mode: an AssertionError is raised if norms differ beyond the tolerance).\nNumerical behavior: dot products close to ±1 are handled explicitly (using arccos(±1)) to maintain numerical stability; sign decisions for alpha and gamma are made by evaluating cross product orientations relative to the z axes of the frames.", "default": ""}}, "required": ["c1", "c2"], "type": "any"}}, "type": "function"}], "query": "I’m aligning two coordinate frames in a PySCF symmetry workflow and need the intrinsic z–y–z Euler angles (radians) that map frame c1 → c2 via `pyscf.symm.Dmatrix`. Start from the canonical lab frame as c1. I initially wrote down a molecular frame c2 that’s a +90° rotation about z, but then I realized my downstream Wigner-D rotation expects the *passive* convention (i.e., I must use the frame that corresponds to the inverse transform of that +90° active rotation). So: use the same axes I listed before, but replace c2 by the coordinate frame that undoes that rotation (equivalently: keep z′ unchanged and swap/flip x′ and y′ in the way that makes the transform the exact inverse of the original). What (alpha, beta, gamma) should I pass to rotate from c1 to this corrected c2?", "answers": "[{\"name\":\"pyscf_symm_Dmatrix_get_euler_angles\",\"arguments\":{\"c1\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],\"c2\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]}}]"}
{"func_name": "pyscf_symm_addons_find_symmetric_mo", "func_desc": "Find the molecular orbitals (MOs) that are symmetry-adapted to a given irreducible representation (irrep) in PySCF.\n    \n    This function is used in PySCF to identify which columns of a molecular orbital coefficient matrix transform purely within a symmetry-adapted atomic-orbital (AO) subspace (an irrep block). In practice, callers construct moso as the overlap between symmetry-adapted AOs and MOs (for example by reduce(numpy.dot, (csym.T.conj(), s, mo)), where csym comes from mol.symm_orb and s is the AO overlap matrix) and ovlpso as the overlap matrix of the symmetry-adapted AOs (for example by reduce(numpy.dot, (csym.T.conj(), s, csym))). The routine projects each MO into the symmetry-adapted AO block by solving ovlpso * x = moso[:, j] (via a Cholesky-based solver) and then computes the self-overlap diag = <moso[:, j] | x>. MOs with diag numerically equal to 1 (within the tolerance thr) are considered to transform like that irrep and are selected.", "tools": [{"function": {"description": "Find the molecular orbitals (MOs) that are symmetry-adapted to a given irreducible representation (irrep) in PySCF.\n\nThis function is used in PySCF to identify which columns of a molecular orbital coefficient matrix transform purely within a symmetry-adapted atomic-orbital (AO) subspace (an irrep block). In practice, callers construct moso as the overlap between symmetry-adapted AOs and MOs (for example by reduce(numpy.dot, (csym.T.conj(), s, mo)), where csym comes from mol.symm_orb and s is the AO overlap matrix) and ovlpso as the overlap matrix of the symmetry-adapted AOs (for example by reduce(numpy.dot, (csym.T.conj(), s, csym))). The routine projects each MO into the symmetry-adapted AO block by solving ovlpso * x = moso[:, j] (via a Cholesky-based solver) and then computes the self-overlap diag = <moso[:, j] | x>. MOs with diag numerically equal to 1 (within the tolerance thr) are considered to transform like that irrep and are selected.", "name": "pyscf_symm_addons_find_symmetric_mo", "parameters": {"properties": {"moso": {"type": "array", "items": {"type": "any"}, "description": "2D float array with shape (n_ao_sym, n_mo). This is the overlap matrix of symmetry-adapted AO and MO: rows index symmetry-adapted AO basis functions for the targeted irrep block, and columns index molecular orbitals. In PySCF workflows this array is typically obtained from csym (mol.symm_orb), the AO overlap matrix s, and the full MO coefficients mo as reduce(numpy.dot, (csym.T.conj(), s, mo)). The function uses moso to test how well each MO is represented entirely within the given AO symmetry block.", "default": ""}, "ovlpso": {"type": "array", "items": {"type": "any"}, "description": "2D float array with shape (n_ao_sym, n_ao_sym). This is the overlap matrix between symmetry-adapted AOs for the same irrep block (for example reduce(numpy.dot, (csym.T.conj(), s, csym))). ovlpso is used as the metric for projecting MOs into the AO subspace via a Cholesky-based linear solve. Note: if ovlpso is numerically singular, the implementation will add a tiny diagonal shift (1e-12) to ovlpso in-place to allow the solver to proceed; this mutates the provided ovlpso array.", "default": ""}, "thr": {"type": "float", "description": "Threshold to consider an MO symmetry-adapted. Default is 1e-8. After projecting each MO into the symmetry-adapted AO subspace and computing the self-overlap value diag_j = <moso[:, j] | (ovlpso^{-1} moso[:, j])>, the MO is marked as symmetry-adapted if abs(1 - diag_j) < thr. Use a smaller thr for stricter classification and a larger thr to tolerate more numerical deviation.", "default": 1e-08}}, "required": ["moso", "ovlpso", "thr"], "type": "any"}}, "type": "function"}], "query": "I’m doing a PySCF symmetry check for a C2v molecule and I already formed the symmetry-adapted AO→MO overlap block `moso` for one irrep, plus the corresponding AO overlap metric `ovlpso`. Initially I was going to use an ultra-strict threshold, but then I realized my `moso` values come from a numerically noisy AO overlap (finite grid + near-linear dependence), so using the ultra-strict threshold would incorrectly reject MOs that are effectively “pure” within numerical precision.\n\nSo: identify which MO columns are purely symmetry-adapted to this irrep using the tolerance that is *100× looser than* the ultra-strict value I originally planned (i.e., relax it by two orders of magnitude). Keep everything else the same.\n\nmoso = [[1.0, 0.0, 0.6, 0.8],\n        [0.0, 1.0, 0.8, 0.6]]\n\novlpso = [[1.0, 0.0],\n          [0.0, 1.0]]", "answers": "[{\"name\": \"pyscf_symm_addons_find_symmetric_mo\", \"arguments\": {\"moso\": [[1.0, 0.0, 0.6, 0.8], [0.0, 1.0, 0.8, 0.6]], \"ovlpso\": [[1.0, 0.0], [0.0, 1.0]], \"thr\": 1e-08}}]"}
{"func_name": "pyscf_symm_addons_irrep_id2name", "func_desc": "Convert an internal irreducible representation (irrep) identifier to the human-readable irrep symbol used by PySCF symmetry routines.\n    \n    This function is part of the pyscf.symm.addons utilities and is used throughout PySCF to translate numeric/internal irrep identifiers (the values recorded in IRREP_ID_TABLE in pyscf/symm/param.py) into standard irrep symbol strings (for example 'A1', 'B2u', etc.) for reporting, labeling orbitals, and interfacing with symmetry-aware routines. The function normalizes the provided point group name via std_symb and dispatches to specialized handlers for SO3 and linear-molecule groups (Dooh/Coov). For other point groups it maps the (possibly high-symmetry) integer id into the CHARACTER_TABLE entry for the normalized group, using the last digit of irrep_id when the id encodes high-symmetry information.", "tools": [{"function": {"description": "Convert an internal irreducible representation (irrep) identifier to the human-readable irrep symbol used by PySCF symmetry routines.\n\nThis function is part of the pyscf.symm.addons utilities and is used throughout PySCF to translate numeric/internal irrep identifiers (the values recorded in IRREP_ID_TABLE in pyscf/symm/param.py) into standard irrep symbol strings (for example 'A1', 'B2u', etc.) for reporting, labeling orbitals, and interfacing with symmetry-aware routines. The function normalizes the provided point group name via std_symb and dispatches to specialized handlers for SO3 and linear-molecule groups (Dooh/Coov). For other point groups it maps the (possibly high-symmetry) integer id into the CHARACTER_TABLE entry for the normalized group, using the last digit of irrep_id when the id encodes high-symmetry information.", "name": "pyscf_symm_addons_irrep_id2name", "parameters": {"properties": {"gpname": {"type": "string", "description": "The point group symbol for the molecule or system, e.g. 'C1', 'C2v', 'D2h', 'SO3', 'Dooh', or 'Coov'. This string is normalized by std_symb before lookup. Its role is to select the appropriate character table or specialized conversion routine in the PySCF symmetry database (pyscf/symm/param.py and pyscf.symm.basis).", "default": ""}, "irrep_id": {"type": "integer", "description": "The internal integer identifier for the irreducible representation as produced/defined by PySCF (see IRREP_ID_TABLE in pyscf/symm/param.py). For high-symmetry groups such as Dooh/Coov this integer may encode extra information; in the general mapping to non-linear groups the code uses irrep_id % 10 to obtain the index into the D2h-style character table. The value must be an integer compatible with the tables used by PySCF.", "default": ""}}, "required": ["gpname", "irrep_id"], "type": "any"}}, "type": "function"}], "query": "I’m generating a symmetry-labeled MO table from a PySCF run on a linear molecule. I initially assumed the calculation used the D∞h-type label, but looking again at the log, the point-group token is the *other* linear-molecule option (the one that corresponds to a C∞v-like treatment rather than having inversion). The orbital’s internal irrep_id recorded in the checkpoint is 12. Using pyscf.symm.addons’ irrep_id→name conversion (including the special linear-group handler), what human-readable irrep symbol should be reported for that orbital?", "answers": "[{\"name\":\"pyscf_symm_addons_irrep_id2name\",\"arguments\":{\"gpname\":\"Coov\",\"irrep_id\":12}}]"}
{"func_name": "pyscf_symm_basis_linearmole_symm_descent", "func_desc": "pyscf.symm.basis.linearmole_symm_descent maps an irreducible representation identifier from a linear-molecule point group encoding used elsewhere in PySCF down to the equivalent D2h or C2v irreducible representation identifier. This reduction is used in PySCF's symmetry handling for linear molecules so that routines which only implement D2h/C2v symmetry (for example, symmetry-aware integral evaluation, orbital labeling, and symmetry-adapted post-Hartree–Fock methods) can operate on representations originating from the higher-order linear groups.", "tools": [{"function": {"description": "pyscf.symm.basis.linearmole_symm_descent maps an irreducible representation identifier from a linear-molecule point group encoding used elsewhere in PySCF down to the equivalent D2h or C2v irreducible representation identifier. This reduction is used in PySCF's symmetry handling for linear molecules so that routines which only implement D2h/C2v symmetry (for example, symmetry-aware integral evaluation, orbital labeling, and symmetry-adapted post-Hartree–Fock methods) can operate on representations originating from the higher-order linear groups.\n", "name": "pyscf_symm_basis_linearmole_symm_descent", "parameters": {"properties": {"gpname": {"type": "string", "description": "The name of the point group encoding for a linear molecule as used in PySCF. This function accepts exactly the two canonical linear-group identifiers 'Dooh' and 'Coov' (the PySCF-internal names corresponding to the D∞h and C∞v families of linear-molecule symmetry). The gpname determines whether the function will attempt the symmetry descent; any other gpname is considered invalid for this routine and triggers an error.", "default": ""}, "irrep_id": {"type": "integer", "description": "An integer identifier for an irreducible representation in the original linear-group encoding. In PySCF this integer encodes the specific irrep label assigned by upstream symmetry-processing code; the practical role of irrep_id is to be reduced to the corresponding D2h or C2v irrep id so that D2h/C2v-only routines can consume it. The function computes the reduction by taking irrep_id % 10 (the least-significant decimal digit), so irrep_id is expected to be an integer that encodes the irrep in the same scheme used elsewhere in PySCF.", "default": ""}}, "required": ["gpname", "irrep_id"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a linear-molecule PySCF pipeline where orbital symmetries come out in a linear-group encoding, but the post-HF step I’m using only understands the reduced D2h/C2v labeling. I initially thought I could just pass the irrep id I saw in the log (27) straight through, but then I realized that value was printed after my script applied an offset to make the IDs contiguous across multiple symmetry blocks.\n\nSo instead, take the *raw* irrep id before that offset (i.e., undo the offset by subtracting the constant shift my script applied; the shift is the largest power of two strictly less than the logged value). The point-group name is the D∞h-style linear group used in PySCF for centrosymmetric linear molecules. Reduce that corrected irrep identifier to the corresponding D2h/C2v irrep id using `pyscf.symm.basis.linearmole_symm_descent`, so I can feed it into a D2h-only post-HF routine for orbital labeling.", "answers": "[{\"name\": \"pyscf_symm_basis_linearmole_symm_descent\", \"arguments\": {\"gpname\": \"Dooh\", \"irrep_id\": 11}}]"}
{"func_name": "pyscf_symm_geom_is_identical_geometry", "func_desc": "pyscf.symm.geom.is_identical_geometry: Fast check whether two molecular geometries are identical using Casimir tensors.\n    \n    Performs a computationally efficient comparison between two sets of atomic coordinates by computing Casimir tensor invariants (orders 1 through 3) for each geometry with the supplied per-atom weights and verifying that the maximum absolute difference for each tensor order is within the module's global tolerance. This function is intended for use inside the PySCF electronic structure framework to quickly determine whether two molecular geometries can be treated as the same geometry (for example, to avoid redundant symmetry analyses, integral rebuilds, or expensive reinitializations) before invoking more expensive routines.", "tools": [{"function": {"description": "pyscf.symm.geom.is_identical_geometry: Fast check whether two molecular geometries are identical using Casimir tensors.\n\nPerforms a computationally efficient comparison between two sets of atomic coordinates by computing Casimir tensor invariants (orders 1 through 3) for each geometry with the supplied per-atom weights and verifying that the maximum absolute difference for each tensor order is within the module's global tolerance. This function is intended for use inside the PySCF electronic structure framework to quickly determine whether two molecular geometries can be treated as the same geometry (for example, to avoid redundant symmetry analyses, integral rebuilds, or expensive reinitializations) before invoking more expensive routines.", "name": "pyscf_symm_geom_is_identical_geometry", "parameters": {"properties": {"coords1": {"type": "array", "items": {"type": "any"}, "description": "Array of atomic coordinates for the first geometry. This should be a numeric NumPy array whose leading dimension indexes atoms and whose remaining dimensions store Cartesian coordinates (commonly shaped (N, 3) for N atoms). The function requires coords1.shape == coords2.shape; if the shapes differ the function returns False immediately. The coordinates are used as input to compute Casimir tensor invariants that summarize the spatial arrangement of the atoms.", "default": ""}, "coords2": {"type": "array", "items": {"type": "any"}, "description": "Array of atomic coordinates for the second geometry to compare against coords1. Must be a numeric NumPy array with the same shape as coords1. The geometry represented by coords2 is converted to Casimir tensor invariants using the same weights and compared order-by-order (orders 1 through 3) to the invariants from coords1.", "default": ""}, "weights": {"type": "array", "items": {"type": "any"}, "description": "Per-atom numeric weights supplied as a NumPy array. These weights are used when forming the Casimir tensors (for example, they may encode atomic masses, nuclear charges, or element-dependent coefficients) and must be aligned with the atom indexing of coords1 and coords2 (i.e., the atom dimension of weights must correspond to the atom dimension of the coordinate arrays). The same weights are applied to both geometries during invariant computation.", "default": ""}}, "required": ["coords1", "coords2", "weights"], "type": "any"}}, "type": "function"}], "query": "I’m trying to de-duplicate geometries inside a PySCF optimization loop before triggering an expensive integral rebuild. I was going to compare the two water structures exactly as printed, but I just realized the optimizer sometimes shuffles atoms when it writes intermediates; to make the comparison meaningful, first reorder geometry B so its atoms follow the same element sequence as geometry A (oxygen first, then the two hydrogens in the same order as in A by choosing the H in B that is closest to A’s first H position, and the remaining H as the second). After that reordering, run the fast Casimir-tensor identity check (orders 1–3) with per-atom weights equal to the nuclear charges (oxygen’s charge for O and hydrogen’s for each H). Use the coordinates (Å): geometry A: O(0.000000, 0.000000, 0.000000), H(0.957200, 0.000000, 0.000000), H(-0.239987, 0.927297, 0.000000). Geometry B (as received, possibly permuted): H(-0.239987, 0.927297, -0.000001), O(0.000000, 0.000000, 0.000000), H(0.957200, 0.000000, 0.000001).", "answers": "[{\"name\":\"pyscf_symm_geom_is_identical_geometry\",\"arguments\":{\"coords1\":[[0.0,0.0,0.0],[0.9572,0.0,0.0],[-0.239987,0.927297,0.0]],\"coords2\":[[0.0,0.0,0.0],[0.9572,0.0,1e-06],[-0.239987,0.927297,-1e-06]],\"weights\":[8,1,1]}}]"}
{"func_name": "pyscf_symm_geom_rotation_mat", "func_desc": "pyscf.symm.geom.rotation_mat returns a 3x3 rotation matrix for rotating 3D Cartesian vectors by an angle theta about an axis specified by vec. The function is used throughout PySCF geometry and symmetry utilities to perform rigid-body rotations of atomic coordinates, molecular frames, and to construct rotation operators needed for point-group symmetry operations and coordinate transformations in electronic structure calculations.", "tools": [{"function": {"description": "pyscf.symm.geom.rotation_mat returns a 3x3 rotation matrix for rotating 3D Cartesian vectors by an angle theta about an axis specified by vec. The function is used throughout PySCF geometry and symmetry utilities to perform rigid-body rotations of atomic coordinates, molecular frames, and to construct rotation operators needed for point-group symmetry operations and coordinate transformations in electronic structure calculations.\n", "name": "pyscf_symm_geom_rotation_mat", "parameters": {"properties": {"vec": {"type": "array", "items": {"type": "any"}, "description": "A length-3 1D array-like Cartesian vector that specifies the rotation axis in 3-dimensional space. The components are interpreted as (x, y, z) in the same Cartesian coordinate system used for molecular coordinates in PySCF. The axis vector is normalized internally by a call to _normalize(vec) before building the rotation matrix, so the caller may pass a non-unit vector; if vec has zero length or cannot be normalized, the underlying normalization routine will fail (raising an exception) and no rotation matrix will be returned. The input must be compatible with numpy.reshape(-1, 1) / reshape(1, -1) to produce a 3-element vector; providing an array with a size other than 3 will trigger a numpy error from the reshape or subsequent operations.", "default": ""}, "theta": {"type": "float", "description": "Rotation angle in radians. The function uses numpy.cos and numpy.sin, so theta is interpreted in radians. Positive theta follows the right-hand rule about the axis given by vec: when vec points toward the observer, a positive theta produces a counterclockwise rotation of vectors in the plane perpendicular to vec.", "default": ""}}, "required": ["vec", "theta"], "type": "any"}}, "type": "function"}], "query": "I’m building a C3 symmetry generator in a PySCF geometry pipeline and initially I was going to rotate by 120° about the (1,1,1) axis. Wait—my downstream code compares the operator against the canonical C3 element used in our point-group table, which is defined as the *minimal positive* rotation that maps the vertices in the opposite handed sense (so I can’t just keep the original sign convention). Keep the same geometric axis direction, but choose the rotation angle that is the complementary turn needed to produce the opposite-handed C3 while still being the smallest positive rotation (and pass it in radians). Using `pyscf.symm.geom.rotation_mat`, what function call should I use to get the 3×3 rotation matrix?", "answers": "[{\"name\": \"pyscf_symm_geom_rotation_mat\", \"arguments\": {\"vec\": [1, 1, 1], \"theta\": 4.1887902047863905}}]"}
{"func_name": "pyscf_symm_geom_symm_identical_atoms", "func_desc": "Symmetry identical atoms\n    \n    Compute groups of symmetry-equivalent atom indices for a molecule in a specified point group. This function is used in the PySCF symmetry utilities to identify which atomic sites are mapped onto each other by the point group operations (for example to reduce integrals, label orbitals, and exploit symmetry in electronic structure calculations). The implementation applies the symmetry operation matrices provided by symm_ops and the operator ordering in OPERATOR_TABLE to the input Cartesian coordinates, compares sorted coordinate arrays using argsort_coords, and groups atom indices that are permuted into each other by all symmetry operations for the given point group.", "tools": [{"function": {"description": "Symmetry identical atoms\n\nCompute groups of symmetry-equivalent atom indices for a molecule in a specified point group. This function is used in the PySCF symmetry utilities to identify which atomic sites are mapped onto each other by the point group operations (for example to reduce integrals, label orbitals, and exploit symmetry in electronic structure calculations). The implementation applies the symmetry operation matrices provided by symm_ops and the operator ordering in OPERATOR_TABLE to the input Cartesian coordinates, compares sorted coordinate arrays using argsort_coords, and groups atom indices that are permuted into each other by all symmetry operations for the given point group.", "name": "pyscf_symm_geom_symm_identical_atoms", "parameters": {"properties": {"gpname": {"type": "string", "description": "The point group name. This is the group identifier used by PySCF symmetry routines (for example 'C2v', 'D2h', 'Dooh', 'Coov', etc.). The function has special-case behavior for linear groups: if gpname == 'Dooh' a single-axis reflection/rotation test based on the 'sz' operator is used; if gpname == 'Coov' the function treats every atom as non-equivalent and returns each atom in its own group. For other group names the function looks up symmetry operation matrices from symm_ops and the operation ordering from OPERATOR_TABLE.", "default": ""}, "atoms": {"type": "array", "items": {"type": "any"}, "description": "A list of atoms where each atom is represented as a two-element sequence or tuple (label, coord). The first element label (typically a string like 'H', 'C', or any atom identifier used by PySCF) is used by other commented routines (e.g. gto.charge) and the second element coord is a sequence-like iterable of Cartesian coordinates (converted to a float numpy array in the function). Coordinates are expected to be given in the coordinate frame consistent with the point group determination (typically the same origin and orientation used to assemble the symmetry operators). The function reads the coordinates as numpy.array([a[1] for a in atoms], dtype=float).", "default": ""}}, "required": ["gpname", "atoms"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a PySCF symmetry-reduced run for a bent triatomic that should be in the same point group as the textbook water example, and I want to use `Symmetry identical atoms` to deduce the symmetry-equivalent atom index groups. Start from these Cartesian coordinates (Å):\n- X (0.000000, 0.000000, 0.000000)\n- Y (0.758602, 0.000000, 0.504284)\n- Y (-0.758602, 0.000000, 0.504284)\n\nInitially I was going to label the point group as the one with a single 2-fold axis and two perpendicular mirror planes, but then I realized our geometry preprocessor sometimes swaps axes, so I should not hard-code the label: instead, use the point group that (i) contains exactly one C2 operation, (ii) has two distinct σ planes, and (iii) is the correct assignment for this coordinate layout where the two terminal atoms are related by flipping the sign of x while keeping y fixed. Using that point group, compute the symmetry-identical atom index groups (by indices in the input order).", "answers": "[{\"name\":\"pyscf_symm_geom_symm_identical_atoms\",\"arguments\":{\"gpname\":\"C2v\",\"atoms\":[[\"X\",[0.0,0.0,0.0]],[\"Y\",[0.758602,0.0,0.504284]],[\"Y\",[-0.758602,0.0,0.504284]]]}}]"}
{"func_name": "pyscf_tools_mo_mapping_mo_1to1map", "func_desc": "Compute a one-to-one mapping from row indices i to column indices j using the absolute overlap matrix <i|j>, typically used in PySCF (Python-based Simulations of Chemistry Framework) to associate molecular orbitals from one set (bras) to the closest matching orbitals in another set (kets). In the quantum chemistry workflow of PySCF, this function is used to produce a simple, greedy correspondence between two orbital bases (for example, when comparing orbitals from two calculations or projecting orbitals between different basis sets) by selecting the largest absolute overlap for each row and preventing reuse of the same column index until all columns are exhausted.\n    \n    The algorithm is greedy: for each row i (0..s.shape[0]-1) it selects the column index k with the largest absolute value in that row, records k as the match for i, and then sets that entire column to zero in an internal working copy so that subsequent rows prefer different columns. The input array is not modified; the function operates on an absolute-value copy of s. Because the method is greedy rather than globally optimal, it may produce suboptimal assignments in cases of strongly tied overlaps or when the number of rows exceeds the number of columns.", "tools": [{"function": {"description": "Compute a one-to-one mapping from row indices i to column indices j using the absolute overlap matrix <i|j>, typically used in PySCF (Python-based Simulations of Chemistry Framework) to associate molecular orbitals from one set (bras) to the closest matching orbitals in another set (kets). In the quantum chemistry workflow of PySCF, this function is used to produce a simple, greedy correspondence between two orbital bases (for example, when comparing orbitals from two calculations or projecting orbitals between different basis sets) by selecting the largest absolute overlap for each row and preventing reuse of the same column index until all columns are exhausted.\n\nThe algorithm is greedy: for each row i (0..s.shape[0]-1) it selects the column index k with the largest absolute value in that row, records k as the match for i, and then sets that entire column to zero in an internal working copy so that subsequent rows prefer different columns. The input array is not modified; the function operates on an absolute-value copy of s. Because the method is greedy rather than globally optimal, it may produce suboptimal assignments in cases of strongly tied overlaps or when the number of rows exceeds the number of columns.", "name": "pyscf_tools_mo_mapping_mo_1to1map", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "any"}, "description": "A 2-D numeric array representing overlaps <i|j>, where rows index the \"bra\" orbitals i and columns index the \"ket\" orbitals j. The function uses abs(s) internally, so negative values are treated by magnitude. The function expects a 2-D array; behavior for inputs that are not 2-D, contain NaNs or non-numeric entries, or otherwise violate the expected numeric array semantics is undefined and may raise numpy exceptions.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "I’m trying to track MOs between two PySCF runs (optimized vs. slightly displaced geometry). Initially I was going to do a straight greedy 1-to-1 map on the full 5×5 MO overlap matrix below, but then I realized one of the ket orbitals is a garbage state from a convergence hiccup: it has uniformly tiny absolute overlap with every bra (i.e., its entire column is near-zero compared to the rest). So, before mapping, drop exactly that ket column (keep all bra rows), then run the same greedy one-to-one mapping on the resulting rectangular overlap matrix (absolute-value selection per row, zeroing out a chosen column after it’s used).\n\nUse this overlap matrix:\n[[ 0.02, -0.91,  0.12,  0.05,  0.10],\n [ 0.88,  0.15, -0.05,  0.03,  0.02],\n [ 0.06,  0.14,  0.85, -0.10,  0.04],\n [-0.04,  0.08,  0.20,  0.79, -0.02],\n [ 0.05,  0.07, -0.02,  0.11,  0.76]]", "answers": "[{\"name\":\"pyscf_tools_mo_mapping_mo_1to1map\",\"arguments\":{\"s\":[[0.02,-0.91,0.12,0.05],[0.88,0.15,-0.05,0.03],[0.06,0.14,0.85,-0.1],[-0.04,0.08,0.2,0.79],[0.05,0.07,-0.02,0.11]]}}]"}
{"func_name": "pyscf_tools_qcschema_load_qcschema_hessian", "func_desc": "Load a molecular Hessian from a QCSchema-formatted dictionary and return it as a 4D NumPy array suitable for PySCF vibrational and normal-mode workflows.", "tools": [{"function": {"description": "Load a molecular Hessian from a QCSchema-formatted dictionary and return it as a 4D NumPy array suitable for PySCF vibrational and normal-mode workflows.\n", "name": "pyscf_tools_qcschema_load_qcschema_hessian", "parameters": {"properties": {"qcschema_dict": {"type": "any", "description": "A Python dictionary containing a QCSchema output JSON already loaded (for example via json.load). This dictionary is expected to follow the QCSchema structure used by PySCF integration and must contain the key \"return_result\" whose value is the flattened Hessian array and the nested key \"molecule\" -> \"symbols\" whose value is an iterable of atomic symbols. The function uses the length of qcschema_dict[\"molecule\"][\"symbols\"] to determine the number of atoms N and uses qcschema_dict[\"return_result\"] as a 1D sequence of length (3*N)**2 representing the Cartesian Hessian flattened in row-major order. qcschema_dict is not modified by this function.", "default": ""}}, "required": ["qcschema_dict"], "type": "any"}}, "type": "function"}], "query": "I’m trying to pipe a QCSchema frequency job result into a PySCF normal-mode workflow, but I realized my pipeline is fragile: sometimes the QCSchema `molecule.symbols` list includes dummy atoms (e.g., a ghost center labeled with a non-element symbol) that should NOT contribute to the vibrational Hessian dimensions. So please do this carefully.\n\nHere’s the already-loaded QCSchema-like dictionary I have right now (it’s *supposed* to be “water”, but treat the atom list as authoritative and filter out any non-chemical symbols before deciding the Hessian shape):\n\n{\"molecule\": {\"symbols\": [\"O\", \"H\", \"H\"]}, \"return_result\": [0.5, -0.1, 0.0, -0.1, 0.4, 0.0, 0.0, 0.0, 0.3, -0.1, 0.4, 0.0, 0.4, -0.2, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.3, 0.0, 0.0, 0.1, 0.3, -0.05, 0.0, 0.0, 0.0, -0.2, 0.0, 0.3, 0.0, -0.05, 0.2, 0.0, 0.0, 0.0, 0.1, 0.0, -0.05, 0.2, 0.0, 0.1, 0.0, 0.15, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0]}\n\nTask: Load the Hessian from this dict into the 4D NumPy array format PySCF expects (natm, natm, 3, 3), where `natm` is the count of *real* atoms after filtering out any invalid/dummy symbols. Also, assume the `return_result` is provided as a flattened row-major representation of the full Cartesian Hessian (3N x 3N) for those real atoms.\n\nReturn the resulting 4D array (or equivalently, produce it via the provided loader function).", "answers": "[{\"name\": \"pyscf_tools_qcschema_load_qcschema_hessian\", \"arguments\": {\"qcschema_dict\": {\"molecule\": {\"symbols\": [\"O\", \"H\", \"H\"]}, \"return_result\": [0.5, -0.1, 0.0, -0.1, 0.4, 0.0, 0.0, 0.0, 0.3, -0.1, 0.4, 0.0, 0.4, -0.2, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.3, 0.0, 0.0, 0.1, 0.3, -0.05, 0.0, 0.0, 0.0, -0.2, 0.0, 0.3, 0.0, -0.05, 0.2, 0.0, 0.0, 0.0, 0.1, 0.0, -0.05, 0.2, 0.0, 0.1, 0.0, 0.15, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.25, 0.0, 0.0, 0.0]}}}]"}
{"func_name": "rdkit_Chem_AtomPairs_Utils_BitsInCommon", "func_desc": "Compute the number of bit identifiers that occur in common between two sorted bit‑id vectors used as molecular fingerprints.\n    \n    This function is part of RDKit's AtomPairs.Utils utilities and is used in cheminformatics tasks that compare fingerprint-like representations of molecules. Each input vector represents the positions (IDs) of set bits in a fingerprint; the function counts how many bit IDs are shared between the two vectors. In practical use within RDKit this count can serve as the numerator for similarity measures (for example, the intersection used when computing Tanimoto similarity) or as a fast overlap check between atom-pair/fingerprint bit lists.", "tools": [{"function": {"description": "Compute the number of bit identifiers that occur in common between two sorted bit‑id vectors used as molecular fingerprints.\n\nThis function is part of RDKit's AtomPairs.Utils utilities and is used in cheminformatics tasks that compare fingerprint-like representations of molecules. Each input vector represents the positions (IDs) of set bits in a fingerprint; the function counts how many bit IDs are shared between the two vectors. In practical use within RDKit this count can serve as the numerator for similarity measures (for example, the intersection used when computing Tanimoto similarity) or as a fast overlap check between atom-pair/fingerprint bit lists.", "name": "rdkit_Chem_AtomPairs_Utils_BitsInCommon", "parameters": {"properties": {"v1": {"type": "any", "description": "A tuple of bit identifiers (integers) representing set bits in the first fingerprint vector. The tuple must be sorted in non-decreasing (ascending) order. Duplicate bit IDs are allowed and are significant: each duplicate occurrence in v1 contributes separately to the count if matched in v2. The function iterates over v1 and advances a pointer through v2 to identify matches efficiently.", "default": ""}, "v2": {"type": "any", "description": "A tuple of bit identifiers (integers) representing set bits in the second fingerprint vector. The tuple must be sorted in non-decreasing (ascending) order. Duplicate bit IDs are allowed and are counted multiple times when matched against duplicates in v1. The function assumes v2 supports len() and index access for forward scanning.", "default": ""}}, "required": ["v1", "v2"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick atom-pair fingerprint overlap check in an RDKit pipeline, but the raw bit-ID vectors coming off our aggregator are messy because they may contain duplicated IDs due to repeated atom-pair paths. Initially I was going to just feed the two sorted lists directly into the overlap counter, but then I realized my downstream similarity code expects the *set-style* intersection (i.e., each bit ID should contribute at most once, regardless of how many times it appears). So before computing the overlap, collapse each vector to its unique IDs while preserving sorted order, then compute the number of bit identifiers in common.\n\nMolecule A bit IDs: [12, 47, 47, 103, 250, 999]\nMolecule B bit IDs: [5, 47, 103, 103, 250, 400, 999, 1200]\n\nWhat is the resulting intersection count after applying that constraint?", "answers": "[{\"name\":\"rdkit_Chem_AtomPairs_Utils_BitsInCommon\",\"arguments\":{\"v1\":[12,47,103,250,999],\"v2\":[5,47,103,250,400,999,1200]}}]"}
{"func_name": "rdkit_Chem_AtomPairs_Utils_Dot", "func_desc": "rdkit.Chem.AtomPairs.Utils.Dot computes the dot product between two sorted integer vectors of bit identifiers used in RDKit fingerprint-like representations.\n    \n    Computes a specialized integer \"dot product\" between two sequences of bit IDs (sparse fingerprint vectors) that are stored as tuples of integers. In the AtomPairs/fingerprints domain of RDKit (cheminformatics and molecular descriptors), these tuples represent the indices of bits set in a fingerprint; this function determines a weighted count of shared bit indices while accounting for duplicate occurrences. The implementation uses a single-pass, merge-like algorithm over the two sorted input tuples so it runs in linear time relative to the sum of the input lengths.", "tools": [{"function": {"description": "rdkit.Chem.AtomPairs.Utils.Dot computes the dot product between two sorted integer vectors of bit identifiers used in RDKit fingerprint-like representations.\n\nComputes a specialized integer \"dot product\" between two sequences of bit IDs (sparse fingerprint vectors) that are stored as tuples of integers. In the AtomPairs/fingerprints domain of RDKit (cheminformatics and molecular descriptors), these tuples represent the indices of bits set in a fingerprint; this function determines a weighted count of shared bit indices while accounting for duplicate occurrences. The implementation uses a single-pass, merge-like algorithm over the two sorted input tuples so it runs in linear time relative to the sum of the input lengths.", "name": "rdkit_Chem_AtomPairs_Utils_Dot", "parameters": {"properties": {"v1": {"type": "any", "description": "A tuple of integers representing bit IDs (indices) in a fingerprint-like sparse vector for the first item. The tuple must be sorted in non-decreasing order and may contain repeated integers; repeated bit IDs are meaningful and counted according to their multiplicity. This parameter plays the role of the left operand of the dot-product calculation in atom-pair/fingerprint comparisons and is typically produced by routines that extract or enumerate fingerprint bits in RDKit.", "default": ""}, "v2": {"type": "any", "description": "A tuple of integers representing bit IDs (indices) in a fingerprint-like sparse vector for the second item. The tuple must be sorted in non-decreasing order and may contain repeated integers; repeated bit IDs are meaningful and counted according to their multiplicity. This parameter is the right operand of the dot-product calculation and is used when comparing or scoring similarity between two fingerprint encodings in cheminformatics workflows.", "default": ""}}, "required": ["v1", "v2"], "type": "any"}}, "type": "function"}], "query": "I’m comparing two RDKit atom-pair sparse fingerprints using the AtomPairs.Utils.Dot merge-style dot product that counts shared bit IDs with multiplicities. I was going to dot the raw bit-ID tuples directly, but I just realized one of our exporters sometimes injects a sentinel/metadata bit at the very beginning that is smaller than any chemically meaningful atom-pair bit in this run; that sentinel must be discarded before computing similarity. So: take compound A’s bits (101, 101, 205, 333, 333, 333, 900) and compound B’s bits (50, 101, 333, 333, 700, 900, 900), first remove the smallest element from the dataset that contains it (only if it appears as that leading sentinel), then compute the AtomPairs.Utils.Dot integer dot product on the cleaned, still-sorted vectors.", "answers": "[{\"name\": \"rdkit_Chem_AtomPairs_Utils_Dot\", \"arguments\": {\"v1\": [101, 101, 205, 333, 333, 333, 900], \"v2\": [101, 333, 333, 700, 900, 900]}}]"}
{"func_name": "rdkit_Chem_CanonSmiles", "func_desc": "rdkit.Chem.CanonSmiles: Return a canonical SMILES string for a molecule represented by an input SMILES.\n    \n    This convenience function is part of the RDKit cheminformatics toolkit and is used to produce a deterministic, canonical SMILES string for a molecule given an input SMILES representation. Canonical SMILES are useful in cheminformatics workflows (for example: deduplication of compound lists, database keys, fingerprinting and descriptor generation for machine learning, and reproducible serialization of molecular structures). The function parses the input SMILES into an RDKit Mol object using MolFromSmiles and then generates a canonical SMILES with MolToSmiles. The useChiral parameter controls whether stereochemical/chiral information is preserved in the output canonicalization.", "tools": [{"function": {"description": "rdkit.Chem.CanonSmiles: Return a canonical SMILES string for a molecule represented by an input SMILES.\n\nThis convenience function is part of the RDKit cheminformatics toolkit and is used to produce a deterministic, canonical SMILES string for a molecule given an input SMILES representation. Canonical SMILES are useful in cheminformatics workflows (for example: deduplication of compound lists, database keys, fingerprinting and descriptor generation for machine learning, and reproducible serialization of molecular structures). The function parses the input SMILES into an RDKit Mol object using MolFromSmiles and then generates a canonical SMILES with MolToSmiles. The useChiral parameter controls whether stereochemical/chiral information is preserved in the output canonicalization.", "name": "rdkit_Chem_CanonSmiles", "parameters": {"properties": {"smi": {"type": "string", "description": "The input SMILES string representing a molecule. This is the textual molecular representation to be parsed by RDKit's MolFromSmiles. The string should follow standard SMILES syntax; if it cannot be parsed, canonicalization cannot proceed and the function will typically propagate an error from the underlying RDKit parsing/serialization functions.", "default": ""}, "useChiral": {"type": "boolean", "description": "Optional flag (default True, expressed in the original signature as 1) that determines whether chiral and stereochemical information is included in the canonicalization and in the returned SMILES. When True, stereochemical markers (e.g., @, @@, /, \\) are considered and emitted in the canonical SMILES where appropriate. When False, stereochemical information is ignored in determining the canonical form and is not included in the returned SMILES.", "default": 1}}, "required": ["smi", "useChiral"], "type": "any"}}, "type": "function"}], "query": "I’m standardizing a small beta-blocker training set for a QSAR run, starting with propranolol. First, take the vendor SMILES string `CC(C)NCC(O)COc1cccc2ccccc12` and canonicalize it for deduplication. Wait—propranolol is marketed as a racemate and this particular entry doesn’t explicitly encode any stereocenters anyway, so forcing stereochemistry preservation won’t change anything but can make the pipeline inconsistent across entries that lack stereo marks. Canonicalize it using the setting that only retains stereochemistry when it is explicitly specified in the input (i.e., don’t force chiral handling beyond what’s present).", "answers": "[{\"name\":\"rdkit_Chem_CanonSmiles\",\"arguments\":{\"smi\":\"CC(C)NCC(O)COc1cccc2ccccc12\",\"useChiral\":false}}]"}
{"func_name": "rdkit_Chem_Draw_SimilarityMaps_GetStandardizedWeights", "func_desc": "GetStandardizedWeights normalizes a sequence of per-atom weights so that the largest absolute weight becomes 1.0.\n    \n    This function is used in the cheminformatics context of rdkit.Chem.Draw.SimilarityMaps to prepare atomic contribution values (weights) for visualization and comparison across molecules or views. Given a list of numeric atomic weights (for example, per-atom similarity contributions or feature importances used when drawing similarity maps), the function rescales every weight by the maximum absolute weight found in the list so that the absolute maximum after scaling equals 1.0. This makes color/size mappings and cross-molecule comparisons meaningful and stable when rendering 2D similarity maps or other atom-level visual representations.", "tools": [{"function": {"description": "GetStandardizedWeights normalizes a sequence of per-atom weights so that the largest absolute weight becomes 1.0.\n\nThis function is used in the cheminformatics context of rdkit.Chem.Draw.SimilarityMaps to prepare atomic contribution values (weights) for visualization and comparison across molecules or views. Given a list of numeric atomic weights (for example, per-atom similarity contributions or feature importances used when drawing similarity maps), the function rescales every weight by the maximum absolute weight found in the list so that the absolute maximum after scaling equals 1.0. This makes color/size mappings and cross-molecule comparisons meaningful and stable when rendering 2D similarity maps or other atom-level visual representations.", "name": "rdkit_Chem_Draw_SimilarityMaps_GetStandardizedWeights", "parameters": {"properties": {"weights": {"type": "array", "items": {"type": "float"}, "description": "A list of numeric per-atom weights. Each element represents an atomic contribution (e.g., similarity contribution, feature importance) that will be used in drawing or comparing molecular similarity maps. The function expects a non-empty list of numeric values; elements must be compatible with math.fabs (typically int or float). The order of elements is significant because the returned normalized list preserves the original atom ordering.", "default": ""}}, "required": ["weights"], "type": "any"}}, "type": "function"}], "query": "I’m preparing an RDKit SimilarityMaps figure from per-atom contribution scores for a 6‑atom fragment, but the raw vector has a subtle preprocessing issue. Here are the contributions in the order of the atoms: [0.15, -0.42, 0.0, 0.85, -0.30, 0.60]. Initially I was going to standardize them directly for visualization, but I just realized the downstream rendering code treats *exact* zeros as “missing” and will drop the annotation for that atom. So before standardizing, replace any exact 0.0 entries with a tiny nonzero sentinel equal to one-thousandth of the largest absolute contribution present in the vector (preserving sign where applicable, though zeros have no sign). After that fix, normalize the full weight vector so that the largest absolute weight becomes 1.0 (i.e., the standard SimilarityMaps weight standardization). Can you return the standardized weights ready for plotting?", "answers": "[{\"name\": \"rdkit_Chem_Draw_SimilarityMaps_GetStandardizedWeights\", \"arguments\": {\"weights\": [0.15, -0.42, 0.00085, 0.85, -0.3, 0.6]}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_CountUpTo", "func_desc": "CountUpTo computes the zero-based rank (the count of preceding combinations) of a given combination of slot indices within the combinatorial enumeration produced by the Pharm2D index-combination generator (_GetIndexCombinations_). This function is part of RDKit's Chem.Pharm2D utilities used when enumerating combinations for descriptor/fingerprint generation in cheminformatics workflows (see RDKit documentation). It traverses the combination space using recursion and combinatorial counts (via NumCombinations) and uses a global cache to speed repeated queries.", "tools": [{"function": {"description": "CountUpTo computes the zero-based rank (the count of preceding combinations) of a given combination of slot indices within the combinatorial enumeration produced by the Pharm2D index-combination generator (_GetIndexCombinations_). This function is part of RDKit's Chem.Pharm2D utilities used when enumerating combinations for descriptor/fingerprint generation in cheminformatics workflows (see RDKit documentation). It traverses the combination space using recursion and combinatorial counts (via NumCombinations) and uses a global cache to speed repeated queries.\n", "name": "rdkit_Chem_Pharm2D_Utils_CountUpTo", "parameters": {"properties": {"nItems": {"type": "integer", "description": "The total number of indistinguishable items to distribute across slots. In the Pharm2D context this represents the pool of discrete units (for example, positions or counts) whose distributions are enumerated. This value is used by the combinatorial counting routine (NumCombinations) to compute how many combinations lie below a given partial selection.", "default": ""}, "nSlots": {"type": "integer", "description": "The number of slots (positions, levels, or places) among which nItems may be distributed. In Pharm2D enumeration, each slot corresponds to a level in the nested combination representation. The function expects to count combinations across these nSlots.", "default": ""}, "vs": {"type": "array", "items": {"type": "any"}, "description": "A list containing the target combination values whose position is sought. Each element of vs is expected to be an integer used as an index/value for the corresponding slot; the function reads vs[idx] during recursion. Practically, vs represents one particular combination (one tuple of slot indices) produced by the index-combination generator.", "default": ""}, "idx": {"type": "integer", "description": "The current recursion depth / slot index being processed. Default is 0. Callers normally invoke CountUpTo with the default (idx=0); internal recursive calls advance idx+1. This parameter must be a non-negative integer and is used only to traverse vs and compute partial counts for subsequent slots.", "default": 0}, "startAt": {"type": "integer", "description": "The minimum value permitted at the current slot during counting. Default is 0. In the combinatorial enumeration used by _GetIndexCombinations_, slots are generated with nondecreasing values and startAt enforces that lower bound as recursion proceeds; callers should not need to set this when initiating a top-level query.", "default": 0}}, "required": ["nItems", "nSlots", "vs", "startAt", "idx"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a Pharm2D fingerprint regression test where the slot-index combinations are generated by RDKit’s nondecreasing index-combination enumerator (_GetIndexCombinations_) for distributing features across slots. I initially planned to compute the zero-based rank for the combination that includes a negative slot index, but that’s clearly an artifact from a corrupted log export and would break the recursion/cache logic. So instead, take the *same* combination after applying the minimal correction that makes it a valid nondecreasing 0-based slot index vector under the same constraints (i.e., keep nItems=5 and nSlots=4, and correct only what’s necessary so all entries fall within the valid slot index range). Using that corrected vector, what does CountUpTo return (with idx and startAt at their defaults)?\n\nCorrupted combination from the log: [-1, 2, 2, 4].", "answers": "[{\"name\": \"rdkit_Chem_Pharm2D_Utils_CountUpTo\", \"arguments\": {\"nItems\": 5, \"nSlots\": 4, \"vs\": [0, 2, 2, 4], \"idx\": 0, \"startAt\": 0}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_GetIndexCombinations", "func_desc": "rdkit.Chem.Pharm2D.Utils.GetIndexCombinations generates all ordered index combinations used by Pharm2D utilities in RDKit for distributing a fixed number of items into a fixed number of slots without producing duplicate permutations. It is commonly used in RDKit pharmacophore/descriptor and fingerprint construction code to enumerate index tuples (for example, atom or feature indices) that represent combinations with repetition where the order of slots is fixed and permutations are suppressed.", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.GetIndexCombinations generates all ordered index combinations used by Pharm2D utilities in RDKit for distributing a fixed number of items into a fixed number of slots without producing duplicate permutations. It is commonly used in RDKit pharmacophore/descriptor and fingerprint construction code to enumerate index tuples (for example, atom or feature indices) that represent combinations with repetition where the order of slots is fixed and permutations are suppressed.\n", "name": "rdkit_Chem_Pharm2D_Utils_GetIndexCombinations", "parameters": {"properties": {"nItems": {"type": "integer", "description": "The number of distinct items (indices) available to place into slots. In the Pharm2D/fingerprint context this typically corresponds to the number of distinct features, atoms, or bins that can be selected. Values are used as exclusive upper bounds for index values (valid indices are 0 .. nItems-1). Supplying non-positive or otherwise small nItems will result in empty combination lists when no valid indices exist.", "default": ""}, "nSlots": {"type": "integer", "description": "The number of slots (positions) to fill in each generated combination. In practical use this is the tuple length for a descriptor/fingerprint element (for example, the arity of a pharmacophore feature tuple). If nSlots <= 0, the function returns an empty list; otherwise the recursion depth is proportional to nSlots.", "default": ""}, "slot": {"type": "integer", "description": "Internal recursion parameter indicating the current slot index being filled. Callers constructing combinations externally should normally leave this at the default 0. When used internally, slot increments from 0 up to nSlots-1; if slot >= nSlots the function returns an empty list. Supplying a non-default slot value alters recursion and bypasses the memoization used when slot == 0.", "default": 0}, "lastItemVal": {"type": "integer", "description": "Internal recursion parameter that defines the minimum item index allowed for the current slot. It enforces non-decreasing order of indices across slots to avoid duplicate permutations. Callers should normally leave this at the default 0; providing a different value restricts the allowed indices for the current and subsequent slots.", "default": 0}}, "required": ["nItems", "nSlots"], "type": "any"}}, "type": "function"}], "query": "I’m regenerating the index-tuples for a Pharm2D-style fingerprint, but my input feature-type list turned out to be messy: it was *supposed* to represent six distinct pharmacophore feature types labeled 0–5, and I initially planned to enumerate all non-decreasing tuples across three fixed slots. Then I realized the pipeline’s feature-normalization step collapses the highest label into the lowest (legacy mapping), so those two labels are not actually distinct for enumeration purposes. Keep the number of fingerprint slots the same as originally intended, but base the enumeration on the count of *truly distinct* feature types after that collapse. Use RDKit’s Pharm2D utility that generates the ordered (non-decreasing) index combinations so I can iterate over unique feature tuples without duplicate permutations.", "answers": "[{\"name\": \"rdkit_Chem_Pharm2D_Utils_GetIndexCombinations\", \"arguments\": {\"nItems\": 5, \"nSlots\": 3}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_GetTriangles", "func_desc": "rdkit.Chem.Pharm2D.Utils.GetTriangles returns the index triples that define the set of triangles used to represent an nPts-pharmacophore in RDKit's Pharm2D utilities. This function is used by the Pharm2D pharmacophore/fingerprint code to enumerate the combinations of three distance positions (points) that form the triangular features used in 3-point pharmacophore representations and downstream fingerprint calculations.\n    \n    The function computes a sliding-window set of triangles starting from the triple (0, 1, nPts-1) and incrementing each index in lock-step until the first index reaches nPts-2. The computed result is cached in the module-level dictionary _trianglesInPharmacophore to avoid recomputation for the same nPts on subsequent calls.", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.GetTriangles returns the index triples that define the set of triangles used to represent an nPts-pharmacophore in RDKit's Pharm2D utilities. This function is used by the Pharm2D pharmacophore/fingerprint code to enumerate the combinations of three distance positions (points) that form the triangular features used in 3-point pharmacophore representations and downstream fingerprint calculations.\n\nThe function computes a sliding-window set of triangles starting from the triple (0, 1, nPts-1) and incrementing each index in lock-step until the first index reaches nPts-2. The computed result is cached in the module-level dictionary _trianglesInPharmacophore to avoid recomputation for the same nPts on subsequent calls.", "name": "rdkit_Chem_Pharm2D_Utils_GetTriangles", "parameters": {"properties": {"nPts": {"type": "integer", "description": "The number of points in the pharmacophore. This integer specifies how many ordered distance positions are present in the pharmacophore model for which triangle index triples are required. The function expects a non-negative integer; callers in the Pharm2D fingerprinting pipeline pass the number of distance bins or point positions used to construct triangular pharmacophore features.", "default": ""}}, "required": ["nPts"], "type": "any"}}, "type": "function"}], "query": "I’m validating an RDKit Pharm2D 3-point fingerprint pipeline for a distance-binned pharmacophore, and I initially planned to hard-code the triangle index triples for the same 7-point setup we used last week. Wait—my binning code inserts an extra placeholder at index 0 for a “null/unknown” distance state, and RDKit’s Pharm2D utilities will treat that as a real point if I pass it through unchanged. So instead of using the raw point count from my bin definitions, compute the triangle index set for the point count after removing exactly that single placeholder (i.e., the effective number of points that actually carry distance information). Please return the triangle index triples that rdkit.Chem.Pharm2D.Utils.GetTriangles would generate for that effective nPts so I can cross-check my enumerator.", "answers": "[{\"name\": \"rdkit_Chem_Pharm2D_Utils_GetTriangles\", \"arguments\": {\"nPts\": 6}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_NumCombinations", "func_desc": "rdkit.Chem.Pharm2D.Utils.NumCombinations: Compute the number of unordered combinations with repetition for placing a set of items into a fixed number of slots. This function implements the \"stars and bars\" combinatorial formula used in RDKit's Pharm2D utilities to count how many distinct multisets of size nSlots can be formed from nItems types (for example, counting ways to assign pharmacophore feature types to fingerprint slots when order does not matter and repeats are allowed).", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.NumCombinations: Compute the number of unordered combinations with repetition for placing a set of items into a fixed number of slots. This function implements the \"stars and bars\" combinatorial formula used in RDKit's Pharm2D utilities to count how many distinct multisets of size nSlots can be formed from nItems types (for example, counting ways to assign pharmacophore feature types to fingerprint slots when order does not matter and repeats are allowed).\n", "name": "rdkit_Chem_Pharm2D_Utils_NumCombinations", "parameters": {"properties": {"nItems": {"type": "integer", "description": "The number of distinct item types (N in the combinatorial formula). In the Pharm2D context this corresponds to the number of distinct feature categories or bins that can be placed into slots. This must be an integer; passing values that are not integers or that lead to invalid binomial parameters will raise the underlying exception from the comb implementation (typically TypeError or ValueError).", "default": ""}, "nSlots": {"type": "integer", "description": "The number of slots to fill (S in the combinatorial formula). In the Pharm2D context this is the number of positions in a descriptor or fingerprint that will be populated from the nItems types. This must be an integer; invalid values (negative or those that make the binomial arguments invalid) will cause the underlying comb call to raise an exception.", "default": ""}}, "required": ["nItems", "nSlots"], "type": "any"}}, "type": "function"}], "query": "I’m setting up an RDKit Pharm2D feature-assignment fingerprint where feature types are treated as indistinguishable except for their type (so slot order doesn’t matter) and repeats are allowed. I initially thought I had 7 feature types and wanted 4 unordered slots, but then I realized one of the “types” is just a protonation-state duplicate of another and should be merged before counting. Also, the 4 slots include a reserved “null/background” slot that shouldn’t be counted as a feature-placement slot in the stars-and-bars calculation. Using the corrected counts implied by those constraints, how many unique multisets are possible?", "answers": "[{\"name\": \"rdkit_Chem_Pharm2D_Utils_NumCombinations\", \"arguments\": {\"nItems\": 6, \"nSlots\": 3}}]"}
{"func_name": "rdkit_Chem_Pharm3D_EmbedLib_CombiEnum", "func_desc": "rdkit.Chem.Pharm3D.EmbedLib.CombiEnum generates all combinations that pick one element from each subsequence in a tuple of subsequences. This generator is a small utility used in RDKit's Pharm3D embedding and pharmacophore-related code to enumerate alternative assignments (for example, alternative placements or feature choices) without materializing the full Cartesian product in memory; it yields each combination as a list in the same order as the input subsequences.", "tools": [{"function": {"description": "rdkit.Chem.Pharm3D.EmbedLib.CombiEnum generates all combinations that pick one element from each subsequence in a tuple of subsequences. This generator is a small utility used in RDKit's Pharm3D embedding and pharmacophore-related code to enumerate alternative assignments (for example, alternative placements or feature choices) without materializing the full Cartesian product in memory; it yields each combination as a list in the same order as the input subsequences.\n", "name": "rdkit_Chem_Pharm3D_EmbedLib_CombiEnum", "parameters": {"properties": {"sequence": {"type": "any", "description": "A tuple of subsequences (each subsequence is expected to be an iterable such as a tuple or list) that provide candidate values for each position in the combination. The role of this parameter in the Pharm3D/embedding domain is to represent, for each pharmacophore position or decision point, the set of alternatives to be enumerated. The function uses len(), indexing and slicing on this tuple. If the top-level tuple is empty (len(sequence) == 0) the generator yields a single empty list (one valid combination of zero choices). If any subsequence is empty, the generator will produce no combinations (i.e., it will be empty), because there is no valid choice for that position.", "default": ""}}, "required": ["sequence"], "type": "any"}}, "type": "function"}], "query": "I’m enumerating alternative pharmacophore feature assignments for a 3-point hypothesis using CombiEnum. Initially I was going to use all candidate IDs as-is, but I just realized the downstream scoring code assumes every candidate list is unique and strictly increasing (it silently overcounts if a subsequence contains repeats or is out of order). So: take the donor candidates [102, 101, 102], the acceptor candidates [201, 202, 203, 203], and the hydrophobe candidates [302, 301]. First, within each position, remove any duplicates and then sort ascending; then generate all combinations by picking exactly one element from each cleaned subsequence, preserving the subsequence order (donor, acceptor, hydrophobe) in each yielded list.", "answers": "[{\"name\": \"rdkit_Chem_Pharm3D_EmbedLib_CombiEnum\", \"arguments\": {\"sequence\": [[101, 102], [201, 202, 203], [301, 302]]}}]"}
{"func_name": "rdkit_Chem_QuickSmartsMatch", "func_desc": "rdkit.Chem.QuickSmartsMatch quickly matches a SMARTS substructure pattern against a SMILES molecule string and returns the atom index matches found in the molecule.", "tools": [{"function": {"description": "rdkit.Chem.QuickSmartsMatch quickly matches a SMARTS substructure pattern against a SMILES molecule string and returns the atom index matches found in the molecule.\n", "name": "rdkit_Chem_QuickSmartsMatch", "parameters": {"properties": {"smi": {"type": "string", "description": "The input molecule specified as a SMILES string. In the RDKit cheminformatics context (see README), SMILES is a compact, ASCII representation of a molecule used as input to RDKit's parser (MolFromSmiles). This function parses smi to an RDKit Mol internally; if parsing fails (invalid SMILES), the function may raise an exception when attempting to perform the substructure search. The returned atom indices refer to the parsed molecule's atom ordering.", "default": ""}, "sma": {"type": "string", "description": "The SMARTS substructure pattern to match against the molecule, expressed as a SMARTS string. SMARTS is the RDKit-supported pattern language for specifying substructures used in substructure searches (MolFromSmarts). If sma cannot be parsed as a valid SMARTS pattern, parsing will fail and the function may raise an exception when attempting to match.", "default": ""}, "unique": {"type": "boolean", "description": "Optional; defaults to True. When True, only unique matches are returned: matches that are equivalent under symmetry or automorphism of the query/molecule are filtered so duplicate-equivalent mappings are omitted. When False, all distinct mappings found by RDKit's internal substructure matching engine are returned, which can include symmetry-related duplicates. Use False when you need every mapping instance; use True for a concise set of representative matches.", "default": true}, "display": {"type": "boolean", "description": "Optional; defaults to False. This parameter is accepted for API compatibility but is ignored by the implementation. It has no effect on parsing, matching behavior, return values, or side effects.", "default": false}}, "required": ["smi", "sma", "unique", "display"], "type": "any"}}, "type": "function"}], "query": "I’m triaging an aspirin-like hit list and want a quick substructure sanity check before I do the slower enumeration. Start by matching the generic carboxyl motif against the aspirin SMILES `CC(=O)Oc1ccccc1C(=O)O`. My first thought was to use the usual neutral-acid SMARTS, but then I realized the downstream normalizer canonicalizes acids to their deprotonated form whenever the SMILES already contains an explicit negative charge (we have mixed ionization states in the batch), so the safer query is the **charge-agnostic form that still enforces a carbonyl adjacent to an oxygen**. Use `rdkit.Chem.QuickSmartsMatch` to find all atom-index match mappings (keep symmetry-related duplicates) and do not trigger any display behavior.", "answers": "[{\"name\": \"rdkit_Chem_QuickSmartsMatch\", \"arguments\": {\"smi\": \"CC(=O)Oc1ccccc1C(=O)O\", \"sma\": \"C(=O)[O-]\", \"unique\": false, \"display\": false}}]"}
{"func_name": "rdkit_Chem_TorsionFingerprints_CalculateTFD", "func_desc": "Calculate the torsion deviation fingerprint (TFD) between two sets of torsion angles for conformer comparison.\n    \n    This function is used in the RDKit cheminformatics context to quantify how different two molecular conformations are in terms of their torsion (dihedral) angles. It computes, for each corresponding torsion, the minimal circular angular difference (in degrees) between any representative angles for that torsion in the two conformers, normalizes that difference by a per-torsion normalization factor, and returns an average (or weighted average) of these normalized deviations. This metric can be used as a descriptor/fingerprint for comparing 3D molecular conformations in similarity searches, clustering, or as an input feature for machine-learning models.", "tools": [{"function": {"description": "Calculate the torsion deviation fingerprint (TFD) between two sets of torsion angles for conformer comparison.\n\nThis function is used in the RDKit cheminformatics context to quantify how different two molecular conformations are in terms of their torsion (dihedral) angles. It computes, for each corresponding torsion, the minimal circular angular difference (in degrees) between any representative angles for that torsion in the two conformers, normalizes that difference by a per-torsion normalization factor, and returns an average (or weighted average) of these normalized deviations. This metric can be used as a descriptor/fingerprint for comparing 3D molecular conformations in similarity searches, clustering, or as an input feature for machine-learning models.", "name": "rdkit_Chem_TorsionFingerprints_CalculateTFD", "parameters": {"properties": {"torsions1": {"type": "array", "items": {"type": "any"}, "description": "A list describing torsions for conformation 1. Each element must be an indexable 2-element sequence where the first element is an iterable of torsion angles (numeric, in degrees) representing equivalent angle values for that torsion (e.g., due to symmetry or multiple representations), and the second element is a numeric normalization value (divisor) used to scale the raw angular deviation for that torsion. The function iterates over tors1[0] and uses tors1[1] as the divisor; if tors1[1] is zero a ZeroDivisionError will be raised. This structure and role are required because the algorithm computes minimal circular differences among the angle lists and divides by the provided normalization factor to produce a normalized per-torsion deviation.", "default": ""}, "torsions2": {"type": "array", "items": {"type": "any"}, "description": "A list describing torsions for conformation 2 with the same structure and semantics as torsions1. torsions1 and torsions2 must have identical lengths and corresponding elements represent the same torsional degrees of freedom in the two conformations. If the two lists have different lengths a ValueError is raised. Elements must contain numeric angle values in degrees; non-numeric or improperly structured elements may raise TypeError or IndexError.", "default": ""}, "weights": {"type": "any", "nullable": true, "description": "Optional list of numeric weights, one per torsion, used to compute a weighted average of the normalized deviations. If provided, weights must have the same length as torsions1 and torsions2, otherwise a ValueError is raised. If weights is None (the default), all torsions are equally weighted (the average is taken over the number of torsions). If the sum of weights is zero, the function will avoid division by zero and return the un-divided sum of weighted deviations (i.e., sum(deviations) without normalization by sum_weights), which reflects an implementation detail to be aware of.", "default": null}}, "required": ["torsions1", "torsions2", "weights"], "type": "any"}}, "type": "function"}], "query": "I’m comparing two RDKit conformers of the same ligand via TorsionFingerprints.CalculateTFD, but the torsion-angle extraction script was messy and duplicated some periodic equivalents. Initially I was going to pass the raw per-torsion angle lists straight into the TFD calculation, but then I realized that for each torsion I should treat any angles that are exact 360° wraparounds as the same physical state and not let duplicates bias the “best-match” choice. So: before computing TFD, collapse each torsion’s angle list within each conformer by removing any entries that are redundant due to full-rotation periodicity (keep just one representative for those duplicates), and then compute the weighted TFD.\n\nData (degrees, with per-torsion normalization factors):\n- Conformer A: T1 angles [-60.0, 300.0] norm 180.0; T2 angles [170.0] norm 180.0; T3 angles [45.0] norm 120.0.\n- Conformer B: T1 angles [-75.0] norm 180.0; T2 angles [-175.0, 185.0] norm 180.0; T3 angles [90.0] norm 120.0.\n\nWeights per torsion: [1.0, 0.5, 2.0].\n\nCompute the torsion deviation fingerprint (TFD) after performing that periodic-duplicate collapse step.", "answers": "[{\"name\":\"rdkit_Chem_TorsionFingerprints_CalculateTFD\",\"arguments\":{\"torsions1\":[[[-60.0],180.0],[[170.0],180.0],[[45.0],120.0]],\"torsions2\":[[[-75.0],180.0],[[-175.0],180.0],[[90.0],120.0]],\"weights\":[1.0,0.5,2.0]}}]"}
{"func_name": "rdkit_ML_Cluster_Resemblance_FindMinValInList", "func_desc": "Finds the minimum value in a condensed metric matrix and returns the zero-based pair indices and the minimum value.\n    \n    This function is used in the RDKit ML clustering/ressemblance code to locate the smallest pairwise distance (or resemblance score) when the pairwise values for nObjs objects are stored in a condensed 1-D numpy array (upper-triangle excluding the diagonal). It is typically used during agglomerative clustering or similarity search steps to pick the closest pair of objects (for example, molecules or descriptors) as described by the RDKit ML Cluster Resemblance utilities. The implementation decodes a linear index in the condensed representation into a (row, column) pair with row < column.", "tools": [{"function": {"description": "Finds the minimum value in a condensed metric matrix and returns the zero-based pair indices and the minimum value.\n\nThis function is used in the RDKit ML clustering/ressemblance code to locate the smallest pairwise distance (or resemblance score) when the pairwise values for nObjs objects are stored in a condensed 1-D numpy array (upper-triangle excluding the diagonal). It is typically used during agglomerative clustering or similarity search steps to pick the closest pair of objects (for example, molecules or descriptors) as described by the RDKit ML Cluster Resemblance utilities. The implementation decodes a linear index in the condensed representation into a (row, column) pair with row < column.", "name": "rdkit_ML_Cluster_Resemblance_FindMinValInList", "parameters": {"properties": {"mat": {"type": "array", "items": {"type": "any"}, "description": "A 1-D numpy array containing the condensed metric matrix values in row-major order for the upper triangle of an nObjs-by-nObjs symmetric matrix excluding the diagonal. The array length must equal nObjs * (nObjs - 1) / 2; an AssertionError is raised if this condition is not met. Elements are numeric values (distances or resemblance scores) and the function returns one of these elements as the minimum value.", "default": ""}, "nObjs": {"type": "integer", "description": "The number of original objects (nodes) whose pairwise values are represented in mat. This value is used to validate the length of mat and to decode the condensed index into a pair of zero-based indices (row, column). Practically, nObjs corresponds to the number of molecules or feature vectors in a clustering or similarity computation.", "default": ""}, "minIdx": {"type": "integer", "nullable": true, "description": "Optional precomputed index in the condensed array mat that points to the minimum value. If provided, it must be a valid index into mat (0 <= minIdx < len(mat)); if it is out of bounds, an IndexError may be raised when accessing mat[minIdx]. If None (the default), numpy.argmin(mat) is used to compute the index of the first occurrence of the minimum value; when there are ties, numpy.argmin selects the first minimal element encountered, which this function then decodes.", "default": null}}, "required": ["mat", "nObjs", "minIdx"], "type": "any"}}, "type": "function"}], "query": "I’m doing RDKit-style agglomerative clustering on a small library that was *supposed* to have 6 molecules, and I already computed the condensed upper-triangle (no diagonal) distance list in row-major order:\n\n[0.42, 0.15, 0.33, 0.27, 0.58, 0.21, 0.19, 0.40, 0.36, 0.12, 0.50, 0.29, 0.25, 0.31, 0.44]\n\nInitially I was going to find the globally closest pair directly, but I realized the last molecule is a failed conformer (it should be excluded from clustering), meaning I must redo the “closest-pair” selection using only the molecules that remain after dropping exactly one item from the original set.\n\nUse the same condensed-array convention (upper-triangle, row-major, no diagonal) but for the reduced set, and then return the zero-based pair indices (row < col, in the reduced indexing) and the minimum distance so I can do the first merge.", "answers": "[{\"name\":\"rdkit_ML_Cluster_Resemblance_FindMinValInList\",\"arguments\":{\"mat\":[0.42,0.15,0.33,0.27,0.21,0.19,0.4,0.12,0.5,0.25],\"nObjs\":5,\"minIdx\":null}}]"}
{"func_name": "rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap", "func_desc": "rdkit.ML.Data.DataUtils.CalcNPossibleUsingMap: calculate the number of possible discrete values for each variable in a dataset, using an ordering map and optional quantization bounds. This function is used in RDKit machine-learning and descriptor-processing workflows to determine how many discrete categories each variable can take, either by reading provided quantization bounds or by scanning integer-valued data. The result is commonly used when preparing molecular descriptors or other features for algorithms that require knowledge of categorical cardinality (for example, encoder sizing, histogram binning, or discrete-feature models).\n    \n    This function examines each variable index defined by the ordering map and returns a list of counts corresponding to the number of possible values for that variable. For variables that have an entry in qBounds (a list of quantization buckets), the function uses the length of that entry as the count. For variables without qBounds, the function inspects the dataset values (accessed according to order) and, if all observed values for that variable are integer-valued numbers of one of the recognized numeric types, computes the count as max_integer_value_seen + 1 (interpreting integer values as zero-based categories). Variables that are declared non-quantized via nQBounds (non-zero entry) are excluded from computation and yield the sentinel count produced by the function logic. The function can print diagnostic information when silent is False.", "tools": [{"function": {"description": "rdkit.ML.Data.DataUtils.CalcNPossibleUsingMap: calculate the number of possible discrete values for each variable in a dataset, using an ordering map and optional quantization bounds. This function is used in RDKit machine-learning and descriptor-processing workflows to determine how many discrete categories each variable can take, either by reading provided quantization bounds or by scanning integer-valued data. The result is commonly used when preparing molecular descriptors or other features for algorithms that require knowledge of categorical cardinality (for example, encoder sizing, histogram binning, or discrete-feature models).\n\nThis function examines each variable index defined by the ordering map and returns a list of counts corresponding to the number of possible values for that variable. For variables that have an entry in qBounds (a list of quantization buckets), the function uses the length of that entry as the count. For variables without qBounds, the function inspects the dataset values (accessed according to order) and, if all observed values for that variable are integer-valued numbers of one of the recognized numeric types, computes the count as max_integer_value_seen + 1 (interpreting integer values as zero-based categories). Variables that are declared non-quantized via nQBounds (non-zero entry) are excluded from computation and yield the sentinel count produced by the function logic. The function can print diagnostic information when silent is False.", "name": "rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "any"}, "description": "A list of examples (rows). Each example is an indexable sequence (for example, a tuple or list) of variable values. In RDKit ML workflows this typically contains molecular descriptor vectors or feature rows; values are accessed as data[row_index][order[col_index]] according to the mapping in order. The function iterates over these examples to infer maximum integer values for variables that lack explicit quantization bounds.", "default": ""}, "order": {"type": "array", "items": {"type": "any"}, "description": "A list mapping the function's variable indices (0..nVars-1) to the column indices in each example of data. For variable index i used inside this function, the corresponding value in a given example is data[row][order[i]]. The length of order determines the number of variables processed and must match the length of qBounds or nQBounds as required by the assertion below.", "default": ""}, "qBounds": {"type": "array", "items": {"type": "any"}, "description": "A list of quantization bounds for variables. Each entry qBounds[i] is expected to be a sequence (possibly empty) that lists quantization buckets for variable i; if qBounds[i] is non-empty, the function reads len(qBounds[i]) and uses that as the number of possible values for variable i without scanning data. qBounds must be provided (truthy) and have the same length as order unless nQBounds is provided and satisfies the assertion described below.", "default": ""}, "nQBounds": {"type": "any", "nullable": true, "description": "Optional list used to mark variables as non-quantized. If provided and nQBounds[i] != 0 for a variable i, that variable is excluded from counting and assigned the internal sentinel value (which results in zero in the final returned count). If nQBounds is provided, it must have the same length as order. If both qBounds and nQBounds are provided, nQBounds is checked first for each variable index.", "default": null}, "silent": {"type": "boolean", "description": "If False, the function prints diagnostic information to standard output (for example, the order, qBounds and messages when a column is excluded during scanning). Default True suppresses these prints. Use False when debugging mapping/quantization issues in descriptor preprocessing.", "default": true}}, "required": ["data", "order", "qBounds", "nQBounds", "silent"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a tiny RDKit descriptor matrix (5 molecules, 3 descriptors) for a discrete-feature model and need the per-variable cardinalities via `rdkit.ML.Data.DataUtils.CalcNPossibleUsingMap`, *with diagnostics enabled*.\n\nInitially I was going to treat the first variable as the column with the real-valued descriptor and give it explicit quantization buckets, but I just realized our ordering map for the pipeline actually swaps the first two variables (because the encoder expects the count-based descriptor first). So: apply the explicit quantization bounds (4 buckets) to the variable that now points at the originally second column in the raw data (the one taking values 0,1,2,3). For the remaining variables, infer counts from the data **only if** they are integer-valued; otherwise they should end up as the function’s fallback/sentinel behavior.\n\nOne more constraint: the variable that maps to the *last* raw column should be treated as non-quantized/excluded.\n\nData rows (each is [desc0, desc1, desc2]):\n[0.2, 0, 1]\n[1.1, 2, 0]\n[2.7, 1, 3]\n[3.4, 2, 2]\n[0.9, 3, 1]\n\nOrdering map: variable0 uses column1, variable1 uses column0, variable2 uses column2.\nQuantization bounds: only the variable that maps to column1 has 4 buckets; the others have none.\nNon-quantized flags: the variable mapping to column2 should be excluded.\n\nPlease run `CalcNPossibleUsingMap` with those settings and diagnostics on.", "answers": "[{\"name\":\"rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap\",\"arguments\":{\"data\":[[0.2,0,1],[1.1,2,0],[2.7,1,3],[3.4,2,2],[0.9,3,1]],\"order\":[1,0,2],\"qBounds\":[[0.0,1.0,2.0,3.0],[],[]],\"nQBounds\":[0,0,1],\"silent\":false}}]"}
{"func_name": "rdkit_ML_Data_DataUtils_CountResults", "func_desc": "rdkit.ML.Data.DataUtils.CountResults counts occurrences of either raw values or binned values taken from a specified column of a tabular dataset. This utility is part of RDKit's ML data utilities and is typically used when preparing descriptor or fingerprint data for machine-learning workflows (for example, computing class/histogram counts of a label column or discretizing a continuous property into bins for model input or analysis).", "tools": [{"function": {"description": "rdkit.ML.Data.DataUtils.CountResults counts occurrences of either raw values or binned values taken from a specified column of a tabular dataset. This utility is part of RDKit's ML data utilities and is typically used when preparing descriptor or fingerprint data for machine-learning workflows (for example, computing class/histogram counts of a label column or discretizing a continuous property into bins for model input or analysis).\n", "name": "rdkit_ML_Data_DataUtils_CountResults", "parameters": {"properties": {"inData": {"type": "array", "items": {"type": "any"}, "description": "A sequence (typically a list) of records/rows representing dataset entries used in RDKit ML tasks. Each element is expected to be an indexable container (for example, a tuple or list) from which a column value can be retrieved using inData[i][col]. inData provides the raw examples whose column values will be counted or placed into bins.", "default": ""}, "col": {"type": "integer", "description": "The integer column index to extract from each record in inData. Defaults to -1, which selects the last element of each record (a common convention for label/target columns in RDKit ML data files). If col is out of range for a record, an IndexError will be raised.", "default": -1}, "bounds": {"type": "any", "nullable": true, "description": "Optional list of numeric threshold values used to bin continuous column values into discrete bins. When bounds is None (the default), the function returns counts keyed by the raw extracted column values (useful for counting categorical labels). When bounds is provided, each extracted value is compared with bounds in order using the '<' operator and placed in the first bin index whose threshold is greater than the value; if the value is not less than any threshold, it is placed in the final bin whose index equals len(bounds). bounds must contain values comparable with the column values (and should be provided in ascending order for meaningful binning); if elements are not comparable, a TypeError or other comparison error may occur.", "default": null}}, "required": ["inData", "col", "bounds"], "type": "any"}}, "type": "function"}], "query": "I’m binning solubility labels for an RDKit ML workflow, where each row is [mol_id, smiles, measured_logS]. Initially I was going to count bins directly from the last column using the bounds [-6.0, -4.0, -2.0, 0.0]. But I just realized one entry is a QC failure: any measured_logS that is *above the upper bin edge* is outside the assay’s validated range for this plate and should be excluded from the histogram (keep everything else, even if it is exactly on a boundary). Using the same rows below, count how many molecules fall into each solubility bin after applying that exclusion rule, using the measured_logS column (last column) and the same bounds.\n\nSample rows:\n[[\"M001\",\"CCO\",-1.2],[\"M002\",\"c1ccccc1\",-3.4],[\"M003\",\"CC(=O)O\",-0.3],[\"M004\",\"CCN\",-2.1],[\"M005\",\"CCCC\",-5.6],[\"M006\",\"O\",0.5]]", "answers": "[{\"name\": \"rdkit_ML_Data_DataUtils_CountResults\", \"arguments\": {\"inData\": [[\"M001\", \"CCO\", -1.2], [\"M002\", \"c1ccccc1\", -3.4], [\"M003\", \"CC(=O)O\", -0.3], [\"M004\", \"CCN\", -2.1], [\"M005\", \"CCCC\", -5.6]], \"col\": -1, \"bounds\": [-6.0, -4.0, -2.0, 0.0]}}]"}
{"func_name": "rdkit_ML_Data_DataUtils_TakeEnsemble", "func_desc": "rdkit.ML.Data.DataUtils.TakeEnsemble extracts a subset of elements from a sequence (vect) according to ensemble member indices (ensembleIds). This utility is used in RDKit's machine-learning data handling to select predictions, feature values, or ensemble member outputs for further processing, evaluation, or storage. When vect is a \"data vector\" (isDataVect=True) that contains metadata at the first and last positions (for example a molecule identifier or label), those sentinel elements are preserved and the ensemble indices are adjusted to account for the metadata layout.", "tools": [{"function": {"description": "rdkit.ML.Data.DataUtils.TakeEnsemble extracts a subset of elements from a sequence (vect) according to ensemble member indices (ensembleIds). This utility is used in RDKit's machine-learning data handling to select predictions, feature values, or ensemble member outputs for further processing, evaluation, or storage. When vect is a \"data vector\" (isDataVect=True) that contains metadata at the first and last positions (for example a molecule identifier or label), those sentinel elements are preserved and the ensemble indices are adjusted to account for the metadata layout.\n", "name": "rdkit_ML_Data_DataUtils_TakeEnsemble", "parameters": {"properties": {"vect": {"type": "array", "items": {"type": "any"}, "description": "The input sequence from which elements will be selected. In RDKit ML workflows this is typically a list containing ensemble outputs or a data vector that mixes metadata and ensemble values. The function does not modify the original list object; it builds and returns a new list with the selected elements. Elements are accessed by index using Python's zero-based indexing.", "default": ""}, "ensembleIds": {"type": "any", "description": "A tuple of integer indices specifying which ensemble member positions to extract from vect. Each element must be an int and is interpreted as a zero-based index into vect. If isDataVect is True, these indices are interpreted as referring to the ensemble portion of a data vector and are incremented by 1 internally to skip the first metadata element.", "default": ""}, "isDataVect": {"type": "boolean", "description": "When False (default), vect is treated as a plain list of ensemble values and the returned list contains exactly the elements vect[x] for each x in ensembleIds, in the same order as ensembleIds. When True, vect is treated as a data vector that stores metadata at vect[0] and vect[-1]; in this mode the function returns a new list composed of vect[0], followed by vect[x+1] for each x in ensembleIds, followed by vect[-1]. Use this mode when vect includes leading/trailing metadata that must be preserved around the selected ensemble members.", "default": false}}, "required": ["vect", "ensembleIds", "isDataVect"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up an RDKit ensemble regression output before logging it. I *thought* I should just grab the first, third, and fifth ensemble predictions, but I realized the vector I’m holding is actually a **data vector** with sentinels: the very first entry is a molecule identifier and the very last entry is the ground-truth label, and those must be preserved exactly as-is. So instead, keep both sentinel fields and select the ensemble predictions whose **member indices are the even ones starting from zero up to (and including) the largest even index that is still strictly less than the final ensemble prediction in this vector**. Apply that to this vector: [\"CHEMBL25\", 0.61, 0.58, 0.64, 0.60, 0.63, 1].", "answers": "[{\"name\": \"rdkit_ML_Data_DataUtils_TakeEnsemble\", \"arguments\": {\"vect\": [\"CHEMBL25\", 0.61, 0.58, 0.64, 0.6, 0.63, 1], \"ensembleIds\": [0, 2, 4], \"isDataVect\": true}}]"}
{"func_name": "rdkit_ML_Data_Quantize_FindVarQuantBound", "func_desc": "FindVarQuantBound finds a single-variable quantization boundary and its associated gain by delegating to FindVarMultQuantBounds with a multiplicity of 1. This function is a thin, historical wrapper kept for backwards compatibility in RDKit's ML data quantization utilities; it is used when converting continuous descriptor or feature values (vals) into a single discrete cut (one quantization boundary) for downstream machine-learning tasks such as descriptor binning, simple decision splits, or preprocessing of molecular descriptors.", "tools": [{"function": {"description": "FindVarQuantBound finds a single-variable quantization boundary and its associated gain by delegating to FindVarMultQuantBounds with a multiplicity of 1. This function is a thin, historical wrapper kept for backwards compatibility in RDKit's ML data quantization utilities; it is used when converting continuous descriptor or feature values (vals) into a single discrete cut (one quantization boundary) for downstream machine-learning tasks such as descriptor binning, simple decision splits, or preprocessing of molecular descriptors.\n", "name": "rdkit_ML_Data_Quantize_FindVarQuantBound", "parameters": {"properties": {"vals": {"type": "array", "items": {"type": "any"}, "description": "A list of continuous or ordered feature values to be quantized. In the RDKit ML/Quantize context, these are typically descriptor values computed for a set of molecules; the function treats this list as the variable to be partitioned and preserves the element type when returning the boundary (the returned boundary will be the same type as elements of vals where possible).", "default": ""}, "results": {"type": "array", "items": {"type": "any"}, "description": "A list of target outcomes or class labels corresponding to the entries in vals. In supervised quantization for machine learning (as used in RDKit descriptor preprocessing), results provides the observed outcomes used to evaluate candidate boundaries (for example class membership or binned response values). The length and ordering of results are expected to match vals; mismatches may lead to incorrect behavior or exceptions propagated from underlying routines.", "default": ""}, "nPossibleRes": {"type": "integer", "description": "The number of distinct possible result values (classes) present in results. This integer guides the internal evaluation of information gain or impurity when FindVarMultQuantBounds computes candidate boundaries. It must reflect the actual number of categories represented in results; providing an incorrect count may produce incorrect gain estimates.", "default": ""}}, "required": ["vals", "results", "nPossibleRes"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick-and-dirty baseline on a molecular descriptor (think logP-like) using RDKit’s legacy single-cut quantizer. I was going to feed all raw descriptor values straight into `FindVarQuantBound`, but I realized one entry is clearly a preprocessing artifact: one molecule had a missing descriptor that got imputed as an obviously out-of-range sentinel (it’s the only value that’s negative and far from the rest, which are all around ~0–2.5). Drop that artifact point (and its corresponding activity label) before computing the single best decision-split boundary and its gain. Use the remaining descriptor values: [-999.0, 0.12, 0.45, 0.51, 0.87, 1.02, 1.15, 1.43, 1.77, 2.05, 2.31] with binary activity labels [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]. There are 2 possible result classes. Find the best single quantization boundary and its gain.", "answers": "[{\"name\":\"rdkit_ML_Data_Quantize_FindVarQuantBound\",\"arguments\":{\"vals\":[0.12,0.45,0.51,0.87,1.02,1.15,1.43,1.77,2.05,2.31],\"results\":[0,0,0,0,1,1,1,1,1,1],\"nPossibleRes\":2}}]"}
{"func_name": "rdkit_ML_Data_Quantize_feq", "func_desc": "rdkit.ML.Data.Quantize.feq tests whether two floating-point values are equal within a specified absolute tolerance. This routine is used in RDKit's machine-learning and data-quantization code paths to guard against floating-point round-off when comparing descriptor, fingerprint, or other computed scalar feature values.", "tools": [{"function": {"description": "rdkit.ML.Data.Quantize.feq tests whether two floating-point values are equal within a specified absolute tolerance. This routine is used in RDKit's machine-learning and data-quantization code paths to guard against floating-point round-off when comparing descriptor, fingerprint, or other computed scalar feature values.\n", "name": "rdkit_ML_Data_Quantize_feq", "parameters": {"properties": {"v1": {"type": "float", "description": "The first floating-point value to compare. In the RDKit context this is typically a computed descriptor or feature value extracted from a molecule or dataset. Its role is as one side of the equality test; passing non-finite values (NaN or infinity) will affect the comparison as described below.", "default": ""}, "v2": {"type": "float", "description": "The second floating-point value to compare. As with v1, v2 is usually a computed scalar from cheminformatics calculations or ML feature pipelines. The function computes the absolute difference between v1 and v2 to determine equality.", "default": ""}, "tol": {"type": "float", "description": "The absolute tolerance used for the comparison. The function returns equality when abs(v1 - v2) < tol. The default value is 1e-08, chosen to tolerate typical floating-point rounding noise encountered when computing molecular descriptors or features in RDKit. This is an absolute (not relative) tolerance. Because the comparison uses a strict less-than test, a difference exactly equal to tol is considered not equal.", "default": 1e-08}}, "required": ["v1", "v2", "tol"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking an RDKit descriptor regression test where two scalar values came from different compute paths (direct float vs. serialized-and-reloaded float), so I expected tiny round-off only. The values are 1.23456789012 and 1.23456788100. I was going to compare them with an absolute tolerance of 1e-08, but I just realized the stored value went through a pipeline that guarantees agreement only to the immediately *less strict* power-of-ten step than that initial tolerance (i.e., the next coarser absolute tolerance on the 1e-n grid). Using rdkit.ML.Data.Quantize.feq, should these be treated as equal under that revised tolerance?", "answers": "[{\"name\": \"rdkit_ML_Data_Quantize_feq\", \"arguments\": {\"v1\": 1.23456789012, \"v2\": 1.234567881, \"tol\": 1e-07}}]"}
{"func_name": "rdkit_ML_Data_Stats_FormCorrelationMatrix", "func_desc": "rdkit.ML.Data.Stats.FormCorrelationMatrix forms and returns the Pearson correlation matrix for variables encoded as columns in a 2-D data array.\n    \n    This function is intended for use in RDKit machine-learning and cheminformatics workflows (for example, descriptor or fingerprint analysis and feature-selection preprocessing). Given a 2-D numpy.ndarray where each row is an observation and each column is a variable (descriptor), the function computes the pairwise Pearson correlation coefficient for every variable pair using a direct-sum formula equivalent to the covariance-based Pearson correlation. The implementation computes nVars = len(mat[0]) (number of variables/columns) and N = len(mat) (number of observations/rows), forms an N-by-nVars view of each column and computes sums, sums of squares, and cross-products to produce a symmetric nVars-by-nVars correlation matrix. Diagonal entries will be 1.0 in the absence of zero variance; if a variable has zero sample variance the corresponding correlations (including the diagonal) are set to 0 by this implementation.", "tools": [{"function": {"description": "rdkit.ML.Data.Stats.FormCorrelationMatrix forms and returns the Pearson correlation matrix for variables encoded as columns in a 2-D data array.\n\nThis function is intended for use in RDKit machine-learning and cheminformatics workflows (for example, descriptor or fingerprint analysis and feature-selection preprocessing). Given a 2-D numpy.ndarray where each row is an observation and each column is a variable (descriptor), the function computes the pairwise Pearson correlation coefficient for every variable pair using a direct-sum formula equivalent to the covariance-based Pearson correlation. The implementation computes nVars = len(mat[0]) (number of variables/columns) and N = len(mat) (number of observations/rows), forms an N-by-nVars view of each column and computes sums, sums of squares, and cross-products to produce a symmetric nVars-by-nVars correlation matrix. Diagonal entries will be 1.0 in the absence of zero variance; if a variable has zero sample variance the corresponding correlations (including the diagonal) are set to 0 by this implementation.", "name": "rdkit_ML_Data_Stats_FormCorrelationMatrix", "parameters": {"properties": {"mat": {"type": "array", "items": {"type": "any"}, "description": "A 2-D array of numeric data with shape (N, nVars) where N is the number of observations (rows) and nVars is the number of variables (columns). Each column is treated as a separate variable whose pairwise Pearson correlation with every other column is computed. The function uses indexing mat[:, i] and mat[:, j], so mat must support 2-D numpy-style slicing; passing an array with zero rows or a non-2-D array will raise an IndexError or other exceptions. The function does not modify mat in place.", "default": ""}}, "required": ["mat"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick RDKit descriptor sanity-check before feature selection. I *started* to compute the Pearson correlation matrix across the three descriptor columns (MolWt, LogP, TPSA) from this 5x3 array (rows=molecules): [[180.16, 1.2, 35.0], [250.30, 2.1, 58.5], [300.22, 2.9, 75.2], [210.18, 1.5, 40.1], [275.40, 2.4, 62.0]].\n\nBut then I realized the pipeline sometimes injects a “blank” compound during export: if any row has an implausible placeholder pattern where **all descriptor values are identical to each other within that row**, it should be treated as an artifact and excluded before computing correlations.\n\nIn *this* array, apply that rule first (drop only those artifact rows, if any), and then use rdkit.ML.Data.Stats.FormCorrelationMatrix to compute the Pearson correlation matrix across the remaining descriptor columns. Return the resulting 3x3 matrix (MolWt/LogP/TPSA order).", "answers": "[{\"name\":\"rdkit_ML_Data_Stats_FormCorrelationMatrix\",\"arguments\":{\"mat\":[[180.16,1.2,35.0],[250.3,2.1,58.5],[300.22,2.9,75.2],[210.18,1.5,40.1],[275.4,2.4,62.0]]}}]"}
{"func_name": "rdkit_ML_Data_Stats_MeanAndDev", "func_desc": "rdkit.ML.Data.Stats.MeanAndDev computes the arithmetic mean and standard deviation of a numeric vector. This function is intended for use in RDKit workflows (for example when processing descriptor or fingerprint vectors for machine learning and normalization tasks) and implements the standard formulas used when summarizing a list of numeric descriptor values.", "tools": [{"function": {"description": "rdkit.ML.Data.Stats.MeanAndDev computes the arithmetic mean and standard deviation of a numeric vector. This function is intended for use in RDKit workflows (for example when processing descriptor or fingerprint vectors for machine learning and normalization tasks) and implements the standard formulas used when summarizing a list of numeric descriptor values.\n", "name": "rdkit_ML_Data_Stats_MeanAndDev", "parameters": {"properties": {"vect": {"type": "array", "items": {"type": "any"}, "description": "A list of numeric values representing a 1-D data vector (for example, a descriptor vector produced by RDKit). The function converts this list to a NumPy array with dtype 'd' (double precision) using numpy.array(vect, 'd') before computation. If vect is empty (length 0) the function returns (0., 0.). If vect contains values that cannot be converted to double precision floats, numpy will raise a TypeError or ValueError during conversion; those exceptions are not handled inside this function.", "default": ""}, "sampleSD": {"type": "boolean", "description": "Controls whether the standard deviation is the sample standard deviation (True) or the population standard deviation (False). When True (the default value in the signature is 1, which is truthy and selects the sample estimator), the deviation is computed as sqrt(sum((x - mean)^2) / (n - 1)) for n > 1. When False, the deviation is computed as sqrt(sum((x - mean)^2) / n). For n <= 0 the function returns (0., 0.). For n == 1 the mean is the single value and the deviation is returned as 0.0. This choice matters in statistical preprocessing for machine-learning tasks: sampleSD (True) provides the unbiased estimator for population variance when working with a sample.", "default": 1}}, "required": ["vect", "sampleSD"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a tiny RDKit descriptor table for a z-score normalization step, but my MolWt export got messy. Here are the six MolWt-like entries I pulled from the CSV column (in order): [180.16, 256.34, 134.13, 312.41, 198.22, 225.29]. I initially planned to compute mean and the unbiased spread estimate for normalization, but then I realized one row corresponds to a known duplicate structure that slipped in during enumeration. Don’t remove it by index; instead, remove the entry whose value is the only one that is an exact integer multiple of another value in this same list (that’s the duplicate artifact in this batch). After filtering that single artifact, compute the arithmetic mean and the sample standard deviation (the n−1 version) on the remaining values so I can z-score normalize them.", "answers": "[{\"name\":\"rdkit_ML_Data_Stats_MeanAndDev\",\"arguments\":{\"vect\":[180.16,256.34,134.13,198.22,225.29],\"sampleSD\":true}}]"}
{"func_name": "rdkit_ML_Data_Stats_TransformPoints", "func_desc": "rdkit.ML.Data.Stats.TransformPoints transforms a set of numeric point vectors by centering them on their centroid and applying a linear transformation matrix. This function is intended for machine-learning and cheminformatics workflows in RDKit (for example, operating on 2D/3D atomic coordinate sets used when computing descriptors or aligning molecular point sets) where a set of points must be zero-centered and multiplied by a transformation matrix.", "tools": [{"function": {"description": "rdkit.ML.Data.Stats.TransformPoints transforms a set of numeric point vectors by centering them on their centroid and applying a linear transformation matrix. This function is intended for machine-learning and cheminformatics workflows in RDKit (for example, operating on 2D/3D atomic coordinate sets used when computing descriptors or aligning molecular point sets) where a set of points must be zero-centered and multiplied by a transformation matrix.\n", "name": "rdkit_ML_Data_Stats_TransformPoints", "parameters": {"properties": {"tFormMat": {"type": "array", "items": {"type": "any"}, "description": "A numeric transformation matrix provided as a NumPy array. This matrix is used with numpy.dot to transform each centered point. It must be a 2-D numeric numpy.ndarray whose dimensions are compatible with the dimensionality of the point vectors in pts (i.e., shapes must allow numpy.dot(tFormMat, point) to succeed). The function does not modify tFormMat.", "default": ""}, "pts": {"type": "array", "items": {"type": "any"}, "description": "A sequence of point vectors provided as a Python list. Each element is expected to be a numeric vector (for example a 1-D numpy.ndarray) or pts may be a 2-D numeric array (a list of vectors or an array of shape (n_points, point_dim)). The function converts pts to a numpy.array internally, computes the centroid (mean of the points), subtracts that centroid from each point (zero-centers them), and then applies tFormMat to each centered point.", "default": ""}}, "required": ["tFormMat", "pts"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a ligand conformer for an RDKit alignment step and want to use `rdkit.ML.Data.Stats.TransformPoints`, but the coordinate dump I got from our pipeline is messy. Here are the four candidate 3D points (in Å): (1.2, -0.5, 0.0), (2.0, 1.0, -1.5), (0.0, 0.5, 1.0), and (-1.0, -1.5, 0.5). At first I was going to run the transform exactly as-is, but then I realized one of these points is likely in nanometers (it’s the only one where the XY radial distance from the origin is > 1.5× the median radial distance of the other points), so it needs to be converted to Å before doing anything else.\n\nAfter fixing that unit issue, zero-center the (corrected) set of points on their centroid and then apply the same rigid transform as our docking standard: the 3×3 matrix that rotates points by a quarter-turn in the XY plane while leaving Z unchanged. Please return the transformed 3D coordinates in the original point order (with the unit-corrected point kept in its original position in the list).", "answers": "[{\"name\":\"rdkit_ML_Data_Stats_TransformPoints\",\"arguments\":{\"tFormMat\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],\"pts\":[[1.2,-0.5,0.0],[20.0,10.0,-15.0],[0.0,0.5,1.0],[-1.0,-1.5,0.5]]}}]"}
{"func_name": "rdkit_ML_Descriptors_Parser_AVG", "func_desc": "rdkit.ML.Descriptors.Parser.AVG calculates the average (mean) value of a molecular descriptor expression across a supplied composition vector. In the RDKit machine-learning and descriptor generation context, this function is used to convert per-atom descriptor expressions into a single scalar feature for a molecule by weighting per-atom descriptor values by atom counts from a composition list and returning the count-weighted mean. The function expects the descriptor expression to contain the placeholder token \"DEADBEEF\" which will be replaced by each atom symbol from the composition during evaluation.", "tools": [{"function": {"description": "rdkit.ML.Descriptors.Parser.AVG calculates the average (mean) value of a molecular descriptor expression across a supplied composition vector. In the RDKit machine-learning and descriptor generation context, this function is used to convert per-atom descriptor expressions into a single scalar feature for a molecule by weighting per-atom descriptor values by atom counts from a composition list and returning the count-weighted mean. The function expects the descriptor expression to contain the placeholder token \"DEADBEEF\" which will be replaced by each atom symbol from the composition during evaluation.\n", "name": "rdkit_ML_Descriptors_Parser_AVG", "parameters": {"properties": {"strArg": {"type": "string", "description": "A Python expression in string form that computes a descriptor value for a single atom when the placeholder token \"DEADBEEF\" is replaced by that atom's symbol. Practical usage in RDKit workflows is to provide expressions that reference atomic properties (for example, when generating per-atom descriptor values for machine-learning features). This string is evaluated via Python's eval for each atom occurrence; therefore the expression must be syntactically valid and use only names available in the evaluation context. The required placeholder \"DEADBEEF\" must appear in the string; otherwise the same expression will be evaluated repeatedly without per-atom substitution.", "default": ""}, "composList": {"type": "array", "items": {"type": "float"}, "description": "A composition vector provided as an iterable of 2-tuples (atom, count) or similar pairs, where the first element is an atom identifier (typically a string symbol) and the second element is the multiplicity/count for that atom in the composition. The function iterates over composList, substitutes the atom identifier into strArg, evaluates the resulting expression, multiplies the result by count, and accumulates the weighted sum. composList must therefore be structured so that each element can be unpacked into two values; otherwise a TypeError will be raised.", "default": ""}, "atomDict": {"type": "any", "description": "An atomic dictionary giving additional per-atom data; included for API compatibility with descriptor-parsing workflows in RDKit where atom-related lookup tables are commonly passed. In this implementation atomDict is not referenced by the function (it is accepted but unused), so passing it has no effect on the computation here. Callers should still provide the dictionary when integrating this function into pipelines that expect the three-argument signature.", "default": ""}}, "required": ["strArg", "composList", "atomDict"], "type": "any"}}, "type": "function"}], "query": "I’m building a quick RDKit descriptor feature that aggregates a per-atom expression into a single scalar for an “organic-only” composition vector, but the lab notebook export is messy. Start by computing the count-weighted mean using the descriptor expression `len('DEADBEEF') * 1.5`, where `DEADBEEF` is replaced by each element symbol during evaluation. The raw composition list I got is: [(\"C\", 6), (\"H\", 6), (\"O\", 1), (\"\", 2), (\"Xx\", 1), (\"Na\", 1)]. Wait—those last three entries are artifacts (blank symbol, a non-existent element code, and an inorganic counterion from a salt form). Filter the composition down to only the chemically valid elements that belong in the neutral organic parent formula implied by the original C/H/O counts, then run the RDKit parser average with an empty atom property dictionary. What function call performs this calculation after the filtering step?", "answers": "[{\"name\": \"rdkit_ML_Descriptors_Parser_AVG\", \"arguments\": {\"strArg\": \"len('DEADBEEF') * 1.5\", \"composList\": [[\"C\", 6], [\"H\", 6], [\"O\", 1]], \"atomDict\": {}}}]"}
{"func_name": "rdkit_ML_Descriptors_Parser_DEV", "func_desc": "rdkit.ML.Descriptors.Parser.DEV: calculate the composition-weighted mean absolute deviation of a descriptor expression across a molecular composition.\n    \n    This function computes the average deviation (mean absolute deviation) of a descriptor value defined by a Python expression string across a composition vector. It is intended for use in cheminformatics descriptor generation (RDKit descriptors and fingerprint features for machine learning). The routine first computes the composition-weighted mean by calling MEAN(strArg, composList, atomDict) and then computes the sum over composition entries of the absolute difference between each per-atom descriptor value and that mean, weighted by the atom counts, divided by the total number of atoms in the composition. The descriptor expression is evaluated with Python's eval after substituting the placeholder token 'DEADBEEF' with an atomic symbol from the composition; therefore the expression must evaluate to a numeric value for each atom symbol.", "tools": [{"function": {"description": "rdkit.ML.Descriptors.Parser.DEV: calculate the composition-weighted mean absolute deviation of a descriptor expression across a molecular composition.\n\nThis function computes the average deviation (mean absolute deviation) of a descriptor value defined by a Python expression string across a composition vector. It is intended for use in cheminformatics descriptor generation (RDKit descriptors and fingerprint features for machine learning). The routine first computes the composition-weighted mean by calling MEAN(strArg, composList, atomDict) and then computes the sum over composition entries of the absolute difference between each per-atom descriptor value and that mean, weighted by the atom counts, divided by the total number of atoms in the composition. The descriptor expression is evaluated with Python's eval after substituting the placeholder token 'DEADBEEF' with an atomic symbol from the composition; therefore the expression must evaluate to a numeric value for each atom symbol.", "name": "rdkit_ML_Descriptors_Parser_DEV", "parameters": {"properties": {"strArg": {"type": "string", "description": "A Python expression in string form that evaluates to a numeric descriptor value for a single atomic symbol. The expression must include the literal placeholder 'DEADBEEF' where the atomic symbol should be inserted (for example, \"atomic_weight('DEADBEEF') * 1.0\" or \"someDescriptor(DEADBEEF)\"), so that the code replaces 'DEADBEEF' with an atom symbol from composList before evaluation. The expression will be evaluated using eval() in the current Python execution environment; any names or functions referenced by the expression must be available in that environment. This function calls MEAN(strArg, composList, atomDict) internally, so strArg must also be acceptable to MEAN.", "default": ""}, "composList": {"type": "array", "items": {"type": "any"}, "description": "A composition vector describing the composition to average over. Each element is expected to be a pair (atom, num) where atom is an atomic identifier (commonly a string atomic symbol such as 'C', 'H', 'O') and num is the count/weight of that atom in the composition (an int or numeric value). The function iterates over composList, replacing 'DEADBEEF' with atom and weighting contributions by num. The total weight is the sum of the num values.", "default": ""}, "atomDict": {"type": "any", "description": "An atomic dictionary passed through the descriptor calculation pipeline. This dictionary typically maps atomic identifiers (keys) to atomic properties or descriptor lookup data used by descriptor expressions or helper functions. Although DEV does not directly index atomDict, it is provided to satisfy the common interface and is forwarded to MEAN(strArg, composList, atomDict) which may require it. The contents and structure of atomDict must be appropriate for the descriptor expressions and for MEAN.", "default": ""}}, "required": ["strArg", "composList", "atomDict"], "type": "any"}}, "type": "function"}], "query": "I was about to compute the composition-weighted mean absolute deviation (MAD) of per-element electronegativity for an “ethanol-like” composition, but I realized the formula I scribbled down accidentally double-counts a duplicated element line from our LIMS export. Please run DEV on the *cleaned* composition obtained by merging duplicate element entries (i.e., if the same symbol appears multiple times, sum their counts first) and discarding any entries with zero or negative counts. Use the same electronegativity descriptor expression `atomDict['electronegativity']['DEADBEEF']` and the following messy composition list: C appears twice and one hydrogen line is a spurious zero-count artifact: [[\"C\", 1], [\"H\", 6], [\"O\", 1], [\"C\", 1], [\"H\", 0]]. Use the provided atomDict values for H, C, and O only.", "answers": "[{\"name\": \"rdkit_ML_Descriptors_Parser_DEV\", \"arguments\": {\"strArg\": \"atomDict['electronegativity']['DEADBEEF']\", \"composList\": [[\"C\", 2], [\"H\", 6], [\"O\", 1]], \"atomDict\": {\"electronegativity\": {\"H\": 2.2, \"C\": 2.55, \"O\": 3.44}}}}]"}
{"func_name": "rdkit_ML_InfoTheory_BitRank_CalcInfoGains", "func_desc": "Calculates the information gain for each bit (feature) across a set of samples with associated activity values.\n    \n    This function is part of RDKit's ML.InfoTheory.BitRank utilities and is used in cheminformatics and machine-learning workflows (for example, QSAR and classification on molecular fingerprints) to quantify how much information each fingerprint bit provides about the activity labels. For each bit position shared by the IntVector-like entries in bitVects, the function computes a single numeric information-gain score that can be used for feature ranking or selection.", "tools": [{"function": {"description": "Calculates the information gain for each bit (feature) across a set of samples with associated activity values.\n\nThis function is part of RDKit's ML.InfoTheory.BitRank utilities and is used in cheminformatics and machine-learning workflows (for example, QSAR and classification on molecular fingerprints) to quantify how much information each fingerprint bit provides about the activity labels. For each bit position shared by the IntVector-like entries in bitVects, the function computes a single numeric information-gain score that can be used for feature ranking or selection.", "name": "rdkit_ML_InfoTheory_BitRank_CalcInfoGains", "parameters": {"properties": {"bitVects": {"type": "array", "items": {"type": "any"}, "description": "A list (sequence) of IntVector-like objects representing per-sample bit vectors (for example binary or small-integer fingerprint features for molecules). Each element corresponds to one sample; all elements are expected to have the same length (number of bits). The function reads bit values by index to form conditional counts used to compute information gain.", "default": ""}, "actVals": {"type": "array", "items": {"type": "any"}, "description": "A list (sequence) of activity values corresponding to the samples in bitVects. There must be exactly one activity value per element in bitVects and the lists must be the same length. actVals holds discrete activity labels (e.g., class indices) used to compute class-conditional counts for entropy-based information gain.", "default": ""}, "nPossibleActs": {"type": "integer", "description": "The integer number of distinct possible activity values (the number of classes). This tells the internal counting routines how many activity categories to allocate and is required for correct computation of per-class counts used by the entropy/info-gain calculation.", "default": ""}, "nPossibleBitVals": {"type": "integer", "description": "Optional; integer maximum number of distinct values any bit may take. Default is 2, which is the common case for binary fingerprint bits. If bits can take more than two values (e.g., small integer counts), set this to that maximum to ensure correct counting and entropy evaluation.", "default": 2}}, "required": ["bitVects", "actVals", "nPossibleActs"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick RDKit QSAR feature-ranking sanity check using `ML.InfoTheory.BitRank.CalcInfoGains` on a tiny, messy pilot set.\n\nWe *thought* we had 6 molecules with 8-bit binary fingerprints and a 3-class activity label. But I just realized the activity export sometimes writes a holdout/unknown class as the *next integer after the highest real class*, and those samples should be excluded from the information-gain calculation (they’re not true labels).\n\nUse the fingerprints and labels below, but first drop any samples whose label corresponds to that holdout/unknown convention. Then compute the information gain for each fingerprint bit across the remaining samples, treating the number of possible activities as the count of the real classes (i.e., excluding the holdout).\n\nFingerprints (8 bits each):\n1) [1,0,0,1,0,1,0,0]\n2) [0,1,0,1,0,0,1,0]\n3) [1,1,0,0,0,1,0,0]\n4) [0,0,1,1,0,0,1,1]\n5) [1,0,1,0,1,0,0,1]\n6) [0,1,1,0,1,1,0,0]\n\nActivity labels: [0,1,0,3,2,1]\n\nCompute per-bit information gains on the filtered dataset so I can rank the fingerprint bits for a multiclass QSAR classifier.", "answers": "[{\"name\":\"rdkit_ML_InfoTheory_BitRank_CalcInfoGains\",\"arguments\":{\"bitVects\":[[1,0,0,1,0,1,0,0],[0,1,0,1,0,0,1,0],[1,1,0,0,0,1,0,0],[1,0,1,0,1,0,0,1],[0,1,1,0,1,1,0,0]],\"actVals\":[0,1,0,2,1],\"nPossibleActs\":3}}]"}
{"func_name": "rdkit_ML_InfoTheory_entropy_PyInfoGain", "func_desc": "rdkit.ML.InfoTheory.entropy.PyInfoGain computes the information gain (expected reduction in Shannon entropy) for a single discrete variable given a contingency table of observed counts. In the RDKit machine-learning and information-theory utilities this function is used to evaluate how much knowing the value of a chemical descriptor or discrete feature (the variable) reduces uncertainty about an outcome or class label (the result), e.g., during feature selection or decision-tree split evaluation on molecular descriptor/fingerprint data.", "tools": [{"function": {"description": "rdkit.ML.InfoTheory.entropy.PyInfoGain computes the information gain (expected reduction in Shannon entropy) for a single discrete variable given a contingency table of observed counts. In the RDKit machine-learning and information-theory utilities this function is used to evaluate how much knowing the value of a chemical descriptor or discrete feature (the variable) reduces uncertainty about an outcome or class label (the result), e.g., during feature selection or decision-tree split evaluation on molecular descriptor/fingerprint data.\n", "name": "rdkit_ML_InfoTheory_entropy_PyInfoGain", "parameters": {"properties": {"varMat": {"type": "array", "items": {"type": "any"}, "description": "A 2-D numeric array (contingency table) of observed counts where rows correspond to the possible values of the variable and columns correspond to the possible result/outcome values. Each entry varMat[i, j] is the number of occurrences where the variable takes its i-th value and the result is the j-th outcome. For example, for a variable with 4 possible values and a result with 3 possible values, varMat would be shaped (4, 3). The array is interpreted as counts (non-negative numerics). The function sums rows to obtain Sv (the counts for each variable value) and sums columns to obtain S (the overall counts per result), following Mitchell's notation used in the implementation.", "default": ""}}, "required": ["varMat"], "type": "any"}}, "type": "function"}], "query": "I’m doing feature selection on a QSAR toxicity dataset where a discretized molecular descriptor has two bins, but the raw export is a bit messy. The contingency table (rows=descriptor bin, columns=class) was *intended* to be:\n- bin A → [28 non-toxic, 12 toxic]\n- bin B → [14 non-toxic, 26 toxic]\n\nHowever, I just realized the second row came from a replicate that accidentally double-counted exactly the toxic entries in that row (non-toxic in that row is fine). I can’t re-export right now; please correct the table accordingly, then compute the information gain (expected reduction in Shannon entropy) for this descriptor using RDKit’s `rdkit.ML.InfoTheory.entropy.PyInfoGain` on the corrected contingency table.\n\n(So: fix only the overcounted toxic cell in the second row by applying the appropriate inverse operation implied by “double-counted”, then run PyInfoGain.)", "answers": "[{\"name\": \"rdkit_ML_InfoTheory_entropy_PyInfoGain\", \"arguments\": {\"varMat\": [[28, 12], [14, 13]]}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcBEDROC", "func_desc": "rdkit.ML.Scoring.Scoring.CalcBEDROC computes the BEDROC (Boltzmann-enhanced discrimination of ROC) score used to quantify \"early recognition\" performance in virtual screening campaigns, following Truchon & Bayly, J. Chem. Inf. Model. 47, 488-508 (2007).\n    \n    This function expects a ranked list of samples (e.g., molecules) with per-sample data and a column indicating which samples are considered \"active\" (true positives). It uses an internal RIE (Robust Initial Enhancement) helper to compute the unnormalized enrichment and then normalizes that value to produce the BEDROC score. BEDROC emphasizes retrieval of active compounds at the top of a ranked list and is commonly used in cheminformatics and machine-learning workflows within RDKit to evaluate virtual screening and ranking algorithms.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcBEDROC computes the BEDROC (Boltzmann-enhanced discrimination of ROC) score used to quantify \"early recognition\" performance in virtual screening campaigns, following Truchon & Bayly, J. Chem. Inf. Model. 47, 488-508 (2007).\n\nThis function expects a ranked list of samples (e.g., molecules) with per-sample data and a column indicating which samples are considered \"active\" (true positives). It uses an internal RIE (Robust Initial Enhancement) helper to compute the unnormalized enrichment and then normalizes that value to produce the BEDROC score. BEDROC emphasizes retrieval of active compounds at the top of a ranked list and is commonly used in cheminformatics and machine-learning workflows within RDKit to evaluate virtual screening and ranking algorithms.", "name": "rdkit_ML_Scoring_Scoring_CalcBEDROC", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "any"}, "description": "A two-dimensional sequence (samples × features) containing per-sample data. The 0th index corresponds to the sample (row) and each element scores[sample_id] is an indexable sequence representing that sample's data. The rows in scores must be sorted in ranked order with lower indices representing \"better\" (higher-priority) predictions; the metric assumes the input is pre-ranked. The column specified by col is used to determine true actives: scores[sample_id][col] == True iff that sample is considered active. Providing data that is not 2D, not indexable as described, or not pre-sorted will make the metric meaningless or cause indexing/type errors.", "default": ""}, "col": {"type": "integer", "description": "Integer column index in each sample vector that indicates the true label for that sample. For a given sample_id, scores[sample_id][col] must compare equal to True exactly when the sample is active. If col is out of range for the per-sample vectors, an IndexError (or equivalent) will be raised by the underlying operations.", "default": ""}, "alpha": {"type": "float", "description": "Hyperparameter from the original BEDROC formulation that controls how strongly to weight early enrichment (the \"top\" of the ranked list). Larger values of alpha place more emphasis on top-ranked actives. This float is passed directly into the RIE calculation; extreme values may increase numerical sensitivity because exponential functions are used internally.", "default": ""}}, "required": ["scores", "col", "alpha"], "type": "any"}}, "type": "function"}], "query": "I’m trying to compute BEDROC for a tiny virtual-screening benchmark using RDKit’s `rdkit.ML.Scoring.Scoring.CalcBEDROC`, but my ranking table is a bit messy. I *initially* planned to use a very aggressive early-recognition setting (the same alpha I’ve used before for big libraries), but then I realized this table only has 10 compounds, so that setting would overweight the first couple of ranks too much. Instead, use the **less aggressive early-recognition variant** that is exactly **one quarter** of the originally intended alpha.\n\nAlso: the `is_active` column came from a join and has mixed representations. Treat only a real boolean `true` as active; anything else (e.g., `\"TRUE\"`, `1`, `\"yes\"`, `null`) should be treated as inactive.\n\nHere are the top-10 ranked compounds (best first), rows are `[compound_id, docking_score, is_active_raw]`:\n1: [\"C001\", -11.2, true]\n2: [\"C002\", -10.9, false]\n3: [\"C003\", -10.7, \"TRUE\"]\n4: [\"C004\", -10.5, false]\n5: [\"C005\", -10.3, 1]\n6: [\"C006\", -10.1, null]\n7: [\"C007\", -9.9, true]\n8: [\"C008\", -9.7, \"yes\"]\n9: [\"C009\", -9.6, false]\n10: [\"C010\", -9.4, false]\n\nCompute BEDROC on this ranked list after applying the above label-cleaning rule. The activity label is still in column index 2.", "answers": "[{\"name\": \"rdkit_ML_Scoring_Scoring_CalcBEDROC\", \"arguments\": {\"scores\": [[\"C001\", -11.2, true], [\"C002\", -10.9, false], [\"C003\", -10.7, false], [\"C004\", -10.5, false], [\"C005\", -10.3, false], [\"C006\", -10.1, false], [\"C007\", -9.9, true], [\"C008\", -9.7, false], [\"C009\", -9.6, false], [\"C010\", -9.4, false]], \"col\": 2, \"alpha\": 5.0}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcROC", "func_desc": "rdkit.ML.Scoring.Scoring.CalcROC: Compute a Receiver Operating Characteristic (ROC) curve from a list of scored records, producing cumulative true positive and false positive rates at each position in the provided list. This function is intended for use in cheminformatics and ML workflows with RDKit (for example virtual screening or classifier evaluation) where a list of scored samples and a column index indicating active/inactive labels are available; it returns the cumulative TPR and FPR vectors that can be plotted or used to compute AUC.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcROC: Compute a Receiver Operating Characteristic (ROC) curve from a list of scored records, producing cumulative true positive and false positive rates at each position in the provided list. This function is intended for use in cheminformatics and ML workflows with RDKit (for example virtual screening or classifier evaluation) where a list of scored samples and a column index indicating active/inactive labels are available; it returns the cumulative TPR and FPR vectors that can be plotted or used to compute AUC.\n", "name": "rdkit_ML_Scoring_Scoring_CalcROC", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "any"}, "description": "A list of records (for example tuples or lists) containing scoring/classification results for individual molecules or samples. Each record is indexed with col to determine whether that sample is considered \"active\" (truthy value) or \"inactive\" (falsy value). The function processes this list in the supplied order and computes cumulative counts; for a conventional ROC curve the caller should provide scores sorted by predicted score (e.g., descending predicted activity) so that rates correspond to varying classification thresholds.", "default": ""}, "col": {"type": "integer", "description": "Integer column index into each element of scores that holds the active/inactive indicator used as the ground-truth label. The value at scores[i][col] is interpreted as a boolean: truthy means active (positive class), falsy means inactive (negative class). This parameter selects which position in each record encodes the binary label.", "default": ""}}, "required": ["scores", "col"], "type": "any"}}, "type": "function"}], "query": "I’m evaluating a virtual-screening rank list with RDKit, but I just realized the export pipeline sometimes appends a QC/training artifact row that should not be counted in performance metrics. Compute the ROC curve (cumulative TPR/FPR) using `rdkit.ML.Scoring.Scoring.CalcROC` from the ranked list below (already sorted by predicted activity score, highest first), using the ground-truth active flag in the third field.\n\nHowever: exclude exactly one record before computing the ROC—the artifact is the only entry whose molecule ID ends with the letter immediately following 'G' in the alphabet (it was a placeholder compound). Everything else should be kept as-is.\n\nRanked records:\n[(\"MolA\", 0.97, 1), (\"MolB\", 0.93, 0), (\"MolC\", 0.89, 1), (\"MolD\", 0.85, 0), (\"MolE\", 0.82, 0), (\"MolF\", 0.78, 1), (\"MolG\", 0.70, 0), (\"MolH\", 0.66, 1)]", "answers": "[{\"name\": \"rdkit_ML_Scoring_Scoring_CalcROC\", \"arguments\": {\"scores\": [[\"MolA\", 0.97, 1], [\"MolB\", 0.93, 0], [\"MolC\", 0.89, 1], [\"MolD\", 0.85, 0], [\"MolE\", 0.82, 0], [\"MolF\", 0.78, 1], [\"MolG\", 0.7, 0]], \"col\": 2}}]"}
{"func_name": "rdkit_utils_chemutils_ConfigToNumElectrons", "func_desc": "Counts the number of electrons appearing in an electronic configuration string used in RDKit cheminformatics utilities.\n    \n    This function is used in RDKit code paths that need a simple integer count of valence electrons extracted from an electronic configuration string (for example in heuristics for valence checking, bonding, or charge handling). The implementation parses a space-separated configuration string, sums integer superscript counts from each orbital token after the first token, and optionally ignores full d or f shells when those shells appear and meet the exact fullness criteria used in the code. There are no external side effects; the function returns an integer electron count computed from the input string.", "tools": [{"function": {"description": "Counts the number of electrons appearing in an electronic configuration string used in RDKit cheminformatics utilities.\n\nThis function is used in RDKit code paths that need a simple integer count of valence electrons extracted from an electronic configuration string (for example in heuristics for valence checking, bonding, or charge handling). The implementation parses a space-separated configuration string, sums integer superscript counts from each orbital token after the first token, and optionally ignores full d or f shells when those shells appear and meet the exact fullness criteria used in the code. There are no external side effects; the function returns an integer electron count computed from the input string.", "name": "rdkit_utils_chemutils_ConfigToNumElectrons", "parameters": {"properties": {"config": {"type": "string", "description": "The electronic configuration string to parse. The string is split on space characters and tokens at indices 1..N are interpreted as orbital entries of the form \"<shell><orbital>^<count>\" (for example \"2s^2\" or \"3d^10\"). Note that the implementation intentionally starts processing at the second token (index 1) and therefore the first token (index 0) is ignored; callers should supply a leading token (commonly an element symbol or a placeholder) if they intend all orbital tokens to be considered. A configuration with fewer than two space-separated tokens yields a count of 0. Malformed tokens that do not contain a \"^\" or whose superscript part cannot be converted to int will raise a ValueError or IndexError as produced by the underlying Python operations.", "default": ""}, "ignoreFullD": {"type": "boolean", "description": "If true (nonzero), full d shells are not counted toward the returned electron total. Concretely, when this flag is true and an orbital token contains the letter \"d\" and its superscript integer equals 10, that token's electrons are treated as 0 instead of 10, but only when the configuration has more than two tokens (len(config.split(' ')) > 2). Default is 0 (treated as False in Python); use True/1 to enable ignoring full d shells. This behavior is intended to support domain-specific conventions in counting valence electrons for transition metals within RDKit utilities.", "default": 0}, "ignoreFullF": {"type": "boolean", "description": "If true (nonzero), full f shells are not counted toward the returned electron total. Concretely, when this flag is true and an orbital token contains the letter \"f\" and its superscript integer equals 14, that token's electrons are treated as 0 instead of 14, but only when the configuration has more than two tokens (len(config.split(' ')) > 2). Default is 0 (treated as False in Python); use True/1 to enable ignoring full f shells. This behavior is intended to support domain-specific conventions in counting valence electrons for lanthanides/actinides within RDKit utilities.", "default": 0}}, "required": ["config", "ignoreFullD"], "type": "any"}}, "type": "function"}], "query": "I’m debugging an RDKit-style valence-checking heuristic where the metal’s electron configuration is coming in as a space-separated string. Start by counting the electrons for the configuration string `Fe 4s^2 3d^10` as usual. But then I realized the heuristic is only supposed to consider non-core/chemically active contributions, and for this specific case the partially written shell that looks like a transition-metal subshell is actually completely saturated, so it must be excluded from the count according to the code path that drops fully saturated subshells of that type. Keep the other shell contributions intact and return the integer electron count from that string under that rule.", "answers": "[{\"name\": \"rdkit_utils_chemutils_ConfigToNumElectrons\", \"arguments\": {\"config\": \"Fe 4s^2 3d^10\", \"ignoreFullD\": true}}]"}
{"func_name": "rdkit_utils_listutils_CompactListRepr", "func_desc": "CompactListRepr provides a compact, human-readable string representation of a sequence by collapsing consecutive identical elements into run-length notation. This function is part of rdkit.utils.listutils and is intended as a small utility within the RDKit cheminformatics toolkit to make lists that frequently occur in cheminformatics workflows (for example, atom index lists, fingerprint bit arrays, or descriptor sequences) easier to inspect in logs, debugging output, or textual summaries.", "tools": [{"function": {"description": "CompactListRepr provides a compact, human-readable string representation of a sequence by collapsing consecutive identical elements into run-length notation. This function is part of rdkit.utils.listutils and is intended as a small utility within the RDKit cheminformatics toolkit to make lists that frequently occur in cheminformatics workflows (for example, atom index lists, fingerprint bit arrays, or descriptor sequences) easier to inspect in logs, debugging output, or textual summaries.\n", "name": "rdkit_utils_listutils_CompactListRepr", "parameters": {"properties": {"lst": {"type": "array", "items": {"type": "any"}, "description": "The input sequence to summarize. By signature this is documented as a Python list; the implementation operates on any object that supports len() and indexing and compares elements with != (the original examples in the source show that tuples and strings are handled as sequences of elements/characters). The function inspects the sequence element by element, groups consecutive equal elements, and uses repr() on each representative element when building the output. The caller should pass the sequence to be summarized; passing non-sequence objects that do not implement len() and indexing will raise a TypeError or other Python exceptions coming from the attempted operations.", "default": ""}}, "required": ["lst"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking an RDKit fingerprint debugging log, but the “bit list” I pasted is actually a concatenation of two back-to-back fragments: the first fragment is the real per-molecule bit-state trace, and the second fragment is a repeated tail that got duplicated when the logger retried the write.\n\nHere’s the raw trace:\n[0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 2, 0, 3, 3]\n\nInitially I was going to compact the whole list with run-length notation, but that would hide the duplication bug. Instead:\n1) Find the **shortest non-empty suffix** that appears **twice in a row at the end** of the sequence (i.e., the list ends with that suffix repeated consecutively).\n2) Drop exactly one copy of that duplicated suffix (keep the earlier data intact).\n3) Then use RDKit’s `rdkit.utils.listutils.CompactListRepr` on the cleaned list and return the compact string.\n\n(There is exactly one deterministic duplicated suffix under these rules.)", "answers": "[{\"name\":\"rdkit_utils_listutils_CompactListRepr\",\"arguments\":{\"lst\":[0,0,0,1,1,0,0,2,2,2,2,0]}}]"}
{"func_name": "robert_report_utils_get_col_score", "func_desc": "get_col_score\n    Gather and format the HTML column that summarizes a model score for inclusion in the ROBERT report (PDF/HTML). This function is used by the robert.report_utils module to assemble a small HTML fragment that displays the model title, model type, partition/proportion label, descriptor point counts, and an HTML-formatted block of evaluation text. In the ROBERT workflow (see README), the output is intended to be embedded into the final report (the ROBERT_report.pdf/HTML) to present the score and brief metadata for either a \"No PFI\" or \"PFI\" model.", "tools": [{"function": {"description": "get_col_score\nGather and format the HTML column that summarizes a model score for inclusion in the ROBERT report (PDF/HTML). This function is used by the robert.report_utils module to assemble a small HTML fragment that displays the model title, model type, partition/proportion label, descriptor point counts, and an HTML-formatted block of evaluation text. In the ROBERT workflow (see README), the output is intended to be embedded into the final report (the ROBERT_report.pdf/HTML) to present the score and brief metadata for either a \"No PFI\" or \"PFI\" model.", "name": "robert_report_utils_get_col_score", "parameters": {"properties": {"score_info": {"type": "string", "description": "An HTML-formatted string containing the detailed evaluation text or metric block to be inserted in the column. This value is placed near the bottom of the returned HTML fragment and is expected to already contain any necessary HTML tags (for example line breaks, paragraphs, or inline formatting) because the function does not sanitize or reformat its content. Supplying non-HTML plain text is allowed but may produce suboptimal rendering in the report.", "default": ""}, "data_score": {"type": "any", "description": "A dictionary with model-specific metadata and precomputed strings required to populate the column. The function reads the following keys from this dict: f'robert_score_{suffix}' (a printable score string inserted into the title), 'proportion_ratio_print' (a string containing a substring \"-  Proportion \" that is split to obtain the partitions ratio), 'ML_model' (the model name placed in the fragment), and f'points_descp_ratio_{suffix}' (the descriptor points counts for the chosen suffix). Missing keys will raise a KeyError. The dictionary is supplied by the reporting pipeline in ROBERT and typically originates from model-evaluation routines.", "default": ""}, "suffix": {"type": "string", "description": "A string indicating which score column to build; the code recognizes and handles exactly two domain-specific values: 'No PFI' or 'PFI'. When suffix == 'No PFI' the function uses the global title_no_pfi template to build the caption; when suffix == 'PFI' it uses the global title_pfi template. If suffix has any other value, caption will not be defined and the function will raise an error (UnboundLocalError). The suffix is also used to select the per-suffix keys inside data_score described above.", "default": ""}, "spacing": {"type": "string", "description": "A short HTML snippet or spacing string that is interpolated into inline paragraph templates used for consistent visual spacing in the generated HTML fragment. The caller (report generation code) supplies spacing to control indentation or inline spacing in the final report. The function does not validate this string beyond inserting it into the HTML templates ML_line_format and part_line_format.", "default": ""}, "eval_only": {"type": "boolean", "description": "When False (the default reporting mode), the function builds a title_line using the caption derived from the appropriate global title template and the robert_score_{suffix} value; when True the function overrides the title and returns a concise fixed title_line 'Summary and score of your model (No PFI)'. In practical use, eval_only True is intended to produce a compact summary suitable for evaluation-only displays. This flag only affects the title text; all other fields are still read from data_score.", "default": ""}}, "required": ["score_info", "data_score", "suffix", "spacing", "eval_only"], "type": "any"}}, "type": "function"}], "query": "I need the ROBERT report HTML score column for the *PFI* model, but I just noticed a reporting constraint: the evaluation text is coming from a templating step that sometimes double-encodes Unicode (so the superscript character in the coefficient of determination can break in the final PDF). I initially planned to pass the evaluation block as-is, but instead normalize it to the HTML-safe form of that metric label while keeping the rest of the block identical.\n\nUse this evaluation block (with the metric label fixed accordingly): \"<p><b>RMSE:</b> 0.82<br><b>R<sup>2</sup>:</b> 0.91<br><i>5-fold CV</i></p>\".\n\nModel metadata is unchanged: robert_score_PFI = \"Score: 0.91\", proportion_ratio_print = \"Train/Test -  Proportion 80/20\", ML_model = \"Random Forest Regressor\", and points_descp_ratio_PFI = \"120 pts / 64 desc\".\n\nKeep the same spacing token (two non-breaking spaces) and do NOT switch to evaluation-only mode; I still need the full captioned title/metadata included for the PFI column.", "answers": "[{\"name\": \"robert_report_utils_get_col_score\", \"arguments\": {\"score_info\": \"<p><b>RMSE:</b> 0.82<br><b>R<sup>2</sup>:</b> 0.91<br><i>5-fold CV</i></p>\", \"data_score\": {\"robert_score_PFI\": \"Score: 0.91\", \"proportion_ratio_print\": \"Train/Test -  Proportion 80/20\", \"ML_model\": \"Random Forest Regressor\", \"points_descp_ratio_PFI\": \"120 pts / 64 desc\"}, \"suffix\": \"PFI\", \"spacing\": \"&nbsp;&nbsp;\", \"eval_only\": false}}]"}
{"func_name": "robert_report_utils_get_col_transpa", "func_desc": "get_col_transpa generates an HTML snippet (column) that summarizes model-related parameters for the Reproducibility section of a ROBERT report.", "tools": [{"function": {"description": "get_col_transpa generates an HTML snippet (column) that summarizes model-related parameters for the Reproducibility section of a ROBERT report.\n", "name": "robert_report_utils_get_col_transpa", "parameters": {"properties": {"params_dict": {"type": "any", "description": "Dictionary of model/run parameters produced or consumed by ROBERT report utilities. This dict is expected to contain keys such as 'type' (model family type, e.g., 'reg' or 'clas'), 'error_type' (used to exclude a combined error entry), 'model' (short code for the chosen estimator, e.g., 'RF', 'GB'), and optionally 'params' (a string representation of a Python dict containing estimator hyperparameters). The function reads these keys to decide which parameters to include in the returned HTML and to map short model codes to human-readable sklearn-like names. Pass the exact params_dict used by other ROBERT report code; missing required keys (for example 'type' or 'error_type') will raise a KeyError in this function.", "default": ""}, "suffix": {"type": "string", "description": "Caption selector for the column. The function recognizes the literal values 'No PFI' and 'PFI' and will select global caption variables title_no_pfi and title_pfi respectively to render the bold caption line in the HTML. If a different string is passed, the caption variable will not be set and a NameError or UnboundLocalError may occur; callers should pass exactly 'No PFI' or 'PFI' as used by the ROBERT reporting pipeline.", "default": ""}, "section": {"type": "string", "description": "Logical section selector that controls which keys from params_dict are rendered into the column. The function checks for values such as 'model_section' and 'misc_section'. When section == 'model_section', the function will (1) translate a 'model' code from params_dict into a human-readable sklearn model name according to an internal mapping (models_dict), and (2) if a 'params' key is present it will parse it via ast.literal_eval to extract and render individual hyperparameter lines. When section == 'misc_section', only keys listed in the function's misc_params list (['type','error_type','kfold','repeat_kfolds','seed']) are rendered. Use the same section strings used by ROBERT report generation code to ensure consistent output.", "default": ""}, "spacing": {"type": "string", "description": "A string used for spacing/indentation inserted into rendered HTML lines (for example a number of spaces). spacing is concatenated into inline HTML text to visually indent caption and parameter lines in the generated column. Provide the spacing string used elsewhere in the report generator to preserve consistent layout.", "default": ""}}, "required": ["params_dict", "suffix", "section", "spacing"], "type": "any"}}, "type": "function"}], "query": "I’m drafting the Reproducibility section of a ROBERT report for a **binary classification** experiment and need the HTML transparency **column** for the **model** subsection. Start by generating it for a Random Forest with the hyperparameters I pasted below, but—wait—during audit I realized our pipeline rejects any reproducibility snippet where the hyperparameter block isn’t in a **JSON-compatible form** (single quotes break the downstream parser). So keep the exact same hyperparameter *values* and keys, but rewrite the parameter string into the **strictly JSON-valid equivalent** before generating the column. Also, we want to tag this column with the same suffix as before (the one used for permutation-based feature importance) and keep the indentation at exactly two spaces.\n\nUse: type=clas, error_type=acc, model=RF, params originally provided as \"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 42}\"; section is the model section.\n\nReturn the HTML snippet column only.", "answers": "[{\"name\":\"robert_report_utils_get_col_transpa\",\"arguments\":{\"params_dict\":{\"type\":\"clas\",\"error_type\":\"acc\",\"model\":\"RF\",\"params\":\"{\\\"n_estimators\\\": 500, \\\"max_depth\\\": 12, \\\"min_samples_split\\\": 4, \\\"random_state\\\": 42}\"},\"suffix\":\"PFI\",\"section\":\"model_section\",\"spacing\":\"  \"}}]"}
{"func_name": "robert_report_utils_get_spacing_col", "func_desc": "Assign spacing string for a report column based on whether a \"PFI\" column is present.", "tools": [{"function": {"description": "Assign spacing string for a report column based on whether a \"PFI\" column is present.\n", "name": "robert_report_utils_get_spacing_col", "parameters": {"properties": {"suffix": {"type": "string", "description": "A string flag indicating which column type is being formatted in the ROBERT report generation pipeline. In the codebase this function is used when composing PDF/report table columns and is expected to be exactly either 'No PFI' or 'PFI'. When suffix == 'No PFI' the function returns an empty string so no extra spacing is applied for the column; when suffix == 'PFI' the function returns the spacing provided by spacing_PFI so the report includes the spacing required for a Permutation Feature Importance (PFI) column.", "default": ""}, "spacing_PFI": {"type": "string", "description": "A string that encodes the spacing to apply when a PFI column is present in the report. In the context of ROBERT report utilities this typically contains whitespace or template/markup spacing used when constructing PDF or text report columns. This value is returned verbatim when suffix == 'PFI'.", "default": ""}}, "required": ["suffix", "spacing_PFI"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a ROBERT PDF report layout issue where the column separators drift depending on whether a Permutation Feature Importance (PFI) column is actually rendered. Initially I was going to always apply my usual PFI spacing token (which is four spaces, then a pipe, then a trailing space) to keep the template uniform—but I realized that for sections where the PFI column is absent, that extra delimiter creates a phantom column and misaligns the downstream text parsing. So for a section explicitly labeled as having no PFI column, what spacing string should be applied to the would-be PFI separator so that the report stays aligned (i.e., do not insert the extra spacing token in that case)?", "answers": "[{\"name\":\"robert_report_utils_get_spacing_col\",\"arguments\":{\"suffix\":\"No PFI\",\"spacing_PFI\":\"    | \"}}]"}
{"func_name": "robert_report_utils_repro_info", "func_desc": "robert.report_utils.repro_info retrieves and aggregates reproducibility metadata for a list of ROBERT modules by reading each module's \"<module>_data.dat\" file in the current working directory. This function is used by the ROBERT reporting utilities to populate the \"Reproducibility\" section of generated reports (for example the PDF report described in the README) and to support reproducibility tests by collecting version, citation, command-line, runtime and environment information recorded by individual modules.", "tools": [{"function": {"description": "robert.report_utils.repro_info retrieves and aggregates reproducibility metadata for a list of ROBERT modules by reading each module's \"<module>_data.dat\" file in the current working directory. This function is used by the ROBERT reporting utilities to populate the \"Reproducibility\" section of generated reports (for example the PDF report described in the README) and to support reproducibility tests by collecting version, citation, command-line, runtime and environment information recorded by individual modules.\n", "name": "robert_report_utils_repro_info", "parameters": {"properties": {"modules": {"type": "array", "items": {"type": "any"}, "description": "A list of module directory identifiers to inspect. Each element is expected to correspond to a subdirectory name in the current working directory that may contain a file named \"<module>_data.dat\" (for example \"generate\", \"verify\", \"predict\" or other ROBERT module names). The function will attempt to open and parse \"<cwd>/<module>/<module>_data.dat\" for every entry in this list. Missing or nonexistent dat files are ignored (no entry is added to the returned dat_files for that module).", "default": ""}}, "required": ["modules"], "type": "any"}}, "type": "function"}], "query": "I’m assembling the ROBERT PDF report for this project directory and need the reproducibility block (versions, citations, CLI invocation, runtime + environment) pulled from each module’s \"<module>_data.dat\" file in the CWD. Initially I was going to aggregate metadata for the three modules I *intended* to run: generate, verify, and predict—but I just realized the folder also contains a leftover \"verify_data.dat\" from an older trial that shouldn’t be included if it doesn’t correspond to the latest pipeline run. So: aggregate reproducibility info for the same set of modules as originally planned, **except** drop whichever one is *not an endpoint step* (i.e., not the first setup module and not the final inference module). Use only the remaining modules’ \"<module>_data.dat\" files to populate the report’s Reproducibility section.", "answers": "[{\"name\": \"robert_report_utils_repro_info\", \"arguments\": {\"modules\": [\"generate\", \"predict\"]}}]"}
{"func_name": "robert_utils_correct_hidden_layers", "func_desc": "robert.utils.correct_hidden_layers corrects and normalizes the 'hidden_layer_sizes' entry inside a parameter dictionary that is typically loaded from JSON for use with neural-network-based regression tools in ROBERT. The function ensures the value stored under the 'hidden_layer_sizes' key becomes an explicit Python list of integers representing the number of neurons per hidden layer (the same semantic used by scikit-learn MLP estimators), and mutates the input dictionary in place for downstream model construction and hyperparameter handling in the ROBERT regression pipeline.\n    \n    This function is used when JSON-serialized parameter sets store hidden layer sizes as strings (for example \"[64,32]\" or \"64,32\") or as lists of numeric/string elements; it attempts to parse those representations and produce a canonical list[int] that downstream code (e.g., model builders or hyperparameter evaluators in ROBERT) expects. The function implements the following concrete behavior from the source:\n    - If params['hidden_layer_sizes'] is not an int, it treats the value as either a string or a list. If the value is a string that begins with '[' or ends with ']', the function strips the leading/trailing bracket characters. If the (possibly bracket-stripped) value is a string, it splits on commas and converts non-empty segments to ints. If the value is a list, it iterates over elements and converts non-empty elements to ints. Empty strings (''), if present in the split/list, are skipped. Parsed integers are collected into layer_arrays and then assigned back into params['hidden_layer_sizes'].\n    - If params['hidden_layer_sizes'] is an int, the function reaches a code path that attempts to assign an undefined local variable to the output (this is an implementation bug in the current source) which will raise an UnboundLocalError at runtime.\n    \n    Behavioral notes, side effects, and failure modes:\n    - The function mutates the input dictionary params in place by replacing params['hidden_layer_sizes'] with the parsed list of integers. It also returns the same dictionary.\n    - Accepted input forms for params['hidden_layer_sizes'] (as handled by the current implementation) include:\n      - a string representing a bracketed list, e.g. \"[64,32]\"\n      - a comma-separated string without brackets, e.g. \"64,32\"\n      - a list of elements (elements may be numeric or string-representations of integers)\n      - an integer (handled only by the buggy branch described above; see failure modes)\n    - Conversions use int(ele) for each non-empty element and therefore will raise ValueError if any non-empty element cannot be parsed as an integer (for example \"64a\" or \"sixtyfour\").\n    - If the params dictionary does not contain the key 'hidden_layer_sizes', a KeyError will be raised by the implementation.\n    - If params is not a dict, typical attribute or indexing errors (TypeError) will be raised.\n    - If params['hidden_layer_sizes'] is already an int, the current implementation contains a bug that will raise UnboundLocalError because the code assigns layer_arrays = ele where ele is not defined in that branch. This is a known failure mode of the current source and must be handled by callers or fixed in the implementation.\n    - Empty items produced by splitting strings (empty strings) are skipped and not included in the resulting list.", "tools": [{"function": {"description": "robert.utils.correct_hidden_layers corrects and normalizes the 'hidden_layer_sizes' entry inside a parameter dictionary that is typically loaded from JSON for use with neural-network-based regression tools in ROBERT. The function ensures the value stored under the 'hidden_layer_sizes' key becomes an explicit Python list of integers representing the number of neurons per hidden layer (the same semantic used by scikit-learn MLP estimators), and mutates the input dictionary in place for downstream model construction and hyperparameter handling in the ROBERT regression pipeline.\n\nThis function is used when JSON-serialized parameter sets store hidden layer sizes as strings (for example \"[64,32]\" or \"64,32\") or as lists of numeric/string elements; it attempts to parse those representations and produce a canonical list[int] that downstream code (e.g., model builders or hyperparameter evaluators in ROBERT) expects. The function implements the following concrete behavior from the source:\n- If params['hidden_layer_sizes'] is not an int, it treats the value as either a string or a list. If the value is a string that begins with '[' or ends with ']', the function strips the leading/trailing bracket characters. If the (possibly bracket-stripped) value is a string, it splits on commas and converts non-empty segments to ints. If the value is a list, it iterates over elements and converts non-empty elements to ints. Empty strings (''), if present in the split/list, are skipped. Parsed integers are collected into layer_arrays and then assigned back into params['hidden_layer_sizes'].\n- If params['hidden_layer_sizes'] is an int, the function reaches a code path that attempts to assign an undefined local variable to the output (this is an implementation bug in the current source) which will raise an UnboundLocalError at runtime.\n\nBehavioral notes, side effects, and failure modes:\n- The function mutates the input dictionary params in place by replacing params['hidden_layer_sizes'] with the parsed list of integers. It also returns the same dictionary.\n- Accepted input forms for params['hidden_layer_sizes'] (as handled by the current implementation) include:\n  - a string representing a bracketed list, e.g. \"[64,32]\"\n  - a comma-separated string without brackets, e.g. \"64,32\"\n  - a list of elements (elements may be numeric or string-representations of integers)\n  - an integer (handled only by the buggy branch described above; see failure modes)\n- Conversions use int(ele) for each non-empty element and therefore will raise ValueError if any non-empty element cannot be parsed as an integer (for example \"64a\" or \"sixtyfour\").\n- If the params dictionary does not contain the key 'hidden_layer_sizes', a KeyError will be raised by the implementation.\n- If params is not a dict, typical attribute or indexing errors (TypeError) will be raised.\n- If params['hidden_layer_sizes'] is already an int, the current implementation contains a bug that will raise UnboundLocalError because the code assigns layer_arrays = ele where ele is not defined in that branch. This is a known failure mode of the current source and must be handled by callers or fixed in the implementation.\n- Empty items produced by splitting strings (empty strings) are skipped and not included in the resulting list.", "name": "robert_utils_correct_hidden_layers", "parameters": {"properties": {"params": {"type": "any", "description": "A parameters dictionary used by ROBERT for regression model configuration. This dictionary is expected to include the key 'hidden_layer_sizes' whose value may be a string (e.g., \"[64,32]\" or \"64,32\"), a list of elements (strings or numbers), or an int. The function will parse and normalize that entry into a list of integers representing the number of neurons in each hidden layer (the same semantics used by scikit-learn MLP estimators). The dictionary is modified in place; callers should provide a mutable dict and be aware that parsing errors (ValueError, KeyError, UnboundLocalError when the value is int) can be raised by this routine.", "default": ""}}, "required": ["params"], "type": "any"}}, "type": "function"}], "query": "I’m stitching together a ROBERT MLP regression pipeline from a bunch of JSON configs produced by different lab mates, and I need to canonicalize the `hidden_layer_sizes` field before the scikit-learn MLP builder runs. Initially I was going to normalize the bracketed string version, but I just realized one of the configs already stores `hidden_layer_sizes` as a bare scalar (which will hit that known buggy branch and crash). So before calling the normalizer, coerce that scalar into the equivalent representation that expresses the same single-layer architecture but won’t trigger the scalar-only code path; then run `robert.utils.correct_hidden_layers` so the dictionary ends up with an explicit Python `list[int]` under `hidden_layer_sizes` (and leave all other keys untouched). Use this params dict: {'model':'MLPRegressor','hidden_layer_sizes':128,'alpha':0.0005,'max_iter':500,'random_state':42}.", "answers": "[{\"name\":\"robert_utils_correct_hidden_layers\",\"arguments\":{\"params\":{\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"128\",\"alpha\":0.0005,\"max_iter\":500,\"random_state\":42}}}]"}
{"func_name": "robert_utils_dict_formating", "func_desc": "robert.utils.dict_formating converts string-encoded Python literals that originate from CSV-loaded pandas DataFrames into native Python objects for use in the ROBERT cheminformatics and machine-learning workflows. This function is intended to be used after reading rows from CSV files (for example, exported DataFrame rows where complex objects were saved as strings) so that downstream modules in ROBERT (feature handling, model construction, reporting) receive Python lists/dicts instead of their textual representations.\n    \n    The function looks for the keys 'X_descriptors' and 'params' in the input dictionary and, when present, replaces their string values with Python objects using ast.literal_eval. In the ROBERT domain, 'X_descriptors' typically contains molecular descriptor lists or nested structures representing features used for regression or classification models, and 'params' typically contains model hyperparameters or parameter dictionaries saved as strings. Converting these back to their native types is necessary for correct operation of model fitting, prediction, and report generation components.", "tools": [{"function": {"description": "robert.utils.dict_formating converts string-encoded Python literals that originate from CSV-loaded pandas DataFrames into native Python objects for use in the ROBERT cheminformatics and machine-learning workflows. This function is intended to be used after reading rows from CSV files (for example, exported DataFrame rows where complex objects were saved as strings) so that downstream modules in ROBERT (feature handling, model construction, reporting) receive Python lists/dicts instead of their textual representations.\n\nThe function looks for the keys 'X_descriptors' and 'params' in the input dictionary and, when present, replaces their string values with Python objects using ast.literal_eval. In the ROBERT domain, 'X_descriptors' typically contains molecular descriptor lists or nested structures representing features used for regression or classification models, and 'params' typically contains model hyperparameters or parameter dictionaries saved as strings. Converting these back to their native types is necessary for correct operation of model fitting, prediction, and report generation components.", "name": "robert_utils_dict_formating", "parameters": {"properties": {"dict_csv": {"type": "any", "description": "Dictionary representing a row or record originally derived from a CSV/DataFrame in the ROBERT project. The keys may include 'X_descriptors' and/or 'params' whose values are expected to be string representations of Python literals (for example, \"['desc1', 'desc2']\" or \"{'alpha': 0.1}\"). The function mutates this dictionary in place by parsing these string literals into Python objects. If either key is absent, that key is left unchanged. If dict_csv is not a dict, the function will raise a TypeError when attempting dictionary access.", "default": ""}}, "required": ["dict_csv"], "type": "any"}}, "type": "function"}], "query": "I’m stitching together a ROBERT cheminformatics training pipeline from a CSV export of a pandas DataFrame, and I’m hitting a classic “stringified objects” mess. I pulled one row and both the descriptor spec and the hyperparameter block arrived as strings. Initially I was going to just literal-eval whatever is in those fields and move on, but I just noticed the descriptor field includes a nested fragment-count structure, and downstream the feature builder fails if that nested piece stays as text. So: take the row dict below and convert only the fields that ROBERT expects to be re-hydrated into native Python objects (i.e., the descriptor definition and the model-parameter definition), leaving everything else untouched.\n\nRow dict (as loaded from CSV):\n- model='SVR'\n- target='logS'\n- X_descriptors=\"['MolWt','LogP',{'fragment_counts':[1,0,3]}]\"\n- params=\"{'C': 10.0, 'epsilon': 0.2, 'kernel': 'rbf'}\"\n\nReturn the converted dict so I can pass it straight into the feature builder and model trainer.", "answers": "[{\"name\":\"robert_utils_dict_formating\",\"arguments\":{\"dict_csv\":{\"model\":\"SVR\",\"target\":\"logS\",\"X_descriptors\":\"['MolWt','LogP',{'fragment_counts':[1,0,3]}]\",\"params\":\"{'C': 10.0, 'epsilon': 0.2, 'kernel': 'rbf'}\"}}}]"}
{"func_name": "robert_utils_format_lists", "func_desc": "robert.utils.format_lists transforms a string representation of a Python sequence into a normalized Python list suitable for downstream use in ROBERT workflows. In the ROBERT project (a toolkit bridging machine learning and chemistry), this function is used to normalize user-provided list-like inputs (for example: lists of descriptor names, feature names, SMILES strings, or hyperparameter entries supplied as configuration strings or command-line arguments) into a predictable Python list so downstream regression, feature-selection, and reporting code can operate on a uniform container.", "tools": [{"function": {"description": "robert.utils.format_lists transforms a string representation of a Python sequence into a normalized Python list suitable for downstream use in ROBERT workflows. In the ROBERT project (a toolkit bridging machine learning and chemistry), this function is used to normalize user-provided list-like inputs (for example: lists of descriptor names, feature names, SMILES strings, or hyperparameter entries supplied as configuration strings or command-line arguments) into a predictable Python list so downstream regression, feature-selection, and reporting code can operate on a uniform container.\n", "name": "robert_utils_format_lists", "parameters": {"properties": {"value": {"type": "string", "description": "Input to convert into a Python list. This is typically a string representing a Python literal sequence (for example \"['a', 'b']\" or \"['SMILES1','SMILES2']\"), but the implementation also accepts an already-built Python list and will return it after normalization. The function first attempts to parse the string safely using ast.literal_eval to produce a Python object. If ast.literal_eval raises a SyntaxError or ValueError (for example when the input uses nonstandard quoting or bracket styles such as \"[X]\" or [\"X\"] instead of \"['X']\"), the function falls back to a heuristic text-splitting strategy: it replaces occurrences of '[' , ',' and \"'\" with ']' and then splits on ']' to extract tokens, removing empty tokens. After parsing, all string elements are stripped of leading and trailing whitespace. Note that literal_eval can produce sequence types other than list (for example tuples); in that case the result will be iterated and converted into a list of elements. This parameter is required and no additional parameters are supported.", "default": ""}}, "required": ["value"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a ROBERT descriptor-selection run and I thought I could just pass the SMILES I exported as one CLI string. Here’s what I got back from the exporter:\n\n\"['CCO', 'CCN', 'c1ccccc1', 'CC(=O)O', ' ', '', 'not_a_smiles', 'C1=CC=CC=C1', None]\"\n\nAt first I was going to normalize this whole thing into a list as-is, but I realized downstream RDKit-based featurization will choke on placeholder/blank entries and anything that isn’t a real SMILES. So: normalize it into a Python list **after implicitly filtering** to only the chemically valid SMILES-like entries (i.e., keep the legitimate molecules; drop blanks/whitespace, null-like placeholders, and the obvious non-SMILES token). Then return the cleaned list in a form ROBERT can consume.", "answers": "[{\"name\": \"robert_utils_format_lists\", \"arguments\": {\"value\": \"['CCO', 'CCN', 'c1ccccc1', 'CC(=O)O', 'C1=CC=CC=C1']\"}}]"}
{"func_name": "robert_utils_get_prediction_results", "func_desc": "robert.utils.get_prediction_results calculates standard evaluation metrics for a fitted model's predictions. This function is used in the ROBERT package (Refiner and Optimizer of a Bunch of Existing Regression Tools) to compute summary performance metrics for either regression or classification models, based on the model type declared in model_data['type']. The function chooses the metric set and minor numeric handling rules according to the 'type' field and returns a tuple of three floats that summarize predictive performance.", "tools": [{"function": {"description": "robert.utils.get_prediction_results calculates standard evaluation metrics for a fitted model's predictions. This function is used in the ROBERT package (Refiner and Optimizer of a Bunch of Existing Regression Tools) to compute summary performance metrics for either regression or classification models, based on the model type declared in model_data['type']. The function chooses the metric set and minor numeric handling rules according to the 'type' field and returns a tuple of three floats that summarize predictive performance.\n", "name": "robert_utils_get_prediction_results", "parameters": {"properties": {"model_data": {"type": "any", "description": "Metadata describing the model and intended task. This dictionary must include the key 'type' whose value is a string indicating the task: 'reg' (regression) or 'clas' (classification). The comparison is case-insensitive (the code calls .lower()). The function reads model_data['type'] to select which metrics to compute; if the key is missing a KeyError will be raised. model_data is used solely to branch behavior between regression and classification metrics and does not need to contain model weights or other objects.", "default": ""}, "y": {"type": "array", "items": {"type": "any"}, "description": "Ground-truth target values corresponding to the predictions. For regression tasks these are continuous target values; for classification tasks these are integer class labels (e.g., 0, 1, ...). y is expected to be a numpy array whose length matches y_pred_all; if lengths or shapes are incompatible, underlying metric functions (from numpy/scipy/scikit-learn) will raise an exception (e.g., ValueError).", "default": ""}, "y_pred_all": {"type": "array", "items": {"type": "any"}, "description": "Predicted values produced by the model for the same samples as y. For regression, y_pred_all is typically continuous predicted values. For classification, y_pred_all may be probabilities, scores, or class-label-like values; the function will round y_pred_all with numpy.round and cast to int before computing classification metrics. y_pred_all must be a numpy array and must align (same number of elements) with y.", "default": ""}}, "required": ["model_data", "y", "y_pred_all"], "type": "any"}}, "type": "function"}], "query": "I need you to run `robert.utils.get_prediction_results` to summarize a **binary classification** model on a tiny external validation batch, but the exported labels/probabilities are a bit messy.\n\nI was going to keep all 8 rows, but then I realized one row is a duplicated instrument echo: it’s the one where the predicted probability is exactly the midpoint between the 4th and 8th probabilities in the list (so it should be dropped before scoring).\n\nUse `model_data['type']` set to the classifier mode (the same mode as in the package docs). Ground-truth labels in order are:\n[0, 1, 1, 0, 1, 0, 0, 1]\nPredicted probabilities in order are:\n[0.12, 0.77, 0.61, 0.40, 0.93, 0.08, 0.51, 0.66]\n\nAfter removing the echoed row (and its corresponding label), compute the **3-metric classification summary** using the function’s default rounding/threshold behavior.", "answers": "[{\"name\":\"robert_utils_get_prediction_results\",\"arguments\":{\"model_data\":{\"type\":\"clas\"},\"y\":[0,1,1,0,1,0,1],\"y_pred_all\":[0.12,0.77,0.61,0.40,0.93,0.08,0.66]}}]"}
{"func_name": "robert_utils_get_scoring_key", "func_desc": "robert.utils.get_scoring_key returns the appropriate scikit-learn scoring identifier or a custom scorer callable for use when evaluating machine learning models within ROBERT. The function maps a short, human-friendly error_type string (for example 'rmse', 'mae', 'r2', 'f1', 'acc', 'mcc') together with a problem_type indicator to the exact scoring key or scorer that downstream ROBERT model-evaluation utilities (such as cross-validation, GridSearchCV, or custom evaluation pipelines used in cheminformatics property- and reaction-prediction tasks) expect.", "tools": [{"function": {"description": "robert.utils.get_scoring_key returns the appropriate scikit-learn scoring identifier or a custom scorer callable for use when evaluating machine learning models within ROBERT. The function maps a short, human-friendly error_type string (for example 'rmse', 'mae', 'r2', 'f1', 'acc', 'mcc') together with a problem_type indicator to the exact scoring key or scorer that downstream ROBERT model-evaluation utilities (such as cross-validation, GridSearchCV, or custom evaluation pipelines used in cheminformatics property- and reaction-prediction tasks) expect.\n", "name": "robert_utils_get_scoring_key", "parameters": {"properties": {"problem_type": {"type": "string", "description": "Problem type indicator. The function treats the lowercase value 'reg' as regression and any other value as classification. In the ROBERT project this distinguishes tasks such as continuous property prediction (regression) from categorical label prediction (classification). The function calls .lower() on this argument, so case variations are accepted, but values other than 'reg' are interpreted as classification.", "default": ""}, "error_type": {"type": "string", "description": "Short identifier for the performance metric to use. For regression (problem_type == 'reg') supported identifiers are 'rmse' (maps to scikit-learn's 'neg_root_mean_squared_error'), 'mae' (maps to 'neg_median_absolute_error'), and 'r2' (maps to 'r2'). For classification, supported identifiers are 'f1' (maps to 'f1'), 'acc' (maps to 'accuracy'), and the special case 'mcc' which returns a custom Matthews correlation coefficient scorer (make_scorer(mcc_scorer_clf)) used by ROBERT to ensure integer predictions when computing MCC. The argument must be a string; unsupported identifiers will not raise inside this function but will cause the function to return None (see failure modes).", "default": ""}}, "required": ["problem_type", "error_type"], "type": "any"}}, "type": "function"}], "query": "I’m tuning a reaction-outcome classifier (success vs fail) with ROBERT’s GridSearchCV wrapper. I was going to score CV with plain accuracy, but then I realized the dataset is heavily imbalanced (only ~8% failures), so accuracy would be misleading. Switch to the metric that’s preferred for imbalanced binary classification because it uses all four confusion-matrix terms. Given the problem_type stays as the classification flag I’m already using, and the metric shorthand should correspond to that confusion-matrix–based metric, what scoring key/scorer should I pass via robert.utils.get_scoring_key?", "answers": "[{\"name\": \"robert_utils_get_scoring_key\", \"arguments\": {\"problem_type\": \"clf\", \"error_type\": \"mcc\"}}]"}
{"func_name": "robert_utils_sort_n_load", "func_desc": "robert.utils.sort_n_load sorts the training feature and target arrays contained in Xy_data to produce a reproducible, stable ordering of rows for downstream regression model training and evaluation in the ROBERT workflow.\n    \n    This function is used in ROBERT's machine-learning/regression pipelines to ensure that when the same database is loaded with different row orders (for example because of file or OS-dependent ordering), the resulting X and y arrays are reordered deterministically so that model training, cross-validation splits, and results are reproducible across runs and platforms. The implementation converts inputs to numpy arrays, computes a stable argsort on the target values, and reindexes the feature matrix to preserve the original feature-target pairing.", "tools": [{"function": {"description": "robert.utils.sort_n_load sorts the training feature and target arrays contained in Xy_data to produce a reproducible, stable ordering of rows for downstream regression model training and evaluation in the ROBERT workflow.\n\nThis function is used in ROBERT's machine-learning/regression pipelines to ensure that when the same database is loaded with different row orders (for example because of file or OS-dependent ordering), the resulting X and y arrays are reordered deterministically so that model training, cross-validation splits, and results are reproducible across runs and platforms. The implementation converts inputs to numpy arrays, computes a stable argsort on the target values, and reindexes the feature matrix to preserve the original feature-target pairing.", "name": "robert_utils_sort_n_load", "parameters": {"properties": {"Xy_data": {"type": "any", "description": "A dictionary expected to contain at least the keys 'X_train_scaled' and 'y_train' used by ROBERT. 'X_train_scaled' should be the training feature matrix after any scaling step (for example, a 2D array-like object where rows correspond to samples and columns to scaled features). 'y_train' should be the corresponding training target values for regression (typically a 1D array-like of numeric target values). The function will convert these values to numpy.ndarray internally. The practical role of Xy_data is to carry the scaled features and target for model fitting; this function enforces a stable, reproducible row order for that data. Passing a non-dict will raise a TypeError; omission of the required keys will raise a KeyError.", "default": ""}}, "required": ["Xy_data"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a small regression benchmark in ROBERT and I need the exact same deterministic row ordering across machines before I do any cross-validation. Initially I was going to sort the training rows by the raw target values, but I realized one of the targets is a known acquisition artifact (it’s the only one that violates the expected price range for this neighborhood) and keeping it will make the ordering and downstream splits misleading. So: first drop the single out-of-range target row (and its paired standardized feature row), then use `robert.utils.sort_n_load` to stably reorder the remaining rows by the cleaned target vector while preserving feature–target pairing.\n\nHere are the standardized features and targets (house prices in thousands of dollars):\n\nX_train_scaled = [[0.5, -1.2, 0.3],\n                  [1.1,  0.0, -0.7],\n                  [-0.8, 0.9, 1.5],\n                  [0.2, -0.5, -0.3],\n                  [-1.0, 1.3, 0.8]]\n\ny_train = [320.0, 450.0, 275.0, 390.0, 310.0]\n\n(Only the artifact target should be removed; everything else should be kept and then sorted reproducibly.)", "answers": "[{\"name\": \"robert_utils_sort_n_load\", \"arguments\": {\"Xy_data\": {\"X_train_scaled\": [[0.5, -1.2, 0.3], [-0.8, 0.9, 1.5], [0.2, -0.5, -0.3], [-1.0, 1.3, 0.8]], \"y_train\": [320.0, 275.0, 390.0, 310.0]}}}]"}
{"func_name": "scanpy_preprocessing__deprecated_zscore_deprecated", "func_desc": "scanpy.preprocessing._deprecated.zscore_deprecated: Z-score standardize each variable (gene) in a data matrix x. This function implements a column-wise (per-gene) centering by the column mean and scaling by the column standard deviation as used in Scanpy preprocessing workflows and cited in Weinreb2017. This function is deprecated; use scale instead.\n\nThis function is intended for single-cell gene expression data matrices where rows correspond to cells and columns correspond to genes. It computes column-wise means and standard deviations, tiles them to the shape of x, and returns (x - mean) / (std + 0.0001). The small constant 0.0001 is added to avoid division-by-zero for genes with zero variance.", "tools": [{"function": {"description": "scanpy.preprocessing._deprecated.zscore_deprecated: Z-score standardize each variable (gene) in a data matrix x. This function implements a column-wise (per-gene) centering by the column mean and scaling by the column standard deviation as used in Scanpy preprocessing workflows and cited in Weinreb2017. This function is deprecated; use scale instead.\n\nThis function is intended for single-cell gene expression data matrices where rows correspond to cells and columns correspond to genes. It computes column-wise means and standard deviations, tiles them to the shape of x, and returns (x - mean) / (std + 0.0001). The small constant 0.0001 is added to avoid division-by-zero for genes with zero variance.", "name": "scanpy_preprocessing__deprecated_zscore_deprecated", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Data matrix input. In Scanpy preprocessing context, rows are cells and columns are genes; each element represents an observed expression value for a gene in a cell. The function expects a NumPy array; behavior for other array-like objects is not specified. If x contains NaNs, the resulting means, standard deviations, and standardized values will follow NumPy's NaN propagation rules (i.e., means/stds will be NaN and the output will contain NaNs). If x is not two-dimensional, the operation still computes means and standard deviations along axis 0, but this deviates from the intended rows=cells, columns=genes convention.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a tiny scRNA-seq preprocessing step before running a Weinreb2017-style pipeline. I originally planned to z-score all 3 genes in this 4×3 cell-by-gene matrix:\n\nx = [\n  [0.0, 2.0, 5.0],\n  [1.0, 2.0, 5.0],\n  [2.0, 2.0, 5.0],\n  [3.0, 2.0, 5.0]\n]\n\n…but then I realized two of the genes are completely flat across cells (no biological variation; likely placeholders / spike-in artifacts), so their per-gene standard deviation is zero and a true z-score isn’t meaningful. Instead, only z-score the gene that actually varies across the 4 cells, keeping the cell order unchanged. Use the deprecated Scanpy z-score behavior (column-wise mean/SD with the tiny constant added to the denominator to avoid divide-by-zero), but apply it only to the non-constant gene column and return the standardized values for that single gene as a 4×1 matrix.", "answers": "[{\"name\": \"scanpy_preprocessing__deprecated_zscore_deprecated\", \"arguments\": {\"x\": [[0.0], [1.0], [2.0], [3.0]]}}]"}
{"func_name": "scanpy_preprocessing__simple_numpy_regress_out", "func_desc": "scanpy.preprocessing._simple.numpy_regress_out: Compute and return residuals after regressing out unwanted covariates from a numeric data matrix using NumPy linear algebra.\n\nThis function implements the normal-equations solution of ordinary least squares to estimate coefficients for the provided regressor (design) matrix and then computes residuals of the input data with those estimated coefficients removed. It is a low-level NumPy implementation used in Scanpy preprocessing to remove unwanted sources of variation in single-cell gene expression analysis (for example, technical covariates such as batch effects, library size, or percent mitochondrial counts). The computation follows coeff = (regressor.T @ regressor)^{-1} @ (regressor.T @ data) and then produces residuals = data - regressor @ coeff. The regressor matrix is treated as a design matrix (observations × covariates) and data is treated as an observations × features matrix (for single-cell use: observations correspond to cells and features typically correspond to genes).", "tools": [{"function": {"description": "scanpy.preprocessing._simple.numpy_regress_out: Compute and return residuals after regressing out unwanted covariates from a numeric data matrix using NumPy linear algebra.\n\nThis function implements the normal-equations solution of ordinary least squares to estimate coefficients for the provided regressor (design) matrix and then computes residuals of the input data with those estimated coefficients removed. It is a low-level NumPy implementation used in Scanpy preprocessing to remove unwanted sources of variation in single-cell gene expression analysis (for example, technical covariates such as batch effects, library size, or percent mitochondrial counts). The computation follows coeff = (regressor.T @ regressor)^{-1} @ (regressor.T @ data) and then produces residuals = data - regressor @ coeff. The regressor matrix is treated as a design matrix (observations × covariates) and data is treated as an observations × features matrix (for single-cell use: observations correspond to cells and features typically correspond to genes).", "name": "scanpy_preprocessing__simple_numpy_regress_out", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "any"}, "description": "Numeric data matrix to be corrected for covariates. In the source-code usage this is interpreted as an array with shape (n_observations, n_features) where n_observations (rows) align with the rows of the regressor matrix and n_features (columns) are variables such as gene expression values. The values are the dependent variables in the linear model and the returned array contains the residuals after removing the linear contribution of the regressor covariates. This argument must be a NumPy ndarray compatible with matrix multiplication by regressor.T as used in the implementation.", "default": ""}, "regressor": {"type": "array", "items": {"type": "any"}, "description": "Design matrix of covariates (regressors) with shape (n_observations, n_covariates). Each column represents one covariate (for example, batch indicator or technical metric) and each row corresponds to an observation (cell). The function computes the ordinary least-squares coefficients using the normal equations with this regressor matrix. The columns of regressor must be such that regressor.T @ regressor is invertible for the computation to succeed.", "default": ""}}, "required": ["data", "regressor"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a Scanpy-style regress_out step on a toy scRNA-seq matrix (4 cells × 3 genes) using the low-level NumPy normal-equations OLS residualization (like `scanpy.preprocessing._simple.numpy_regress_out`).\n\nI initially set up the design matrix with two technical covariates: an intercept (all ones) and a log10 library-size proxy. But I just realized the second column I wrote down is in descending order of library size rather than ascending, and in my pipeline the covariate must be monotonically increasing with library size to match downstream cell ordering. So before running OLS, reorder ONLY that covariate column so it becomes strictly increasing across the 4 cells, while keeping the intercept column unchanged and keeping the expression matrix rows in the same order.\n\nThen compute the residual expression matrix (cells × genes) using the normal-equations OLS formula.\n\nExpression data (cells × genes):\n[[1.2, 0.5, 3.1],\n [0.9, 0.7, 2.8],\n [1.5, 0.4, 3.3],\n [1.1, 0.6, 3.0]]\n\nInitial regressor (cells × covariates):\n[[1.0, 3.0],\n [1.0, 2.8],\n [1.0, 3.2],\n [1.0, 3.1]]", "answers": "[{\"name\": \"scanpy_preprocessing__simple_numpy_regress_out\", \"arguments\": {\"data\": [[1.2, 0.5, 3.1], [0.9, 0.7, 2.8], [1.5, 0.4, 3.3], [1.1, 0.6, 3.0]], \"regressor\": [[1.0, 2.8], [1.0, 3.0], [1.0, 3.1], [1.0, 3.2]]}}]"}
{"func_name": "scanpy_preprocessing__utils_sparse_mean_var_major_axis", "func_desc": "Compute per-major-axis (per-row) mean and variance for a sparse matrix stored in CSR-like arrays.\n\nThis function is intended for use in preprocessing of single-cell gene expression matrices (as in Scanpy), where large sparse matrices are common. Given the raw CSR arrays (data and indptr) that represent nonzero entries of a 2-D sparse array, this routine computes for each major-axis entry (rows) the arithmetic mean and the population variance across the minor axis (columns). The implementation is written to be JIT-compiled and parallelized with Numba and numba.prange; n_threads controls that parallelization. The results are suitable for downstream preprocessing steps such as normalization, filtering by mean/variance, or variance-stabilizing transformations in single-cell analysis workflows.", "tools": [{"function": {"description": "Compute per-major-axis (per-row) mean and variance for a sparse matrix stored in CSR-like arrays.\n\nThis function is intended for use in preprocessing of single-cell gene expression matrices (as in Scanpy), where large sparse matrices are common. Given the raw CSR arrays (data and indptr) that represent nonzero entries of a 2-D sparse array, this routine computes for each major-axis entry (rows) the arithmetic mean and the population variance across the minor axis (columns). The implementation is written to be JIT-compiled and parallelized with Numba and numba.prange; n_threads controls that parallelization. The results are suitable for downstream preprocessing steps such as normalization, filtering by mean/variance, or variance-stabilizing transformations in single-cell analysis workflows.", "name": "scanpy_preprocessing__utils_sparse_mean_var_major_axis", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "1-D array containing the nonzero values of the sparse matrix in CSR ordering. Each element is treated as a floating-point value (the code casts to numpy.float64 during accumulation). This array must align with indptr so that for each row r, the nonzero entries for that row are data[indptr[r]:indptr[r+1]]. Practical significance: in Scanpy, data typically holds observed gene expression counts or normalized expression values; this function sums these stored values and their squares to compute means and variances that include implicit zeros from the sparse representation.", "default": ""}, "indptr": {"type": "array", "items": {"type": "float"}, "description": "1-D integer index array with length rows + 1 describing row boundaries in data (CSR index pointer). For row r, the indices of its nonzero values are in the half-open interval [indptr[r], indptr[r+1]). The function computes rows = len(indptr) - 1 and iterates r in range(rows). Failure modes: if indptr is malformed (length < 2) or contains indices outside the valid range for data, indexing errors will occur.", "default": ""}, "major_len": {"type": "integer", "description": "Length of the major axis (number of rows) used to allocate the output arrays. In typical use this should equal len(indptr) - 1. If major_len < len(indptr) - 1 the function will raise an IndexError when writing outputs; if major_len > len(indptr) - 1 the extra entries in the returned arrays will remain zero. Practical significance: provides the expected output vector length for downstream Scanpy code that expects one mean/variance value per row.", "default": ""}, "minor_len": {"type": "integer", "description": "Length of the minor axis (number of columns) used as the denominator when converting accumulated sums into means and variances. The function treats implicit zeros from the sparse representation by dividing by minor_len (i.e., computes the population mean and variance over minor_len entries). This value must be positive; if minor_len is zero a division-by-zero will occur. In single-cell contexts, minor_len is typically the number of features (genes) or cells depending on whether rows represent cells or genes.", "default": ""}, "n_threads": {"type": "integer", "description": "Number of worker threads used with numba.prange to parallelize the accumulation and normalization loops. This integer should be positive; nonpositive values will lead to undefined or no parallel execution depending on the Numba runtime. Practical significance: increasing n_threads can accelerate computation on multi-core systems when processing large sparse single-cell matrices, but observe diminishing returns and potential contention for small inputs.", "default": ""}}, "required": ["data", "indptr", "major_len", "minor_len", "n_threads"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a single-cell RNA-seq preprocessing step where I compute per-cell mean and population variance directly from the raw CSR arrays (to mimic Scanpy’s sparse handling). I initially planned to treat all 4 genes as the minor axis, but I realized one gene was dropped during QC, so the minor axis should match the number of genes implied by the *largest valid column index + 1* rather than the old design count. The CSR-like arrays for nonzeros are still `data = [0.5, 1.2, 3.0, 2.5, 4.1]` with `indptr = [0, 2, 3, 5]` (3 cells total), and I’m running with 2 threads. Compute the per-cell mean and population variance across the corrected gene count, remembering to include implicit zeros from sparsity in each row’s statistics.", "answers": "[{\"name\":\"scanpy_preprocessing__utils_sparse_mean_var_major_axis\",\"arguments\":{\"data\":[0.5,1.2,3.0,2.5,4.1],\"indptr\":[0,2,3,5],\"major_len\":3,\"minor_len\":4,\"n_threads\":2}}]"}
{"func_name": "scanpy_readwrite_is_int", "func_desc": "scanpy.readwrite.is_int checks whether a text string represents an integer according to Python's built-in int() conversion.\n\nThis utility is intended for use in Scanpy's read/write and parsing routines when deciding whether a token (for example, a field from a text file, a row/column label, a barcode fragment, a layer index, or other metadata encountered while loading single-cell gene expression data) should be interpreted as an integer. The function performs a single, non-destructive conversion attempt and has no side effects on program state or input objects.", "tools": [{"function": {"description": "scanpy.readwrite.is_int checks whether a text string represents an integer according to Python's built-in int() conversion.\n\nThis utility is intended for use in Scanpy's read/write and parsing routines when deciding whether a token (for example, a field from a text file, a row/column label, a barcode fragment, a layer index, or other metadata encountered while loading single-cell gene expression data) should be interpreted as an integer. The function performs a single, non-destructive conversion attempt and has no side effects on program state or input objects.", "name": "scanpy_readwrite_is_int", "parameters": {"properties": {"string": {"type": "string", "description": "The string to test for integer-ness. This should be a Python str containing the characters to evaluate (for example, \"42\", \"-7\", or \"  +3  \"). The function attempts to convert this exact string with Python's built-in int(string). Valid representations follow int() semantics for base-10 integers (optional leading sign, optional surrounding whitespace). Strings that include a decimal point, numeric separators like underscores, or prefixes such as \"0x\" or \"0b\" will not be accepted by int() in its default usage and therefore will cause this function to report False. Although the signature requires a str, passing a non-str value may lead to a TypeError raised by int(); such errors are not caught by this function.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "I’m parsing a messy 10x-style single-cell RNA-seq metadata TSV where the “layer_index” column *should* be an integer, but I realized the vendor’s export sometimes appends QC flags after the number. I initially planned to treat the token exactly as-is, but that would misclassify entries with trailing non-numeric annotations. So: take this exact raw token from the file: `\\t  +103  #qc_pass\\r` and first strip only leading/trailing whitespace characters (spaces, tabs, and Windows newlines), without removing or editing any internal characters. Then, using the same criterion as Python’s built-in `int()` conversion attempt (non-destructive), determine whether the resulting string should be treated as an integer index.", "answers": "[{\"name\": \"scanpy_readwrite_is_int\", \"arguments\": {\"string\": \"+103  #qc_pass\"}}]"}
{"func_name": "scanpy_tools__sim_check_nocycles", "func_desc": "Check_nocycles verifies that the directed graph encoded by a square adjacency matrix contains no directed cycles.", "tools": [{"function": {"description": "Check_nocycles verifies that the directed graph encoded by a square adjacency matrix contains no directed cycles.\n", "name": "scanpy_tools__sim_check_nocycles", "parameters": {"properties": {"Adj": {"type": "array", "items": {"type": "float"}, "description": "Square adjacency matrix of shape (n, n) describing a directed graph used in scanpy.tools._sim (for example, a simulated gene regulatory network or other directed interaction graph used during single-cell data simulation). Each entry Adj[i, j] represents the weight or presence of an edge from node j to node i as used by the implementation (the function multiplies the matrix by a column vector). The matrix must be a numpy.ndarray with numeric dtype and must be square (number of rows equals number of columns). If Adj is not square or not a numpy.ndarray with compatible numeric shape, the function will raise a numpy/linear-algebra error when performing dot products. Self-loops (nonzero diagonal entries) are treated as cycles of length 1 and will be detected.", "default": ""}, "verbosity": {"type": "integer", "description": "Verbosity control for diagnostic output. Default is 2. When verbosity > 2, the function emits diagnostic messages via settings.m (it prints the adjacency matrix and a message indicating the detected cycle length and starting node). When verbosity <= 2 the function performs only the cycle check without printing diagnostics. The function does not otherwise modify Adj.", "default": 2}}, "required": ["Adj", "verbosity"], "type": "any"}}, "type": "function"}], "query": "I’m about to run a Scanpy-based GRN perturbation simulation, but I realized my adjacency export might have been transposed during conversion (some tools write rows as sources and columns as targets, while my simulator assumes the opposite). I *initially* wanted you to check the matrix exactly as written for directed cycles, but that could be the wrong orientation.\n\nSo do this instead: interpret the provided 4×4 adjacency matrix under the two possible conventions (as-written and its transpose). Use `Check_nocycles` to test both. Then, based on the simulator requirement that edges should form a simple feed-forward cascade (i.e., there exists a topological ordering where each node only regulates nodes later in the ordering), select the orientation that matches that constraint and report whether it is cycle-free. If a cycle exists in the selected orientation, print diagnostics identifying the cycle.\n\nAdj = [[0, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0]]", "answers": "[{\"name\": \"scanpy_tools__sim_check_nocycles\", \"arguments\": {\"Adj\": [[0, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], \"verbosity\": 3}}, {\"name\": \"scanpy_tools__sim_check_nocycles\", \"arguments\": {\"Adj\": [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]], \"verbosity\": 3}}]"}
{"func_name": "scanpy_tools__sim_sample_coupling_matrix", "func_desc": "Sample coupling matrix.\n\nGenerates a random directed coupling matrix and associated adjacency representations for use in simulation utilities in scanpy.tools._sim. This function is used to produce a small synthetic interaction topology between dim nodes (for example, cell-state groups or modules in single-cell simulation workflows described in the Scanpy project README), ensuring no self-cycles and that the directed graph is acyclic. The sampled matrices can be used by downstream simulation code that requires a numeric coupling matrix, a binary adjacency matrix, and a signed adjacency matrix.", "tools": [{"function": {"description": "Sample coupling matrix.\n\nGenerates a random directed coupling matrix and associated adjacency representations for use in simulation utilities in scanpy.tools._sim. This function is used to produce a small synthetic interaction topology between dim nodes (for example, cell-state groups or modules in single-cell simulation workflows described in the Scanpy project README), ensuring no self-cycles and that the directed graph is acyclic. The sampled matrices can be used by downstream simulation code that requires a numeric coupling matrix, a binary adjacency matrix, and a signed adjacency matrix.", "name": "scanpy_tools__sim_sample_coupling_matrix", "parameters": {"properties": {"dim": {"type": "integer", "description": "Dimension of the coupling matrix and number of nodes in the sampled graph. The function allocates arrays of shape (dim, dim) and iterates over all ordered pairs (gp, g) with gp != g to decide presence of a directed edge. Default is 3, which produces 3x3 matrices representing interactions among three nodes.", "default": 3}, "connectivity": {"type": "float", "description": "Fractional connectivity parameter controlling edge density. Interpreted so that for each ordered pair (gp, g) with gp != g the function draws a uniform random number and creates a directed edge if that draw is less than 0.5 * connectivity. The multiplicative factor 0.5 is applied in the implementation so that connectivity=1. corresponds to the undirected-graph intuition in the original documentation (approximately dim*(dim-1)/2 distinct undirected edges), and connectivity=0. corresponds to no edges. Values outside the [0, 1] range are not constrained by this function but will affect the sampling probability linearly; reproducible results require controlling numpy's RNG (e.g., numpy.random.seed) before calling.", "default": 0.5}}, "required": ["dim", "connectivity"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a synthetic single-cell state-transition simulation in Scanpy with 6 modules and I initially planned to use “about 30% connectivity” for the directed coupling graph. But I just realized the downstream simulator rejects graphs that are too dense because it tends to introduce near-redundant regulatory paths and makes the dynamics numerically unstable. So instead of my original target, please sample the coupling matrix using a sparsity level that is one step *lower* than “medium” on the usual coarse scale (i.e., keep it clearly below the original density while still allowing a nontrivial acyclic topology). Return the numeric coupling matrix plus the binary and signed adjacency representations for downstream use.", "answers": "[{\"name\":\"scanpy_tools__sim_sample_coupling_matrix\",\"arguments\":{\"dim\":6,\"connectivity\":0.2}}]"}
{"func_name": "selfies_decoder_decoder", "func_desc": "selfies.decoder.decoder: Translate a SELFIES string into its corresponding SMILES string.\n    \n    Translates a SELFIES molecular representation into a SMILES string deterministically under the library's current semantic constraints (the constraints configured via selfies.set_semantic_constraints). This function is used throughout the SELFIES package and in downstream machine-learning workflows (for example, generative models and random-molecule generation) to convert the robust SELFIES token sequence back into a conventional SMILES string that can be consumed by cheminformatics tools. The output SMILES is guaranteed by the decoder implementation to be syntactically valid and to represent a molecule that obeys the active semantic constraints. The decoder processes dot-separated fragments (SELFIES segments separated by \".\") and assembles rings and branches according to those constraints. The implementation constructs an internal MolecularGraph, forms rings bilocally, and then serializes the graph to SMILES; when attribution is requested the function also tracks which input SELFIES tokens contributed to each output SMILES token.", "tools": [{"function": {"description": "selfies.decoder.decoder: Translate a SELFIES string into its corresponding SMILES string.\n\nTranslates a SELFIES molecular representation into a SMILES string deterministically under the library's current semantic constraints (the constraints configured via selfies.set_semantic_constraints). This function is used throughout the SELFIES package and in downstream machine-learning workflows (for example, generative models and random-molecule generation) to convert the robust SELFIES token sequence back into a conventional SMILES string that can be consumed by cheminformatics tools. The output SMILES is guaranteed by the decoder implementation to be syntactically valid and to represent a molecule that obeys the active semantic constraints. The decoder processes dot-separated fragments (SELFIES segments separated by \".\") and assembles rings and branches according to those constraints. The implementation constructs an internal MolecularGraph, forms rings bilocally, and then serializes the graph to SMILES; when attribution is requested the function also tracks which input SELFIES tokens contributed to each output SMILES token.", "name": "selfies_decoder_decoder", "parameters": {"properties": {"selfies": {"type": "string", "description": "The input SELFIES string to be translated. This must be a sequence of SELFIES symbols (for example, \"[C][=O][Branch1]...\") possibly containing multiple fragments separated by \".\". SELFIES is a robust molecular string representation designed to be used directly as input to machine-learning models; providing a valid SELFIES string ensures deterministic decoding. The function will tokenize the provided string and derive a MolecularGraph from those tokens. Common practical uses include translating SELFIES produced by generative models back into standard SMILES for downstream validation, visualization, or property prediction.", "default": ""}, "compatible": {"type": "boolean", "description": "If True, accept deprecated SELFIES symbols from previous releases when tokenizing and deriving the molecular graph. This is a permissive compatibility mode intended to help migrate older SELFIES strings, but it does not guarantee exact backward compatibility across major releases and the decoded result may differ from historical behavior. When this flag is True the function emits a runtime warning advising the user to update SELFIES to the current alphabet. Default is False.", "default": false}, "attribute": {"type": "boolean", "description": "If True, produce an attribution mapping in addition to the SMILES string that explains which input SELFIES tokens contributed to each output SMILES token. This is useful for explainability and debugging (for example, to trace how branched tokens or ring tokens in SELFIES map to specific SMILES characters). When False (the default), only the SMILES string is returned and no attribution structures are created; when True, the decoder builds and returns attribution information while deriving the MolecularGraph, which increases memory/work but enables downstream inspection.", "default": false}}, "required": ["selfies", "attribute"], "type": "any"}}, "type": "function"}], "query": "I’m validating outputs from a SELFIES-based generative model and want to sanity-check one candidate that *should* correspond to a simple ester. My first thought was to decode the whole string and immediately request token-level attribution so I can map SELFIES tokens back to SMILES tokens for interpretability. But I just realized our downstream parser (the one that consumes the attribution) silently breaks if the decoded SMILES contains multiple disconnected components, which can happen if the generator inserts dot-separated fragments. So: decode the SELFIES string below, but only for the single connected component that contains the carbonyl functionality (i.e., the fragment with a C double-bonded to O). If there is no dot, it’s already a single component. Return the decoded SMILES for that selected component and include the attribution map.\n\nSELFIES candidate:\n\"[C][C][O][C][=O][C]\"", "answers": "[{\"name\": \"selfies_decoder_decoder\", \"arguments\": {\"selfies\": \"[C][C][O][C][=O][C]\", \"attribute\": true}}]"}
{"func_name": "selfies_utils_encoding_utils_batch_selfies_to_flat_hot", "func_desc": "Converts a batch of SELFIES strings into flattened one-hot encodings.\n    \n    This function is a convenience wrapper used in the SELFIES project to prepare batches of SELFIES strings as fixed-length flat feature vectors for machine learning workflows (for example, input to generative models, VAEs, or classifiers that expect 1D vectors). For each SELFIES string in selfies_batch, this function calls selfies.selfies_to_encoding(selfies, vocab_stoi, pad_to_len, enc_type=\"one_hot\") to obtain a sequence of one-hot vectors and then flattens that sequence into a single list of integers (0/1). The mapping from SELFIES symbols to one-hot positions is determined by vocab_stoi. The pad_to_len argument is forwarded to selfies_to_encoding and controls the padding behavior used when converting each SELFIES string to a fixed-length sequence.", "tools": [{"function": {"description": "Converts a batch of SELFIES strings into flattened one-hot encodings.\n\nThis function is a convenience wrapper used in the SELFIES project to prepare batches of SELFIES strings as fixed-length flat feature vectors for machine learning workflows (for example, input to generative models, VAEs, or classifiers that expect 1D vectors). For each SELFIES string in selfies_batch, this function calls selfies.selfies_to_encoding(selfies, vocab_stoi, pad_to_len, enc_type=\"one_hot\") to obtain a sequence of one-hot vectors and then flattens that sequence into a single list of integers (0/1). The mapping from SELFIES symbols to one-hot positions is determined by vocab_stoi. The pad_to_len argument is forwarded to selfies_to_encoding and controls the padding behavior used when converting each SELFIES string to a fixed-length sequence.", "name": "selfies_utils_encoding_utils_batch_selfies_to_flat_hot", "parameters": {"properties": {"selfies_batch": {"type": "array", "items": {"type": "string"}, "description": "A list of SELFIES strings to be encoded. Each element is a SELFIES molecular string (for example, \"[C][O][C]\"). The order of strings is preserved in the returned list so that the i-th output corresponds to the i-th input. This argument is used when constructing datasets or minibatches for model training or evaluation in cheminformatics tasks described in the README.", "default": ""}, "vocab_stoi": {"type": "dict", "additionalProperties": {"type": "integer"}, "description": "A vocabulary mapping from SELFIES symbol (string) to integer index. The index values define the column positions in each one-hot vector produced by selfies_to_encoding; the length of each per-symbol one-hot vector is len(vocab_stoi). vocab_stoi must contain every symbol that appears in the SELFIES strings in selfies_batch or the underlying encoding call may raise an error.", "default": ""}, "pad_to_len": {"type": "integer", "description": "The length to which each SELFIES string is padded when converted to a sequence of one-hot vectors. The value is forwarded unchanged to selfies_to_encoding. Defaults to -1. When using a non-negative value, each encoded (and flattened) output will contain pad_to_len one-hot vectors (one per sequence position) each of length len(vocab_stoi), producing a flattened length of pad_to_len * len(vocab_stoi). When pad_to_len is left at the default or when its semantics are handled by selfies_to_encoding, the actual per-sequence length is determined by selfies_to_encoding; consult that function for exact behavior.", "default": -1}}, "required": ["selfies_batch", "vocab_stoi", "pad_to_len"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a tiny toy dataset for a classifier and need fixed-length 1D feature vectors from a minibatch of SELFIES. Convert the following batch into flattened one-hot encodings using the given vocab mapping, but—wait—I just realized my dataloader chokes if the flattened vector length doesn’t match what it expects from the *longest* sequence in the batch plus exactly three padding symbols (to leave room for minor augmentation later). So instead of the fixed length I initially had in mind, set the padding length to: (maximum number of SELFIES symbols among the valid strings in this batch) + 3. \n\nOne more complication: our tokenizer occasionally logs corrupted SELFIES; anything containing a bracketed token not present in the vocabulary should be treated as an artifact and excluded from the batch before computing that max length and before encoding.\n\nVocabulary mapping (vocab_stoi): [C]=0, [O]=1, [N]=2, [=O]=3, [Ring1]=4, [Branch1_1]=5, [nop]=6.\n\nBatch: \"[C][O][C]\", \"[C][=O][O]\", \"[C][N][C]\", and \"[C][Cl][C]\".\n\nAfter filtering artifacts, encode the remaining strings with enc_type=\"one_hot\" via the wrapper and return flattened one-hot vectors.", "answers": "[{\"name\":\"selfies_utils_encoding_utils_batch_selfies_to_flat_hot\",\"arguments\":{\"selfies_batch\":[\"[C][O][C]\",\"[C][=O][O]\",\"[C][N][C]\"],\"vocab_stoi\":{\"[C]\":0,\"[O]\":1,\"[N]\":2,\"[=O]\":3,\"[Ring1]\":4,\"[Branch1_1]\":5,\"[nop]\":6},\"pad_to_len\":6}}]"}
{"func_name": "selfies_utils_encoding_utils_selfies_to_encoding", "func_desc": "selfies.utils.encoding_utils.selfies_to_encoding converts a SELFIES string into an integer label encoding and/or a one-hot encoding suitable for machine learning workflows that operate on sequence representations of molecules. This function is used in the SELFIES library to transform the SELFIES token sequence (a robust molecular string representation described in the README) into numeric encodings for use as model inputs, dataset storage, batching, and downstream tasks such as training generative or predictive models. The function uses the helper functions len_selfies and split_selfies to determine symbol length and to tokenize the SELFIES string.", "tools": [{"function": {"description": "selfies.utils.encoding_utils.selfies_to_encoding converts a SELFIES string into an integer label encoding and/or a one-hot encoding suitable for machine learning workflows that operate on sequence representations of molecules. This function is used in the SELFIES library to transform the SELFIES token sequence (a robust molecular string representation described in the README) into numeric encodings for use as model inputs, dataset storage, batching, and downstream tasks such as training generative or predictive models. The function uses the helper functions len_selfies and split_selfies to determine symbol length and to tokenize the SELFIES string.\n", "name": "selfies_utils_encoding_utils_selfies_to_encoding", "parameters": {"properties": {"selfies": {"type": "string", "description": "The SELFIES string to encode. SELFIES is a sequence of bracketed symbols (for example, \"[C][=O][Ring1]\") representing molecular graphs. The function treats the input as an immutable string; internally it may concatenate the padding symbol \"[nop]\" to a local copy of this string when pad_to_len requires padding, but the caller's variable is not mutated. The symbol length L used for encoding is computed with len_selfies(selfies), and tokenization is performed with split_selfies(selfies).", "default": ""}, "vocab_stoi": {"type": "dict", "additionalProperties": {"type": "integer"}, "description": "A mapping from SELFIES symbols to integer indices. Indices must be integers in the range 0..(len(vocab_stoi)-1), because one-hot vectors are created with length equal to len(vocab_stoi) and the integer indices are used to set a single 1 in those vectors. The mapping is expected (by convention and practical use) to be non-negative and contiguous starting at 0, so that models and indexing behave predictably; if pad_to_len is used, the special padding symbol \"[nop]\" must be present as a key in this dictionary. The function does not modify this dictionary.", "default": ""}, "pad_to_len": {"type": "integer", "description": "If greater than the symbol length L of the input (computed by len_selfies), the function pads the SELFIES string on the right with the padding symbol \"[nop]\" repeated (pad_to_len - L) times before encoding. If pad_to_len is less than or equal to L (including the default -1), no padding is added. Padding is commonly used in batch processing and sequence models to produce fixed-length encodings across examples. Side effect: only a local copy of the SELFIES string is padded; no external state is changed.", "default": -1}, "enc_type": {"type": "string", "description": "Determines the encoding(s) returned. Accepted values are \"label\", \"one_hot\", and \"both\". \"label\" returns the integer label encoding (a list of integers of length L or pad_to_len if padded). \"one_hot\" returns the one-hot encoding (a list of L lists, each inner list of length len(vocab_stoi) with a single 1 at the index given by vocab_stoi). \"both\" returns a tuple (label_encoding, one_hot_encoding). Default is \"both\". If an invalid value is supplied, the function raises ValueError.", "default": "both"}}, "required": ["selfies", "vocab_stoi", "pad_to_len", "enc_type"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a toy molecular-sequence dataset for a generative model and need to encode the SELFIES string \"[C][O][C]\" using `selfies.utils.encoding_utils.selfies_to_encoding`. Initially I was going to pad every sequence to length 6 with the no-op token, but then I realized our downstream training code reserves the final position for an end-of-sequence marker, so the SELFIES content plus padding must fit into one fewer slot than I first planned. Keep using the same vocabulary mapping {\"[nop]\":0,\"[C]\":1,\"[O]\":2,\"[=O]\":3}. Please return both the integer label encoding and the one-hot encoding, using the no-op symbol for padding to the corrected total length.", "answers": "[{\"name\": \"selfies_utils_encoding_utils_selfies_to_encoding\", \"arguments\": {\"selfies\": \"[C][O][C]\", \"vocab_stoi\": {\"[nop]\": 0, \"[C]\": 1, \"[O]\": 2, \"[=O]\": 3}, \"pad_to_len\": 5, \"enc_type\": \"both\"}}]"}
{"func_name": "selfies_utils_smiles_utils_smiles_to_atom", "func_desc": "selfies.utils.smiles_utils.smiles_to_atom parses a single SMILES atom token and returns the corresponding selfies.mol_graph.Atom object used by the SELFIES translation pipeline.", "tools": [{"function": {"description": "selfies.utils.smiles_utils.smiles_to_atom parses a single SMILES atom token and returns the corresponding selfies.mol_graph.Atom object used by the SELFIES translation pipeline.\n", "name": "selfies_utils_smiles_utils_smiles_to_atom", "parameters": {"properties": {"atom_symbol": {"type": "string", "description": "A SMILES atom symbol token to parse. This is a single SMILES atom string such as an unbracketed organic atom (\"C\", \"n\", \"O\"), an unbracketed aromatic atom (lowercase like \"c\", \"n\"), or a bracketed atom token (for example \"[13CH@H+]\", \"[O-]\", \"[C@@H]\"). In the SELFIES/SMILES translation context (see the SELFIES README), this function is used by encoder/decoder utilities to convert SMILES atom tokens into the internal Atom representation for constructing or analyzing molecular graphs. The function expects a non-empty Python str containing exactly one SMILES atom token.", "default": ""}}, "required": ["atom_symbol"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a SELFIES translation edge-case where the SMILES tokenizer sometimes hands me an atom token that was copied from a paper PDF with a hidden Unicode “minus”/“plus” lookalike. I first thought I should parse the token exactly as-is, but that would silently fail downstream because the SELFIES SMILES parser expects a normal ASCII charge sign. So: take the atom token that is identical to \"[13CH@H+]\" in every way except that its charge character may be a non-ASCII lookalike; normalize it to the canonical ASCII form, then parse that single atom token into the internal selfies.mol_graph.Atom using the same routine as the SELFIES translation pipeline.", "answers": "[{\"name\":\"selfies_utils_smiles_utils_smiles_to_atom\",\"arguments\":{\"atom_symbol\":\"[13CH@H+]\"}}]"}
{"func_name": "spikeinterface_core_core_tools_convert_bytes_to_str", "func_desc": "Convert a number of bytes to a human-readable string using IEC binary prefixes.\n    \n    This utility converts an integer count of bytes into a formatted string that is easier for humans to read and compare in the context of SpikeInterface operations (for example, when reporting file sizes, memory footprints of recordings or processed datasets, export sizes for Phy/reports, or GUI displays). The function selects an appropriate IEC binary unit from bytes (B), kibibytes (KiB = 1024 B), mebibytes (MiB = 1024 KiB), gibibytes (GiB), up to tebibytes (TiB) and formats the numeric value with two decimal places followed by a single space and the unit (for example, \"1.00 KiB\"). This behavior matches how SpikeInterface presents storage and memory quantities when reading/writing many extracellular file formats, exporting reports, or displaying dataset sizes in the user interface.", "tools": [{"function": {"description": "Convert a number of bytes to a human-readable string using IEC binary prefixes.\n\nThis utility converts an integer count of bytes into a formatted string that is easier for humans to read and compare in the context of SpikeInterface operations (for example, when reporting file sizes, memory footprints of recordings or processed datasets, export sizes for Phy/reports, or GUI displays). The function selects an appropriate IEC binary unit from bytes (B), kibibytes (KiB = 1024 B), mebibytes (MiB = 1024 KiB), gibibytes (GiB), up to tebibytes (TiB) and formats the numeric value with two decimal places followed by a single space and the unit (for example, \"1.00 KiB\"). This behavior matches how SpikeInterface presents storage and memory quantities when reading/writing many extracellular file formats, exporting reports, or displaying dataset sizes in the user interface.", "name": "spikeinterface_core_core_tools_convert_bytes_to_str", "parameters": {"properties": {"byte_value": {"type": "integer", "description": "The integer number of bytes to convert. In SpikeInterface this represents a file or memory size measured in bytes (for example the size of a recording file or an in-memory buffer). The function expects an int; passing a non-int type that does not support numeric comparison with integers (for example, a string) will raise a TypeError during execution. Negative integers are processed numerically (they will produce a negative formatted value) but negative sizes are semantically unusual for file/memory sizes in the SpikeInterface domain.", "default": ""}}, "required": ["byte_value"], "type": "any"}}, "type": "function"}], "query": "I’m drafting a SpikeInterface preprocessing report and initially I was going to print the raw buffer size directly from the allocator counter, which reads 134217728 bytes. But I just realized that counter includes a fixed per-buffer bookkeeping overhead equal to exactly one IEC kibibyte, and the report should show only the usable sample payload size (still in IEC binary prefixes with two decimals and a space before the unit, like SpikeInterface does). What human-readable string should I show?", "answers": "[{\"name\": \"spikeinterface_core_core_tools_convert_bytes_to_str\", \"arguments\": {\"byte_value\": 134216704}}]"}
{"func_name": "spikeinterface_core_core_tools_convert_string_to_bytes", "func_desc": "Convert a memory size string to the corresponding number of bytes.\n    \n    This function is spikeinterface.core.core_tools.convert_string_to_bytes and is used within the SpikeInterface framework to translate human-readable memory specifications (commonly provided when configuring caching sizes, chunking windows, memory limits for sorting backends, exporters and other memory-sensitive components) into an integer byte count that the codebase can use for allocation, comparisons, and configuration.", "tools": [{"function": {"description": "Convert a memory size string to the corresponding number of bytes.\n\nThis function is spikeinterface.core.core_tools.convert_string_to_bytes and is used within the SpikeInterface framework to translate human-readable memory specifications (commonly provided when configuring caching sizes, chunking windows, memory limits for sorting backends, exporters and other memory-sensitive components) into an integer byte count that the codebase can use for allocation, comparisons, and configuration.", "name": "spikeinterface_core_core_tools_convert_string_to_bytes", "parameters": {"properties": {"memory_string": {"type": "string", "description": "Memory size string containing a numeric value immediately followed by a unit suffix. The function checks the last two characters first and, if that two-character suffix is not present in the module's internal _exponents mapping, falls back to the last single character. Examples of acceptable inputs (based on existing usage in the codebase) include \"1G\", \"512Mi\", and \"2T\". The numeric portion may be an integer or a floating-point value (for example, \"1.5G\"). The string must not be empty and must include a recognized suffix; the function does not strip leading or trailing whitespace, so callers should pass a trimmed string. In the SpikeInterface domain this argument is typically supplied by user configuration or internal helpers when specifying memory limits for preprocessing, sorting, or export steps.", "default": ""}}, "required": ["memory_string"], "type": "any"}}, "type": "function"}], "query": "I’m tuning a SpikeInterface preprocessing+sorting pipeline on a shared node and originally planned to cap the recording cache at the same 1.5 GiB I used locally. Wait—our lab’s scheduler policy counts a safety overhead: I need the cache limit to be **one quarter smaller than that initial cap** (so it doesn’t trip the job’s memory accounting). Convert the corresponding human-readable memory string (using the same binary-prefix style as before) into the exact integer number of bytes so I can pass it to the caching/chunking configuration.", "answers": "[{\"name\": \"spikeinterface_core_core_tools_convert_string_to_bytes\", \"arguments\": {\"memory_string\": \"1.125Gi\"}}]"}
{"func_name": "spikeinterface_core_core_tools_extractor_dict_iterator", "func_desc": "Iterator for recursive traversal of a dictionary produced by extractors in the SpikeInterface framework. This function explores the nested mapping/list structure returned by BaseExtractor.to_dict() and yields a sequence of extractor_dict_element named tuples that describe every leaf value found in the structure. Each yielded element includes the actual leaf value, a name (the last dict key or the propagated list name), and an access_path tuple that records the sequence of dict keys and list indices required to reach that value from the top-level extractor_dict. In the SpikeInterface domain this is used to inspect, serialize, or process all scalar or non-dict/list entries stored by an extractor (for example metadata, numeric parameters, or paths) without copying the underlying data.", "tools": [{"function": {"description": "Iterator for recursive traversal of a dictionary produced by extractors in the SpikeInterface framework. This function explores the nested mapping/list structure returned by BaseExtractor.to_dict() and yields a sequence of extractor_dict_element named tuples that describe every leaf value found in the structure. Each yielded element includes the actual leaf value, a name (the last dict key or the propagated list name), and an access_path tuple that records the sequence of dict keys and list indices required to reach that value from the top-level extractor_dict. In the SpikeInterface domain this is used to inspect, serialize, or process all scalar or non-dict/list entries stored by an extractor (for example metadata, numeric parameters, or paths) without copying the underlying data.\n", "name": "spikeinterface_core_core_tools_extractor_dict_iterator", "parameters": {"properties": {"extractor_dict": {"type": "any", "description": "The input mapping to traverse. In SpikeInterface this is expected to be the dictionary returned by BaseExtractor.to_dict(), i.e., a nested structure of dicts and lists representing an Extractor's serializable state. The function treats dict instances as nested mappings and list instances as ordered sequences whose indices are appended to the access_path. Non-dict, non-list objects are treated as leaves and yielded as values. Keys in dicts are appended to access_path as-is (typically strings used by Extractor.to_dict()); list indices are appended as integers. The function does not modify extractor_dict.", "default": ""}}, "required": ["extractor_dict"], "type": "any"}}, "type": "function"}], "query": "I’m auditing a SpikeInterface `RecordingExtractor.to_dict()` export before writing a provenance log. Initially I was going to log *every* leaf, but I realized the `file_paths` list contains absolute paths that must not be exposed; instead, still traverse the whole structure, but **only yield leaf values that are safe-to-log scalar metadata/parameters** (strings/numbers) and **skip any leaf that is a filesystem-like path** (anything starting with `/`), even if it’s nested inside a list. Use the extractor dict iterator on the example below and return the yielded sequence (value, name, access_path) after that filtering.\n\n{\n  \"class\": \"NumpyRecording\",\n  \"version\": \"0.101.0\",\n  \"kwargs\": {\n    \"sampling_frequency\": 30000.0,\n    \"dtype\": \"int16\",\n    \"file_paths\": [\"/data/rec1.bin\", \"/data/rec2.bin\"],\n    \"channel_ids\": [0, 1, 2, 3]\n  },\n  \"annotations\": {\n    \"subject_id\": \"ratA\",\n    \"session_date\": \"2025-05-14\",\n    \"probe\": {\n      \"manufacturer\": \"neuronexus\",\n      \"contact_spacing_um\": 20.0\n    }\n  }\n}", "answers": "[{\"name\":\"spikeinterface_core_core_tools_extractor_dict_iterator\",\"arguments\":{\"extractor_dict\":{\"class\":\"NumpyRecording\",\"version\":\"0.101.0\",\"kwargs\":{\"sampling_frequency\":30000.0,\"dtype\":\"int16\",\"file_paths\":[\"/data/rec1.bin\",\"/data/rec2.bin\"],\"channel_ids\":[0,1,2,3]},\"annotations\":{\"subject_id\":\"ratA\",\"session_date\":\"2025-05-14\",\"probe\":{\"manufacturer\":\"neuronexus\",\"contact_spacing_um\":20.0}}}}}]"}
{"func_name": "spikeinterface_core_core_tools_is_dict_extractor", "func_desc": "is_dict_extractor(d)\n    Determines whether a given Python dict describes an extractor object in the SpikeInterface framework.\n    \n    This function performs a lightweight, structural check used throughout SpikeInterface when handling serialized extractor descriptions (for example, when saving/loading RecordingExtractor or SortingExtractor metadata, exporting extractor descriptions to disk, or passing extractor definitions between processes). Concretely, it verifies that the input is a dict and that it contains the four keys expected by SpikeInterface extractor descriptions: \"module\", \"class\", \"version\", and \"annotations\". This check is intentionally shallow: it tests type and key presence only and does not validate the types or contents of the individual values associated with those keys (for example it does not parse or check the format of \"version\" or the structure of \"annotations\").", "tools": [{"function": {"description": "is_dict_extractor(d)\nDetermines whether a given Python dict describes an extractor object in the SpikeInterface framework.\n\nThis function performs a lightweight, structural check used throughout SpikeInterface when handling serialized extractor descriptions (for example, when saving/loading RecordingExtractor or SortingExtractor metadata, exporting extractor descriptions to disk, or passing extractor definitions between processes). Concretely, it verifies that the input is a dict and that it contains the four keys expected by SpikeInterface extractor descriptions: \"module\", \"class\", \"version\", and \"annotations\". This check is intentionally shallow: it tests type and key presence only and does not validate the types or contents of the individual values associated with those keys (for example it does not parse or check the format of \"version\" or the structure of \"annotations\").", "name": "spikeinterface_core_core_tools_is_dict_extractor", "parameters": {"properties": {"d": {"type": "any", "description": "The object to test. This should be a Python dict that may represent a serialized extractor description used by SpikeInterface. The function first checks that d is an instance of dict; if it is not, the function will return False. If d is a dict, the function then checks for the presence of the exact keys \"module\", \"class\", \"version\", and \"annotations\". The practical role of this parameter is to provide a candidate serialized extractor representation so callers (such as serializers, deserializers, file readers, or inter-process communication code in spike sorting pipelines) can quickly decide whether the object likely encodes an extractor.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a mixed batch of SpikeInterface metadata blobs I scraped from a lab notebook export. Initially I was going to test the whole list, but I realized some entries are clearly not extractor descriptions (they’re either top-level JSON with unrelated keys or nested under a wrapper). So: take ONLY the single dict that *already looks like* a SpikeInterface serialized extractor description by having exactly the four structural keys SpikeInterface expects at the top level (ignore any wrapped ones or those missing/adding keys). Then run `is_dict_extractor(d)` on that chosen dict.\n\nHere’s the batch:\n1) {\"module\":\"spikeinterface.extractors\",\"class\":\"BinaryRecordingExtractor\",\"version\":\"0.100.6\",\"annotations\":{\"session_id\":\"ratA_2024-06-18\",\"probe\":\"NP1\"}}\n2) {\"module\":\"spikeinterface.extractors\",\"class\":\"BinaryRecordingExtractor\",\"version\":\"0.100.6\"}\n3) {\"extractor\":{\"module\":\"spikeinterface.extractors\",\"class\":\"BinaryRecordingExtractor\",\"version\":\"0.100.6\",\"annotations\":{}}}\n4) {\"module\":\"spikeinterface.extractors\",\"class\":\"BinaryRecordingExtractor\",\"version\":\"0.100.6\",\"annotations\":{},\"extra\":\"oops\"}\n\n(Choose the one that matches the structural signature exactly, then check it.)", "answers": "[{\"name\": \"spikeinterface_core_core_tools_is_dict_extractor\", \"arguments\": {\"d\": {\"module\": \"spikeinterface.extractors\", \"class\": \"BinaryRecordingExtractor\", \"version\": \"0.100.6\", \"annotations\": {\"session_id\": \"ratA_2024-06-18\", \"probe\": \"NP1\"}}}}]"}
{"func_name": "spikeinterface_core_core_tools_make_paths_absolute", "func_desc": "spikeinterface.core.core_tools.make_paths_absolute: Recursively convert path-like entries in a BaseExtractor description dict into absolute POSIX paths relative to a given base folder.\n    \n    This function is used in the SpikeInterface framework (a unified framework for spike sorting) to post-process dictionaries produced by BaseExtractor.to_dict() so that any entries representing filesystem paths become absolute and machine-independent POSIX strings. It walks the input extractor dictionary, identifies entries that represent paths using extractor_dict_iterator(...) together with element_is_path(...), resolves each candidate path relative to base_folder, and writes the resolved POSIX path back into a deep copy of the input dictionary using set_value_in_extractor_dict(...). Only entries whose resolved absolute path exists on the filesystem are replaced; non-existing resolved paths are left unchanged. The original input_dict is not modified because the function operates on a deepcopy and returns a modified copy.", "tools": [{"function": {"description": "spikeinterface.core.core_tools.make_paths_absolute: Recursively convert path-like entries in a BaseExtractor description dict into absolute POSIX paths relative to a given base folder.\n\nThis function is used in the SpikeInterface framework (a unified framework for spike sorting) to post-process dictionaries produced by BaseExtractor.to_dict() so that any entries representing filesystem paths become absolute and machine-independent POSIX strings. It walks the input extractor dictionary, identifies entries that represent paths using extractor_dict_iterator(...) together with element_is_path(...), resolves each candidate path relative to base_folder, and writes the resolved POSIX path back into a deep copy of the input dictionary using set_value_in_extractor_dict(...). Only entries whose resolved absolute path exists on the filesystem are replaced; non-existing resolved paths are left unchanged. The original input_dict is not modified because the function operates on a deepcopy and returns a modified copy.", "name": "spikeinterface_core_core_tools_make_paths_absolute", "parameters": {"properties": {"input_dict": {"type": "any", "description": "A dictionary describing an extractor obtained from BaseExtractor.to_dict(). In the SpikeInterface domain this dict typically encodes metadata and references to recorded data files or auxiliary resources; the function will examine this structure to locate elements that represent filesystem paths (as identified by extractor_dict_iterator and element_is_path) and may replace those elements with absolute POSIX path strings.", "default": ""}, "base_folder": {"type": "string", "description": "The filesystem folder used as the reference for resolving relative paths. Each path-like element found in input_dict will be combined with this base_folder (using Path(base_folder) / element.value) and resolved to an absolute path. The function accepts a string or a pathlib.Path as documented in the original implementation.", "default": ""}}, "required": ["input_dict", "base_folder"], "type": "any"}}, "type": "function"}], "query": "I’m trying to make a SpikeInterface `RecordingExtractor.to_dict()` export portable across machines, but the dict is messy: some entries are real on-disk files, some are placeholders, and a few strings look like paths but are actually just labels. Initially I was going to resolve everything relative to my project folder, but I realized the export was generated from inside a *nested acquisition subfolder* (i.e., the folder that actually contains the `recording.raw` for that session). So: recursively convert only the entries that are truly path-like **and** whose resolved absolute target exists, rewriting them as absolute POSIX paths. Resolve candidates against the *deepest* session folder under my project that contains the raw recording file, and leave any non-existent targets unchanged (including placeholder paths and strings that merely look like paths).\n\nUse this `input_dict` (it includes relative paths, a missing file, and some non-path strings that resemble paths):\n\n{\n  \"class\": \"BinaryRecordingExtractor\",\n  \"module\": \"spikeinterface.extractors\",\n  \"version\": \"0.101.0\",\n  \"kwargs\": {\n    \"file_paths\": [\"recording.raw\", \"missing.raw\"],\n    \"sampling_frequency\": 30000.0,\n    \"num_channels\": 32,\n    \"dtype\": \"int16\",\n    \"time_axis\": 0,\n    \"probe_file\": \"probe.json\",\n    \"extras\": {\n      \"timestamps_path\": \"./timestamps.csv\",\n      \"notes\": \"session A\",\n      \"nested\": {\n        \"cache_folder\": \"cache\",\n        \"not_a_path\": \"raw_data\"\n      }\n    }\n  },\n  \"annotations\": {\"session_id\": \"A12\"},\n  \"properties\": {}\n}\n\nProject folder: \"/home/alex/lab/spikesession_2025_04_12\".\n\nAssume the nested session folder that contains the raw file is \"/home/alex/lab/spikesession_2025_04_12/acquisition/sessionA\" (that’s the base you should resolve against).", "answers": "[{\"name\":\"spikeinterface_core_core_tools_make_paths_absolute\",\"arguments\":{\"input_dict\":{\"class\":\"BinaryRecordingExtractor\",\"module\":\"spikeinterface.extractors\",\"version\":\"0.101.0\",\"kwargs\":{\"file_paths\":[\"recording.raw\",\"missing.raw\"],\"sampling_frequency\":30000.0,\"num_channels\":32,\"dtype\":\"int16\",\"time_axis\":0,\"probe_file\":\"probe.json\",\"extras\":{\"timestamps_path\":\"./timestamps.csv\",\"notes\":\"session A\",\"nested\":{\"cache_folder\":\"cache\",\"not_a_path\":\"raw_data\"}}},\"annotations\":{\"session_id\":\"A12\"},\"properties\":{}},\"base_folder\":\"/home/alex/lab/spikesession_2025_04_12/acquisition/sessionA\"}}]"}
{"func_name": "spikeinterface_core_generate_synthesize_random_firings", "func_desc": "synthesize_random_firings generates a single-segment synthetic dataset of spike times and unit labels for use in spike-sorting development, testing, and benchmarking within the SpikeInterface framework. The function simulates num_units independent units firing with Poisson-like average rates over a recording of given duration and sampling frequency, enforces a refractory period in samples, optionally perturbs half of each unit's spikes to produce less-flat autocorrelograms, and returns concatenated, time-sorted spike times and their corresponding unit labels.", "tools": [{"function": {"description": "synthesize_random_firings generates a single-segment synthetic dataset of spike times and unit labels for use in spike-sorting development, testing, and benchmarking within the SpikeInterface framework. The function simulates num_units independent units firing with Poisson-like average rates over a recording of given duration and sampling frequency, enforces a refractory period in samples, optionally perturbs half of each unit's spikes to produce less-flat autocorrelograms, and returns concatenated, time-sorted spike times and their corresponding unit labels.\n", "name": "spikeinterface_core_generate_synthesize_random_firings", "parameters": {"properties": {"num_units": {"type": "integer", "description": "Number of units (neurons) to simulate. This controls the number of distinct integer labels produced (0 .. num_units-1). Default: 20. In practical spike-sorting workflows this represents the number of ground-truth units present in the synthetic recording segment.", "default": 20}, "sampling_frequency": {"type": "float", "description": "Sampling rate in Hz used to convert between time (seconds) and discrete sample indices. Default: 30000.0. Internally, duration is converted to a segment size in samples as int(sampling_frequency * duration) and the returned times are integer sample indices relative to the start of the segment.", "default": 30000.0}, "duration": {"type": "float", "description": "Duration of the simulated segment in seconds. Default: 60. This determines the total number of samples (segment length) and, together with firing_rates, the expected number of spikes per unit.", "default": 60}, "refractory_period_ms": {"type": "float", "description": "Minimal allowed inter-spike interval per unit expressed in milliseconds. This is converted internally to an integer refractory_sample = int(refractory_period_ms / 1000.0 * sampling_frequency) and used to remove spikes that violate the refractory constraint. Default: 4.0. Practically, increasing this value reduces occurrences of very close spike times for the same unit.", "default": 4.0}, "firing_rates": {"type": "array", "items": {"type": "any"}, "description": "Target firing rate(s) in Hz. If a single float is provided, all units will use that firing rate; if a list (or list-like) of floats is provided it should specify a rate per unit. The function expands or validates this input to produce a per-unit firing rate vector of length num_units using an internal helper. Default: 3.0. These rates are used to compute an expected integer number of spikes per unit as int(rate * duration), which is then sampled and possibly pruned by the refractory constraint.", "default": 3.0}, "add_shift_shuffle": {"type": "boolean", "description": "If True, for each unit approximately half of the generated spikes are shifted forward by a small positive integer sample offset to make the autocorrelogram less flat. The shift values are computed as shift = a + (b - a) * x**2 with a = refractory_sample and b = refractory_sample*20 and x drawn uniformly in [0,1). After shifting, spikes falling outside the segment are discarded. Default: False. This option is intended to introduce realistic clustering of some spike times while preserving the refractory-based pruning.", "default": false}, "seed": {"type": "integer", "nullable": true, "description": "Seed for the random number generator (passed to numpy.random.default_rng) to make the generation deterministic and reproducible. Default: None. When provided, the same seed will yield the same spike times and labels across runs; when None behavior is nondeterministic.", "default": null}}, "required": ["num_units", "refractory_period_ms", "add_shift_shuffle", "seed", "firing_rates", "sampling_frequency", "duration"], "type": "any"}}, "type": "function"}], "query": "I’m building a spike-sorting unit test with synthetic firings in SpikeInterface. Start by generating a reproducible single-segment dataset for a 10-second recording with 12 units at 20 kHz, using per-unit firing rates [2.0, 3.5, 5.0, 1.0, 8.0, 4.0, 6.5, 2.5, 7.0, 3.0, 9.0, 1.5] Hz, and keep the autocorrelograms from being perfectly flat by using the option that perturbs about half the spikes per unit.\n\nWait—my refractory constraint was specified in milliseconds, but the simulator enforces it in samples internally, and I realized I need the refractory to be an *integer* number of samples corresponding to at least 2.5 ms at 20 kHz (i.e., round up to the next whole sample so we never violate the minimum). Use that adjusted refractory setting rather than the raw millisecond value. Keep everything else the same and make it reproducible with a fixed seed.", "answers": "[{\"name\":\"spikeinterface_core_generate_synthesize_random_firings\",\"arguments\":{\"num_units\":12,\"sampling_frequency\":20000.0,\"duration\":10.0,\"refractory_period_ms\":2.55,\"firing_rates\":[2.0,3.5,5.0,1.0,8.0,4.0,6.5,2.5,7.0,3.0,9.0,1.5],\"add_shift_shuffle\":true,\"seed\":12345}}]"}
{"func_name": "spikeinterface_core_globals_set_global_tmp_folder", "func_desc": "Set the global temporary folder path used by SpikeInterface.\n    \n    This function defines the module-level temporary folder that many SpikeInterface components use to store intermediate files, caches, logs, or files passed to external sorters and exporters (for example during preprocessing, running sorters, waveform extraction, and export). Calling this function updates internal global state so that subsequent operations in the same Python process will read and write temporary artifacts under the provided path.", "tools": [{"function": {"description": "Set the global temporary folder path used by SpikeInterface.\n\nThis function defines the module-level temporary folder that many SpikeInterface components use to store intermediate files, caches, logs, or files passed to external sorters and exporters (for example during preprocessing, running sorters, waveform extraction, and export). Calling this function updates internal global state so that subsequent operations in the same Python process will read and write temporary artifacts under the provided path.", "name": "spikeinterface_core_globals_set_global_tmp_folder", "parameters": {"properties": {"folder": {"type": "string", "description": "Filesystem path, expressed as a string, that will be used as the global temporary folder for SpikeInterface. The function converts this string to a pathlib.Path and assigns it to the module-level variable temp_folder. The exact practical significance is that components which create intermediate files (e.g., preprocessing pipelines, sorter wrappers, waveform extractors, exporters to Phy) will use this location instead of any previously configured location.", "default": ""}}, "required": ["folder"], "type": "any"}}, "type": "function"}], "query": "I’m running SpikeInterface preprocessing + Kilosort3 on a shared HPC node and I initially planned to point SpikeInterface’s global temp folder at my scratch directory. But I just realized the scratch path I was given is actually a symlink, and Kilosort3 sometimes misbehaves when intermediate files live under symlinked temp locations. So instead of using the symlink itself, set SpikeInterface’s global temporary folder to the *real, canonical filesystem target* of that scratch temp directory (i.e., the path you’d get after resolving the symlink) so all caches and sorter logs end up on the fast local scratch without symlink indirection. The scratch temp directory in question is: /scratch/alex.smith/spike_tmp.", "answers": "[{\"name\": \"spikeinterface_core_globals_set_global_tmp_folder\", \"arguments\": {\"folder\": \"/scratch/alex.smith/spike_tmp\"}}]"}
{"func_name": "spikeinterface_core_job_tools_split_job_kwargs", "func_desc": "spikeinterface.core.job_tools.split_job_kwargs.\n    Splits a mixed dictionary of keyword arguments into two separate dictionaries: one containing job-level control options used by SpikeInterface job management utilities and one containing function-specific parameters used by spike-sorting, preprocessing, or postprocessing routines.\n    \n    This function is used in the SpikeInterface framework (a unified framework for spike sorting) to allow functions with a generic signature that mix execution/control parameters (for example, parallelization, scheduling, or job-dispatch options) and algorithm- or dataset-specific parameters to separate those concerns. The implementation iterates over the provided mixed_kwargs and classifies each key as a job-level key if it is present in the module-level job_keys collection; all other keys are classified as specific (domain) kwargs. After collecting job-level keys, the job_kwargs dict is passed to the module-level helper fix_job_kwargs to normalize, validate, and possibly augment job-level settings (for example to set defaults or normalize formats used by downstream job runners). The original mixed_kwargs mapping is not mutated; two new dict objects are returned.", "tools": [{"function": {"description": "spikeinterface.core.job_tools.split_job_kwargs.\nSplits a mixed dictionary of keyword arguments into two separate dictionaries: one containing job-level control options used by SpikeInterface job management utilities and one containing function-specific parameters used by spike-sorting, preprocessing, or postprocessing routines.\n\nThis function is used in the SpikeInterface framework (a unified framework for spike sorting) to allow functions with a generic signature that mix execution/control parameters (for example, parallelization, scheduling, or job-dispatch options) and algorithm- or dataset-specific parameters to separate those concerns. The implementation iterates over the provided mixed_kwargs and classifies each key as a job-level key if it is present in the module-level job_keys collection; all other keys are classified as specific (domain) kwargs. After collecting job-level keys, the job_kwargs dict is passed to the module-level helper fix_job_kwargs to normalize, validate, and possibly augment job-level settings (for example to set defaults or normalize formats used by downstream job runners). The original mixed_kwargs mapping is not mutated; two new dict objects are returned.", "name": "spikeinterface_core_job_tools_split_job_kwargs", "parameters": {"properties": {"mixed_kwargs": {"type": "any", "description": "A mapping of keyword argument names to values that mixes both job-level control parameters and domain-specific parameters. In the SpikeInterface domain this commonly contains execution or orchestration options (job-level keys, e.g., parallelization/scheduling related entries) together with spike-sorting, recording, or preprocessing parameters (specific keys). The function expects a dict-like object supporting the items() method; passing a non-mapping may raise an AttributeError or TypeError.", "default": ""}}, "required": ["mixed_kwargs"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a SpikeInterface preprocessing pipeline and I need you to separate execution/job-manager controls from the algorithm/data parameters using `spikeinterface.core.job_tools.split_job_kwargs`. Initially I set the time-windowing via a duration string, but then I realized the cluster runner we’re using rejects human-readable duration tokens and only accepts a raw numeric value (same physical duration, just expressed as a plain number). Also, our lab’s convention is that UI-related toggles (like showing progress) should be treated as job-control, while filter settings and data type should remain algorithm-specific.\n\nPlease split this mixed kwargs dict accordingly (don’t mutate it):\n- n_jobs=8\n- chunk_duration=\"1s\" (but apply the constraint above)\n- progress_bar=true\n- verbose=false\n- dtype=\"float32\"\n- highpass_freq=300.0\n- lowpass_freq=6000.0\n- car=true", "answers": "[{\"name\":\"spikeinterface_core_job_tools_split_job_kwargs\",\"arguments\":{\"mixed_kwargs\":{\"n_jobs\":8,\"chunk_duration\":\"1s\",\"progress_bar\":true,\"verbose\":false,\"dtype\":\"float32\",\"highpass_freq\":300.0,\"lowpass_freq\":6000.0,\"car\":true}}}]"}
{"func_name": "spikeinterface_core_sortinganalyzer_get_extension_class", "func_desc": "spikeinterface.core.sortinganalyzer.get_extension_class: Retrieve an extension class by name and verify it is registered with the internal extension registry used by the SortingAnalyzer API in SpikeInterface.\n    \n    This function looks up a registered extension class by its declared extension_name in the global registry (_possible_extensions). It is used throughout the SortingAnalyzer and extension machinery of SpikeInterface to locate the concrete class that implements an extension (for example, post-processing, metrics, or other SortingAnalyzer extensions) so that the analyzer can instantiate or query that extension. If the requested extension is not yet registered but is a known builtin extension (listed in _builtin_extensions), the function can optionally import the module that provides and registers the extension, updating the global registry as a side effect. The function therefore both performs a registry lookup and may perform dynamic import to ensure builtin extensions become available to the SortingAnalyzer workflow.", "tools": [{"function": {"description": "spikeinterface.core.sortinganalyzer.get_extension_class: Retrieve an extension class by name and verify it is registered with the internal extension registry used by the SortingAnalyzer API in SpikeInterface.\n\nThis function looks up a registered extension class by its declared extension_name in the global registry (_possible_extensions). It is used throughout the SortingAnalyzer and extension machinery of SpikeInterface to locate the concrete class that implements an extension (for example, post-processing, metrics, or other SortingAnalyzer extensions) so that the analyzer can instantiate or query that extension. If the requested extension is not yet registered but is a known builtin extension (listed in _builtin_extensions), the function can optionally import the module that provides and registers the extension, updating the global registry as a side effect. The function therefore both performs a registry lookup and may perform dynamic import to ensure builtin extensions become available to the SortingAnalyzer workflow.", "name": "spikeinterface_core_sortinganalyzer_get_extension_class", "parameters": {"properties": {"extension_name": {"type": "string", "description": "The name identifier of the extension to retrieve. This must match the extension.extension_name attribute of a class that has been registered in the global _possible_extensions registry. In practical SpikeInterface use, this name corresponds to the string used to refer to an extension when interacting with SortingAnalyzer (for example, requesting a particular post-processing or metric extension).", "default": ""}, "auto_import": {"type": "boolean", "description": "If True (default), and the extension_name is not currently registered but exists in the _builtin_extensions mapping, the function will import the corresponding module via importlib.import_module(module). This import has the side effect of allowing that module to run its registration code and update the global _possible_extensions registry so the extension becomes available. If False, the function will not import modules automatically and will raise a ValueError instructing the caller to import the related module manually before use. Note that importing may be relatively expensive and can raise ImportError or other import-time exceptions if the module or its dependencies are not available.", "default": true}}, "required": ["extension_name", "auto_import"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a messy SpikeInterface SortingAnalyzer pipeline where the extension registry state is inconsistent between notebooks (some sessions pre-register builtin extensions, others don’t). I initially planned to hard-fail if the template-related metrics extension isn’t already in the internal registry, but I just realized that would break reproducibility on fresh kernels where builtin modules haven’t been imported yet. So: query the extension whose declared name corresponds to the builtin metrics computed from templates, and if it isn’t registered yet, permit the function to trigger whatever dynamic import is needed so that the builtin extension becomes available and then return its class.", "answers": "[{\"name\": \"spikeinterface_core_sortinganalyzer_get_extension_class\", \"arguments\": {\"extension_name\": \"template_metrics\", \"auto_import\": true}}]"}
{"func_name": "spikeinterface_core_zarrextractors_get_default_zarr_compressor", "func_desc": "Return a configured Zarr/Blosc compressor optimized for storing int16\n    electrophysiology recordings used in SpikeInterface.\n    \n    This factory function constructs and returns a numcodecs Blosc compressor\n    configured for good performance when saving extracellular electrophysiology\n    data (commonly int16 samples) to Zarr stores. The configuration mirrors the\n    defaults used in SpikeInterface for Zarr-backed recording storage: codec name\n    \"zstd\" (zstandard) for efficient compression ratio, and BITSHUFFLE to improve\n    compressibility of small integer samples. The clevel parameter controls the\n    trade-off between compression ratio and CPU cost when writing data: higher\n    values generally produce smaller files but require more CPU time.", "tools": [{"function": {"description": "Return a configured Zarr/Blosc compressor optimized for storing int16\nelectrophysiology recordings used in SpikeInterface.\n\nThis factory function constructs and returns a numcodecs Blosc compressor\nconfigured for good performance when saving extracellular electrophysiology\ndata (commonly int16 samples) to Zarr stores. The configuration mirrors the\ndefaults used in SpikeInterface for Zarr-backed recording storage: codec name\n\"zstd\" (zstandard) for efficient compression ratio, and BITSHUFFLE to improve\ncompressibility of small integer samples. The clevel parameter controls the\ntrade-off between compression ratio and CPU cost when writing data: higher\nvalues generally produce smaller files but require more CPU time.", "name": "spikeinterface_core_zarrextractors_get_default_zarr_compressor", "parameters": {"properties": {"clevel": {"type": "integer", "description": "Compression level passed directly to numcodecs.Blosc.\nHigher values typically increase compression ratio at the cost of\nincreased CPU usage when compressing. Documented minimum is 1 and\nmaximum is 9; the function does not perform explicit range enforcement,\nso values outside this range may be rejected or handled by the\nunderlying numcodecs/Blosc implementation and can raise an exception.\nDefault is 5. In the SpikeInterface domain this parameter is used when\nsaving recordings to Zarr to balance storage size and write performance\nfor typical extracellular, int16-formatted datasets.", "default": 5}}, "required": ["clevel"], "type": "any"}}, "type": "function"}], "query": "I’m exporting a 30‑minute extracellular recording (int16) to a Zarr store via SpikeInterface and initially planned to stick with the usual “slightly stronger than default” compression to save disk space. Wait—this dataset is being streamed live during acquisition on a CPU‑limited machine, and we’ve noticed dropped chunks when the encoder work gets too heavy. So keep the same SpikeInterface-style Blosc configuration (zstd + BITSHUFFLE) but dial the compression strength back to the largest level that is still *one notch below* what I originally considered, so writes stay responsive while still improving size a bit over the baseline. Return the configured compressor.", "answers": "[{\"name\": \"spikeinterface_core_zarrextractors_get_default_zarr_compressor\", \"arguments\": {\"clevel\": 6}}]"}
{"func_name": "spikeinterface_curation_auto_merge_estimate_contamination", "func_desc": "Estimate the contamination of a spike train by counting refractory period violations and converting that count into a contamination fraction used in spike sorting curation.", "tools": [{"function": {"description": "Estimate the contamination of a spike train by counting refractory period violations and converting that count into a contamination fraction used in spike sorting curation.\n", "name": "spikeinterface_curation_auto_merge_estimate_contamination", "parameters": {"properties": {"spike_train": {"type": "array", "items": {"type": "any"}, "description": "The unit's spike train as a 1-D numpy array of spike times expressed in sample indices (integer sample timestamps). In the context of SpikeInterface (a unified framework for spike sorting), this array represents the detected spike events for one unit. The function will cast this array to numpy.int64 internally before computing violations; the original array is not modified.", "default": ""}, "sf": {"type": "float", "description": "The sampling frequency (in Hz) of the spike train. This value is used to convert refractory period limits given in milliseconds to sample counts (t_c and t_r are computed as refractory_period[*] * 1e-3 * sf). sf must be provided in the same temporal units expected for conversion to samples.", "default": ""}, "T": {"type": "integer", "description": "The duration of the spike train in samples (integer). This is the total number of samples in the recording epoch used to normalize the contamination estimate; providing an incorrect T (for example smaller than the largest spike index) will yield an incorrect estimate or may trigger runtime errors.", "default": ""}, "refractory_period": {"type": "tuple", "prefixItems": [{"type": "float"}, {"type": "float"}], "description": "The censored and refractory period (t_c, t_r) used, expressed in milliseconds. The first element is the censored time t_c (ms) and the second is the refractory time t_r (ms). These two values are converted to sample counts via sf and are used to define the time windows for counting violations and computing the contamination metric.", "default": ""}}, "required": ["spike_train", "sf", "T", "refractory_period"], "type": "any"}}, "type": "function"}], "query": "I’m doing spike-sorting curation on a single putative unit and want a contamination estimate from refractory period violations. Use the spike train below (sample indices), recorded at 30 kHz over a 10-minute session. Initially I was going to use the two refractory windows I wrote down (a shorter censor window and a longer “true” refractory window), but I just realized those were specified in milliseconds for the 20 kHz rig—we ran this session at 30 kHz, so don’t keep the window sizes in *samples* from that note. Keep the *time* windows the same in milliseconds, and convert appropriately for this dataset before estimating contamination. Also, total duration is T=18,000,000 samples.\n\nSpike train:\n[1200, 45000, 90010, 135050, 180000, 225020, 225150, 270000, 315000, 360000, 405000, 450000, 495000, 540000, 585000, 630000, 675000, 720000, 765000, 810000, 855000, 900000, 945000, 990000, 1035000, 1080000, 1125000, 1170000, 1215000, 1260000, 1305000, 1350000, 1395000, 1440000, 1485000, 1530000, 1575000, 1620000, 1665000, 1710000]\n\nUse the same censor and refractory durations as in the note (0.5 ms and 1.5 ms), but ensure they’re interpreted as milliseconds (not samples) at 30 kHz when calling the contamination estimator.", "answers": "[{\"name\":\"spikeinterface_curation_auto_merge_estimate_contamination\",\"arguments\":{\"spike_train\":[1200,45000,90010,135050,180000,225020,225150,270000,315000,360000,405000,450000,495000,540000,585000,630000,675000,720000,765000,810000,855000,900000,945000,990000,1035000,1080000,1125000,1170000,1215000,1260000,1305000,1350000,1395000,1440000,1485000,1530000,1575000,1620000,1665000,1710000],\"sf\":30000,\"T\":18000000,\"refractory_period\":[0.5,1.5]}}]"}
{"func_name": "spikeinterface_curation_auto_merge_get_unit_adaptive_window", "func_desc": "Computes an adaptive window size from a correlogram for unit curation/auto-merge workflows in SpikeInterface. This function identifies the first relevant peak in the correlogram (interpreted as the first peak nearest to the center) by locating peaks in the negative second derivative of the correlogram, filtering those peaks by a minimum amplitude threshold, and selecting the last qualifying peak before the correlogram center. The resulting window size is computed as the distance (in array indices / samples) from that peak to the correlogram center and is intended to be used by downstream curation logic (for example, to define the temporal window for deciding whether two units should be merged based on their cross-correlogram).", "tools": [{"function": {"description": "Computes an adaptive window size from a correlogram for unit curation/auto-merge workflows in SpikeInterface. This function identifies the first relevant peak in the correlogram (interpreted as the first peak nearest to the center) by locating peaks in the negative second derivative of the correlogram, filtering those peaks by a minimum amplitude threshold, and selecting the last qualifying peak before the correlogram center. The resulting window size is computed as the distance (in array indices / samples) from that peak to the correlogram center and is intended to be used by downstream curation logic (for example, to define the temporal window for deciding whether two units should be merged based on their cross-correlogram).\n", "name": "spikeinterface_curation_auto_merge_get_unit_adaptive_window", "parameters": {"properties": {"auto_corr": {"type": "array", "items": {"type": "any"}, "description": "1-D numpy array containing the correlogram values used to compute the adaptive window. The correlogram is expected to be centered such that the central bin index is auto_corr.shape[0] // 2. Values represent correlogram amplitudes (e.g., counts or normalized correlation) across time lags. This array is processed by computing its second derivative (via numpy.gradient) and finding peaks in the negated second derivative using scipy.signal.find_peaks; incorrect dimensionality (non-1D) or non-numeric contents may produce exceptions or undefined behavior.", "default": ""}, "threshold": {"type": "float", "description": "Minimum amplitude threshold used to filter candidate peaks in the correlogram. Only peaks whose correlogram amplitude at the peak index is greater than or equal to this threshold are kept. If no peaks meet this threshold, the function will recursively retry with threshold / 2 until either a peak is found or the threshold falls below 1e-5. The threshold represents an absolute amplitude cutoff on auto_corr values as used in curation decisions.", "default": ""}}, "required": ["auto_corr", "threshold"], "type": "any"}}, "type": "function"}], "query": "I’m running SpikeInterface auto-merge on a unit, and I need the adaptive coincidence window from its autocorrelogram. Initially I was going to use the center bin at index 10, but I realized my correlogram array has an odd length, so the true center is the middle element (i.e., the index that splits the array into equal left/right halves). Use the correlogram values [-0.02, 0.00, 0.05, 0.10, 0.18, 0.28, 0.45, 0.60, 0.55, 0.40, 0.30, 0.38, 0.50, 0.62, 0.48, 0.30, 0.20, 0.12, 0.06, 0.02, 0.00] and compute the adaptive window using a peak threshold that is stricter than 0.35 by exactly 0.05 (to suppress small curvature artifacts in this recording).", "answers": "[{\"name\":\"spikeinterface_curation_auto_merge_get_unit_adaptive_window\",\"arguments\":{\"auto_corr\":[-0.02,0.0,0.05,0.1,0.18,0.28,0.45,0.6,0.55,0.4,0.3,0.38,0.5,0.62,0.48,0.3,0.2,0.12,0.06,0.02,0.0],\"threshold\":0.4}}]"}
{"func_name": "spikeinterface_postprocessing_localization_tools_make_shells", "func_desc": "Get neighbor-channel \"shells\" for each electrode channel based on inter-channel distances.\n    \n    This function is used in spike sorting post-processing (spikeinterface.postprocessing.localization_tools) to determine, for every recording channel, which other channels lie within successive nearest-neighbor distance shells. In practical spike-sorting workflows this is useful to restrict computations (for example waveform extraction, localization, template updates, or quality metric calculations) to channels that are spatially close to a given channel. The radius that defines each shell is computed from the empirical distances between channels: first the distance to the closest other channel, then the next distinct larger distance, and so on, up to n_jumps distinct distance levels.", "tools": [{"function": {"description": "Get neighbor-channel \"shells\" for each electrode channel based on inter-channel distances.\n\nThis function is used in spike sorting post-processing (spikeinterface.postprocessing.localization_tools) to determine, for every recording channel, which other channels lie within successive nearest-neighbor distance shells. In practical spike-sorting workflows this is useful to restrict computations (for example waveform extraction, localization, template updates, or quality metric calculations) to channels that are spatially close to a given channel. The radius that defines each shell is computed from the empirical distances between channels: first the distance to the closest other channel, then the next distinct larger distance, and so on, up to n_jumps distinct distance levels.", "name": "spikeinterface_postprocessing_localization_tools_make_shells", "parameters": {"properties": {"geom": {"type": "array", "items": {"type": "any"}, "description": "Array of channel coordinates. Expected to have shape (n_channels, n_dimensions) where each row gives the spatial coordinate of one electrode channel. The function uses these coordinates to compute pairwise Euclidean distances between channels and to determine the successive nearest-neighbor distance thresholds. The channel ordering (row index) defines the channel indices returned in the output; indices are zero-based and correspond to the rows of geom.", "default": ""}, "n_jumps": {"type": "integer", "description": "Number of successive nearest-neighbor distance levels (shells) to include for each channel. A value of 1 includes only the channels at the minimum nonzero distance from the reference channel (the immediate neighbors). A value of 2 includes those and also the channels at the next larger distinct distance, and so on. Must be a positive integer (>= 1). If n_jumps exceeds the number of distinct nonzero inter-channel distance levels, the returned shells for a channel will include all other channels (i.e., the shell grows until no further distinct distances remain).", "default": 1}}, "required": ["geom", "n_jumps"], "type": "any"}}, "type": "function"}], "query": "I’m doing spike-sorting post-processing on an 8‑channel 2D probe (µm) with geometry [[0,0],[20,0],[40,0],[0,20],[20,20],[40,20],[0,40],[20,40]]. I initially planned to compute neighbor-channel shells using the first 3 distinct nearest-neighbor distance levels, but I realized a boundary-channel artifact: the last shell would pull in channels that are only reachable at a distance equal to the longest edge-to-edge separation present in this layout, which makes downstream localization comparisons unfair across channels. So instead, compute shells only up to the largest distance level that is still strictly less than that maximum separation, keeping everything else the same.", "answers": "[{\"name\":\"spikeinterface_postprocessing_localization_tools_make_shells\",\"arguments\":{\"geom\":[[0,0],[20,0],[40,0],[0,20],[20,20],[40,20],[0,40],[20,40]],\"n_jumps\":2}}]"}
{"func_name": "spikeinterface_preprocessing_detect_bad_channels_detrend", "func_desc": "spikeinterface.preprocessing.detect_bad_channels.detrend subtracts a median-filtered trend from a 1-D signal (vector) using endpoint tapering. This function is used in spike preprocessing (for example, in detect_bad_channels) to remove slowly varying baseline or trend from an extracellular recording channel prior to computing metrics for bad-channel detection or other downstream analyses.", "tools": [{"function": {"description": "spikeinterface.preprocessing.detect_bad_channels.detrend subtracts a median-filtered trend from a 1-D signal (vector) using endpoint tapering. This function is used in spike preprocessing (for example, in detect_bad_channels) to remove slowly varying baseline or trend from an extracellular recording channel prior to computing metrics for bad-channel detection or other downstream analyses.\n", "name": "spikeinterface_preprocessing_detect_bad_channels_detrend", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "any"}, "description": "Input 1-D array (vector) containing the sampled signal for a single channel or trace. In the SpikeInterface preprocessing context, this typically represents a time series of extracellular voltage values for one channel. The function treats x as read-only and returns a new array; it does not modify x in place. x must be non-empty because the implementation accesses x[0] and x[-1].", "default": ""}, "nmed": {"type": "integer", "description": "Number of points used for the median filter kernel (median filter length). This integer controls the time scale of the trend that is removed: larger values remove slower-varying trends. There is no default; the caller must supply a positive integer. The implementation computes ntap = ceil(nmed / 2) and pads the signal by repeating the first and last samples ntap times before applying scipy.signal.medfilt with kernel size nmed.", "default": ""}}, "required": ["x", "nmed"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a single-channel extracellular snippet for bad-channel detection, but I just realized the detrending step in our pipeline uses a *median* trend and the kernel must be an odd length. I initially planned to use the same kernel length as the number of unique ADC codes in this short segment, but that count includes one duplicated code due to quantization, so it would make the kernel even and invalid. Instead, set the median-filter kernel length to the nearest valid odd length *below* that unique-code count, and detrend (median-trend subtraction with endpoint tapering) this 1-D trace before computing metrics:\n\nx = [12.0, 12.5, 13.0, 13.8, 14.2, 14.0, 13.6, 13.1, 12.7, 12.2].", "answers": "[{\"name\":\"spikeinterface_preprocessing_detect_bad_channels_detrend\",\"arguments\":{\"x\":[12.0,12.5,13.0,13.8,14.2,14.0,13.6,13.1,12.7,12.2],\"nmed\":9}}]"}
{"func_name": "spikeinterface_preprocessing_motion_get_motion_parameters_preset", "func_desc": "Get the parameters tree for a named preset used by the motion-correction steps in the preprocessing pipeline.\n    \n    This function is part of SpikeInterface's preprocessing.motion utilities and returns a fully-resolved parameters dictionary (a \"parameters tree\") for a given preset name. In the spike sorting domain, motion correction is a preprocessing stage that compensates for probe and tissue movement by running a sequence of algorithmic steps (for example: selecting peaks, estimating drift, splitting/merging segments). Each step can have multiple methods and method-specific parameters. Presets are high-level named configurations that select methods and override a subset of parameters for those steps. This function takes the named preset, deep-copies the preset definition from the internal registry, merges it with the framework's default motion parameters for each step and method, and returns the resulting parameter tree that the motion-correction pipeline consumes to configure and run each step.", "tools": [{"function": {"description": "Get the parameters tree for a named preset used by the motion-correction steps in the preprocessing pipeline.\n\nThis function is part of SpikeInterface's preprocessing.motion utilities and returns a fully-resolved parameters dictionary (a \"parameters tree\") for a given preset name. In the spike sorting domain, motion correction is a preprocessing stage that compensates for probe and tissue movement by running a sequence of algorithmic steps (for example: selecting peaks, estimating drift, splitting/merging segments). Each step can have multiple methods and method-specific parameters. Presets are high-level named configurations that select methods and override a subset of parameters for those steps. This function takes the named preset, deep-copies the preset definition from the internal registry, merges it with the framework's default motion parameters for each step and method, and returns the resulting parameter tree that the motion-correction pipeline consumes to configure and run each step.", "name": "spikeinterface_preprocessing_motion_get_motion_parameters_preset", "parameters": {"properties": {"preset": {"type": "string", "description": "The preset name to resolve into a parameters tree. The string must be a key present in the internal preset registry (motion_options_preset) and typically corresponds to a predefined motion-correction configuration provided by SpikeInterface. The original implementation documented a default of None; in practice the function expects a valid preset name and will raise a KeyError if the provided name is not found in the preset registry. Use spikeinterface.preprocessing.get_motion_presets() to list available preset names. The preset controls which methods are selected for each motion-correction step and any preset-specific parameter overrides; this function will merge those overrides with the framework defaults for each selected method.", "default": ""}}, "required": ["preset"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up motion correction in a SpikeInterface preprocessing pipeline and wanted to start by inspecting the fully-resolved motion-parameter tree for the preset that mimics the classic Kilosort-style workflow. Wait—my downstream comparison script keys off the preset name string and only accepts the built-in preset whose name is the same as that Kilosort-like preset but written in all-caps (it’s a case-sensitive registry lookup). Please resolve and return the full motion-parameter tree for that case-variant preset so I can verify which methods and parameter overrides it selects at each step.", "answers": "[{\"name\": \"spikeinterface_preprocessing_motion_get_motion_parameters_preset\", \"arguments\": {\"preset\": \"KILOSORT_LIKE\"}}]"}
{"func_name": "spikeinterface_preprocessing_phase_shift_apply_frequency_shift", "func_desc": "Apply a sub-sample-accurate frequency (phase) shift to a multi-channel signal buffer.\n    \n    This function is intended for preprocessing extracellular recordings in SpikeInterface prior to spike sorting or waveform extraction. It shifts each channel by a (possibly fractional) number of samples using the Fourier shift theorem: the signal is transformed to the frequency domain with a real FFT (rFFT), a complex phase rotation that corresponds to the requested time shift is applied per-frequency-bin and per-channel, and the result is transformed back to the time domain with an inverse real FFT (irFFT). This produces time shifts that are accurate below the sampling-period resolution and so are useful for aligning channels, correcting propagation delays between channels, or fine temporal registration of multichannel recordings before downstream spike-sorting, quality-metric computation, or waveform extraction.", "tools": [{"function": {"description": "Apply a sub-sample-accurate frequency (phase) shift to a multi-channel signal buffer.\n\nThis function is intended for preprocessing extracellular recordings in SpikeInterface prior to spike sorting or waveform extraction. It shifts each channel by a (possibly fractional) number of samples using the Fourier shift theorem: the signal is transformed to the frequency domain with a real FFT (rFFT), a complex phase rotation that corresponds to the requested time shift is applied per-frequency-bin and per-channel, and the result is transformed back to the time domain with an inverse real FFT (irFFT). This produces time shifts that are accurate below the sampling-period resolution and so are useful for aligning channels, correcting propagation delays between channels, or fine temporal registration of multichannel recordings before downstream spike-sorting, quality-metric computation, or waveform extraction.", "name": "spikeinterface_preprocessing_phase_shift_apply_frequency_shift", "parameters": {"properties": {"signal": {"type": "array", "items": {"type": "any"}, "description": "Input real-valued signal array to be shifted. This array contains the time samples along the axis specified by the axis parameter and the channel layout on the remaining axes. For typical SpikeInterface preprocessing, signal is a 2D array with shape (num_time_samples, num_channels) and dtype compatible with scipy.fft.rfft/irfft. The function will perform an rFFT along the specified axis to compute the frequency-domain representation.", "default": ""}, "shift_samples": {"type": "array", "items": {"type": "any"}, "description": "1-D array of sample shifts, one entry per channel. Each value is the desired shift for the corresponding channel expressed in units of samples; fractional values are allowed to request sub-sample shifts. Conceptually, the time shift in seconds equals shift_samples / sampling_rate (sampling_rate is not an argument to this function and must be handled by the caller). The array length (shift_samples.size) must match the number of channels in signal along the non-shift axis (for axis=0 this means the second dimension: signal.shape[1] when signal is 2D). If sizes do not match, NumPy broadcasting or elementwise multiplication will fail and a ValueError (or related broadcasting exception) can be raised.", "default": ""}, "axis": {"type": "integer", "description": "Axis along which to perform the shift. Default is 0. Currently, only axis=0 is supported by this implementation and passing any other value will cause a NotImplementedError. The axis specifies the time/sample axis over which the rFFT/irFFT are computed and therefore which axis is translated in time by shift_samples.", "default": 0}}, "required": ["signal", "shift_samples", "axis"], "type": "any"}}, "type": "function"}], "query": "I’m preprocessing a short 8-sample, 3-channel extracellular snippet (time x channels) in SpikeInterface and need sub-sample delay correction via Fourier phase rotation along the time axis (axis=0). I initially planned to apply the measured per-channel shifts directly, but I realized my sign convention was flipped (my cross-correlation delays were computed as ‘channel leads reference’ rather than ‘channel lags reference’), so the correction must use the opposite sign for each channel. Apply the phase-based time shift to the buffer below using that corrected sign convention (keep the same magnitudes as originally measured for each channel).\n\nBuffer:\n[[12.0, -3.0, 5.0],\n [10.0, -2.5, 4.2],\n [8.0, -2.0, 3.5],\n [6.0, -1.5, 2.8],\n [4.0, -1.0, 2.1],\n [2.0, -0.5, 1.4],\n [0.0, 0.0, 0.7],\n [-2.0, 0.5, 0.0]]", "answers": "[{\"name\":\"spikeinterface_preprocessing_phase_shift_apply_frequency_shift\",\"arguments\":{\"signal\":[[12.0,-3.0,5.0],[10.0,-2.5,4.2],[8.0,-2.0,3.5],[6.0,-1.5,2.8],[4.0,-1.0,2.1],[2.0,-0.5,1.4],[0.0,0.0,0.7],[-2.0,0.5,0.0]],\"shift_samples\":[-0.25,0.5,-1.0],\"axis\":0}}]"}
{"func_name": "spikeinterface_qualitymetrics_misc_metrics_presence_ratio", "func_desc": "Calculate the presence ratio for a single unit across a recording by dividing the number of temporal bins in which the unit is \"active\" by the total number of temporal bins. This metric is used in spike sorting quality assessment (see SpikeInterface quality metrics) to quantify how consistently a sorted unit is present throughout the recording: values close to 1 indicate the unit fires across most of the recording, values close to 0 indicate the unit is only present in a small fraction of the recording.", "tools": [{"function": {"description": "Calculate the presence ratio for a single unit across a recording by dividing the number of temporal bins in which the unit is \"active\" by the total number of temporal bins. This metric is used in spike sorting quality assessment (see SpikeInterface quality metrics) to quantify how consistently a sorted unit is present throughout the recording: values close to 1 indicate the unit fires across most of the recording, values close to 0 indicate the unit is only present in a small fraction of the recording.\n", "name": "spikeinterface_qualitymetrics_misc_metrics_presence_ratio", "parameters": {"properties": {"spike_train": {"type": "array", "items": {"type": "any"}, "description": "1-D array of spike times for this unit, expressed in samples. These are the event times used to assign spikes to temporal bins; they do not need to be sorted. In the spike-sorting domain this represents the sample indices of detected spikes for a single unit produced by a sorter or post-processing step.", "default": ""}, "total_length": {"type": "integer", "description": "Total length of the recording in samples. This parameter documents the expected recording duration (in samples) for the spike_train and is part of the function API in SpikeInterface quality metrics; note that the current implementation does not use this value internally but callers should supply the recording length in samples for API consistency and future compatibility.", "default": ""}, "bin_edges": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional. An explicit array of bin edge positions (in samples) to use to partition the recording into temporal bins. Mutually exclusive with num_bin_edges. If provided, these edges are passed directly to numpy.histogram and the effective number of histogram bins is len(bin_edges) - 1. Values in spike_train that fall outside the provided edges are not counted in the returned histogram bins (consistent with numpy.histogram behavior). Providing bin_edges allows callers to define uneven or externally computed bin boundaries (for example, to align bins to behavioral epochs).", "default": null}, "num_bin_edges": {"type": "integer", "nullable": true, "description": "Optional. The number of bin edges to use to compute the presence ratio (mutually exclusive with bin_edges). Interpreted as the number of edges, so the effective number of histogram bins used is num_bin_edges - 1. If bin_edges is not provided, num_bin_edges is passed to numpy.histogram as the bins argument (an integer), which causes numpy to compute an even partitioning across the range of spike times. This parameter controls the temporal resolution of the presence ratio: larger num_bin_edges (hence more bins) yields a finer-grained assessment of presence across time.", "default": null}, "bin_n_spikes_thres": {"type": "integer", "description": "Minimum number of spikes required within a bin for that bin to be considered \"active\" (default: 0). In practice, with the default 0 a bin is considered active if it contains at least one spike; setting this to a larger integer makes the presence ratio stricter by requiring more spikes within a bin to count it as presence. Must be >= 0.", "default": 0}}, "required": ["spike_train", "total_length", "bin_n_spikes_thres", "num_bin_edges"], "type": "any"}}, "type": "function"}], "query": "I’m computing the SpikeInterface-style presence ratio for unit #7 across a 10‑minute recording sampled at 30 kHz (so the recording spans 18,000,000 samples). Start by binning the spike train into 60 equal temporal bins using evenly spaced bin edges across the full recording (so the number of bin edges is one more than the number of bins). Initially I was going to count a bin as ‘active’ only if it has at least two spikes, but I realized that with this unit’s low firing rate that criterion would undercount presence; instead, use the *most permissive nonzero-activity criterion* so that any bin with at least one spike counts as active. Also, before binning, drop any spikes that are outside the valid sample index range for this recording (i.e., not within the recording duration). Given the spike times (in samples): [1500, 29990, 60010, 120000, 450000, 900000, 1200500, 2400000, 3600000, 5400000, 7200000, 9000000, 10800000, 12600000, 14400000, 16200000, 17900000], compute the presence ratio (active_bins / total_bins).", "answers": "[{\"name\":\"spikeinterface_qualitymetrics_misc_metrics_presence_ratio\",\"arguments\":{\"spike_train\":[1500,29990,60010,120000,450000,900000,1200500,2400000,3600000,5400000,7200000,9000000,10800000,12600000,14400000,16200000,17900000],\"total_length\":18000000,\"num_bin_edges\":61,\"bin_n_spikes_thres\":1}}]"}
{"func_name": "spikeinterface_qualitymetrics_pca_metrics_lda_metrics", "func_desc": "spikeinterface.qualitymetrics.pca_metrics.lda_metrics computes a d-prime separability measure for a single sorted unit using Linear Discriminant Analysis (LDA). This function is used within the SpikeInterface quality-metrics workflow to quantify how well the principal-component (PC) feature distribution of spikes assigned to a target unit (this_unit_id) is separated from the distribution of all other spikes; a larger d-prime indicates better separability and therefore higher presumed unit quality during spike sorting validation and curation.", "tools": [{"function": {"description": "spikeinterface.qualitymetrics.pca_metrics.lda_metrics computes a d-prime separability measure for a single sorted unit using Linear Discriminant Analysis (LDA). This function is used within the SpikeInterface quality-metrics workflow to quantify how well the principal-component (PC) feature distribution of spikes assigned to a target unit (this_unit_id) is separated from the distribution of all other spikes; a larger d-prime indicates better separability and therefore higher presumed unit quality during spike sorting validation and curation.\n", "name": "spikeinterface_qualitymetrics_pca_metrics_lda_metrics", "parameters": {"properties": {"all_pcs": {"type": "array", "items": {"type": "any"}, "description": "A 2-D array of principal-component scores for every spike in the recording, organized as [num_spikes, num_PCs]. In the SpikeInterface workflow this array is typically produced by applying PCA to extracted spike waveforms and is used here as the feature matrix X for LDA. The function requires a numeric numpy.ndarray; shape must be (N, P) where N is the number of spikes and P is the number of retained PCs. If this array is not two-dimensional or its first dimension does not match the length of all_labels, the function will produce an error or invalid results.", "default": ""}, "all_labels": {"type": "array", "items": {"type": "any"}, "description": "A 1-D array of integer cluster labels for each spike, with length equal to the number of spikes N. These labels come from a spike-sorting output (the cluster assignments to be evaluated). The function builds a boolean target mask by comparing all_labels to this_unit_id; therefore all_labels must be indexable and align elementwise with all_pcs rows. If lengths mismatch or the array is not 1-D, the behavior is undefined and an exception may be raised.", "default": ""}, "this_unit_id": {"type": "integer", "description": "The integer identifier of the unit (cluster) to evaluate. This identifier is compared against all_labels to create the positive class for LDA. In practice this is the unit ID returned by a spike sorting algorithm or a SortingExtractor; the function measures how distinct the PC features of spikes labeled this_unit_id are from all other spikes.", "default": ""}}, "required": ["all_pcs", "all_labels", "this_unit_id"], "type": "any"}}, "type": "function"}], "query": "I’m curating a spike sorting output and I initially planned to compute LDA d-prime separability for unit 7 using the first 3 PCA features exactly as exported. But I just noticed the PCA export includes a diagnostic column at the end (a non-PC value) that got concatenated to the PC matrix. To avoid contaminating the LDA, please compute the LDA-based separability metric for unit 7 using only the true PC features (i.e., drop the extra trailing column) given the PCA-like scores and cluster labels below.\n\nPCA-like scores (rows = spikes):\n[\n  [0.12, -1.05, 0.33, 999],\n  [0.18, -0.98, 0.41, 999],\n  [0.09, -1.12, 0.29, 999],\n  [2.31, 0.44, -0.87, 999],\n  [2.18, 0.51, -0.92, 999],\n  [2.26, 0.39, -0.81, 999],\n  [-1.44, 1.22, 0.15, 999],\n  [-1.51, 1.18, 0.11, 999],\n  [-1.39, 1.27, 0.19, 999],\n  [0.15, -1.01, 0.36, 999],\n  [2.22, 0.48, -0.89, 999],\n  [-1.47, 1.25, 0.13, 999]\n]\n\nCluster labels:\n[7, 7, 7, 3, 3, 3, 12, 12, 12, 7, 3, 12]\n\nTarget unit: 7", "answers": "[{\"name\": \"spikeinterface_qualitymetrics_pca_metrics_lda_metrics\", \"arguments\": {\"all_pcs\": [[0.12, -1.05, 0.33], [0.18, -0.98, 0.41], [0.09, -1.12, 0.29], [2.31, 0.44, -0.87], [2.18, 0.51, -0.92], [2.26, 0.39, -0.81], [-1.44, 1.22, 0.15], [-1.51, 1.18, 0.11], [-1.39, 1.27, 0.19], [0.15, -1.01, 0.36], [2.22, 0.48, -0.89], [-1.47, 1.25, 0.13]], \"all_labels\": [7, 7, 7, 3, 3, 3, 12, 12, 12, 7, 3, 12], \"this_unit_id\": 7}}]"}
{"func_name": "spikeinterface_sorters_container_tools_find_recording_folders", "func_desc": "spikeinterface.sorters.container_tools.find_recording_folders finds the minimal set of filesystem folders that contain recording file paths described in a SpikeInterface-style dictionary and prepares them for use as container mount points.\n    \n    This function is used in the container tooling of SpikeInterface (a framework to run spike sorters in Docker/Singularity containers) to determine which host folders need to be mounted into a container so the sorter can access the recording files. It extracts file paths from the provided dictionary using the internal helper _get_paths_list, resolves each path, reduces them to their parent directories, and attempts to compute a single common parent folder when possible to minimize the number of mounts. The function performs only path computations and does not perform any I/O, create mounts, or modify the input dictionary.", "tools": [{"function": {"description": "spikeinterface.sorters.container_tools.find_recording_folders finds the minimal set of filesystem folders that contain recording file paths described in a SpikeInterface-style dictionary and prepares them for use as container mount points.\n\nThis function is used in the container tooling of SpikeInterface (a framework to run spike sorters in Docker/Singularity containers) to determine which host folders need to be mounted into a container so the sorter can access the recording files. It extracts file paths from the provided dictionary using the internal helper _get_paths_list, resolves each path, reduces them to their parent directories, and attempts to compute a single common parent folder when possible to minimize the number of mounts. The function performs only path computations and does not perform any I/O, create mounts, or modify the input dictionary.", "name": "spikeinterface_sorters_container_tools_find_recording_folders", "parameters": {"properties": {"d": {"type": "any", "description": "A dictionary containing recording metadata and file paths in the format expected by SpikeInterface container utilities. The helper _get_paths_list(d=d) is called to extract a list of raw file-system paths (strings) from this dictionary. The caller is responsible for providing a dictionary where recording file paths can be found by that helper; if _get_paths_list raises an exception for an unexpected structure or missing keys, this function will not catch it.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "I’m about to containerize a SpikeInterface spike-sorting run, and I initially assumed I could just mount the top-level `/data` folder to keep things simple. But that’s not acceptable on our cluster because mounts must be as narrow as possible (only what’s needed for the recording), and we also discovered one of the paths in the recording dict is a leftover artifact that points to a derived analysis file rather than a raw recording dependency.\n\nGiven the following SpikeInterface-style recording dictionary, compute the minimal set of host folders that must be used as container mount points **after**:\n1) resolving paths and reducing them to their parent directories,\n2) collapsing to a single common parent folder *only if* it doesn’t broaden access beyond what’s necessary,\n3) excluding the artifact path (it’s the only one that is not a raw acquisition dependency and lives alongside raw data but is not needed by the extractor).\n\nRecording dictionary:\n{\n  \"recording\": {\n    \"extractor\": \"BinaryRecordingExtractor\",\n    \"kwargs\": {\n      \"file_paths\": [\n        \"/data/neuropixels/session_2025-05-14/recording.bin\",\n        \"/data/neuropixels/session_2025-05-14/channel_positions.npy\",\n        \"/data/neuropixels/session_2025-05-15/recording.bin\"\n      ],\n      \"sampling_frequency\": 30000.0,\n      \"num_channels\": 384\n    }\n  }\n}", "answers": "[{\"name\":\"spikeinterface_sorters_container_tools_find_recording_folders\",\"arguments\":{\"d\":{\"recording\":{\"extractor\":\"BinaryRecordingExtractor\",\"kwargs\":{\"file_paths\":[\"/data/neuropixels/session_2025-05-14/recording.bin\",\"/data/neuropixels/session_2025-05-14/channel_positions.npy\",\"/data/neuropixels/session_2025-05-15/recording.bin\"],\"sampling_frequency\":30000.0,\"num_channels\":384}}}}}]"}
{"func_name": "spikeinterface_sorters_sorterlist_get_default_sorter_params", "func_desc": "Returns the default parameter dictionary for a given spike sorter implementation, resolving the sorter either from a registered name or from a Sorter class. This function is part of the SpikeInterface unified framework for spike sorting and is used to obtain canonical runtime configuration values that downstream code (sorting pipelines, benchmarking, GUIs, containerized runs) rely on to run a sorter with its default settings.", "tools": [{"function": {"description": "Returns the default parameter dictionary for a given spike sorter implementation, resolving the sorter either from a registered name or from a Sorter class. This function is part of the SpikeInterface unified framework for spike sorting and is used to obtain canonical runtime configuration values that downstream code (sorting pipelines, benchmarking, GUIs, containerized runs) rely on to run a sorter with its default settings.\n", "name": "spikeinterface_sorters_sorterlist_get_default_sorter_params", "parameters": {"properties": {"sorter_name_or_class": {"type": "string", "description": "Identifier of the sorter whose default parameters are requested. This argument can be either:\n- a string key that must exist in the module-level sorter_dict mapping (the registered name for a sorter implementation), or\n- a SorterClass object that must be present in the module-level sorter_full_list (the class implementing a sorter).\nThe value is used to resolve the concrete SorterClass and then obtain its defaults. In the spike sorting domain, callers pass this to retrieve the canonical configuration for a specific sorter implementation so they can run the sorter, display defaults in a user interface, or compare default settings across sorters.", "default": ""}}, "required": ["sorter_name_or_class"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a SpikeInterface sorter-selection UI that should show the *canonical* default config the same way our containerized runner will see it. Initially I was going to query defaults by the registered name for the Kilosort2 entry, but I just realized our lab fork has a wrapper class registered under multiple aliases and the display must be tied to the exact sorter implementation object we import (to avoid picking the wrong alias). So: retrieve the default parameter dictionary by passing the Sorter **class object** corresponding to the same implementation that would be resolved by the Kilosort2 registered entry, and return the defaults exactly as SpikeInterface reports them.", "answers": "[{\"name\": \"spikeinterface_sorters_sorterlist_get_default_sorter_params\", \"arguments\": {\"sorter_name_or_class\": \"Kilosort2Sorter\"}}]"}
{"func_name": "spikeinterface_sorters_sorterlist_get_sorter_params_description", "func_desc": "Returns a description of the parameters supported by a specific spike sorter.\n    \n    This function is part of SpikeInterface, a unified framework for spike sorting (see README). It retrieves the parameter documentation that a given sorter exposes so callers (CLI, GUIs, workflow code, container wrappers, or benchmarking scripts) can present, validate, or programmatically set the parameters needed to run that sorter. The function accepts either the registered sorter name or the Sorter class object itself and returns the sorter-provided parameters description without modifying global state.", "tools": [{"function": {"description": "Returns a description of the parameters supported by a specific spike sorter.\n\nThis function is part of SpikeInterface, a unified framework for spike sorting (see README). It retrieves the parameter documentation that a given sorter exposes so callers (CLI, GUIs, workflow code, container wrappers, or benchmarking scripts) can present, validate, or programmatically set the parameters needed to run that sorter. The function accepts either the registered sorter name or the Sorter class object itself and returns the sorter-provided parameters description without modifying global state.", "name": "spikeinterface_sorters_sorterlist_get_sorter_params_description", "parameters": {"properties": {"sorter_name_or_class": {"type": "string", "description": "Identifier of the sorter whose parameters description is requested. If a string is provided, it must be the key of a sorter registered in the module-level sorter_dict registry (the canonical name used by SpikeInterface to reference available sorters). If a SorterClass object is provided, it must be one of the classes listed in the module-level sorter_full_list registry. This argument determines which SorterClass.params_description() method is invoked to obtain the description. Supplying the sorter name is the typical usage when code or user interfaces allow selecting a sorter by name; supplying the SorterClass is useful when the caller already has the class object (for example when iterating over sorter classes).", "default": ""}}, "required": ["sorter_name_or_class"], "type": "any"}}, "type": "function"}], "query": "In my SpikeInterface benchmarking script I’m building a generic “preflight” panel that shows the help/tooltip text for every configurable option of whichever sorter I’m about to benchmark. I initially hard-coded the registered sorter name as a string, but I just realized the benchmarking harness sometimes passes me the sorter as the actual Sorter class object (because it’s already imported and validated earlier), and I don’t want this codepath to depend on registry lookups that could vary across environments. So: given that I already have the sorter class corresponding to the same algorithm family as the commonly used GPU-based sorter I was going to run, fetch the parameter descriptions using the class object interface instead of the name-string interface, and return whatever the sorter exposes (no global state changes).", "answers": "[{\"name\": \"spikeinterface_sorters_sorterlist_get_sorter_params_description\", \"arguments\": {\"sorter_name_or_class\": \"Kilosort2Sorter\"}}]"}
{"func_name": "stk_utilities_utilities_get_projection", "func_desc": "Get the projection of start onto target.\n    \n    Compute the orthogonal projection of the vector represented by start onto the vector represented by target using the linear-algebra formula\n    target * (np.dot(start, target) / np.dot(target, target)).\n    In the stk domain this routine is used when manipulating molecular geometry or vector quantities (for example, projecting an atomic displacement, an orientation vector, or a placement direction onto another direction when aligning building blocks, positioning functional groups, or extracting directional components of properties during molecular construction and analysis).", "tools": [{"function": {"description": "Get the projection of start onto target.\n\nCompute the orthogonal projection of the vector represented by start onto the vector represented by target using the linear-algebra formula\ntarget * (np.dot(start, target) / np.dot(target, target)).\nIn the stk domain this routine is used when manipulating molecular geometry or vector quantities (for example, projecting an atomic displacement, an orientation vector, or a placement direction onto another direction when aligning building blocks, positioning functional groups, or extracting directional components of properties during molecular construction and analysis).", "name": "stk_utilities_utilities_get_projection", "parameters": {"properties": {"start": {"type": "array", "items": {"type": "float"}, "description": "Numeric array holding the vector to be projected. In practical stk use this commonly represents a coordinate difference, displacement vector, or other per-atom/per-feature vector whose component along target is required. Must be a numpy.ndarray for which np.dot(start, target) is defined; if it is not (for example, due to incompatible shapes or non-numeric contents) numpy will raise an error.", "default": ""}, "target": {"type": "array", "items": {"type": "float"}, "description": "Numeric array holding the vector onto which start is projected. In stk workflows this typically represents an axis, bond direction, or placement/orientation vector used to align or decompose other vectors. Must be a numpy.ndarray for which np.dot(start, target) and np.dot(target, target) are defined. If target is the zero vector (so np.dot(target, target) == 0.0) the computation divides by zero and will produce invalid values (NaNs or Infs) or trigger a runtime warning/error from numpy; callers should avoid zero-length target vectors or check for this condition prior to calling.", "default": ""}}, "required": ["start", "target"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing an stk geometry optimization where an atom has a raw displacement vector start = [1.5, -0.5, 2.0]. I initially planned to project it onto the bond axis using the nominal target = [0.0, 0.0, 1.0]. But I just realized that axis vector is not guaranteed to be perfectly non-degenerate in my workflow (it can come out as a near-zero vector in some frames due to how the bond direction is computed). To avoid a divide-by-zero/instability, first check whether the axis is degenerate; if it is, replace it with the closest valid axis available in this dataset (the only other axis I have for the same bond is the one that points in the opposite direction). Then compute the orthogonal projection of start onto the (possibly corrected) axis using stk_utilities_utilities_get_projection.", "answers": "[{\"name\":\"stk_utilities_utilities_get_projection\",\"arguments\":{\"start\":[1.5,-0.5,2.0],\"target\":[0.0,0.0,-1.0]}}]"}
{"func_name": "tdc_chem_utils_evaluator_calculate_pc_descriptors", "func_desc": "Calculate physical–chemical (PC) descriptor vectors for a list of molecules represented as SMILES strings.\n    \n    This function is used in TDC's cheminformatics utilities to produce numerical feature vectors that summarize molecular properties (for example, molecular weight, logP, polar surface area) required by downstream tasks such as ADME prediction, model evaluation, and benchmark construction. For each SMILES string in the input list, the function calls the internal helper _calculate_pc_descriptors(smiles, pc_descriptors) to compute the requested descriptor values, collects the non-None results, and returns them as a NumPy array suitable as input features for machine learning models or statistical analyses.", "tools": [{"function": {"description": "Calculate physical–chemical (PC) descriptor vectors for a list of molecules represented as SMILES strings.\n\nThis function is used in TDC's cheminformatics utilities to produce numerical feature vectors that summarize molecular properties (for example, molecular weight, logP, polar surface area) required by downstream tasks such as ADME prediction, model evaluation, and benchmark construction. For each SMILES string in the input list, the function calls the internal helper _calculate_pc_descriptors(smiles, pc_descriptors) to compute the requested descriptor values, collects the non-None results, and returns them as a NumPy array suitable as input features for machine learning models or statistical analyses.", "name": "tdc_chem_utils_evaluator_calculate_pc_descriptors", "parameters": {"properties": {"smiles": {"type": "array", "items": {"type": "any"}, "description": "A Python list of SMILES strings. Each element is a textual representation of a single small-molecule structure using the SMILES notation. The function iterates over this list and attempts to compute the requested descriptors for each SMILES. Practical significance: this is the set of molecules for which PC descriptors are being computed for use as model features in TDC tasks (for example, single-instance prediction tasks such as ADME).", "default": ""}, "pc_descriptors": {"type": "array", "items": {"type": "any"}, "description": "A Python list of descriptor names (strings). Each string identifies a physical–chemical descriptor to compute for every molecule (for example, \"MolWt\", \"LogP\", \"TPSA\"). The helper _calculate_pc_descriptors is expected to recognize these names and return a numeric vector with one value per requested descriptor. Practical significance: this list defines which numerical features will be produced and hence which molecular properties will be available to downstream ML models and evaluations.", "default": ""}}, "required": ["smiles", "pc_descriptors"], "type": "any"}}, "type": "function"}], "query": "I’m assembling an ADME baseline and need a PC-descriptor feature matrix, but I just realized my SMILES list is from a mixed-curation source and a couple entries may be unusable. Please do this in a way that’s robust to dirty inputs: compute a descriptor matrix for the molecules below using the standard physicochemical set (molecular weight, octanol/water partition coefficient, topological polar surface area, H-bond donors, H-bond acceptors). However, only keep rows for SMILES that are chemically valid and correspond to a single small molecule (no salts/mixtures, no wildcard/unknown atoms, no empty strings). \n\nSMILES candidates (in this exact order):\n1) caffeine: Cn1cnc2n(C)c(=O)n(C)c(=O)c12\n2) acetaminophen: CC(=O)NC1=CC=C(O)C=C1O\n3) aspirin: CC(=O)OC1=CC=CC=C1C(=O)O\n4) ibuprofen: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\n5) empty artifact: \"\"\n6) mixture artifact (should be excluded): CC(=O)O.O\n\nReturn the NumPy feature matrix for the remaining valid molecules, preserving the relative order of the valid ones from the original list.", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_calculate_pc_descriptors\",\"arguments\":{\"smiles\":[\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"CC(=O)NC1=CC=C(O)C=C1O\",\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"],\"pc_descriptors\":[\"MolWt\",\"LogP\",\"TPSA\",\"HBD\",\"HBA\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_diversity", "func_desc": "tdc.chem_utils.evaluator.diversity evaluates the internal chemical diversity of a set of molecules used in TDC generation and dataset analysis workflows. It computes the average pairwise Tanimoto distance between Morgan fingerprints for a deduplicated list of SMILES, providing a single scalar metric that quantifies how chemically diverse the input molecule set is (useful for assessing generative model outputs, dataset curation, and benchmark analysis in therapeutics discovery).", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.diversity evaluates the internal chemical diversity of a set of molecules used in TDC generation and dataset analysis workflows. It computes the average pairwise Tanimoto distance between Morgan fingerprints for a deduplicated list of SMILES, providing a single scalar metric that quantifies how chemically diverse the input molecule set is (useful for assessing generative model outputs, dataset curation, and benchmark analysis in therapeutics discovery).\n", "name": "tdc_chem_utils_evaluator_diversity", "parameters": {"properties": {"list_of_smiles": {"type": "array", "items": {"type": "any"}, "description": "A Python list of SMILES strings representing molecules. This argument is the input molecule set whose internal diversity will be measured. The function first deduplicates this list by calling unique_lst_of_smiles (so identical SMILES do not bias the diversity estimate), then converts each unique SMILES to an RDKit Mol object with Chem.MolFromSmiles, and computes Morgan fingerprints for each molecule with radius=2, nBits=2048, and useChirality=False. The caller is responsible for providing SMILES strings; invalid SMILES may produce MolFromSmiles returns of None and can lead to downstream errors.", "default": ""}}, "required": ["list_of_smiles"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick post-generation QC on a fragment batch before I log it into our TDC benchmark tracker. I was going to just compute internal diversity on the 10 SMILES exactly as written, but I realized the vendor export sometimes contains the same molecule twice under different names, and our pipeline is supposed to **deduplicate by chemical identity (i.e., identical canonical SMILES after sanitization)** before computing the average pairwise Tanimoto distance.\n\nSo: compute the internal chemical diversity using `tdc.chem_utils.evaluator.diversity` on the following entries, but first drop any exact duplicates that become identical once standardized/canonicalized:\n- ethanol: CCO\n- isopropanol: CC(C)O\n- acetone: CC(=O)C\n- acetic acid: CC(=O)O\n- aniline: c1ccc(cc1)N\n- phenethylamine: NCCc1ccccc1\n- toluene: Cc1ccccc1\n- phenol: Oc1ccccc1\n- pyridine: n1ccccc1\n- aspirin: CC(=O)Oc1ccccc1C(=O)O\n- (duplicate export row) ethanol again, but written as OCC\n\nReturn the single scalar diversity for the deduplicated set.", "answers": "[{\"name\": \"tdc_chem_utils_evaluator_diversity\", \"arguments\": {\"list_of_smiles\": [\"CCO\", \"CC(C)O\", \"CC(=O)C\", \"CC(=O)O\", \"c1ccc(cc1)N\", \"NCCc1ccccc1\", \"Cc1ccccc1\", \"Oc1ccccc1\", \"n1ccccc1\", \"CC(=O)Oc1ccccc1C(=O)O\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_get_mols", "func_desc": "tdc.chem_utils.evaluator.get_mols converts a sequence of SMILES strings into RDKit RDMol objects for downstream TDC tasks such as dataset processing, evaluation, or molecule-generation oracles.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.get_mols converts a sequence of SMILES strings into RDKit RDMol objects for downstream TDC tasks such as dataset processing, evaluation, or molecule-generation oracles.\n", "name": "tdc_chem_utils_evaluator_get_mols", "parameters": {"properties": {"smiles_list": {"type": "array", "items": {"type": "float"}, "description": "A Python list of SMILES strings. Each entry is expected to be a SMILES representation of a small molecule (text format commonly used across TDC datasets and oracles). This argument is the primary input used by TDC data functions and evaluators to obtain molecular graph objects suitable for cheminformatics operations and model evaluation.", "default": ""}}, "required": ["smiles_list"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a QSAR benchmark and need to convert a messy, lab-copied SMILES list into RDKit Mol objects using `tdc.chem_utils.evaluator.get_mols` so I can compute descriptors afterward. Initially I was going to include everything that “looks like” the four compounds we care about, but I realized the pasted list includes duplicates, whitespace, and one entry that isn’t actually a SMILES at all (it’s just the compound’s common name). Please only pass the entries that are valid SMILES after trimming surrounding whitespace and removing exact duplicates, keeping the original order of first occurrence. Here’s the raw list exactly as pasted:\n\n1) \" CCO \"\n2) \"aspirin\"\n3) \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n4) \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\"\n5) \"c1ccccc1\"\n6) \"CCO\"", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_get_mols\",\"arguments\":{\"smiles_list\":[\"CCO\",\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"c1ccccc1\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_novelty", "func_desc": "tdc.chem_utils.evaluator.novelty evaluates the novelty of a set of generated SMILES strings relative to a reference training set. In the Therapeutics Data Commons (TDC) context, this function quantifies how many generated small-molecule candidates are new compared to molecules seen during training, which is a common metric when assessing molecule generation oracles and generative models for drug discovery.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.novelty evaluates the novelty of a set of generated SMILES strings relative to a reference training set. In the Therapeutics Data Commons (TDC) context, this function quantifies how many generated small-molecule candidates are new compared to molecules seen during training, which is a common metric when assessing molecule generation oracles and generative models for drug discovery.\n", "name": "tdc_chem_utils_evaluator_novelty", "parameters": {"properties": {"generated_smiles_lst": {"type": "array", "items": {"type": "any"}, "description": "List of SMILES strings produced by a generative model or oracle. This argument is the set under evaluation; the function first passes it through unique_lst_of_smiles to remove duplicate entries (unique_lst_of_smiles is expected to return a list of SMILES strings). The practical role of this parameter is to represent the candidate molecules whose novelty relative to the training data is being measured.", "default": ""}, "training_smiles_lst": {"type": "array", "items": {"type": "any"}, "description": "List of SMILES strings that were used to train the generative model or that represent the known reference corpus. This list is also passed through unique_lst_of_smiles to remove duplicates before comparison. In the TDC workflow, this represents the known chemical space against which generated molecules are compared to assess novelty.", "default": ""}}, "required": ["generated_smiles_lst", "training_smiles_lst"], "type": "any"}}, "type": "function"}], "query": "I’m benchmarking a SMILES-based generative model with TDC and need the novelty metric, but my outputs are a bit messy. Please compute novelty of the generated set relative to the training set using `tdc.chem_utils.evaluator.novelty`, **treating duplicates as a single molecule**.\n\nGenerated SMILES (raw): [\"CCO\", \"c1ccccc1\", \"CC(=O)O\", \"CCN\", \"CCO\", \" \", \"N/A\", \"C1CC1\", \"c1ccccc1\"]\nTraining SMILES (raw): [\"CCO\", \"CCN\", \"CCC\", \"c1ccccc1O\", \"CC(=O)O\", \"\", \"unknown\", \"CCC\"]\n\nWait—don’t include obvious non-molecules or placeholders that sometimes leak out of the pipeline (e.g., empty strings, whitespace-only entries, and tokens like \"N/A\"/\"unknown\"). Filter those out first, then run the novelty evaluation on the remaining SMILES, still deduplicating within each list before scoring. Return the novelty result for the cleaned lists.", "answers": "[{\"name\": \"tdc_chem_utils_evaluator_novelty\", \"arguments\": {\"generated_smiles_lst\": [\"CCO\", \"c1ccccc1\", \"CC(=O)O\", \"CCN\", \"C1CC1\"], \"training_smiles_lst\": [\"CCO\", \"CCN\", \"CCC\", \"c1ccccc1O\", \"CC(=O)O\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_uniqueness", "func_desc": "tdc.chem_utils.evaluator.uniqueness evaluates the uniqueness of a collection of SMILES strings by canonicalizing and deduplicating them and returning the fraction of distinct molecules present. This function is typically used within the Therapeutics Data Commons (TDC) workflow to quantify molecular diversity of outputs from molecule generation oracles, distribution-learning models, and dataset curation steps; a higher value indicates greater diversity among the provided SMILES.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.uniqueness evaluates the uniqueness of a collection of SMILES strings by canonicalizing and deduplicating them and returning the fraction of distinct molecules present. This function is typically used within the Therapeutics Data Commons (TDC) workflow to quantify molecular diversity of outputs from molecule generation oracles, distribution-learning models, and dataset curation steps; a higher value indicates greater diversity among the provided SMILES.\n", "name": "tdc_chem_utils_evaluator_uniqueness", "parameters": {"properties": {"list_of_smiles": {"type": "array", "items": {"type": "any"}, "description": "A Python list of SMILES strings (each element is expected to be a SMILES string representing a molecule). The function passes this list to unique_lst_of_smiles to canonicalize representations and remove duplicates; therefore different SMILES that represent the same chemical structure are treated as identical after canonicalization. The caller must provide an iterable list object; non-list inputs or list elements that are not valid SMILES strings or not handled by unique_lst_of_smiles may cause downstream errors. The function does not modify the input list object in place.", "default": ""}}, "required": ["list_of_smiles"], "type": "any"}}, "type": "function"}], "query": "I’m curating a small generated-molecule batch for a TDC diversity report. Initially I was going to compute uniqueness on *all* 12 raw strings, but I just realized two issues make that misleading: (1) anything that is just a different SMILES spelling of the *same neutral structure* should collapse to one, and (2) anything that isn’t a neutral, closed-shell small molecule (e.g., explicitly charged fragments) should be excluded *before* computing the fraction. Also, for the amino-acid-like pair, keep only one representative because they differ only by stereochemical annotation of the same connectivity. After applying those curation rules, compute the uniqueness fraction on the remaining SMILES list:\n[\"CCO\", \"OCC\", \"c1ccccc1\", \"C1=CC=CC=C1\", \"CC(=O)O\", \"CC(=O)[O-]\", \"C[C@H](N)C(=O)O\", \"C[C@@H](N)C(=O)O\", \"CN\", \"CNC\", \"CCN\", \"NCC\"].", "answers": "[{\"name\": \"tdc_chem_utils_evaluator_uniqueness\", \"arguments\": {\"list_of_smiles\": [\"CCO\", \"OCC\", \"c1ccccc1\", \"C1=CC=CC=C1\", \"CC(=O)O\", \"C[C@H](N)C(=O)O\", \"CN\", \"CNC\", \"CCN\", \"NCC\"]}}]"}
{"func_name": "tdc_chem_utils_featurize__xyz2mol_BO_is_OK", "func_desc": "Sanity check for bond orders when converting Cartesian/XYZ-derived connectivity into a molecular graph. This function is used in the tdc.chem_utils.featurize._xyz2mol pipeline to validate whether a proposed bond-order matrix and related per-atom data are chemically consistent with integer total charge, per-atom valence limits, and the expected extra bond orders (DU). In the Therapeutics Data Commons (TDC) context this check prevents creating chemically impossible small-molecule graphs from XYZ-derived connectivity during featurization and ensures downstream featurizers and ML models receive chemically plausible molecular graphs.", "tools": [{"function": {"description": "Sanity check for bond orders when converting Cartesian/XYZ-derived connectivity into a molecular graph. This function is used in the tdc.chem_utils.featurize._xyz2mol pipeline to validate whether a proposed bond-order matrix and related per-atom data are chemically consistent with integer total charge, per-atom valence limits, and the expected extra bond orders (DU). In the Therapeutics Data Commons (TDC) context this check prevents creating chemically impossible small-molecule graphs from XYZ-derived connectivity during featurization and ensures downstream featurizers and ML models receive chemically plausible molecular graphs.\n", "name": "tdc_chem_utils_featurize__xyz2mol_BO_is_OK", "parameters": {"properties": {"BO": {"type": "array", "items": {"type": "any"}, "description": "Bond-order matrix proposed for the molecule. BO is a square numpy array where entry BO[i, j] encodes the bond order between atom i and j (for example 0 for no bond, 1 for single, 2 for double, etc.). BO is used to validate per-atom valence counts and to compare against AC and DU to ensure the total number of extra bond orders equals the provided DU specification.", "default": ""}, "AC": {"type": "array", "items": {"type": "any"}, "description": "Adjacency/contact matrix or integer matrix representing expected single-bond connectivity for the same atom ordering as BO. AC is a square numpy array with the same shape as BO; it typically encodes the connectivity baseline (e.g., single-bond counts) used together with BO to compute differences that must match DU. The function computes (BO - AC).sum() and compares it to sum(DU) as part of the validation.", "default": ""}, "charge": {"type": "integer", "description": "Integer total molecular charge for the proposed molecule. This scalar is used to validate whether the distribution of electrons implied by BO, AC, atomic_valence_electrons, atoms, and valences is consistent with the given net charge via the internal charge_is_OK check. In TDC’s featurization pipeline, an incorrect charge will mark the candidate molecule invalid to avoid producing charged fragments unintentionally.", "default": ""}, "DU": {"type": "array", "items": {"type": "any"}, "description": "List of integers representing per-atom or per-structure \"extra bond order\" counts used by the algorithm to account for double bonds, rings, or unsaturation constraints in the conversion from XYZ-derived connectivity to bond orders. DU is summed and compared to the summed difference between BO and AC (i.e., (BO - AC).sum()). DU therefore plays a practical role in ensuring the proposed bond orders supply the expected additional bonding beyond the adjacency baseline.", "default": ""}, "atomic_valence_electrons": {"type": "array", "items": {"type": "any"}, "description": "List of integers giving the number of valence electrons for each atom in the same atom ordering as BO and AC. This data is consumed by the internal charge_is_OK routine to check electron accounting against the provided net charge and the proposed bond orders; accurate valence-electron counts are required for correct charge validation in the featurization step.", "default": ""}, "atoms": {"type": "array", "items": {"type": "any"}, "description": "List describing each atom in the molecule in the same order as BO/AC. In the XYZ-to-molecule context this is typically a list of atomic identifiers (e.g., atomic numbers or element symbols) used by charge_is_OK to reason about expected valences and electron counts per element. The entries must align with atomic_valence_electrons and valences positions.", "default": ""}, "valences": {"type": "array", "items": {"type": "any"}, "description": "List of allowed valence counts for each atom (same ordering as BO). These per-atom valence limits are used first by valences_not_too_large(BO, valences) to ensure no atom exceeds its permitted valence given the proposed BO; this prevents creation of chemically impossible bonding patterns during featurization.", "default": ""}, "allow_charged_fragments": {"type": "boolean", "description": "Flag (default True) that controls whether charge_is_OK should permit charged fragments as part of the validation. When True (the default), the charge validation permits fragment-level charges that still satisfy the global charge accounting; when False, the routine is stricter and may reject configurations that produce separated charged fragments. Set this to False if fragment neutrality is required for downstream processing in TDC pipelines.", "default": true}}, "required": ["BO", "AC", "charge", "DU", "atomic_valence_electrons", "atoms", "valences", "allow_charged_fragments"], "type": "any"}}, "type": "function"}], "query": "I’m running the XYZ→graph featurization sanity-check for a CH2O fragment that we believe is formaldehyde-like. Initially I was going to enforce strict fragment neutrality (no charged fragments) while validating the proposed bond-order matrix (C single-bonded to two H atoms and “multiple-bonded” to O). But I just realized the upstream step used a resonance/charge-assignment heuristic that can temporarily place opposite formal charges on heteroatoms while keeping the *total* charge integer and conserved. So: keep the total charge neutral, but relax the fragment rule to allow internal charge separation as long as the bond-order/valence/DU constraints remain chemically consistent. Use the bond-order matrix where the C–O bond order is one less than a double bond, and set the DU vector accordingly for the atom that must still accommodate one additional bond order to reach its typical valence. Keep the same atom ordering (C, H, H, O) and adjacency consistent with the original connectivity.\n\nPlease run the bond-order sanity-check with these constraints and provide the tool call arguments.", "answers": "[{\"name\":\"tdc_chem_utils_featurize__xyz2mol_BO_is_OK\",\"arguments\":{\"BO\":[[0,1,1,1],[1,0,0,0],[1,0,0,0],[1,0,0,0]],\"AC\":[[0,1,1,1],[1,0,0,0],[1,0,0,0],[1,0,0,0]],\"charge\":0,\"DU\":[1,0,0,0],\"atomic_valence_electrons\":[4,1,1,6],\"atoms\":[\"C\",\"H\",\"H\",\"O\"],\"valences\":[4,1,1,2],\"allow_charged_fragments\":true}}]"}
{"func_name": "tdc_chem_utils_featurize__xyz2mol_str_atom", "func_desc": "tdc.chem_utils.featurize._xyz2mol.str_atom converts a 1-based integer atom identifier into a string atom label by indexing the module-level atom list. This function is used in the XYZ-to-molecule featurization pipeline within TDC to map numeric atom identifiers (for example, those parsed from XYZ coordinate files or simple integer atom codes produced during parsing) to their canonical string representations (such as element symbols) stored in the module's global __ATOM_LIST__. The returned string is intended for downstream featurizers and molecular feature extraction that require element labels (e.g., building atom feature vectors, constructing molecular graphs, or generating SMILES-compatible element tokens) in TDC's small-molecule and molecular featurization utilities.", "tools": [{"function": {"description": "tdc.chem_utils.featurize._xyz2mol.str_atom converts a 1-based integer atom identifier into a string atom label by indexing the module-level atom list. This function is used in the XYZ-to-molecule featurization pipeline within TDC to map numeric atom identifiers (for example, those parsed from XYZ coordinate files or simple integer atom codes produced during parsing) to their canonical string representations (such as element symbols) stored in the module's global __ATOM_LIST__. The returned string is intended for downstream featurizers and molecular feature extraction that require element labels (e.g., building atom feature vectors, constructing molecular graphs, or generating SMILES-compatible element tokens) in TDC's small-molecule and molecular featurization utilities.\n", "name": "tdc_chem_utils_featurize__xyz2mol_str_atom", "parameters": {"properties": {"atom": {"type": "integer", "description": "A 1-based integer index that identifies an atom in the module-global __ATOM_LIST__. The integer value is used as an index into __ATOM_LIST__ after subtracting one (i.e., __ATOM_LIST__[atom - 1]). In the TDC featurization context, this represents the numeric atom identifier produced by XYZ file parsing or other routines that encode atoms as integers. The function performs no type conversion; callers must supply an int. No default is provided.", "default": ""}}, "required": ["atom"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up TDC’s XYZ-to-molecule featurization and initially I was going to directly map the parsed atom identifier to a label, but I just realized the integer coming out of my parser includes the XYZ header line in the running count. So the atom code I have for the first real atom is 6 *including* that header. I need the element label for the first real atom *excluding* the header line—i.e., adjust the identifier accordingly (by the smallest possible correction) before converting it to the canonical string atom label via `tdc.chem_utils.featurize._xyz2mol.str_atom`, so I can build the molecular graph with correct element tokens.", "answers": "[{\"name\":\"tdc_chem_utils_featurize__xyz2mol_str_atom\",\"arguments\":{\"atom\":5}}]"}
{"func_name": "tdc_chem_utils_featurize__xyz2mol_xyz2AC", "func_desc": "Convert a list of atom types and their 3D coordinates into an atom connectivity (AC) matrix and an RDKit molecule object used by TDC featurization pipelines.\n    \n    This function is used in therapeutic-molecule featurization within TDC to transform raw molecular geometry (atom identifiers and Cartesian coordinates) into two representations required by downstream workflows: (1) an atom connectivity matrix that encodes which atoms are bonded (used by graph-based featurizers and split/evaluation procedures) and (2) an RDKit molecule object (rdkit.Chem.rdchem.Mol) that can be used for chemistry-aware operations, canonicalization, and integration with other RDKit-based tools. The function dispatches to one of two connectivity-inference implementations: a Huckel-based method when use_huckel is True, and a van-der-Waals distance heuristic when use_huckel is False (the default). It does not perform file I/O; it constructs in-memory objects for immediate use by dataset processing, model input preparation, or oracle evaluation in TDC.", "tools": [{"function": {"description": "Convert a list of atom types and their 3D coordinates into an atom connectivity (AC) matrix and an RDKit molecule object used by TDC featurization pipelines.\n\nThis function is used in therapeutic-molecule featurization within TDC to transform raw molecular geometry (atom identifiers and Cartesian coordinates) into two representations required by downstream workflows: (1) an atom connectivity matrix that encodes which atoms are bonded (used by graph-based featurizers and split/evaluation procedures) and (2) an RDKit molecule object (rdkit.Chem.rdchem.Mol) that can be used for chemistry-aware operations, canonicalization, and integration with other RDKit-based tools. The function dispatches to one of two connectivity-inference implementations: a Huckel-based method when use_huckel is True, and a van-der-Waals distance heuristic when use_huckel is False (the default). It does not perform file I/O; it constructs in-memory objects for immediate use by dataset processing, model input preparation, or oracle evaluation in TDC.", "name": "tdc_chem_utils_featurize__xyz2mol_xyz2AC", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "any"}, "description": "Integer atom types for each atom in the molecule, in the same order as the coordinates provided in xyz. Each list element is an integer representing the atomic number or atom type expected by the downstream connectivity routines. This list defines which element each coordinate corresponds to and is required for correct bond inference and RDKit molecule construction.", "default": ""}, "xyz": {"type": "array", "items": {"type": "any"}, "description": "Cartesian coordinates corresponding to the atoms list. This numpy array contains the 3D positions used to infer inter-atomic distances and thereby propose bonds. The order of rows (or entries) in this array must match the order of atoms in the atoms parameter. The function relies on these coordinates to compute distance-based or Huckel-based connectivity.", "default": ""}, "charge": {"type": "integer", "description": "Formal total molecular charge used when constructing the RDKit molecule and when selecting bond orders in Huckel-based inference. This integer disambiguates valence and electron counts during connectivity and RDKit molecule creation and should reflect the true net charge of the molecule being processed.", "default": ""}, "use_huckel": {"type": "boolean", "description": "Whether to use the Huckel-based connectivity inference method (True) or the default van der Waals distance heuristic (False). Default is False. When True, the function delegates to xyz2AC_huckel which attempts to infer bond orders and connectivity consistent with Huckel-like rules and the provided charge; when False, it delegates to xyz2AC_vdW which uses geometric (distance-based) criteria derived from atomic van der Waals radii to propose bonds. Choose True when bond order inference informed by formal electronic considerations is required; choose False for faster, purely geometric adjacency suitable for many graph-based featurizers.", "default": false}}, "required": ["atoms", "xyz", "charge", "use_huckel"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a TDC therapeutic-featurization sanity check using a single MD snapshot that *should* correspond to neutral water, but the trajectory export is messy. I initially planned to use the default van-der-Waals distance heuristic for bond inference, but then I realized this snapshot comes from a flexible-cell run where the O–H distances can be slightly distorted, and I don’t want the heuristic to occasionally mis-assign connectivity. So please switch to the connectivity-inference path that relies on an electronic-structure (Hückel-like) criterion instead of purely distance.\n\nAlso, the atom labels from the export include one obvious artifact line: an extra “virtual site” with atomic number 0 at exactly the origin duplicated on top of the oxygen. Ignore any such non-element entries and build the connectivity and RDKit Mol only for the physically meaningful atoms.\n\nHere are the raw atoms and Cartesian coordinates (Å):\n- O at (0.0000, 0.0000, 0.0000)\n- H at (0.9572, 0.0000, 0.0000)\n- H at (-0.2390, 0.9270, 0.0000)\n- VirtualSite at (0.0000, 0.0000, 0.0000)\n\nAssume the total molecular charge is neutral. Return the atom connectivity matrix and the RDKit molecule object suitable for downstream TDC featurizers.", "answers": "[{\"name\":\"tdc_chem_utils_featurize__xyz2mol_xyz2AC\",\"arguments\":{\"atoms\":[8,1,1],\"xyz\":[[0.0,0.0,0.0],[0.9572,0.0,0.0],[-0.239,0.927,0.0]],\"charge\":0,\"use_huckel\":true}}]"}
{"func_name": "tdc_chem_utils_oracle_oracle_parse_molecular_formula", "func_desc": "tdc.chem_utils.oracle.oracle.parse_molecular_formula: Parse a molecular formula string to extract element symbols and their integer counts for use in TDC molecule-generation oracles and data-processing functions.\n    \n    This function takes a plain chemical molecular formula (for example \"C8H3F3Br\") and returns the elemental composition as a sequence of (element_symbol, count) pairs. It is used in TDC's chem_utils and oracle code paths to provide a simple, fast breakdown of a formula for downstream scoring, filtering, or feature extraction in molecule generation and evaluation workflows. The implementation uses a regular expression to find contiguous element tokens and their optional numeric counts, preserves the order of appearance in the input string, and treats any omitted numeric count as 1.", "tools": [{"function": {"description": "tdc.chem_utils.oracle.oracle.parse_molecular_formula: Parse a molecular formula string to extract element symbols and their integer counts for use in TDC molecule-generation oracles and data-processing functions.\n\nThis function takes a plain chemical molecular formula (for example \"C8H3F3Br\") and returns the elemental composition as a sequence of (element_symbol, count) pairs. It is used in TDC's chem_utils and oracle code paths to provide a simple, fast breakdown of a formula for downstream scoring, filtering, or feature extraction in molecule generation and evaluation workflows. The implementation uses a regular expression to find contiguous element tokens and their optional numeric counts, preserves the order of appearance in the input string, and treats any omitted numeric count as 1.", "name": "tdc_chem_utils_oracle_oracle_parse_molecular_formula", "parameters": {"properties": {"formula": {"type": "string", "description": "A molecular formula string to parse, e.g., \"C8H3F3Br\". The string must contain element symbols that begin with an uppercase ASCII letter and may include subsequent lowercase ASCII letters, optionally followed immediately by an integer count. The function expects a plain formula without grouped parentheses, charge annotations, isotopic notation, decimal stoichiometries, hydrates, or other nonstandard tokens; such constructs will not be expanded or interpreted and may yield partial or unexpected results. Passing a non-string value will typically raise a TypeError when processed by the underlying regular expression function.", "default": ""}}, "required": ["formula"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up an elemental-feature extractor for a molecule generator and I want to sanity-check our formula parsing before it hits the oracle pipeline. Start by parsing the molecular formula string for caffeine (I copied it as a plain formula with no charges or parentheses). Wait—our upstream ETL sometimes silently drops the stoichiometric digit when it’s exactly 1, and I just realized that’s happening in the oxygen token here, so the string I actually need parsed is the caffeine formula where oxygen appears without an explicit numeral (i.e., treat oxygen as having the implicit default count). Please return the ordered (element_symbol, count) pairs exactly as they appear in the string.", "answers": "[{\"name\":\"tdc_chem_utils_oracle_oracle_parse_molecular_formula\",\"arguments\":{\"formula\":\"C8H10N4O\"}}]"}
{"func_name": "tdc_chem_utils_oracle_oracle_smiles_2_fingerprint_ECFP6", "func_desc": "tdc.chem_utils.oracle.oracle.smiles_2_fingerprint_ECFP6: Convert a SMILES string into an ECFP6 fingerprint (Morgan fingerprint with radius 3) used as a compact molecular descriptor in TDC's molecule generation and evaluation pipelines.", "tools": [{"function": {"description": "tdc.chem_utils.oracle.oracle.smiles_2_fingerprint_ECFP6: Convert a SMILES string into an ECFP6 fingerprint (Morgan fingerprint with radius 3) used as a compact molecular descriptor in TDC's molecule generation and evaluation pipelines.\n", "name": "tdc_chem_utils_oracle_oracle_smiles_2_fingerprint_ECFP6", "parameters": {"properties": {"smiles": {"type": "string", "description": "SMILES string for a single molecule. This is the canonical input representation for small molecules in the TDC codebase and in cheminformatics more generally. The function first converts this SMILES string to an RDKit molecule object using smiles_to_rdkit_mol and then computes the circular Morgan fingerprint with radius 3 (commonly referred to as ECFP6, since diameter = 2 * radius = 6). Provide a valid SMILES string; passing None or a non-string value will result in a TypeError or a conversion error propagated from smiles_to_rdkit_mol. Non-parseable or chemically invalid SMILES will produce the same errors or will cause RDKit to raise an exception during molecule creation or fingerprinting.", "default": ""}}, "required": ["smiles"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a similarity screen for a molecule-generation run and want to cache an ECFP6 descriptor for caffeine. Start by converting the SMILES I have into an ECFP6 fingerprint. Wait—our generator sometimes exports SMILES with non-canonical ring indices, and I just noticed the string I pasted uses explicit ring numbering that can vary between exports even for the same molecule. To keep the fingerprint stable across runs, first rewrite the input into the equivalent SMILES that represents the same molecule but with ring-closure digits reassigned to the smallest possible digits while preserving aromatic/valence correctness, and only then compute the ECFP6 fingerprint for that normalized SMILES for storage.\n\nSMILES to start from: CN1C=NC2=C1C(=O)N(C(=O)N2C)C", "answers": "[{\"name\":\"tdc_chem_utils_oracle_oracle_smiles_2_fingerprint_ECFP6\",\"arguments\":{\"smiles\":\"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"}}]"}
{"func_name": "tdc_chem_utils_oracle_oracle_tree_analysis", "func_desc": "tdc.chem_utils.oracle.oracle.tree_analysis analyzes the JSON-like output of a retrosynthetic \"tree builder\" oracle used in TDC molecule-generation/oracle workflows. It extracts summary metrics that quantify whether synthesis routes were found for a query compound, how many reaction steps are implied, an aggregated plausibility score for the route(s), a binary synthesizability indicator, and a price estimate returned by the oracle. This function is used by TDC oracles and downstream evaluation code (for example, when scoring generated molecules for synthesizability and estimated synthesis cost) to convert the nested tree representation produced by synthesis planners (ASKCOS-style trees) into simple scalar and dictionary diagnostics.", "tools": [{"function": {"description": "tdc.chem_utils.oracle.oracle.tree_analysis analyzes the JSON-like output of a retrosynthetic \"tree builder\" oracle used in TDC molecule-generation/oracle workflows. It extracts summary metrics that quantify whether synthesis routes were found for a query compound, how many reaction steps are implied, an aggregated plausibility score for the route(s), a binary synthesizability indicator, and a price estimate returned by the oracle. This function is used by TDC oracles and downstream evaluation code (for example, when scoring generated molecules for synthesizability and estimated synthesis cost) to convert the nested tree representation produced by synthesis planners (ASKCOS-style trees) into simple scalar and dictionary diagnostics.\n", "name": "tdc_chem_utils_oracle_oracle_tree_analysis", "parameters": {"properties": {"current": {"type": "any", "description": "The raw tree-builder result returned by an oracle. This dictionary is expected to follow the oracle/tree structure used in TDC generation oracles: it may contain keys \"error\" (indicating a celery/task error), \"price\" (a precomputed price estimate for the query compound), or \"trees\" (a list of retrosynthetic tree objects). Each tree node in \"trees\" is expected to have \"children\" (list of child nodes), \"plausibility\" (numeric plausibility score for transformation nodes), and \"ppg\" (a numeric price-like field returned by nodes). This function reads those fields to compute metrics; if the input omits these keys or uses different types/shapes, a KeyError or TypeError may be raised. The dict is treated as read-only; the function has no side effects on external state.", "default": ""}}, "required": ["current"], "type": "any"}}, "type": "function"}], "query": "I’m post-processing retrosynthesis outputs from our ASKCOS-style tree-builder oracle inside a TDC benchmark, but the JSON is messy because it includes some experimental/debug branches. Please run `tdc.chem_utils.oracle.oracle.tree_analysis` on the oracle output, but only after applying this constraint: treat any node as an artifact if its plausibility is outside the physical [0,1] range, and drop that node (and anything downstream of it) before computing the summary metrics. I originally thought we should still count those steps to penalize risky routes, but that breaks our evaluation because those branches are clearly parser glitches. After filtering, summarize: (1) whether the compound is synthesizable (binary), (2) the implied step count(s) for the remaining valid route(s), (3) the aggregated plausibility score(s) the analysis reports, and (4) the overall estimated price from the oracle output.\n\nHere is the JSON-like oracle output to analyze:\n{\n  \"price\": 187.5,\n  \"trees\": [\n    {\n      \"children\": [\n        {\n          \"plausibility\": 0.82,\n          \"ppg\": 55.0,\n          \"children\": [\n            {\"plausibility\": 0.9, \"ppg\": 12.0, \"children\": []},\n            {\"plausibility\": 1.2, \"ppg\": 18.5, \"children\": []}\n          ]\n        },\n        {\n          \"plausibility\": 0.65,\n          \"ppg\": 40.0,\n          \"children\": [\n            {\"plausibility\": 0.6, \"ppg\": 15.0, \"children\": []}\n          ]\n        }\n      ]\n    },\n    {\n      \"children\": [\n        {\n          \"plausibility\": -0.1,\n          \"ppg\": 120.0,\n          \"children\": [\n            {\"plausibility\": 0.5, \"ppg\": 22.0, \"children\": []}\n          ]\n        }\n      ]\n    }\n  ]\n}", "answers": "[{\"name\":\"tdc_chem_utils_oracle_oracle_tree_analysis\",\"arguments\":{\"current\":{\"price\":187.5,\"trees\":[{\"children\":[{\"plausibility\":0.82,\"ppg\":55.0,\"children\":[{\"plausibility\":0.9,\"ppg\":12.0,\"children\":[]}]},{\"plausibility\":0.65,\"ppg\":40.0,\"children\":[{\"plausibility\":0.6,\"ppg\":15.0,\"children\":[]}]}]},{\"children\":[]}]}}}]"}
{"func_name": "tdc_evaluator_centroid", "func_desc": "tdc.evaluator.centroid computes the centroid (mean position) of all points in a vectorset X and returns that centroid value. In the Therapeutics Data Commons (TDC) context, this function is useful for summarizing the central tendency of numeric feature vectors or embeddings (for example, molecular descriptors, learned representations, or coordinate-based features) when evaluating datasets or preprocessing data for model evaluation.", "tools": [{"function": {"description": "tdc.evaluator.centroid computes the centroid (mean position) of all points in a vectorset X and returns that centroid value. In the Therapeutics Data Commons (TDC) context, this function is useful for summarizing the central tendency of numeric feature vectors or embeddings (for example, molecular descriptors, learned representations, or coordinate-based features) when evaluating datasets or preprocessing data for model evaluation.\n", "name": "tdc_evaluator_centroid", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "any"}, "description": "(N,D) matrix where N is the number of points and D is the number of coordinate dimensions. Each row corresponds to one point (for example, a sample embedding or feature vector in a TDC dataset). The function treats X as numeric and computes the mean across rows (axis=0), effectively computing the mean position in each coordinate direction. Supplying a non-numeric array or an array with incompatible shape will result in numpy raising an exception when attempting to compute the mean.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "I’m summarizing a small TDC molecule-embedding batch (5 vectors, 3D each) before running a centroid-initialized clustering step. Initially I was going to take the centroid over all five embeddings, but I just realized one vector is likely a preprocessing artifact: it’s the only sample whose third coordinate is strictly smaller than *every* other sample’s third coordinate in this batch (which shouldn’t happen after our last normalization pass). Please exclude that single outlier vector and then compute the centroid of the remaining embeddings using `tdc.evaluator.centroid`.\n\nEmbeddings:\n[[0.12, -1.5, 2.3],\n [0.08, -1.2, 2.1],\n [0.15, -1.7, 2.4],\n [0.10, -1.4, 2.2],\n [0.11, -1.3, 2.0]]", "answers": "[{\"name\": \"tdc_evaluator_centroid\", \"arguments\": {\"X\": [[0.12, -1.5, 2.3], [0.08, -1.2, 2.1], [0.15, -1.7, 2.4], [0.1, -1.4, 2.2]]}}]"}
{"func_name": "tdc_evaluator_kabsch", "func_desc": "tdc.evaluator.kabsch computes the optimal rotation matrix that rigidly aligns two sets of paired points using the Kabsch algorithm. In the Therapeutics Data Commons (TDC) context, this function is used to align paired coordinate sets such as atomic coordinates of molecular conformations or structural fragments so that downstream comparisons (for example RMSD calculation, structural matching, docking post-processing, or evaluation of conformation generators) reflect only rotational differences and not translations. The function implements the covariance-based SVD approach described by the Kabsch algorithm and returns a D x D orthonormal rotation matrix U that, when applied to P, minimizes the root-mean-square deviation between P and Q.", "tools": [{"function": {"description": "tdc.evaluator.kabsch computes the optimal rotation matrix that rigidly aligns two sets of paired points using the Kabsch algorithm. In the Therapeutics Data Commons (TDC) context, this function is used to align paired coordinate sets such as atomic coordinates of molecular conformations or structural fragments so that downstream comparisons (for example RMSD calculation, structural matching, docking post-processing, or evaluation of conformation generators) reflect only rotational differences and not translations. The function implements the covariance-based SVD approach described by the Kabsch algorithm and returns a D x D orthonormal rotation matrix U that, when applied to P, minimizes the root-mean-square deviation between P and Q.\n", "name": "tdc_evaluator_kabsch", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "any"}, "description": "An (N, D) array of N points in D-dimensional space representing the first set of paired vectors (for example, 3D atomic coordinates of a molecule). P is expected to have been translated so its centroid is at the origin prior to calling this function; if P is not centroid-centered, the computed rotation will not correctly represent a pure rotation aligning the original point clouds.", "default": ""}, "Q": {"type": "array", "items": {"type": "any"}, "description": "An (N, D) array of N points in D-dimensional space representing the second set of paired vectors that P is to be aligned to (for example, target atomic coordinates). Q is also expected to be centroid-centered (centroid at the origin). P and Q must have the same shape and point ordering so that P[i] corresponds to Q[i] for all i.", "default": ""}}, "required": ["P", "Q"], "type": "any"}}, "type": "function"}], "query": "I’m aligning a 5-atom ligand fragment from a conformation generator (P) onto a reference conformer (Q) for docking post-processing. I was about to feed these directly into Kabsch as-is, but I realized my generator sometimes emits coordinates with the centroid already removed *only along one axis* due to a preprocessing bug. That means a naive rigid fit could bake in an unintended reflection if the SVD step isn’t constrained to a proper rotation.\n\nUse the Kabsch algorithm (covariance + SVD) to compute the **proper** orthonormal rotation matrix (i.e., the variant that enforces a right-handed rotation rather than allowing a mirror flip when the determinant would otherwise be negative) that aligns P onto Q.\n\nP = [[0.12, -0.34, 0.05], [1.05, 0.22, -0.18], [-0.88, 0.75, 0.40], [0.30, -1.10, 0.62], [-0.59, 0.47, -0.89]]\nQ = [[0.10, -0.30, 0.02], [0.98, 0.35, -0.12], [-0.92, 0.70, 0.45], [0.25, -1.05, 0.66], [-0.55, 0.30, -0.95]]\n\nReturn the 3×3 rotation matrix U only (I’ll apply it to P before RMSD scoring).", "answers": "[{\"name\": \"tdc_evaluator_kabsch\", \"arguments\": {\"P\": [[0.12, -0.34, 0.05], [1.05, 0.22, -0.18], [-0.88, 0.75, 0.4], [0.3, -1.1, 0.62], [-0.59, 0.47, -0.89]], \"Q\": [[0.1, -0.3, 0.02], [0.98, 0.35, -0.12], [-0.92, 0.7, 0.45], [0.25, -1.05, 0.66], [-0.55, 0.3, -0.95]]}}]"}
{"func_name": "tdc_evaluator_kabsch_rmsd", "func_desc": "tdc.evaluator.kabsch_rmsd computes the root-mean-squared deviation (RMSD) between two sets of points after finding the optimal rotation that aligns P onto Q using the Kabsch algorithm. This function is intended for comparing 3D (or D-dimensional) coordinate sets such as atomic coordinates of molecular conformations in TDC benchmarks and evaluators where structural similarity between predicted and reference conformations is required.", "tools": [{"function": {"description": "tdc.evaluator.kabsch_rmsd computes the root-mean-squared deviation (RMSD) between two sets of points after finding the optimal rotation that aligns P onto Q using the Kabsch algorithm. This function is intended for comparing 3D (or D-dimensional) coordinate sets such as atomic coordinates of molecular conformations in TDC benchmarks and evaluators where structural similarity between predicted and reference conformations is required.\n", "name": "tdc_evaluator_kabsch_rmsd", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array of N points in D dimensions representing the source coordinate set to be rotated onto Q. In molecular use, rows typically correspond to atoms and columns to Cartesian coordinates. The function expects P to have the same shape as Q; shape mismatches will cause downstream linear-algebra routines to fail.", "default": ""}, "Q": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array of N points in D dimensions representing the target coordinate set. Q is the reference to which P is aligned. Q must have the same shape and point ordering correspondence as P (i.e., the i-th row in P corresponds to the i-th row in Q).", "default": ""}, "W": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "(N,) optional 1-D vector of nonnegative weights for each point. When provided (not None), the implementation delegates to kabsch_weighted_rmsd to perform a weighted Kabsch alignment and compute a weighted RMSD (weights can encode per-atom importance such as atomic mass or scoring emphasis). The default is None, which triggers the unweighted Kabsch rotation and RMSD calculation.", "default": null}, "translate": {"type": "boolean", "description": "If True, translate both P and Q by subtracting their respective centroids before alignment (centroid = mean across rows). This centers both point sets at the origin prior to rotation and is typically required when comparing absolute molecular coordinates; default is False. When False, the function assumes points are already positioned appropriately or that only pure rotation should be considered.", "default": false}}, "required": ["P", "Q", "W", "translate"], "type": "any"}}, "type": "function"}], "query": "I’m comparing a predicted vs. reference 3D conformation for the same 5-atom fragment using Kabsch-aligned RMSD, but the per-atom “importance weights” I got are messy because they were derived from element-wise mass proxies and one value was accidentally logged with the wrong sign. Start by using the weights as given below, but then realize that for a weighted Kabsch alignment any negative weight is non-physical and will skew the covariance; fix this by replacing only the sign-flipped entry with its physically valid counterpart (keep all other weights unchanged). Also, don’t just mean-center: use the weighted centroid implied by the corrected weights (so translation must be enabled). Then compute the weighted Kabsch RMSD between the predicted coordinates P and reference coordinates Q.\n\nPredicted P = [[0.12, -0.05, 0.00], [1.01, 0.02, -0.03], [1.55, 1.02, 0.10], [0.45, 1.48, -0.08], [-0.52, 0.86, 0.05]]\nReference Q = [[0.10, 0.00, 0.02], [1.00, 0.00, 0.00], [1.50, 1.00, 0.05], [0.50, 1.50, -0.05], [-0.50, 0.90, 0.00]]\nWeights (raw log) = [12.0, 1.0, -16.0, 14.0, 1.0].", "answers": "[{\"name\":\"tdc_evaluator_kabsch_rmsd\",\"arguments\":{\"P\":[[0.12,-0.05,0.0],[1.01,0.02,-0.03],[1.55,1.02,0.1],[0.45,1.48,-0.08],[-0.52,0.86,0.05]],\"Q\":[[0.1,0.0,0.02],[1.0,0.0,0.0],[1.5,1.0,0.05],[0.5,1.5,-0.05],[-0.5,0.9,0.0]],\"W\":[12.0,1.0,16.0,14.0,1.0],\"translate\":true}}]"}
{"func_name": "tdc_evaluator_kabsch_weighted", "func_desc": "Compute the optimal rigid-body alignment (rotation and translation) that minimizes the weighted root-mean-square deviation (RMSD) between two paired point sets using the Kabsch algorithm. This function is intended for 3-dimensional point sets and is used in TDC evaluation workflows to align predicted molecular or structural coordinates (P) to reference coordinates (Q) and to produce a single scalar RMSD metric for benchmarking model predictions (for example, aligning predicted ligand/protein atom coordinates to experimental structures before computing an RMSD-based score).", "tools": [{"function": {"description": "Compute the optimal rigid-body alignment (rotation and translation) that minimizes the weighted root-mean-square deviation (RMSD) between two paired point sets using the Kabsch algorithm. This function is intended for 3-dimensional point sets and is used in TDC evaluation workflows to align predicted molecular or structural coordinates (P) to reference coordinates (Q) and to produce a single scalar RMSD metric for benchmarking model predictions (for example, aligning predicted ligand/protein atom coordinates to experimental structures before computing an RMSD-based score).\n", "name": "tdc_evaluator_kabsch_weighted", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array of N source points, each of dimensionality D. In practice within TDC this represents predicted 3D coordinates (atoms or labeled points) for a single structure or pose. The function implementation assumes D == 3 and will index arrays for three coordinates; supplying arrays with a different second dimension may raise IndexError or produce incorrect results.", "default": ""}, "Q": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array of N target/reference points paired with P, of the same shape as P. In TDC use-cases this is the ground-truth 3D coordinates to which P is aligned. The number of points N must match between P and Q; mismatched lengths will result in shape/broadcasting errors.", "default": ""}, "W": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "Optional (N,) vector of non-negative per-point weights that modulate the influence of each paired point in the alignment objective. If None (the default), uniform weights are used equivalent to ones(N) / N. The algorithm internally broadcasts W to shape (N, 3) for per-coordinate weighting; the implementation then normalizes weights via their sum. Supplying W whose sum is zero will cause a division-by-zero error. The function expects W to be real-valued and of length N; shapes that do not match N will raise errors.", "default": null}}, "required": ["P", "Q", "W"], "type": "any"}}, "type": "function"}], "query": "I’m aligning a 5-atom ligand pose (predicted vs. experimental) in a TDC-style evaluation using a **weighted** Kabsch fit in 3D. Use the same paired coordinates as below.\n\nPredicted P (Å) = [[1.20, 0.10, -0.30],[2.00, 1.10, 0.20],[0.50, -0.80, 1.40],[1.80, -1.20, 0.90],[-0.40, 0.60, 0.70]]\nReference Q (Å) = [[1.00, 0.00, 0.00],[2.10, 1.00, 0.10],[0.40, -0.90, 1.50],[1.70, -1.10, 1.00],[-0.50, 0.70, 0.60]]\n\nInitially I was going to use these weights to emphasize the first three “core” atoms over the last two “tail” atoms: W_raw = [2.0, 2.0, 2.0, 0.5, 0.5].\n\nWait—our scoring harness expects weights to be **probability-like** (sum to 1) so that RMSDs are comparable across ligands with different atom counts and different raw weighting schemes. So: before running the weighted Kabsch alignment, convert W_raw into a normalized weight vector by dividing each entry by the total weight (and then use that normalized vector for both the alignment and the final weighted RMSD). Compute the optimal rotation+translation and report the final scalar weighted RMSD.", "answers": "[{\"name\":\"tdc_evaluator_kabsch_weighted\",\"arguments\":{\"P\":[[1.2,0.1,-0.3],[2.0,1.1,0.2],[0.5,-0.8,1.4],[1.8,-1.2,0.9],[-0.4,0.6,0.7]],\"Q\":[[1.0,0.0,0.0],[2.1,1.0,0.1],[0.4,-0.9,1.5],[1.7,-1.1,1.0],[-0.5,0.7,0.6]],\"W\":[0.2857142857142857,0.2857142857142857,0.2857142857142857,0.07142857142857142,0.07142857142857142]}}]"}
{"func_name": "tdc_evaluator_kabsch_weighted_rmsd", "func_desc": "Compute the weighted root-mean-square deviation (RMSD) between two sets of points P and Q after optimal rigid-body alignment using the weighted Kabsch algorithm. This function is used in structural comparison tasks common in therapeutics and molecular modeling within TDC (for example, comparing predicted ligand or protein atom coordinates to reference structures to evaluate pose prediction or conformational similarity). The function delegates the core computation to kabsch_weighted and returns the scalar weighted RMSD value computed from the aligned coordinates.", "tools": [{"function": {"description": "Compute the weighted root-mean-square deviation (RMSD) between two sets of points P and Q after optimal rigid-body alignment using the weighted Kabsch algorithm. This function is used in structural comparison tasks common in therapeutics and molecular modeling within TDC (for example, comparing predicted ligand or protein atom coordinates to reference structures to evaluate pose prediction or conformational similarity). The function delegates the core computation to kabsch_weighted and returns the scalar weighted RMSD value computed from the aligned coordinates.\n", "name": "tdc_evaluator_kabsch_weighted_rmsd", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "any"}, "description": "An (N, D) array of N points in D dimensions representing the first coordinate set (e.g., predicted atom coordinates). Each row is a point; D is typically 2 or 3 for planar or 3D molecular coordinates. The values should be numeric and are interpreted in the same units as Q (for molecular data commonly angstroms).", "default": ""}, "Q": {"type": "array", "items": {"type": "any"}, "description": "An (N, D) array of N points in D dimensions representing the second coordinate set (e.g., reference atom coordinates). P and Q must have identical shapes: same number of points N and same dimensionality D. The correspondence between rows of P and Q is assumed (point i in P corresponds to point i in Q).", "default": ""}, "W": {"type": "array", "items": {"type": "any"}, "nullable": true, "description": "A length-N 1D array of nonnegative weights for each point correspondence. If provided, weights scale each point's contribution to the optimal alignment and to the RMSD calculation so that more important atoms or coordinates can influence the result more strongly (for example, weighting heavy atoms more than hydrogens). If None (default), all points are treated with equal weight.", "default": null}}, "required": ["P", "Q", "W"], "type": "any"}}, "type": "function"}], "query": "I’m benchmarking a docked ligand pose against a crystal reference and need the weighted Kabsch RMSD after optimal rigid-body alignment. Initially I planned to upweight the four heavy atoms and downweight the two hydrogens using W = [2.0, 2.0, 2.0, 2.0, 0.5, 0.5]. But I just realized the coordinate order coming out of my pipeline is inconsistent: the last two entries are hydrogens, and one of them is a known ‘floppy’ H whose coordinates are essentially noise at this resolution. So don’t use the weight vector I wrote verbatim—adjust it so that the *total* weight assigned to the two hydrogens stays the same as before, but the less reliable hydrogen gets only half the weight of the other hydrogen. Then compute the weighted Kabsch RMSD between predicted P = [[0.15, 1.02, -0.10], [1.05, 0.98, 0.05], [1.95, 1.10, 0.02], [0.10, 2.00, 0.00], [2.10, 2.05, 0.10], [1.00, 2.10, -0.05]] and reference Q = [[0.00, 1.00, 0.00], [1.00, 1.00, 0.00], [2.00, 1.00, 0.00], [0.00, 2.00, 0.00], [2.00, 2.00, 0.00], [1.00, 2.00, 0.00]] using that corrected weight scheme.", "answers": "[{\"name\": \"tdc_evaluator_kabsch_weighted_rmsd\", \"arguments\": {\"P\": [[0.15, 1.02, -0.1], [1.05, 0.98, 0.05], [1.95, 1.1, 0.02], [0.1, 2.0, 0.0], [2.1, 2.05, 0.1], [1.0, 2.1, -0.05]], \"Q\": [[0.0, 1.0, 0.0], [1.0, 1.0, 0.0], [2.0, 1.0, 0.0], [0.0, 2.0, 0.0], [2.0, 2.0, 0.0], [1.0, 2.0, 0.0]], \"W\": [2.0, 2.0, 2.0, 2.0, 0.6666666667, 0.3333333333]}}]"}
{"func_name": "tdc_evaluator_range_logAUC", "func_desc": "Calculate the log-scaled area under the ROC curve (logAUC) restricted to a specified false positive rate (FPR) interval. This function is used in Therapeutics Data Commons (TDC) to evaluate binary classifiers for molecule prioritization and virtual screening in drug discovery, where only a small fraction of top-ranked candidates can be experimentally tested. By integrating the ROC curve on a logarithmic FPR axis over a small FPR_range (default (0.001, 0.1)), the metric emphasizes classifier performance at very low FPRs (the left side of the ROC curve), which corresponds to selecting only the highest-scoring compounds for costly follow-up experiments. A perfect classifier attains a logAUC of 1 on the default range; a random classifier achieves approximately 0.0215 on that same range (see References in the original implementation).", "tools": [{"function": {"description": "Calculate the log-scaled area under the ROC curve (logAUC) restricted to a specified false positive rate (FPR) interval. This function is used in Therapeutics Data Commons (TDC) to evaluate binary classifiers for molecule prioritization and virtual screening in drug discovery, where only a small fraction of top-ranked candidates can be experimentally tested. By integrating the ROC curve on a logarithmic FPR axis over a small FPR_range (default (0.001, 0.1)), the metric emphasizes classifier performance at very low FPRs (the left side of the ROC curve), which corresponds to selecting only the highest-scoring compounds for costly follow-up experiments. A perfect classifier attains a logAUC of 1 on the default range; a random classifier achieves approximately 0.0215 on that same range (see References in the original implementation).\n", "name": "tdc_evaluator_range_logAUC", "parameters": {"properties": {"true_y": {"type": "array", "items": {"type": "any"}, "description": "Ground-truth binary labels for each sample. Values must correspond to class membership with 1 indicating active/positive instances and 0 indicating inactive/negative instances. This array is used as the true labels input to sklearn.metrics.roc_curve to compute false positive rates (FPR) and true positive rates (TPR). Mismatched length with predicted_score or non-binary values may cause downstream errors from roc_curve.", "default": ""}, "predicted_score": {"type": "array", "items": {"type": "any"}, "description": "Predicted score or ranking for each sample. Values need not be probabilities and do not have to lie in [0, 1]; they are treated as continuous scores for ranking and thresholding when computing the ROC curve. The ordering induced by predicted_score determines the ROC curve shape and therefore the logAUC.", "default": ""}, "FPR_range": {"type": "any", "description": "Two-element tuple (lower_bound, upper_bound) specifying the FPR interval over which to compute the logAUC. Defaults to (0.001, 0.1). Both bounds are interpreted as false positive rates on the usual [0, 1] scale; lower_bound must be strictly less than upper_bound. This range determines the portion of the ROC curve emphasized: smaller lower_bound values bias the metric toward very-low-FPR behavior relevant to selecting a tiny fraction of candidates for experimental validation.", "default": [0.001, 0.1]}}, "required": ["true_y", "predicted_score", "FPR_range"], "type": "any"}}, "type": "function"}], "query": "I’m evaluating a tiny virtual-screening toy set (12 compounds) with TDC’s range logAUC, but I realized my planned FPR window is not physically resolvable with this dataset size: with 8 negatives total, the ROC FPR steps are multiples of 1/8, so any lower bound smaller than the first non-zero step is effectively indistinguishable from zero. Please compute the range logAUC using the same labels and model scores below, but set the FPR interval to start at the smallest *non-zero* FPR that can occur for this dataset and end at the first FPR step that is still \u00166% (i.e., no more than one sixth) of the negatives.\n\ntrue_y = [0,1,0,0,1,0,0,1,0,0,1,0]\npredicted_score = [0.12,0.91,0.33,0.05,0.77,0.22,0.18,0.66,0.09,0.44,0.81,0.15]", "answers": "[{\"name\":\"tdc_evaluator_range_logAUC\",\"arguments\":{\"true_y\":[0,1,0,0,1,0,0,1,0,0,1,0],\"predicted_score\":[0.12,0.91,0.33,0.05,0.77,0.22,0.18,0.66,0.09,0.44,0.81,0.15],\"FPR_range\":[0.125,0.125]}}]"}
{"func_name": "tdc_evaluator_rmsd", "func_desc": "tdc.evaluator.rmsd calculates the root-mean-square deviation (RMSD) between two sets of vectors. In the Therapeutics Data Commons (TDC) context, this function is used as a numeric evaluator to quantify average Euclidean deviation between predicted and reference vector representations (for example, predicted molecular coordinates, embeddings, or other D-dimensional descriptors) across N points. The implementation computes RMSD as sqrt(sum((V - W)**2) / N), where N is the number of points (rows) in V. The function converts its inputs to numpy arrays internally and returns a single scalar RMSD value that can be used to compare model predictions to ground truth in benchmarking and evaluation workflows.", "tools": [{"function": {"description": "tdc.evaluator.rmsd calculates the root-mean-square deviation (RMSD) between two sets of vectors. In the Therapeutics Data Commons (TDC) context, this function is used as a numeric evaluator to quantify average Euclidean deviation between predicted and reference vector representations (for example, predicted molecular coordinates, embeddings, or other D-dimensional descriptors) across N points. The implementation computes RMSD as sqrt(sum((V - W)**2) / N), where N is the number of points (rows) in V. The function converts its inputs to numpy arrays internally and returns a single scalar RMSD value that can be used to compare model predictions to ground truth in benchmarking and evaluation workflows.\n", "name": "tdc_evaluator_rmsd", "parameters": {"properties": {"V": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array representing N points with D dimensions each. In TDC usage, V typically holds reference (ground-truth) vectors such as experimentally-determined coordinates or target embeddings. The function will internally call numpy.array(V) to ensure array semantics. N is determined as len(V). If V is empty (len(V) == 0), a division-by-zero will occur.", "default": ""}, "W": {"type": "array", "items": {"type": "any"}, "description": "(N, D) array representing N points with D dimensions each corresponding to the predicted vectors to be compared against V. In TDC usage, W typically holds model predictions. The function will internally call numpy.array(W). V and W are expected to have the same shape (N, D); mismatched shapes may trigger numpy broadcasting or runtime errors and can lead to incorrect results.", "default": ""}}, "required": ["V", "W"], "type": "any"}}, "type": "function"}], "query": "I’m benchmarking a docking model in TDC and want a single RMSD score for one ligand pose. I started to compute RMSD directly between the 5 predicted 3D atom coordinates and the 5 reference coordinates, but then I realized one of the predicted rows is a docking artifact: it’s the only atom whose coordinate-wise absolute errors are all >= 0.15 Å (i.e., |dx|, |dy|, and |dz| are each at least 0.15 when compared to its corresponding reference atom). Exclude that one outlier row from BOTH the reference and predicted sets (keeping the remaining rows in their original order), and then compute RMSD with tdc.evaluator.rmsd on the filtered coordinate sets.\n\nReference V = [[0.0, 1.2, -0.5], [1.1, 0.0, 2.3], [-0.7, 1.8, 1.5], [2.0, -1.0, 0.4], [0.5, 0.5, 0.5]]\nPredicted W = [[0.1, 1.0, -0.6], [1.0, 0.2, 2.1], [-0.6, 1.9, 1.7], [1.8, -1.1, 0.6], [0.4, 0.6, 0.7]]", "answers": "[{\"name\": \"tdc_evaluator_rmsd\", \"arguments\": {\"V\": [[0.0, 1.2, -0.5], [1.1, 0.0, 2.3], [-0.7, 1.8, 1.5], [2.0, -1.0, 0.4]], \"W\": [[0.1, 1.0, -0.6], [1.0, 0.2, 2.1], [-0.6, 1.9, 1.7], [1.8, -1.1, 0.6]]}}]"}
{"func_name": "tdc_utils_label_binarize", "func_desc": "Binarization of a label list given a pre-specified numeric threshold for use in TDC data processing and label transformation pipelines.\n    \n    This function converts a list of continuous or ordinal labels into binary labels (0/1) according to a threshold. It is used in TDC's data processing utilities when transforming regression-style or scored outputs into binary classes for downstream single-instance prediction tasks, leaderboard evaluation, or classifier training. The output is a new numpy array of integer 0/1 values and the input list is not modified in place.", "tools": [{"function": {"description": "Binarization of a label list given a pre-specified numeric threshold for use in TDC data processing and label transformation pipelines.\n\nThis function converts a list of continuous or ordinal labels into binary labels (0/1) according to a threshold. It is used in TDC's data processing utilities when transforming regression-style or scored outputs into binary classes for downstream single-instance prediction tasks, leaderboard evaluation, or classifier training. The output is a new numpy array of integer 0/1 values and the input list is not modified in place.", "name": "tdc_utils_label_binarize", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "any"}, "description": "A list of labels to be binarized. In the TDC context this typically contains numeric assay measurements, predicted scores, or other scalar labels produced by datasets or model outputs. Each element of y is compared to threshold with a numeric comparison; elements must be comparable to a float (for example, ints or floats). The function will produce a 1-D output array with the same length as len(y).", "default": ""}, "threshold": {"type": "float", "description": "The numeric threshold used to decide class membership. For order=\"ascending\", elements strictly greater than threshold become 1 and all others become 0. For order=\"descending\", elements strictly less than threshold become 1 and all others become 0. Note that values exactly equal to threshold are treated as \"not greater\" and \"not less\" and therefore become 0 in both ordering modes.", "default": ""}, "order": {"type": "string", "description": "Determines the direction of the binarization rule and defaults to \"ascending\". If order is \"ascending\", a label is mapped to 1 when label > threshold and to 0 otherwise. If order is \"descending\", a label is mapped to 1 when label < threshold and to 0 otherwise. Any other string value for order is invalid and triggers an AttributeError. This parameter is useful when converting a continuous score that is either positively or negatively correlated with the desired positive class (for example, higher potency scores -> positive versus lower toxicity scores -> positive).", "default": "ascending"}}, "required": ["y", "threshold", "order"], "type": "any"}}, "type": "function"}], "query": "I’m prepping a TDC single-instance classification benchmark from continuous binding affinity scores and need to binarize them, but I realized the screening readout includes a couple of obvious QC artifacts. First, I was going to binarize everything with the usual ascending rule (higher score = more active) using the same cutoff as the 75th-percentile gate from our protocol. Wait—two entries are not physically meaningful for this assay and should be excluded before binarization: one is a negative affinity score (instrument baseline subtraction went wrong) and one is a missing value placeholder. After removing only those invalid entries, binarize the remaining scores with the same cutoff rule: assign 1 only when a score is strictly above the cutoff, else 0. Scores: [0.12, 0.75, 0.76, 1.03, -0.20, 0.50, 0.91, NaN].", "answers": "[{\"name\": \"tdc_utils_label_binarize\", \"arguments\": {\"y\": [0.12, 0.75, 0.76, 1.03, 0.5, 0.91], \"threshold\": 0.75, \"order\": \"ascending\"}}]"}
{"func_name": "tdc_utils_label_label_transform", "func_desc": "tdc.utils.label.label_transform transforms a list of raw labels used in TDC benchmarks into a numpy array suitable for downstream evaluation or model training by optionally performing binarization or log-scale conversion.\n    \n    This helper function is used in TDC data processing workflows to prepare target values from datasets (for example continuous biochemical measurements such as Kd or IC50 reported in nM) into either binary class labels for classification tasks or into a log-transformed scale commonly used in pharmacology (e.g., p-scale). It is intended to be called by dataset loaders or preprocessing pipelines when a task requires label normalization or thresholding.", "tools": [{"function": {"description": "tdc.utils.label.label_transform transforms a list of raw labels used in TDC benchmarks into a numpy array suitable for downstream evaluation or model training by optionally performing binarization or log-scale conversion.\n\nThis helper function is used in TDC data processing workflows to prepare target values from datasets (for example continuous biochemical measurements such as Kd or IC50 reported in nM) into either binary class labels for classification tasks or into a log-transformed scale commonly used in pharmacology (e.g., p-scale). It is intended to be called by dataset loaders or preprocessing pipelines when a task requires label normalization or thresholding.", "name": "tdc_utils_label_label_transform", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "any"}, "description": "A list of labels in the original dataset order. Elements are expected to be numeric when thresholding or log-conversion is requested. This list may contain continuous values (for example binding affinities in nM) or already-binary values; the function preserves the input order and returns an array of the same length.", "default": ""}, "binary": {"type": "boolean", "description": "If True and the input list y contains more than two unique values, perform binarization using the supplied threshold and order. Binarization converts a continuous label into a binary class label (1 or 0) to support binary classification benchmarks in TDC (for example converting potency measurements into active/inactive labels). If binary is False, no binarization is attempted.", "default": ""}, "threshold": {"type": "float", "description": "Numeric threshold used for binarization when binary is True and y contains more than two unique values. The threshold is compared elementwise against y to assign class 1 or 0 according to the order parameter. The threshold is not applied when the function does not perform binarization.", "default": ""}, "convert_to_log": {"type": "boolean", "description": "If True and the input list y contains more than two unique values and binarization does not occur (binary is False or not applicable), convert continuous values to a log-scale using the internal convert_y_unit function with unit conversion from \"nM\" to \"p\". This option is useful when preparing continuous pharmacological measurements for regression tasks where a negative-log transform (p-scale) is conventional.", "default": ""}, "verbose": {"type": "boolean", "description": "Whether to print intermediate processing statements to standard error. When True, the function prints a message before binarization and before log conversion. Default is True. The messages are written to sys.stderr and flushed immediately.", "default": true}, "order": {"type": "string", "description": "Determines the direction of binarization when binary is True. If \"descending\", a label is assigned 1 when the original value is strictly less than threshold (y < threshold) and 0 otherwise. If \"ascending\", a label is assigned 1 when the original value is strictly greater than threshold (y > threshold) and 0 otherwise. Default is \"descending\". This parameter controls whether smaller values are considered the positive class (\"descending\") or larger values are considered the positive class (\"ascending\").", "default": "descending"}}, "required": ["y", "binary", "threshold", "convert_to_log"], "type": "any"}}, "type": "function"}], "query": "I’m preparing a TDC-style binary benchmark from IC50 values (nM) for 8 compounds: [3.2, 8.5, 15.0, 120.0, 950.0, 2000.0, 45.0, 600.0]. Initially I was going to use 100 nM as the activity cutoff, but I just realized our assay report and the downstream evaluation script define the cutoff in a different unit scale (µM) and I need the numerically equivalent threshold expressed back in nM to keep the labels consistent. Convert the raw IC50 list into active/inactive labels where smaller IC50 means active, using that corrected threshold, and do not apply any log (p-scale) conversion.", "answers": "[{\"name\":\"tdc_utils_label_label_transform\",\"arguments\":{\"y\":[3.2,8.5,15.0,120.0,950.0,2000.0,45.0,600.0],\"binary\":true,\"threshold\":100.0,\"convert_to_log\":false}}]"}
{"func_name": "tdc_utils_misc_fuzzy_search", "func_desc": "Fuzzy matching between a user-provided dataset name and the canonical dataset name used by the TDC (Therapeutics Data Commons) library. This function is used inside TDC data-loading and utility workflows to resolve minor differences in user input (case differences, an optional \"tdc.\" prefix, or small typographical variations) to the exact dataset identifier that TDC expects when retrieving a dataset, evaluating a benchmark, or recording leaderboard submissions.", "tools": [{"function": {"description": "Fuzzy matching between a user-provided dataset name and the canonical dataset name used by the TDC (Therapeutics Data Commons) library. This function is used inside TDC data-loading and utility workflows to resolve minor differences in user input (case differences, an optional \"tdc.\" prefix, or small typographical variations) to the exact dataset identifier that TDC expects when retrieving a dataset, evaluating a benchmark, or recording leaderboard submissions.\n", "name": "tdc_utils_misc_fuzzy_search", "parameters": {"properties": {"name": {"type": "string", "description": "Input dataset name provided by a user or calling code. In the TDC domain this is typically a short identifier for a benchmark or dataset (for example, names returned by TDC retrieval utilities or passed to data loaders). The function treats this string case-insensitively, strips a leading \"tdc.\" prefix if present, and then attempts to match it against the canonical dataset identifier(s) for that task.", "default": ""}, "dataset_names": {"type": "string", "description": "The exact dataset name(s) used by TDC against which the input name is compared. In the TDC workflow this value represents the canonical dataset identifier(s) registered for a given task or dataset collection. The function tests membership of the normalized input name in this value and, if no exact case-insensitive match is found, computes a closest fuzzy match using the internal get_closet_match helper to propose the canonical name to return.", "default": ""}}, "required": ["name", "dataset_names"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up an automated TDC loader for a cardiotox screening pipeline, and the dataset name is coming from a labmate’s notes. I initially planned to pass the string `tdc.HERG` straight into the resolver, but I just realized the downstream bookkeeping step treats a leading namespace prefix as part of the identifier and will reject it. So: first strip any optional library-style prefix (if present), then fuzzy-match what remains against the canonical TDC dataset IDs: `hERG, herg_karim, herg_moltox, hiv, tox21`. Return the exact canonical identifier that should be used for loading and leaderboard logging.", "answers": "[{\"name\": \"tdc_utils_misc_fuzzy_search\", \"arguments\": {\"name\": \"HERG\", \"dataset_names\": \"hERG, herg_karim, herg_moltox, hiv, tox21\"}}]"}
{"func_name": "tdc_utils_retrieve_retrieve_benchmark_names", "func_desc": "tdc.utils.retrieve.retrieve_benchmark_names returns all benchmark dataset names associated with a queried benchmark group registered in TDC. This function is used by higher-level TDC utilities and user code to enumerate available benchmarks (datasets) that belong to a logically defined benchmark group (for example, 'ADMET_Group' as used in TDC leaderboards and examples in the README). It performs a fuzzy match of the provided group name against the internal benchmark registry and then collects dataset names from every learning task contained in the matched group so downstream code can present, iterate over, or submit results for those benchmarks.", "tools": [{"function": {"description": "tdc.utils.retrieve.retrieve_benchmark_names returns all benchmark dataset names associated with a queried benchmark group registered in TDC. This function is used by higher-level TDC utilities and user code to enumerate available benchmarks (datasets) that belong to a logically defined benchmark group (for example, 'ADMET_Group' as used in TDC leaderboards and examples in the README). It performs a fuzzy match of the provided group name against the internal benchmark registry and then collects dataset names from every learning task contained in the matched group so downstream code can present, iterate over, or submit results for those benchmarks.\n", "name": "tdc_utils_retrieve_retrieve_benchmark_names", "parameters": {"properties": {"name": {"type": "string", "description": "the name of the benchmark group to query. This should be a string identifying a benchmark group in TDC (for example, \"ADMET_Group\"); the function applies fuzzy matching against the internal registry of benchmark group names to select the closest available group. The parameter represents the logical grouping of related benchmarks (used for leaderboards, curated comparisons, and dataset discovery).", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "I’m wiring up a leaderboard submission script and need to enumerate the benchmark dataset names from TDC, but I just realized my config file sometimes contains a *slightly misspelled* benchmark-group string because it’s scraped from README snippets. I initially planned to query the group whose name indicates it’s the ADMET collection, but I’m worried the fuzzy matcher could silently match the wrong registry entry if multiple groups share that substring. \n\nSo: use `tdc.utils.retrieve.retrieve_benchmark_names` to list the dataset names for the benchmark group that (i) is the ADMET-themed leaderboard group and (ii) would be the *closest fuzzy match* to the string you’d get by taking the canonical ADMET group name and removing exactly one underscore character. Return the dataset names for that intended group so I can iterate over them deterministically in my script.", "answers": "[{\"name\": \"tdc_utils_retrieve_retrieve_benchmark_names\", \"arguments\": {\"name\": \"ADMETGroup\"}}]"}
{"func_name": "torchdrug_data_dataloader_graph_collate", "func_desc": "torchdrug.data.dataloader.graph_collate\n    Collate a list of identically-structured samples into a single batched container suitable for model input and DataLoader output.\n    \n    This function is used in TorchDrug to prepare batches of graph-structured data (for example, molecules or interaction graphs used in drug discovery tasks) and other nested containers so they can be processed efficiently by downstream models and by PyTorch DataLoader workers. It supports torch.Tensor stacking with optional shared-memory allocation when running in a DataLoader worker process, conversion of numeric scalars into tensors, preservation of raw string/bytes lists, packing of Graph objects via data.Graph.pack for batched graph processing, and recursive collation of Mapping and Sequence containers. The caller must provide a non-empty list of samples where each sample has the same nested container structure and element types; mismatches or empty input are failure modes documented below.", "tools": [{"function": {"description": "torchdrug.data.dataloader.graph_collate\nCollate a list of identically-structured samples into a single batched container suitable for model input and DataLoader output.\n\nThis function is used in TorchDrug to prepare batches of graph-structured data (for example, molecules or interaction graphs used in drug discovery tasks) and other nested containers so they can be processed efficiently by downstream models and by PyTorch DataLoader workers. It supports torch.Tensor stacking with optional shared-memory allocation when running in a DataLoader worker process, conversion of numeric scalars into tensors, preservation of raw string/bytes lists, packing of Graph objects via data.Graph.pack for batched graph processing, and recursive collation of Mapping and Sequence containers. The caller must provide a non-empty list of samples where each sample has the same nested container structure and element types; mismatches or empty input are failure modes documented below.", "name": "torchdrug_data_dataloader_graph_collate", "parameters": {"properties": {"batch": {"type": "array", "items": {"type": "any"}, "description": "list of samples with the same nested container. Each sample is expected to be one of: torch.Tensor, float, int, str, bytes, data.Graph, Mapping, or Sequence (as used in the source). The role of this parameter is to supply per-sample data produced by a dataset so that graph_collate can merge them into a single batch for training or inference in TorchDrug. In practice, batch is the list passed by torch.utils.data.DataLoader when batch_size > 1.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "I’m batching a tiny ligand-screening toy set for TorchDrug, but I just noticed my preprocessing exported labels as strings. I initially wanted to collate them as raw strings for later parsing, but that would break the loss computation in the next step (it expects numeric tensors). So: please collate the two samples into a single DataLoader batch, stacking the per-atom feature matrices as usual, and convert the labels into numeric scalars suitable for a classification loss.\n\nEach sample is a mapping with keys {\"x\", \"y\"}:\n- Sample 1: x = [[1.0, 0.0, 0.5], [0.2, 1.0, -0.1]], y = \"1\"\n- Sample 2: x = [[0.0, 1.0, 0.3], [0.4, 0.6, 0.9]], y = \"0\"\n\nReturn the collated batch exactly as torchdrug.data.dataloader.graph_collate would produce it (i.e., tensors for numeric fields, recursively collated by key).", "answers": "[{\"name\":\"torchdrug_data_dataloader_graph_collate\",\"arguments\":{\"batch\":[{\"x\":[[1.0,0.0,0.5],[0.2,1.0,-0.1]],\"y\":1},{\"x\":[[0.0,1.0,0.3],[0.4,0.6,0.9]],\"y\":0}]}}]"}
{"func_name": "torchdrug_utils_pretty_long_array", "func_desc": "Format a list as a concise, human-readable string suitable for logging and display in TorchDrug utilities.\n    \n    This function is used throughout TorchDrug to produce compact textual summaries of potentially long Python lists (for example, lists of node indices for graphs, atom or bond index lists for molecules, dataset index lists, or other long enumerations) so that logs, visualizations, and printed representations remain readable. If the list length does not exceed the truncation threshold, the full list is returned as its standard Python string representation. If the list is longer than the truncation threshold, the function returns a summary that shows the first few and last few elements separated by the literal \", ..., \" to indicate omitted middle elements. The function does not modify the input list.", "tools": [{"function": {"description": "Format a list as a concise, human-readable string suitable for logging and display in TorchDrug utilities.\n\nThis function is used throughout TorchDrug to produce compact textual summaries of potentially long Python lists (for example, lists of node indices for graphs, atom or bond index lists for molecules, dataset index lists, or other long enumerations) so that logs, visualizations, and printed representations remain readable. If the list length does not exceed the truncation threshold, the full list is returned as its standard Python string representation. If the list is longer than the truncation threshold, the function returns a summary that shows the first few and last few elements separated by the literal \", ..., \" to indicate omitted middle elements. The function does not modify the input list.", "name": "torchdrug_utils_pretty_long_array", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "any"}, "description": "The input list to format. In TorchDrug this commonly holds things like node or atom indices, attribute lists, or other ordered collections that benefit from compact display. The function expects a Python list (an object that supports len() and slicing); passing an object without these behaviors may raise a TypeError or produce unexpected output.", "default": ""}, "truncation": {"type": "integer", "description": "Threshold length above which the input list will be summarized by truncation. If len(array) is less than or equal to truncation, the function returns the full string representation of array. If len(array) is greater than truncation, the function returns a truncated summary showing the beginning and end of the list. Defaults to 10.", "default": 10}, "display": {"type": "integer", "description": "Number of elements to include from both the start and the end of the list when producing a truncated summary. For a truncated list, the output will contain the string form of array[:display] (with its closing bracket removed), then the literal \", ..., \", then the string form of array[-display:] (with its opening bracket removed). Defaults to 3. Note: if display is zero, negative, or larger than the list length, the produced string may be redundant or formatted in an unexpected way because slicing behavior and simple string slicing are used to assemble the result.", "default": 3}}, "required": ["array", "truncation", "display"], "type": "any"}}, "type": "function"}], "query": "I’m sanity-checking a TorchDrug dataloader log for a molecule graph, and I need a compact, human-readable summary of the atom index list. Initially I was going to truncate at the threshold used in our config, but I just realized that threshold counts the total list length, and we want the summary to trigger only when the list is strictly longer than a dozen items. Keep the number of items shown at the head and tail equal, and set that count to one third of the truncation threshold (rounding down if needed). Format this index list accordingly: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "answers": "[{\"name\":\"torchdrug_utils_pretty_long_array\",\"arguments\":{\"array\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\"truncation\":12,\"display\":4}}]"}
{"func_name": "torchdrug_utils_pretty_time", "func_desc": "torchdrug.utils.pretty.time formats a duration given in seconds into a concise, human-readable string suitable for logging and display in TorchDrug training, evaluation, and data processing workflows (for example, epoch time, batch processing time, or dataset loading time).\n    \n    This function converts a numeric elapsed time in seconds to one of four unit strings (\"secs\", \"mins\", \"hours\", \"days\") using fixed thresholds. It is intended to be used in monitoring and reporting runtime information in the TorchDrug domain (PyTorch-based graph and molecular ML tasks) where readable elapsed-time strings simplify experiment logs and user-facing output. The conversion uses constant thresholds sec_per_min = 60, sec_per_hour = 3600, and sec_per_day = 86400 and selects units by strict greater-than comparisons: values strictly greater than a threshold are expressed in the next larger unit. Formatting is performed with two decimal places using \"%.2f\".\n    \n    Behavior details, side effects, defaults, and failure modes: The function has no side effects and does not modify input. There is no default for seconds; the caller must supply a float. Numeric values that exactly equal the thresholds (60, 3600, 86400) are formatted in the smaller unit because the comparisons are strict (for example, 60.0 -> \"60.00 secs\", 3600.0 -> \"60.00 mins\", 86400.0 -> \"24.00 hours\"). Negative float values are accepted by the implementation and will be formatted as negative durations in seconds (for example, -5.0 -> \"-5.00 secs\"). Non-numeric inputs or types incompatible with Python float formatting may raise a TypeError or produce unexpected output; callers should pass a float or a value coercible to float. Special float values such as NaN or infinity will be formatted according to Python's float-to-string behavior.", "tools": [{"function": {"description": "torchdrug.utils.pretty.time formats a duration given in seconds into a concise, human-readable string suitable for logging and display in TorchDrug training, evaluation, and data processing workflows (for example, epoch time, batch processing time, or dataset loading time).\n\nThis function converts a numeric elapsed time in seconds to one of four unit strings (\"secs\", \"mins\", \"hours\", \"days\") using fixed thresholds. It is intended to be used in monitoring and reporting runtime information in the TorchDrug domain (PyTorch-based graph and molecular ML tasks) where readable elapsed-time strings simplify experiment logs and user-facing output. The conversion uses constant thresholds sec_per_min = 60, sec_per_hour = 3600, and sec_per_day = 86400 and selects units by strict greater-than comparisons: values strictly greater than a threshold are expressed in the next larger unit. Formatting is performed with two decimal places using \"%.2f\".\n\nBehavior details, side effects, defaults, and failure modes: The function has no side effects and does not modify input. There is no default for seconds; the caller must supply a float. Numeric values that exactly equal the thresholds (60, 3600, 86400) are formatted in the smaller unit because the comparisons are strict (for example, 60.0 -> \"60.00 secs\", 3600.0 -> \"60.00 mins\", 86400.0 -> \"24.00 hours\"). Negative float values are accepted by the implementation and will be formatted as negative durations in seconds (for example, -5.0 -> \"-5.00 secs\"). Non-numeric inputs or types incompatible with Python float formatting may raise a TypeError or produce unexpected output; callers should pass a float or a value coercible to float. Special float values such as NaN or infinity will be formatted according to Python's float-to-string behavior.", "name": "torchdrug_utils_pretty_time", "parameters": {"properties": {"seconds": {"type": "float", "description": "Elapsed time in seconds to format. In the TorchDrug context this typically represents measured durations such as per-batch, per-epoch, data-loading, or inference times during model training and evaluation. The value is compared against the constants sec_per_min (60), sec_per_hour (3600), and sec_per_day (86400) using strict greater-than checks to decide which unit to display. The function expects a numeric float; supply values already converted to seconds.", "default": ""}}, "required": ["seconds"], "type": "any"}}, "type": "function"}], "query": "I’m cleaning up my TorchDrug experiment logs for a molecule-graph model and I realized my epoch timer is unreliable: it sometimes reports the wall-clock elapsed time plus a one-time dataloader warm-up penalty, and the easiest way to detect that is when the recorded duration exceeds a full day. I initially planned to format the raw measured duration for display, but if it crosses that day boundary I want to *remove exactly one hour* (that warm-up constant) before formatting; otherwise, just format the raw number.\n\nFor the epoch I’m logging right now, the measured duration is 90500.25 seconds. Please output the concise human-readable string using TorchDrug’s pretty time formatting rules (strict greater-than thresholds; two decimals).", "answers": "[{\"name\": \"torchdrug_utils_pretty_time\", \"arguments\": {\"seconds\": 86900.25}}]"}
{"func_name": "torchdrug_utils_torch_sparse_coo_tensor", "func_desc": "Construct a sparse COO tensor without performing index validation. This function is a thin, high-performance wrapper used in the torchdrug library to build sparse tensors (COO format) for graph-structured data such as adjacency matrices or sparse node/edge features. It delegates construction to a low-level backend (torch_ext.sparse_coo_tensor_unsafe) and therefore avoids the index checks performed by torch.sparse_coo_tensor, providing faster construction for workloads in TorchDrug where indices are known to be valid in advance (for example, creating adjacency representations of molecular graphs or minibatch graph unions during training).", "tools": [{"function": {"description": "Construct a sparse COO tensor without performing index validation. This function is a thin, high-performance wrapper used in the torchdrug library to build sparse tensors (COO format) for graph-structured data such as adjacency matrices or sparse node/edge features. It delegates construction to a low-level backend (torch_ext.sparse_coo_tensor_unsafe) and therefore avoids the index checks performed by torch.sparse_coo_tensor, providing faster construction for workloads in TorchDrug where indices are known to be valid in advance (for example, creating adjacency representations of molecular graphs or minibatch graph unions during training).\n", "name": "torchdrug_utils_torch_sparse_coo_tensor", "parameters": {"properties": {"indices": {"type": "array", "items": {"type": "any"}, "description": "2D indices describing the nonzero locations in COO format. Expected shape is (2, n), where the first row contains row indices and the second row contains column indices for n nonzero entries. In TorchDrug this is typically used to represent edge endpoints or coordinate pairs for sparse features; the function uses these indices verbatim without checking bounds or uniqueness, so the caller is responsible for ensuring they are valid for the provided size.", "default": ""}, "values": {"type": "array", "items": {"type": "any"}, "description": "1D tensor of length n containing the values corresponding to each column in indices. Each entry in values is placed at the coordinate given by the corresponding column in indices. In graph and molecular workflows within TorchDrug, values often represent edge weights, adjacency indicators, or sparse feature values. The function does not validate alignment beyond relying on the backend, so mismatched lengths (values length != n) will result in a backend error.", "default": ""}, "size": {"type": "array", "items": {"type": "any"}, "description": "List specifying the desired size (shape) of the resulting sparse tensor. This list defines the overall dimensions of the sparse COO tensor (for example, [num_rows, num_cols] for a 2D sparse matrix). The provided indices are interpreted against this size; if indices contain out-of-range entries relative to size, undefined behavior or errors from the underlying implementation may occur because no bounds checking is performed.", "default": ""}}, "required": ["indices", "values", "size"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a minibatch union graph in TorchDrug and need a fast COO adjacency build (no index validation) because upstream code already sanitized everything. I was going to encode the edges as a 4×4 directed adjacency with 0→1, 1→2, 2→3, plus a self-loop on the last node, and weights [1.0, 0.5, 2.0, 1.0]. Wait—our GNN block expects an *undirected* adjacency (i.e., every directed edge must be mirrored), but self-loops should not be duplicated when mirroring. Using the same per-edge weights for the mirrored edges, construct the resulting sparse COO tensor of the appropriate square size using the unsafe sparse constructor (skip index checks).", "answers": "[{\"name\": \"torchdrug_utils_torch_sparse_coo_tensor\", \"arguments\": {\"indices\": [[0, 1, 2, 3, 1, 2, 3], [1, 2, 3, 3, 0, 1, 2]], \"values\": [1.0, 0.5, 2.0, 1.0, 1.0, 0.5, 2.0], \"size\": [4, 4]}}]"}
{"func_name": "torchio_utils_get_batch_images_and_size", "func_desc": "Get names of image entries in a batch and the number of channels (size) per image.", "tools": [{"function": {"description": "Get names of image entries in a batch and the number of channels (size) per image.\n", "name": "torchio_utils_get_batch_images_and_size", "parameters": {"properties": {"batch": {"type": "any", "description": "Batch dictionary produced by a torchio SubjectsLoader when extracting\ndata from a torchio SubjectsDataset. In the TorchIO data pipeline, each entry\nin this dictionary is expected to map a string image name (for example\n't1', 'flair', or 'label') to an image-like dictionary (the representation\nused internally by torchio.Image/SubjectsLoader). This function inspects the\nbatch to locate those entries that are dictionaries containing the key\nconstants.DATA. The value at constants.DATA is expected to be a sequence\nor tensor-like object supporting len() (in TorchIO this is typically a\nPyTorch tensor with channels-first shape, e.g. (C, W, H, D)). The function\nuses that len() to determine the per-image \"size\" (practically, the number\nof channels C). This parameter must be the raw batch mapping produced by\nthe loader; passing other structures will likely result in no images found\nor a RuntimeError.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "I’m debugging a TorchIO `SubjectsLoader` batch and I *thought* I could just iterate over the batch dict and report every key that has a `.data` tensor (like `t1`, `flair`, `label`) along with its channel count. But I just realized that this batch sometimes contains **non-image payloads** that can also be dict-like (e.g., metadata blobs or preprocessing artifacts) and I don’t want those to be mistaken as images. So: please **only** treat an entry as an image if it looks like a TorchIO image tensor, meaning it has a `data` field whose value is a numeric array with at least 4 dimensions (channels-first), and then report the image entry names and the number of channels for each (size = the first dimension of that `data`). Use the sample batch below.\n\nSample batch:\n{\n  \"t1\": {\"data\": [[[[0.0, 0.1], [0.2, 0.3]], [[0.4, 0.5], [0.6, 0.7]]], [[[0.8, 0.9], [1.0, 1.1]], [[1.2, 1.3], [1.4, 1.5]]]]},\n  \"flair\": {\"data\": [[[[1.0, 1.1], [1.2, 1.3]], [[1.4, 1.5], [1.6, 1.7]]]]},\n  \"label\": {\"data\": [[[[0, 1], [1, 0]], [[1, 0], [0, 1]]]]},\n  \"subject_id\": \"sub-001\",\n  \"age\": 57\n}\n\nReturn the discovered image keys and their per-image channel counts.", "answers": "[{\"name\": \"torchio_utils_get_batch_images_and_size\", \"arguments\": {\"batch\": {\"t1\": {\"data\": [[[[0.0, 0.1], [0.2, 0.3]], [[0.4, 0.5], [0.6, 0.7]]], [[[0.8, 0.9], [1.0, 1.1]], [[1.2, 1.3], [1.4, 1.5]]]]}, \"flair\": {\"data\": [[[[1.0, 1.1], [1.2, 1.3]], [[1.4, 1.5], [1.6, 1.7]]]]}, \"label\": {\"data\": [[[[0, 1], [1, 0]], [[1, 0], [0, 1]]]]}, \"subject_id\": \"sub-001\", \"age\": 57}}}]"}
{"func_name": "useful_rdkit_utils_ring_systems_get_min_ring_frequency", "func_desc": "useful_rdkit_utils.ring_systems.get_min_ring_frequency returns the ring identifier and its minimum observed frequency from a ring-frequency list produced by RingSystemLookup.process_smiles, used in cheminformatics workflows that analyze ring systems with RDKit.", "tools": [{"function": {"description": "useful_rdkit_utils.ring_systems.get_min_ring_frequency returns the ring identifier and its minimum observed frequency from a ring-frequency list produced by RingSystemLookup.process_smiles, used in cheminformatics workflows that analyze ring systems with RDKit.\n", "name": "useful_rdkit_utils_ring_systems_get_min_ring_frequency", "parameters": {"properties": {"ring_list": {"type": "array", "items": {"type": "any"}, "description": "A list produced by RingSystemLookup.process_smiles containing ring frequency records. Each element is expected to be a two-element sequence where the first element is a ring identifier (typically a string) and the second element is the observed frequency (typically an integer). This function will sort this list in-place by the second element (frequency) and therefore mutates the provided list. Provide an empty list for acyclic molecules (no ring systems); in that case the function returns the sentinel [\"\", -1].", "default": ""}}, "required": ["ring_list"], "type": "any"}}, "type": "function"}], "query": "I’m triaging an RDKit ring-system summary before feeding it into our RingSystemLookup-based diversity metrics. Initially I was going to just ask for the minimum frequency from this raw output of `RingSystemLookup.process_smiles`: [[\"benzene\", 12], [\"pyridine\", 7], [\"imidazole\", 7], [\"cyclohexane\", 15]]. But then I realized the downstream QC step requires a deterministic single hit: if there’s a tie for the minimum count, we must break it by picking the ring identifier that would come first in a case-insensitive alphabetical sort of the tied identifiers. Using that rule, which ring identifier should be reported as the minimum, and what is the minimum observed frequency? Use `useful_rdkit_utils.ring_systems.get_min_ring_frequency` only after applying the tie-break so the input list has a unique minimum.", "answers": "[{\"name\": \"useful_rdkit_utils_ring_systems_get_min_ring_frequency\", \"arguments\": {\"ring_list\": [[\"imidazole\", 7]]}}]"}
