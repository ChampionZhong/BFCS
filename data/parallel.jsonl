{"func_name": "Bio_AlignIO_PhylipIO_sanitize_name", "func_desc": "Bio.AlignIO.PhylipIO.sanitize_name: Sanitize a sequence identifier string for safe output in PHYLIP-style alignment files used by Biopython's AlignIO/PhylipIO code.\n    \n    This function prepares a Python string intended to be used as a sequence identifier when writing alignment files in the PHYLIP family of formats. It applies a fixed, deterministic sequence of transformations to ensure the identifier does not contain characters banned or problematic for PHYLIP writers and optionally truncates the identifier to a specified maximum width. This routine is used by Biopython's PHYLIP output code to improve interoperability of produced files with downstream tools that expect simple identifiers.", "tools": [{"function": {"description": "Bio.AlignIO.PhylipIO.sanitize_name: Sanitize a sequence identifier string for safe output in PHYLIP-style alignment files used by Biopython's AlignIO/PhylipIO code.\n\nThis function prepares a Python string intended to be used as a sequence identifier when writing alignment files in the PHYLIP family of formats. It applies a fixed, deterministic sequence of transformations to ensure the identifier does not contain characters banned or problematic for PHYLIP writers and optionally truncates the identifier to a specified maximum width. This routine is used by Biopython's PHYLIP output code to improve interoperability of produced files with downstream tools that expect simple identifiers.", "name": "Bio_AlignIO_PhylipIO_sanitize_name", "parameters": {"properties": {"name": {"type": "string", "description": "The input sequence identifier to sanitize. The function will first remove leading and trailing whitespace (via str.strip()), then delete any occurrences of the characters '[', ']', '(', ')', and ',' from the string, and finally replace any ':' or ';' characters with the pipe character '|' to avoid using punctuation that can break PHYLIP parsers. This parameter must be a Python str; passing another type that does not implement str.strip() will raise an exception (typically AttributeError).", "default": ""}, "width": {"type": "integer", "nullable": true, "description": "Optional maximum number of characters to keep from the sanitized name. If specified as an int, the sanitized name is truncated by Python slicing name[:width]. If width is None (the default), no truncation is performed. If a non-int, non-None value is provided, slicing will raise a TypeError. Negative integer values for width follow Python slicing semantics (for example, width == -1 will omit the final character), so callers should pass a non-negative int when the intent is a simple left-side truncation.", "default": null}}, "required": ["name", "width"], "type": "any"}}, "type": "function"}], "query": "We’re exporting a mixed-source multiple-sequence alignment to a PHYLIP-derived format and need identifiers sanitized in a way that mimics our real ingest rules. Given these raw identifiers from different pipelines:\n\n1) \"Homo_sapiens[chr1]:BRCA1;v2 (sample, test)\"\n2) \"  Homo_sapiens[BRCA1](isoform-1):v2  \"\n\nRun the PHYLIP sanitation step on each identifier using Biopython’s deterministic rules. Then apply a cohort-derived width policy: compute the trimmed (leading/trailing whitespace removed) raw length for each identifier; if the trimmed length is strictly greater than the median trimmed length of the cohort, cap the sanitized identifier to 15 characters, otherwise cap it to 20 characters. Return the finalized PHYLIP-ready identifiers in the same order.", "answers": "[{\"name\":\"Bio_AlignIO_PhylipIO_sanitize_name\",\"arguments\":{\"name\":\"Homo_sapiens[chr1]:BRCA1;v2 (sample, test)\",\"width\":15}},{\"name\":\"Bio_AlignIO_PhylipIO_sanitize_name\",\"arguments\":{\"name\":\"  Homo_sapiens[BRCA1](isoform-1):v2  \",\"width\":20}}]"}
{"func_name": "Bio_Cluster_distancematrix", "func_desc": "Compute and return a triangular distance matrix from a 2-D NumPy array for use in clustering and other computational-molecular-biology analyses.", "tools": [{"function": {"description": "Compute and return a triangular distance matrix from a 2-D NumPy array for use in clustering and other computational-molecular-biology analyses.\n", "name": "Bio_Cluster_distancematrix", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "nrows x ncolumns 2-D array containing the numeric data values.\nIn the Bio.Cluster context this represents the matrix of observations used to\ncompute pairwise distances (for example rows = samples and columns = features,\nor vice versa when transpose=True). This function calls internal validation\n(__check_data) and requires a numeric, two-dimensional numpy.ndarray.", "default": ""}, "mask": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "nrows x ncolumns array of integers indicating missing\nvalues (same shape as data). If mask[i, j] == 0 then data[i, j] is treated as\nmissing and excluded from pairwise distance calculations. If None (the default),\nno values are treated as missing. The mask must align with data shape exactly;\nsupplying an incorrectly shaped mask will raise an error.", "default": null}, "weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1-D array of weights applied to the data when\ncalculating distances. The length of weight must match the number of items over\nwhich pairwise distances are computed (that is, the number of columns when\ntranspose=False, or the number of rows when transpose=True). If None (the\ndefault) equal weighting is assumed. The function validates weight length via\n__check_weight and will raise an error on length mismatch.", "default": null}, "transpose": {"type": "boolean", "description": "If False (default), distances are computed between rows of data.\nIf True, distances are computed between columns of data. This is provided to\nsupport common analysis patterns in molecular data where either samples or\nmeasured features may be compared.", "default": false}, "dist": {"type": "string", "description": "Single-character code selecting the distance or similarity metric to\nuse (default 'e'). The supported codes and their meanings are:\n'e' -- Euclidean distance\n'b' -- City Block (Manhattan) distance\n'c' -- Pearson correlation\n'a' -- Absolute value of the Pearson correlation\n'u' -- Uncentered correlation\n'x' -- Absolute value of the uncentered correlation\n's' -- Spearman's rank correlation\n'k' -- Kendall's tau\nThe chosen code controls the mathematical form used for pairwise comparisons;\ninvalid or unsupported codes will result in an error from the underlying\nroutine.", "default": "e"}}, "required": ["data", "transpose", "dist", "mask", "weight"], "type": "any"}}, "type": "function"}], "query": "We’re prepping inter-tissue distance inputs for clustering from a noisy pilot expression panel (4 tissues × 3 genes) where the lab reported mixed assay reliability. Start from two raw matrices collected on the same tissues:\n\n1) Quantification matrix Q (intended for absolute-scale distance): [[5.2, 3.1, 4.8], [4.9, 2.8, 4.5], [6.0, 3.5, 5.1], [5.5, 3.0, 4.9]].\n2) Spike-in–contaminated matrix S (values of 0.0 indicate a non-detect that should be considered missing only when corroborated by the mask): [[5.2, 3.1, 0.0], [4.8, 2.9, 0.5], [6.0, 3.5, 0.2], [5.5, 3.0, 0.1]] with mask M = [[1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1]].\n\nRun a two-arm clustering-prep batch with protocol branching based on intrinsic signal properties:\n- Arm A (absolute-scale baseline): use Q as-is (no transpose). Use Euclidean distances only if every gene shows non-trivial across-tissue dynamic range (max−min) of at least 0.5 in Q; otherwise fall back to Pearson-correlation distance.\n- Arm B (robustness replicate): use S with mask M and keep orientation unchanged (no transpose). Use per-gene weights that depend on how dynamic each gene is in the baseline Q: compute each gene’s range in Q and set weight = 1.0 for genes with range < 0.5, 1.5 for genes with 0.5 ≤ range < 1.0, and 2.0 for genes with range ≥ 1.0. For the distance metric, use Pearson-correlation distance if any masked non-detect is present in M; otherwise use Euclidean.\n\nReturn the triangular inter-tissue distance matrix for each arm.", "answers": "[{\"name\":\"Bio_Cluster_distancematrix\",\"arguments\":{\"data\":[[5.2,3.1,4.8],[4.9,2.8,4.5],[6.0,3.5,5.1],[5.5,3.0,4.9]],\"mask\":null,\"weight\":null,\"transpose\":false,\"dist\":\"e\"}},{\"name\":\"Bio_Cluster_distancematrix\",\"arguments\":{\"data\":[[5.2,3.1,0.0],[4.8,2.9,0.5],[6.0,3.5,0.2],[5.5,3.0,0.1]],\"mask\":[[1,1,0],[1,1,1],[1,1,1],[1,1,1]],\"weight\":[1.5,1.5,1.0],\"transpose\":false,\"dist\":\"c\"}}]"}
{"func_name": "Bio_Cluster_kcluster", "func_desc": "Perform k-means clustering on a numeric matrix, returning cluster assignments,\n    the within-cluster sum of distances for the best solution found, and how many\n    times that best solution was observed. This implementation is used in the\n    Biopython project for clustering rows or columns of biological data matrices\n    (e.g., gene expression data: genes as rows, samples as columns), and is\n    suitable for computational molecular biology workflows that require repeated\n    k-means runs with different initializations or a single deterministic run\n    starting from a provided initial clustering.", "tools": [{"function": {"description": "Perform k-means clustering on a numeric matrix, returning cluster assignments,\nthe within-cluster sum of distances for the best solution found, and how many\ntimes that best solution was observed. This implementation is used in the\nBiopython project for clustering rows or columns of biological data matrices\n(e.g., gene expression data: genes as rows, samples as columns), and is\nsuitable for computational molecular biology workflows that require repeated\nk-means runs with different initializations or a single deterministic run\nstarting from a provided initial clustering.", "name": "Bio_Cluster_kcluster", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "nrows x ncolumns array containing the data values.\nRows correspond to items and columns to variables by default. The shape\nexpectation is preserved: if transpose is False, the number of items\nclustered (nitems) equals data.shape[0]; if transpose is True, nitems\nequals data.shape[1]. The array must be a NumPy ndarray containing\nnumeric values; internal validation will raise an error if the input\nis not compatible.", "default": ""}, "nclusters": {"type": "integer", "description": "number of clusters (the \"k\" in k-means). This controls\nhow many cluster centroids the algorithm fits. nclusters should be a\npositive integer; if it is greater than the number of items, the\nunderlying implementation or validation will raise an error.", "default": 2}, "mask": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "nrows x ncolumns array of integers indicating missing\nentries. mask[i,j] == 0 marks data[i,j] as missing and that element\nwill be ignored in distance and centroid calculations. If None (the\ndefault), all data are treated as present. The mask must have the same\nshape as data when provided.", "default": null}, "weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "one-dimensional array of length equal to the\nnumber of variables (ndata) containing weights applied when\ncalculating distances and cluster centers. If None (the default),\nequal weighting is assumed. The internal validation enforces the\nrequired length and numeric type.", "default": null}, "transpose": {"type": "boolean", "description": "if False (default), rows of data are treated as the\nitems to be clustered and columns as variables; if True, columns are\ntreated as items and rows as variables. This option lets the function\ncluster either genes (rows) or samples (columns) in biological matrices\nwithout copying or transposing the input externally.", "default": false}, "npass": {"type": "integer", "description": "number of times to perform the k-means algorithm with\ndifferent random initial clusterings when initialid is None. The\nroutine returns the best solution found across these npass runs and\nhow many times that best solution was encountered. Default is 1.\nIf initialid is provided, npass is ignored and the algorithm runs\nonce deterministically from the supplied initial clustering.", "default": 1}, "method": {"type": "string", "description": "how to compute the center of a cluster. Supported codes are\n'a' for arithmetic mean (default) and 'm' for median. This affects the\ncentroid update step and therefore the fitted clusters.", "default": "a"}, "dist": {"type": "string", "description": "specifies the distance or similarity metric used to assign\nitems to clusters. Supported codes (and their meanings) are:\n'e' for Euclidean distance (default), 'b' for City Block distance,\n'c' for Pearson correlation, 'a' for absolute value of the\ncorrelation, 'u' for uncentered correlation, 'x' for absolute\nuncentered correlation, 's' for Spearman's rank correlation, and\n'k' for Kendall's tau. The chosen metric determines how distances\nbetween items and cluster centers are computed and therefore impacts\nclustering results.", "default": "e"}, "initialid": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "initial clustering assignment used to start\nthe algorithm. If provided (an array of length equal to the number of\nitems, containing integer cluster indices), the routine carries out the\nk-means (EM) algorithm exactly once from that initial partition and\ndoes not randomize the order in which items are reassigned; the run\nis therefore fully deterministic. If None (default), npass randomized\nstarts are used and results may vary between runs unless an external\nrandom seed is set.", "default": null}}, "required": ["data", "method", "npass", "nclusters", "dist", "initialid", "transpose", "weight", "mask"], "type": "any"}}, "type": "function"}], "query": "We’re re-running gene-level k-means on two expression cohorts (genes are rows; do not transpose) under a QC-aware protocol that mimics real assay messiness. Use Euclidean distance and arithmetic-mean cluster centers throughout.\n\nCohort A (6 genes × 3 samples): matrix = [[1.2,1.0,0.8],[1.1,0.9,0.7],[5.0,5.2,4.9],[4.8,5.1,5.0],[9.5,9.7,9.6],[9.6,9.8,9.7]]. Treat this as a clean run: no masking and no per-sample weights. Set the number of clusters to the number of visually separable expression bands in the cohort (low/mid/high) and use 10 random-start passes.\n\nCohort B (8 genes × 5 samples): matrix = [[5.2,3.1,0.0,2.8,4.0],[4.9,0.0,2.2,3.0,3.8],[0.0,1.2,3.5,2.9,4.1],[6.0,3.5,0.0,3.1,4.3],[2.1,1.9,2.3,0.0,1.8],[2.4,2.2,2.6,1.9,0.0],[7.2,3.9,4.1,0.0,5.0],[1.8,0.0,1.9,1.5,2.0]]. Here, interpret any 0.0 as a missing measurement and construct a binary mask from the matrix accordingly (1 where observed, 0 where missing). Apply the per-sample weights [1.0, 0.5, 1.5, 1.0, 2.0]. Use the same cluster-count rule as above and run fewer random-start passes for higher sparsity: set npass to 5 if the fraction of missing entries exceeds 20%, otherwise 10. Use random initialization (no provided initial clustering).", "answers": "[{\"name\":\"Bio_Cluster_kcluster\",\"arguments\":{\"data\":[[1.2,1.0,0.8],[1.1,0.9,0.7],[5.0,5.2,4.9],[4.8,5.1,5.0],[9.5,9.7,9.6],[9.6,9.8,9.7]],\"nclusters\":3,\"mask\":null,\"weight\":null,\"transpose\":false,\"npass\":10,\"method\":\"a\",\"dist\":\"e\",\"initialid\":null}},{\"name\":\"Bio_Cluster_kcluster\",\"arguments\":{\"data\":[[5.2,3.1,0.0,2.8,4.0],[4.9,0.0,2.2,3.0,3.8],[0.0,1.2,3.5,2.9,4.1],[6.0,3.5,0.0,3.1,4.3],[2.1,1.9,2.3,0.0,1.8],[2.4,2.2,2.6,1.9,0.0],[7.2,3.9,4.1,0.0,5.0],[1.8,0.0,1.9,1.5,2.0]],\"nclusters\":3,\"mask\":[[1,1,0,1,1],[1,0,1,1,1],[0,1,1,1,1],[1,1,0,1,1],[1,1,1,0,1],[1,1,1,1,0],[1,1,1,0,1],[1,0,1,1,1]],\"weight\":[1.0,0.5,1.5,1.0,2.0],\"transpose\":false,\"npass\":5,\"method\":\"a\",\"dist\":\"e\",\"initialid\":null}}]"}
{"func_name": "Bio_Cluster_kmedoids", "func_desc": "Perform k-medoids clustering on a distance matrix.\n    \n    This function performs k-medoids clustering (partitioning around medoids) on a set of items given a distance matrix and returns the best clustering found, the within-cluster sum of distances (objective) for that clustering, and how many times that optimal solution was discovered across repeated restarts. In the context of Biopython and computational molecular biology, this is typically used to cluster biological items (for example, sequences, profiles, or other pairwise comparisons) when a precomputed pairwise distance matrix is available. The routine accepts three different representations of a symmetric distance matrix (full 2D array, condensed 1D array, or a list of lower-triangular rows) and uses only the lower-triangular part when a full 2D NumPy array is provided. The implementation validates the distance input, prepares an initial clustering (either provided or generated randomly), and delegates the core computation to the underlying clustering routine. If an explicit initial clustering is supplied via initialid, the algorithm runs once deterministically using that initialization; otherwise it performs npass independent random restarts and returns the best solution found.", "tools": [{"function": {"description": "Perform k-medoids clustering on a distance matrix.\n\nThis function performs k-medoids clustering (partitioning around medoids) on a set of items given a distance matrix and returns the best clustering found, the within-cluster sum of distances (objective) for that clustering, and how many times that optimal solution was discovered across repeated restarts. In the context of Biopython and computational molecular biology, this is typically used to cluster biological items (for example, sequences, profiles, or other pairwise comparisons) when a precomputed pairwise distance matrix is available. The routine accepts three different representations of a symmetric distance matrix (full 2D array, condensed 1D array, or a list of lower-triangular rows) and uses only the lower-triangular part when a full 2D NumPy array is provided. The implementation validates the distance input, prepares an initial clustering (either provided or generated randomly), and delegates the core computation to the underlying clustering routine. If an explicit initial clustering is supplied via initialid, the algorithm runs once deterministically using that initialization; otherwise it performs npass independent random restarts and returns the best solution found.", "name": "Bio_Cluster_kmedoids", "parameters": {"properties": {"distance": {"type": "array", "items": {"type": "float"}, "description": "The pairwise distance matrix between items. Accepted formats are: (1) a 2D NumPy array (only the left-lower / lower-triangular part is accessed), (2) a 1D NumPy array containing the condensed distances consecutively (as in SciPy/cluster condensed format), or (3) a list of 1D arrays where each element i contains the i-th row of the lower-triangular part (the first element may be empty). This argument is required and is validated by the function; malformed shapes or incompatible dimensions will raise a ValueError. The distances represent domain-specific dissimilarities (for example, sequence edit distances, profile distances, or other biologically meaningful dissimilarity measures) and must be non-negative as expected by k-medoids algorithms.", "default": ""}, "nclusters": {"type": "integer", "description": "The desired number of clusters (the k in k-medoids). Must be a positive integer less than or equal to the number of items represented by distance. Default is 2. If nclusters is larger than the number of items, the routine will raise an error.", "default": 2}, "npass": {"type": "integer", "description": "The number of independent runs (random restarts) of the k-medoids algorithm to perform when no initialid is provided. Each pass uses a different random initial clustering to reduce the chance of converging to a poor local optimum. Default is 1. If initialid is supplied, npass is ignored and the algorithm runs exactly once using the provided initial clustering.", "default": 1}, "initialid": {"type": "any", "nullable": true, "description": "An optional initial clustering assignment used to start the algorithm deterministically. When given, initialid should specify a clustering consistent with the number of items (typically as an array-like of medoid indices or cluster assignments compatible with the internal checks); the routine will perform the EM-style re-assignment/medoid update sequence once starting from this initialization and will not randomize the order in which items are assigned to clusters. Providing initialid makes the run deterministic (useful for reproducible experiments or continuing clustering from a previous result). When initialid is None (the default), the routine will perform npass randomized starts and return the best solution found.", "default": null}}, "required": ["distance", "npass", "initialid", "nclusters"], "type": "any"}}, "type": "function"}], "query": "We’re benchmarking PAM/k-medoids stability under heterogeneous experimental noise for two independent 5-structure protein panels. Treat each provided 5×5 matrix as a precomputed dissimilarity, but choose clustering settings *from the matrix itself*.\n\nFor each matrix:\n1) First, screen for numerical pathologies typical of failed distance calculations: proceed with clustering only if all off-diagonal distances are non-negative and the matrix is symmetric within a small tolerance (|d_ij − d_ji| ≤ 1e-6).\n2) Use exactly 2 clusters for any matrix whose off-diagonal distance dynamic range (max − min over i<j) is ≥ 1.0; otherwise use 3 clusters to avoid degenerate bipartitions on near-uniform distance fields.\n3) Set the number of random restarts (npass) proportional to distance-field heterogeneity: use 25 restarts when the coefficient of variation (std/mean over i<j) exceeds 0.6; otherwise use 10 restarts.\n4) Do not supply any user-defined initialization (initialid must be absent/None).\n\nReport the best clustering found per matrix under these rules.\n\nMatrices:\nA (RMSD-like):\n[[0.0, 1.2, 2.5, 3.1, 2.9],\n [1.2, 0.0, 1.8, 2.7, 2.4],\n [2.5, 1.8, 0.0, 1.3, 1.6],\n [3.1, 2.7, 1.3, 0.0, 0.9],\n [2.9, 2.4, 1.6, 0.9, 0.0]]\n\nB (sequence-distance-like):\n[[0.0, 0.12, 0.55, 0.58, 0.6],\n [0.12, 0.0, 0.57, 0.59, 0.61],\n [0.55, 0.57, 0.0, 0.1, 0.15],\n [0.58, 0.59, 0.1, 0.0, 0.11],\n [0.6, 0.61, 0.15, 0.11, 0.0]]", "answers": "[{\"name\":\"Bio_Cluster_kmedoids\",\"arguments\":{\"distance\":[[0.0,1.2,2.5,3.1,2.9],[1.2,0.0,1.8,2.7,2.4],[2.5,1.8,0.0,1.3,1.6],[3.1,2.7,1.3,0.0,0.9],[2.9,2.4,1.6,0.9,0.0]],\"nclusters\":2,\"npass\":10,\"initialid\":null}},{\"name\":\"Bio_Cluster_kmedoids\",\"arguments\":{\"distance\":[[0.0,0.12,0.55,0.58,0.6],[0.12,0.0,0.57,0.59,0.61],[0.55,0.57,0.0,0.1,0.15],[0.58,0.59,0.1,0.0,0.11],[0.6,0.61,0.15,0.11,0.0]],\"nclusters\":3,\"npass\":10,\"initialid\":null}}]"}
{"func_name": "Bio_Cluster_somcluster", "func_desc": "Calculate a Self-Organizing Map (SOM) on a rectangular grid and return the cluster assignments and cluster centroids.", "tools": [{"function": {"description": "Calculate a Self-Organizing Map (SOM) on a rectangular grid and return the cluster assignments and cluster centroids.\n", "name": "Bio_Cluster_somcluster", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "Two-dimensional numeric array containing the data values to cluster. For typical use in computational molecular biology (e.g., clustering gene expression profiles, microarray columns/rows, or other high-dimensional experimental measurements), data is an nrows x ncolumns array: rows conventionally represent items (e.g., genes, samples) and columns represent measured features. This function will validate and may convert the input via internal checks; the input array itself is treated as read-only by callers, but an internal copy or view may be created for computation.", "default": ""}, "mask": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional two-dimensional integer array with the same shape as data indicating missing values. A mask element equal to 0 signals that the corresponding data element is missing and should be ignored during distance and centroid calculations. If None (the default), no missing entries are indicated. The function will validate the mask shape against the data and may raise an error if incompatible.", "default": null}, "weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional one-dimensional numeric array of length equal to the number of features used when calculating distances (the number of columns when clustering rows, or the number of rows when clustering columns). These weights scale the contribution of each feature to the distance metric. If None (the default), all features are treated equally. The function validates the length of weight against the data dimensionality.", "default": null}, "transpose": {"type": "boolean", "description": "If False (default), the function clusters rows of data (each row is an item to be assigned to a SOM cell). If True, the function clusters columns of data (each column is an item). This flag determines how the input array is interpreted and therefore the shapes of outputs: when clustering rows, the number of features used for distance computations is the number of columns; when clustering columns, it is the number of rows.", "default": false}, "nxgrid": {"type": "integer", "description": "Horizontal dimension of the rectangular SOM map (number of cells along the x axis). Must be a positive integer; the function raises ValueError if nxgrid < 1. Default is 2. In practice this controls the granularity of the 2D map used to organize items.", "default": 2}, "nygrid": {"type": "integer", "description": "Vertical dimension of the rectangular SOM map (number of cells along the y axis). Must be a positive integer; the function raises ValueError if nygrid < 1. Default is 1. Together nxgrid and nygrid define the total number of map cells (nxgrid * nygrid) used to represent clusters.", "default": 1}, "inittau": {"type": "float", "description": "Initial value of the neighborhood function parameter tau which determines how strongly nearby SOM cells influence each other's centroids during training. The parameter is a positive floating point scalar; default is 0.02. Smaller or larger values change the smoothing/neighborhood size during the iterative training and therefore affect topology preservation.", "default": 0.02}, "niter": {"type": "integer", "description": "Number of iterations of the SOM training algorithm to perform. Each iteration refines the centroids across the grid; higher values increase computation time but may improve map organization. Default is 1. The routine performs the specified number of iterations using the chosen distance metric and neighborhood schedule starting from inittau.", "default": 1}, "dist": {"type": "string", "description": "One-character code selecting the distance function used to compare items and centroids. Accepted values and their meanings are: 'e' for Euclidean distance; 'b' for City Block (Manhattan) distance; 'c' for Pearson correlation (centered correlation); 'a' for the absolute value of the Pearson correlation; 'u' for uncentered correlation; 'x' for the absolute value of the uncentered correlation; 's' for Spearman's rank correlation; 'k' for Kendall's tau. The string must match one of these documented codes; invalid codes will result in an error in the underlying implementation.", "default": "e"}}, "required": ["data", "transpose", "dist", "nygrid", "niter", "inittau", "mask", "nxgrid", "weight"], "type": "any"}}, "type": "function"}], "query": "I’m doing SOM-based exploratory clustering on three messy gene-expression cohorts where each cohort needs a protocol chosen from its intrinsic data quality. For each cohort matrix (genes×conditions/timepoints; cluster genes/rows; Euclidean distance; no transposition), first quantify per-gene observation completeness as the fraction of non-missing entries from the provided binary mask (0=missing, 1=observed). Only if the cohort’s median per-gene completeness is at least 0.875, treat it as “high-coverage” and use a 3×2 rectangular SOM; otherwise treat it as “low-coverage” and use a 2×2 SOM. Set niter per cohort to 10×(number of conditions/timepoints). Use the same tau initialization (0.05) everywhere. Feature-weighting is also cohort-adaptive: if the last two columns contain at least one missing value anywhere in the cohort mask, upweight those last two columns by 2× relative to the first two; otherwise use equal weights. Run this protocol on the following cohorts (use their matrices and masks exactly as given) and return both per-gene cluster assignments and SOM centroids for each cohort.\n\nCohort A data (8×4): [[5.1,4.9,0.0,5.0],[4.8,5.2,4.9,5.1],[1.2,1.0,1.1,1.3],[1.0,1.1,0.9,1.2],[3.5,3.6,3.7,3.4],[3.6,0.0,3.5,3.7],[7.8,8.0,7.9,8.1],[8.2,7.9,8.1,0.0]]; mask: [[1,1,0,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,0,1,1],[1,1,1,1],[1,1,1,0]].\n\nCohort B data (6×4): [[1.2,0.9,1.5,1.7],[0.8,1.1,1.0,0.9],[2.0,2.1,1.9,2.2],[1.9,2.2,2.0,2.1],[0.2,0.1,0.3,0.4],[0.0,0.2,0.1,0.3]]; mask: [[1,1,1,1],[1,1,1,1],[1,1,0,1],[1,1,1,1],[1,0,1,1],[1,1,1,1]].\n\nCohort C data (6×4): [[1.2,0.9,1.1,1.0],[2.5,2.7,2.6,2.4],[0.1,0.2,0.0,0.1],[3.0,3.1,2.9,3.2],[1.8,1.6,1.7,1.9],[0.5,0.6,0.4,0.5]]; mask: [[1,1,1,1],[1,1,1,1],[1,1,0,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]].", "answers": "[{\"name\":\"Bio_Cluster_somcluster\",\"arguments\":{\"data\":[[5.1,4.9,0.0,5.0],[4.8,5.2,4.9,5.1],[1.2,1.0,1.1,1.3],[1.0,1.1,0.9,1.2],[3.5,3.6,3.7,3.4],[3.6,0.0,3.5,3.7],[7.8,8.0,7.9,8.1],[8.2,7.9,8.1,0.0]],\"mask\":[[1,1,0,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,0,1,1],[1,1,1,1],[1,1,1,0]],\"weight\":[1.0,1.0,2.0,2.0],\"transpose\":false,\"nxgrid\":3,\"nygrid\":2,\"inittau\":0.05,\"niter\":40,\"dist\":\"e\"}},{\"name\":\"Bio_Cluster_somcluster\",\"arguments\":{\"data\":[[1.2,0.9,1.5,1.7],[0.8,1.1,1.0,0.9],[2.0,2.1,1.9,2.2],[1.9,2.2,2.0,2.1],[0.2,0.1,0.3,0.4],[0.0,0.2,0.1,0.3]],\"mask\":[[1,1,1,1],[1,1,1,1],[1,1,0,1],[1,1,1,1],[1,0,1,1],[1,1,1,1]],\"weight\":[1.0,1.0,2.0,2.0],\"transpose\":false,\"nxgrid\":3,\"nygrid\":2,\"inittau\":0.05,\"niter\":40,\"dist\":\"e\"}},{\"name\":\"Bio_Cluster_somcluster\",\"arguments\":{\"data\":[[1.2,0.9,1.1,1.0],[2.5,2.7,2.6,2.4],[0.1,0.2,0.0,0.1],[3.0,3.1,2.9,3.2],[1.8,1.6,1.7,1.9],[0.5,0.6,0.4,0.5]],\"mask\":[[1,1,1,1],[1,1,1,1],[1,1,0,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]],\"weight\":[1.0,1.0,2.0,2.0],\"transpose\":false,\"nxgrid\":3,\"nygrid\":2,\"inittau\":0.05,\"niter\":40,\"dist\":\"e\"}}]"}
{"func_name": "Bio_Data_CodonTable_list_ambiguous_codons", "func_desc": "Bio.Data.CodonTable.list_ambiguous_codons: Extend a list of codon strings by adding unambiguous and ambiguous codon codes that represent only the provided codons, used for example when constructing codon tables in Biopython (e.g., to extend a list of stop codons with ambiguity codes like 'TAR' or 'URA').\n    \n    This function is used in the Biopython codon table utilities to compute ambiguous codon symbols that are safe to add to an existing set of codons (for example, stop codons). It examines the nucleotides observed at each of the three codon positions in the provided codons and identifies ambiguous nucleotide letters (from ambiguous_nucleotide_values) whose concrete nucleotide meanings are fully represented at that position across the input codons. It then forms candidate ambiguous codons by combining such letters for the three positions and retains only those candidates whose full expansion (all concrete codons implied by the ambiguous letters) are present in the input codons. The function returns a new list containing the original codons (in their original order) followed by any added ambiguous codons (in a deterministic order produced by the algorithm). The function does not modify the input codons list or the ambiguous_nucleotide_values mapping.", "tools": [{"function": {"description": "Bio.Data.CodonTable.list_ambiguous_codons: Extend a list of codon strings by adding unambiguous and ambiguous codon codes that represent only the provided codons, used for example when constructing codon tables in Biopython (e.g., to extend a list of stop codons with ambiguity codes like 'TAR' or 'URA').\n\nThis function is used in the Biopython codon table utilities to compute ambiguous codon symbols that are safe to add to an existing set of codons (for example, stop codons). It examines the nucleotides observed at each of the three codon positions in the provided codons and identifies ambiguous nucleotide letters (from ambiguous_nucleotide_values) whose concrete nucleotide meanings are fully represented at that position across the input codons. It then forms candidate ambiguous codons by combining such letters for the three positions and retains only those candidates whose full expansion (all concrete codons implied by the ambiguous letters) are present in the input codons. The function returns a new list containing the original codons (in their original order) followed by any added ambiguous codons (in a deterministic order produced by the algorithm). The function does not modify the input codons list or the ambiguous_nucleotide_values mapping.", "name": "Bio_Data_CodonTable_list_ambiguous_codons", "parameters": {"properties": {"codons": {"type": "array", "items": {"type": "float"}, "description": "A list of codon strings to extend. Each element is expected to be a three-character nucleotide triplet (DNA or RNA letters such as 'A', 'C', 'G', 'T' or 'U'), for example ['TAG', 'TAA'] or ['UAG', 'UGA']. In Biopython this argument is typically a set of codons with a specific biological meaning (for example, stop codons). The function uses these codons as the authoritative set: ambiguous codons will only be added if every concrete codon they represent is already present in this list. The input list is not modified; it is copied into the returned list.", "default": ""}, "ambiguous_nucleotide_values": {"type": "any", "description": "A mapping from single-letter ambiguous nucleotide codes to the string (or sequence) of concrete nucleotides they represent, for example {'R': 'AG', 'Y': 'CT'} (or using 'U' for RNA). This mapping is used to (1) determine which ambiguous letters cover all observed nucleotides at each codon position, and (2) expand candidate ambiguous codons into their concrete codons for validation. Keys must be single-character strings corresponding to ambiguity codes used in the candidates; values must be iterable strings of concrete nucleotide characters. If a required ambiguity code is missing from this mapping, a KeyError will be raised during expansion.", "default": ""}}, "required": ["codons", "ambiguous_nucleotide_values"], "type": "any"}}, "type": "function"}], "query": "We’re curating stop-codon annotations from a mixed-source genetic code export where codon tokens may be DNA- or RNA-context and may include duplicates from multiple submitters. Start with the raw stop-codon pool:\n\n[\"UAA\",\"UAG\",\"UGA\",\"TAA\",\"TAG\",\"TGA\",\"UAA\",\"tag\",\"UGA\",\"TAA\"]\n\n1) Normalize for analysis by treating codons case-insensitively and partitioning them into an RNA cohort (codons whose letters are restricted to {A,C,G,U}) and a DNA cohort (restricted to {A,C,G,T}).\n2) For each cohort, de-duplicate while preserving first-seen order, and then run the codon-table finalization step: extend the cohort’s stop-codon list with any IUPAC ambiguous codon symbols whose full expansion is entirely contained within that cohort’s observed stops.\n3) Use the standard RNA IUPAC ambiguity mapping for the RNA cohort and the standard DNA IUPAC ambiguity mapping for the DNA cohort.\n4) As a deterministic-output verification, rerun the extension on the DNA cohort a second time using the same de-duplicated DNA stops and mapping, and compare outputs downstream.\n\nReturn the three extended lists (RNA extended, DNA extended, DNA replicate extended).", "answers": "[{\"name\":\"Bio_Data_CodonTable_list_ambiguous_codons\",\"arguments\":{\"codons\":[\"UAA\",\"UAG\",\"UGA\"],\"ambiguous_nucleotide_values\":{\"A\":\"A\",\"C\":\"C\",\"G\":\"G\",\"U\":\"U\",\"R\":\"AG\",\"Y\":\"CU\",\"S\":\"GC\",\"W\":\"AU\",\"K\":\"GU\",\"M\":\"AC\",\"B\":\"CGU\",\"D\":\"AGU\",\"H\":\"ACU\",\"V\":\"ACG\",\"N\":\"ACGU\"}}},{\"name\":\"Bio_Data_CodonTable_list_ambiguous_codons\",\"arguments\":{\"codons\":[\"TAA\",\"TAG\",\"TGA\"],\"ambiguous_nucleotide_values\":{\"A\":\"A\",\"C\":\"C\",\"G\":\"G\",\"T\":\"T\",\"R\":\"AG\",\"Y\":\"CT\",\"S\":\"GC\",\"W\":\"AT\",\"K\":\"GT\",\"M\":\"AC\",\"B\":\"CGT\",\"D\":\"AGT\",\"H\":\"ACT\",\"V\":\"ACG\",\"N\":\"ACGT\"}}},{\"name\":\"Bio_Data_CodonTable_list_ambiguous_codons\",\"arguments\":{\"codons\":[\"TAA\",\"TAG\",\"TGA\"],\"ambiguous_nucleotide_values\":{\"A\":\"A\",\"C\":\"C\",\"G\":\"G\",\"T\":\"T\",\"R\":\"AG\",\"Y\":\"CT\",\"S\":\"GC\",\"W\":\"AT\",\"K\":\"GT\",\"M\":\"AC\",\"B\":\"CGT\",\"D\":\"AGT\",\"H\":\"ACT\",\"V\":\"ACG\",\"N\":\"ACGT\"}}}]"}
{"func_name": "Bio_Data_CodonTable_register_ncbi_table", "func_desc": "Bio.Data.CodonTable.register_ncbi_table registers a single NCBI-style codon table definition with the CodonTable module by converting the provided raw table data into multiple internal codon table objects used throughout Biopython for DNA/RNA translation and ambiguous-base handling. This function is intended for internal (PRIVATE) use when populating the module-level codon table registries used by sequence translation and related utilities in computational molecular biology workflows.", "tools": [{"function": {"description": "Bio.Data.CodonTable.register_ncbi_table registers a single NCBI-style codon table definition with the CodonTable module by converting the provided raw table data into multiple internal codon table objects used throughout Biopython for DNA/RNA translation and ambiguous-base handling. This function is intended for internal (PRIVATE) use when populating the module-level codon table registries used by sequence translation and related utilities in computational molecular biology workflows.\n", "name": "Bio_Data_CodonTable_register_ncbi_table", "parameters": {"properties": {"name": {"type": "string", "description": "The primary name string for the codon table as provided by NCBI. This string may contain multiple synonymous names separated by \"; \", \", \" or the word \" and \". The function parses this string into a list of individual names (splitting on \"; \", converting \" and \" to \"; \", and trimming whitespace) and uses those names when registering the created codon table objects in the module-level name-to-table mappings. The parsed names are important for lookups by human-readable table names in Biopython translation functions.", "default": ""}, "alt_name": {"type": "string", "description": "An alternate or legacy single name for the codon table (may be None). If not None, this alternate name is appended to the parsed names list and also used when registering the table objects. alt_name provides backward compatibility with older naming conventions and is stored with the generated objects and registries.", "default": ""}, "id": {"type": "integer", "description": "The numeric NCBI identifier for the codon table (for example, 1 for the standard code). This integer is used as the key when storing the generated table objects in the module-level id-indexed registries (e.g., unambiguous_dna_by_id, unambiguous_rna_by_id, generic_by_id, ambiguous_*_by_id). If id equals 1, the function also sets the module-level globals standard_dna_table and standard_rna_table to the created DNA and RNA objects respectively. Care should be taken because registering a table with an id that already exists will overwrite the existing entries in these registries.", "default": ""}, "table": {"type": "any", "description": "A mapping of codon strings to encoded amino acid values representing the unambiguous DNA codon-to-amino-acid assignments as provided by NCBI. Keys are expected to be codon strings using the letter \"T\" for thymine (e.g., \"ATG\", \"TAA\"); values are the corresponding amino acid symbol(s) or stop indicators as used by Biopython. The function uses this dictionary to build three variants of unambiguous lookup tables: a DNA-specific table (T-based), an RNA-specific table (T replaced by U), and a generic table that contains both T- and U-based codons for flexible lookup across DNA/RNA sequences.", "default": ""}, "start_codons": {"type": "array", "items": {"type": "float"}, "description": "A list of codon strings (using \"T\" for thymine) that are considered valid translation initiation (start) codons for this genetic code. These are used to populate corresponding start-codon lists for the DNA-specific, RNA-specific (with \"T\" replaced by \"U\"), and generic tables. The implementation takes care to add both T- and U-forms where appropriate but only adds the U-form when the original contains \"T\" to reduce accidental duplicates.", "default": ""}, "stop_codons": {"type": "array", "items": {"type": "float"}, "description": "A list of codon strings (using \"T\" for thymine) that are considered translation termination (stop) codons for this genetic code. These are used to populate the stop-codon lists for the DNA-specific, RNA-specific (with \"T\" replaced by \"U\"), and generic tables in the same manner as start_codons, with both T- and U-forms added where applicable.", "default": ""}}, "required": ["name", "alt_name", "id", "table", "start_codons", "stop_codons"], "type": "any"}}, "type": "function"}], "query": "We are stress-testing our Biopython translation stack against a messy upstream feed of candidate NCBI-style codon-table records coming from three sources (plasmid build sheets, organism cohort configs, and a simulated organelle compartment). Each record includes a proposed NCBI `id`, `name`, `alt_name`, a sparse codon `table`, and proposed `start_codons`/`stop_codons`.\n\nRegister only those records that are internally consistent for use in ambiguous-base translation: (i) the `id` must be unique across the batch, (ii) `start_codons` and `stop_codons` must be disjoint sets, and (iii) every listed start/stop codon must also appear in the provided `table` (with start codons mapping to amino acids and stop codons mapping to '*'). For accepted records, normalize the code identifier metadata by keeping the given `name`/`alt_name` unchanged.\n\nRaw candidate records:\n1) id=99, name=\"Synthetic Minimal Code; SMC and Lab Variant\", alt_name=\"SMC_legacy\", table={ATG:M, TTT:F, TTC:F, TAA:*, TAG:*, TGA:*, GCT:A, GCC:A, GCA:A, GCG:A}, start_codons=[ATG,TTG], stop_codons=[TAA,TAG,TGA]\n2) id=99, name=\"Synthetic Code X; SynX and Experimental\", alt_name=\"SynX-legacy\", table={ATG:M, TGG:W, TAA:*, TAG:*, TGA:*}, start_codons=[ATG,TTG], stop_codons=[TAA,TAG,TGA]\n3) id=33, name=\"Simulated Mitochondrial; Sim-MT\", alt_name=\"SimMT Code\", table={ATG:M, ATA:M, TAA:*, TAG:*, TTT:F, TTC:F, GAA:E, GAG:E}, start_codons=[ATG,ATA], stop_codons=[TAA,TAG]\n\nPopulate the registry by registering the accepted records via `Bio.Data.CodonTable.register_ncbi_table`.", "answers": "[{\"name\":\"Bio_Data_CodonTable_register_ncbi_table\",\"arguments\":{\"name\":\"Simulated Mitochondrial; Sim-MT\",\"alt_name\":\"SimMT Code\",\"id\":33,\"table\":{\"ATG\":\"M\",\"ATA\":\"M\",\"TAA\":\"*\",\"TAG\":\"*\",\"TTT\":\"F\",\"TTC\":\"F\",\"GAA\":\"E\",\"GAG\":\"E\"},\"start_codons\":[\"ATG\",\"ATA\"],\"stop_codons\":[\"TAA\",\"TAG\"]}}]"}
{"func_name": "Bio_ExPASy_get_prodoc_entry", "func_desc": "Get a text handle to a PRODOC entry at the ExPASy PROSITE web service in HTML format.\n    \n    This function is used in computational molecular biology and bioinformatics workflows (see the Biopython project README) to retrieve the PRODOC documentation page for a given PROSITE entry identifier from the ExPASy web server. The returned handle provides the raw HTML text of the PRODOC entry, which can be read and stored or parsed to extract human-readable documentation about sequence motifs, patterns, and associated annotations curated in the PROSITE/PRODOC resource.", "tools": [{"function": {"description": "Get a text handle to a PRODOC entry at the ExPASy PROSITE web service in HTML format.\n\nThis function is used in computational molecular biology and bioinformatics workflows (see the Biopython project README) to retrieve the PRODOC documentation page for a given PROSITE entry identifier from the ExPASy web server. The returned handle provides the raw HTML text of the PRODOC entry, which can be read and stored or parsed to extract human-readable documentation about sequence motifs, patterns, and associated annotations curated in the PROSITE/PRODOC resource.", "name": "Bio_ExPASy_get_prodoc_entry", "parameters": {"properties": {"id": {"type": "string", "description": "The PROSITE PRODOC identifier to fetch, for example \"PDOC00001\". This identifier is appended to the CGI endpoint as a query string and identifies the specific PRODOC record to retrieve from the remote PROSITE database. The value must be a string representing a valid PRODOC key known to the ExPASy service.", "default": ""}, "cgi": {"type": "string", "description": "The base CGI URL of the ExPASy PROSITE \"get-prodoc-entry\" endpoint. Default is \"https://prosite.expasy.org/cgi-bin/prosite/get-prodoc-entry\". This string is used verbatim and the function will construct the full request URL by appending \"?{id}\" to this CGI string. Override this parameter only to point to an alternative compatible CGI endpoint (for testing or mirrored services); do not change the format (it must accept the identifier as a query string).", "default": "https://prosite.expasy.org/cgi-bin/prosite/get-prodoc-entry"}}, "required": ["id"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed-format manifest of PROSITE/PRODOC documentation references coming from three upstream annotation sources (manual curation notes, a legacy spreadsheet export, and an automated motif scan). Before archiving HTML handles for the report pipeline, normalize each reference to a canonical PRODOC identifier and then pull documentation only for entries that are plausibly valid PRODOC accessions: the identifier must resolve to the PRODOC namespace (i.e., a PDOC accession) and be exactly 9 characters long after normalization (prefix PDOC plus 5 digits). Treat whitespace, case differences, and common delimiter artifacts (e.g., 'PDOC:00016', 'pdoc00171', 'PDOC 00394') as normalization targets. Manifest: [' PDOC00171 ', 'PS00171', 'PDOC:00016', 'pdoc00394', 'PDOC 0039A', 'PDOC000160', 'PDOC 171']. Retrieve the raw HTML handle from the standard ExPASy PROSITE CGI endpoint for each normalized, valid PRODOC entry that passes these rules.", "answers": "[{\"name\":\"Bio_ExPASy_get_prodoc_entry\",\"arguments\":{\"id\":\"PDOC00171\"}},{\"name\":\"Bio_ExPASy_get_prodoc_entry\",\"arguments\":{\"id\":\"PDOC00016\"}},{\"name\":\"Bio_ExPASy_get_prodoc_entry\",\"arguments\":{\"id\":\"PDOC00394\"}}]"}
{"func_name": "Bio_ExPASy_get_prosite_raw", "func_desc": "Get a text handle to a raw PROSITE or PRODOC record at the ExPASy web service.\n    \n    This function is used in Biopython to retrieve the plain-text PROSITE/PRODOC entry for a given PROSITE accession (for example \"PS00001\") from the ExPASy host (https://prosite.expasy.org/). The returned handle is a readable text file-like object that can be parsed by Biopython parsers such as Bio.ExPASy.Prosite.read to produce a Prosite record object used for motif, pattern, and protein family annotations in computational molecular biology workflows. The function performs an HTTP GET for the URL https://prosite.expasy.org/{id}.txt using the package-internal opener. Because the handle represents a network resource, prefer using it as a context manager (with statement) so it is closed automatically when no longer needed.", "tools": [{"function": {"description": "Get a text handle to a raw PROSITE or PRODOC record at the ExPASy web service.\n\nThis function is used in Biopython to retrieve the plain-text PROSITE/PRODOC entry for a given PROSITE accession (for example \"PS00001\") from the ExPASy host (https://prosite.expasy.org/). The returned handle is a readable text file-like object that can be parsed by Biopython parsers such as Bio.ExPASy.Prosite.read to produce a Prosite record object used for motif, pattern, and protein family annotations in computational molecular biology workflows. The function performs an HTTP GET for the URL https://prosite.expasy.org/{id}.txt using the package-internal opener. Because the handle represents a network resource, prefer using it as a context manager (with statement) so it is closed automatically when no longer needed.", "name": "Bio_ExPASy_get_prosite_raw", "parameters": {"properties": {"id": {"type": "string", "description": "The PROSITE or PRODOC identifier to fetch, for example \"PS00001\". This value is formatted into the ExPASy PROSITE URL as {id}.txt. It is the primary lookup key and must match an existing PROSITE accession; otherwise the function will report the entry as not found.", "default": ""}, "cgi": {"type": "string", "nullable": true, "description": "Deprecated and ignored. Historically used to select CGI-style access on the ExPASy site, but due to changes in the ExPASy website this argument is no longer used; pass None or omit it. The default value is None.", "default": null}}, "required": ["id", "cgi"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed batch of candidate PROSITE identifiers exported from a legacy motif table for a zinc-finger annotation run:\n\n`['PS00001', 'ps00028', 'PS00028 ', 'PS0002O', 'PDOC00028', 'PS99999', 'PS00028#qc']`\n\nBefore downstream parsing, pull raw ExPASy plain-text records only for entries that (i) are PROSITE accessions (PS-prefix), (ii) have exactly 5 digits, and (iii) represent distinct accessions after normalizing case and trimming surrounding whitespace. Any trailing non-accession characters (e.g., inline QC tags) should be treated as invalid for retrieval. Fetch each qualifying accession via the standard .txt endpoint (no special CGI/options) and return readable handles for parsing.", "answers": "[{\"name\":\"Bio_ExPASy_get_prosite_raw\",\"arguments\":{\"id\":\"PS00001\"}},{\"name\":\"Bio_ExPASy_get_prosite_raw\",\"arguments\":{\"id\":\"PS00028\"}}]"}
{"func_name": "Bio_Geo_Record_out_block", "func_desc": "Bio.Geo.Record.out_block formats a long text string into fixed-width blocks of up to 80 characters per block (slicing the input text every 80 characters) and returns the assembled lines with an optional prefix prepended to each line. This function is used in the Biopython codebase (for example in record formatting within Bio.Geo.Record) to produce human-readable and file-format-friendly multiline fields when writing out record annotations or other long text fields in computational molecular biology workflows.\n    \n    Behavior: The input text is split by simple character slices of length 80 (text[0:80], text[80:160], ...). Each slice is emitted as a separate line with a trailing newline character. After all blocks are emitted an additional newline is appended. If text is empty the function returns a single newline. Note that the slicing is performed only on the text content; the prefix is prepended to each output line after slicing and therefore can increase the printed line length beyond 80 characters. The function does not perform word-aware wrapping and may split words across block boundaries. This routine does not perform any I/O operations; it builds and returns a string suitable for inclusion in file output or display.", "tools": [{"function": {"description": "Bio.Geo.Record.out_block formats a long text string into fixed-width blocks of up to 80 characters per block (slicing the input text every 80 characters) and returns the assembled lines with an optional prefix prepended to each line. This function is used in the Biopython codebase (for example in record formatting within Bio.Geo.Record) to produce human-readable and file-format-friendly multiline fields when writing out record annotations or other long text fields in computational molecular biology workflows.\n\nBehavior: The input text is split by simple character slices of length 80 (text[0:80], text[80:160], ...). Each slice is emitted as a separate line with a trailing newline character. After all blocks are emitted an additional newline is appended. If text is empty the function returns a single newline. Note that the slicing is performed only on the text content; the prefix is prepended to each output line after slicing and therefore can increase the printed line length beyond 80 characters. The function does not perform word-aware wrapping and may split words across block boundaries. This routine does not perform any I/O operations; it builds and returns a string suitable for inclusion in file output or display.", "name": "Bio_Geo_Record_out_block", "parameters": {"properties": {"text": {"type": "string", "description": "The input text to be formatted into 80-character blocks. In the Biopython context this typically holds long annotation strings or record field content that must be broken into fixed-width lines for textual record output. The function treats this as plain Python text (Unicode string) and slices it by character indices; multi-codepoint or multi-byte characters count as individual Python characters for slicing. Supplying a non-string value may cause a runtime error when slicing is attempted.", "default": ""}, "prefix": {"type": "string", "description": "Optional string to prepend to every output line. By default this is the empty string (prefix = \"\"), meaning no extra characters are added before each block. In practical use within Biopython this prefix can be used to add field labels, indentation, or format-specific line headers. Because prefix is added after slicing the text into 80-character chunks, a non-empty prefix will make the visible line length exceed 80 characters.", "default": ""}}, "required": ["text", "prefix"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a mixed-source Geo/Genomics flat-file release where annotation fields arrive with inconsistent provenance and must be formatted into fixed-width multiline blocks for downstream parsers. Use Bio.Geo.Record.out_block to format only the annotation payloads that are non-empty after trimming and represent a single-field value (i.e., they do not contain any embedded newline characters). Apply a provenance-dependent prefix rule: if the payload contains any taxonomy-style Latin binomial (two tokens where the first starts with an uppercase letter and the second is all lowercase), treat it as a GenBank-derived biological note and prepend four spaces to each emitted line; otherwise treat it as a field-logger campaign note and prepend the literal prefix \"ANNOT: \" to each emitted line. Format the following raw payloads accordingly:\n1) \"This gene is involved in the early stages of chlorophyll biosynthesis and is highly conserved across multiple plant species, including Arabidopsis thaliana, Oryza sativa, and Zea mays. Experimental evidence suggests a regulatory role under low-light conditions.\"\n2) \"This coastal sampling campaign focused on microbial communities in surface ocean waters collected along a 500 km transect. Environmental parameters including temperature, salinity, chlorophyll concentration, and nutrient levels were recorded at each station for downstream metagenomic correlation analyses.\"\n3) \"   \"\n4) \"Operator log:\\nInstrument rebooted mid-cast; see attached report.\"", "answers": "[{\"name\":\"Bio_Geo_Record_out_block\",\"arguments\":{\"text\":\"This gene is involved in the early stages of chlorophyll biosynthesis and is highly conserved across multiple plant species, including Arabidopsis thaliana, Oryza sativa, and Zea mays. Experimental evidence suggests a regulatory role under low-light conditions.\",\"prefix\":\"    \"}},{\"name\":\"Bio_Geo_Record_out_block\",\"arguments\":{\"text\":\"This coastal sampling campaign focused on microbial communities in surface ocean waters collected along a 500 km transect. Environmental parameters including temperature, salinity, chlorophyll concentration, and nutrient levels were recorded at each station for downstream metagenomic correlation analyses.\",\"prefix\":\"ANNOT: \"}}]"}
{"func_name": "Bio_KEGG_REST_kegg_find", "func_desc": "KEGG find - Data search.\n    Finds KEGG entries whose keywords or other query data match the provided query string(s) by issuing a call to the KEGG REST API. This function is part of Biopython's KEGG.REST client and is used in computational molecular biology workflows to locate KEGG database entries (for example, compounds, drugs, pathways, enzymes, organisms) that match textual keywords, chemical formulae, or mass values.", "tools": [{"function": {"description": "KEGG find - Data search.\nFinds KEGG entries whose keywords or other query data match the provided query string(s) by issuing a call to the KEGG REST API. This function is part of Biopython's KEGG.REST client and is used in computational molecular biology workflows to locate KEGG database entries (for example, compounds, drugs, pathways, enzymes, organisms) that match textual keywords, chemical formulae, or mass values.", "name": "Bio_KEGG_REST_kegg_find", "parameters": {"properties": {"database": {"type": "string", "description": "The KEGG database or organism code to search. This argument selects which KEGG collection to query (for example \"pathway\", \"module\", \"disease\", \"drug\", \"environ\", \"ko\", \"genome\", a KEGG organism code or T number for an organism, \"compound\", \"glycan\", \"reaction\", \"rpair\", \"rclass\", \"enzyme\", \"genes\", \"ligand\"). The special cases \"compound\" and \"drug\" permit an additional option (see the option parameter). The value is passed directly to the KEGG REST endpoint and determines the domain of entries returned, so choosing the correct database is important when mapping experimental results to KEGG identifiers or when looking up biochemical entities.", "default": ""}, "query": {"type": "string", "description": "The search terms to match in the selected database. Typical uses are keyword queries (e.g., enzyme name, pathway keyword), chemical formulae, or numeric mass values depending on the database and option. If a list of strings is provided (handled by the implementation), the list elements are joined into a single query with the \"+\" character (equivalent to KEGG REST multi-term queries) before sending the request; each element must therefore be a string. The query content directly affects which KEGG entries are matched and returned by the KEGG server.", "default": ""}, "option": {"type": "string", "nullable": true, "description": "Optional search modifier used only when database is \"compound\" or \"drug\". Accepted values are \"formula\", \"exact_mass\", or \"mol_weight\". When option is \"formula\", the search matches chemical formulae as a partial match and ignores atom order (useful for looking up compounds by their molecular formula). When option is \"exact_mass\" or \"mol_weight\", numeric matching is performed by rounding the KEGG-stored value to the same decimal precision as the provided query; a numeric range may be specified in the query using the minus (\"-\") sign to find entries within a range. The default is None, meaning no option-specific field search is requested. If option is provided for a database other than \"compound\" or \"drug\", the function raises a ValueError (see Failure modes).", "default": null}}, "required": ["database", "query", "option"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a messy glycolysis-adjacent metabolomics annotation sheet before downstream pathway mapping. The raw identifiers are a mix of molecular formulae, high-resolution exact masses, and contaminated placeholders: [\"C6H12O6\", \"180.0634\", \"C6H12O7\", \"179.0561\", \"C6H12O6 \", \"180.06340\", \"N/A\", \"C6H12O6+Na\", \"-180.0634\", \"C5H10O5\"]. For each entry, decide the KEGG compound lookup mode intrinsically: treat strings that are clean neutral formulae (only element symbols and integer stoichiometries, no charge/adduct notation, no whitespace) as formula queries; treat strings that are strictly positive decimal numbers as exact-mass queries. Then run KEGG find against the compound database for the entries that satisfy either criterion. Additionally, include a replicated stability check by repeating the first valid formula-based query at the end of the batch.", "answers": "[{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"C6H12O6\",\"option\":\"formula\"}},{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"180.0634\",\"option\":\"exact_mass\"}},{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"C6H12O7\",\"option\":\"formula\"}},{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"179.0561\",\"option\":\"exact_mass\"}},{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"C5H10O5\",\"option\":\"formula\"}},{\"name\":\"Bio_KEGG_REST_kegg_find\",\"arguments\":{\"database\":\"compound\",\"query\":\"C6H12O6\",\"option\":\"formula\"}}]"}
{"func_name": "Bio_KEGG_REST_kegg_info", "func_desc": "KEGG info - Retrieve current statistics for a KEGG database or organism.\n    \n    This function issues a synchronous REST request to the KEGG REST API endpoint\n    (https://rest.kegg.jp/info/<database>) to obtain human-readable statistics and\n    summary information for a KEGG database or an organism entry. In the context of\n    Biopython (a toolkit for computational molecular biology and bioinformatics),\n    kegg_info is used to programmatically discover the current size, versioning,\n    and basic counts (for example number of entries) for resources hosted by KEGG\n    before performing downstream retrieval or parsing operations. Typical uses\n    include checking the number of pathway, gene, compound, or organism entries,\n    or obtaining a list of supported organism codes via kegg_info('organism').", "tools": [{"function": {"description": "KEGG info - Retrieve current statistics for a KEGG database or organism.\n\nThis function issues a synchronous REST request to the KEGG REST API endpoint\n(https://rest.kegg.jp/info/<database>) to obtain human-readable statistics and\nsummary information for a KEGG database or an organism entry. In the context of\nBiopython (a toolkit for computational molecular biology and bioinformatics),\nkegg_info is used to programmatically discover the current size, versioning,\nand basic counts (for example number of entries) for resources hosted by KEGG\nbefore performing downstream retrieval or parsing operations. Typical uses\ninclude checking the number of pathway, gene, compound, or organism entries,\nor obtaining a list of supported organism codes via kegg_info('organism').", "name": "Bio_KEGG_REST_kegg_info", "parameters": {"properties": {"database": {"type": "string", "description": "The KEGG database name, KEGG database official abbreviation,\nKEGG organism code, or T number to query. Examples: 'pathway' or 'path'\nfor the KEGG pathway database, 'compound' for the compound database,\nan organism code such as 'hsa' for human, or a T number such as 'T01001'.\nThe provided string is interpolated directly into the KEGG REST URL\nsegment <database> and therefore must match KEGG's expected identifiers.\nThere is no default value; the caller must supply a non-empty string.\nThis parameter controls which KEGG resource's statistics are returned.", "default": ""}}, "required": ["database"], "type": "any"}}, "type": "function"}], "query": "Before kicking off downstream KEGG downloads, we’re running a noisy-preflight audit over a mixed batch of resource identifiers coming from two sources: an internal config and a lab notebook. The raw identifiers are: ['pathway', 'PATHWAY', 'hsa', 'HSA', 'hsa ', ' homo sapiens ', 'mmu', 'organism', 'pathway/extra', '', 'n/a']. For two independent pipeline replicates (A and B), do the following: normalize identifiers by trimming surrounding whitespace and lowercasing; treat any identifier that matches an official KEGG organism code pattern as an organism entry, but only proceed with the one corresponding to Homo sapiens; also include the global pathway resource if present after normalization. For each replicate, query KEGG info in this order: pathway first (if eligible), then the Homo sapiens organism entry (if eligible).", "answers": "[{\"name\":\"Bio_KEGG_REST_kegg_info\",\"arguments\":{\"database\":\"pathway\"}},{\"name\":\"Bio_KEGG_REST_kegg_info\",\"arguments\":{\"database\":\"hsa\"}},{\"name\":\"Bio_KEGG_REST_kegg_info\",\"arguments\":{\"database\":\"pathway\"}},{\"name\":\"Bio_KEGG_REST_kegg_info\",\"arguments\":{\"database\":\"hsa\"}}]"}
{"func_name": "Bio_KEGG_REST_kegg_list", "func_desc": "KEGG list - Retrieve an entry list from a KEGG database or a database subset restricted to an organism.\n    \n    This function is part of the Bio.KEGG.REST module in Biopython and is used to query the KEGG REST API (see https://rest.kegg.jp/list/<database> and https://rest.kegg.jp/list/<database>/<org>) to obtain plain-text lists of database entries. In the computational molecular biology domain, this is useful for programmatically obtaining lists of pathways, modules, compounds, enzymes, organisms, and other KEGG resources for downstream processing (for example, mapping identifiers, filtering by organism, or constructing local indexes). Behavior depends on the `database` and `org` arguments as detailed below.", "tools": [{"function": {"description": "KEGG list - Retrieve an entry list from a KEGG database or a database subset restricted to an organism.\n\nThis function is part of the Bio.KEGG.REST module in Biopython and is used to query the KEGG REST API (see https://rest.kegg.jp/list/<database> and https://rest.kegg.jp/list/<database>/<org>) to obtain plain-text lists of database entries. In the computational molecular biology domain, this is useful for programmatically obtaining lists of pathways, modules, compounds, enzymes, organisms, and other KEGG resources for downstream processing (for example, mapping identifiers, filtering by organism, or constructing local indexes). Behavior depends on the `database` and `org` arguments as detailed below.", "name": "Bio_KEGG_REST_kegg_list", "parameters": {"properties": {"database": {"type": "string", "description": "Name of the KEGG database to list entries from. Typical values include \"pathway\", \"module\", \"compound\", \"enzyme\", \"organism\", and others documented by KEGG. Although the function signature annotates this parameter as str, the implementation also accepts a Python list of database names; in that case the list elements are concatenated using \"+\" into a single query string to request multiple databases at once (see behavior notes below). When a single string is provided and `org` is supplied, the function only accepts \"pathway\" or \"module\" as the `database` value; supplying a different non-empty database string together with `org` raises ValueError. The practical significance is that `database` selects which KEGG resource you will receive an entry list for, and controls which REST endpoint is invoked.", "default": ""}, "org": {"type": "string", "nullable": true, "description": "Optional KEGG organism code (for example \"hsa\" for Homo sapiens) or KEGG T number. When provided, and when `database` is \"pathway\" or \"module\", the request is directed to the KEGG REST endpoint that restricts the list to entries for that organism (i.e., /list/<database>/<org>). This parameter defaults to None, meaning no organism restriction. Passing `org` with a `database` that is a non-empty string other than \"pathway\" or \"module\" will cause a ValueError to be raised as described below.", "default": null}}, "required": ["database", "org"], "type": "any"}}, "type": "function"}], "query": "We’re building a KEGG-derived reference set for a multi-omics human project where samples come from mixed sources. For each incoming sample sheet row below, decide whether to pull an organism-restricted pathway catalog or a global pathway catalog: if the row has a valid KEGG organism code (3–4 lowercase letters) use that as `org`; otherwise treat it as an untrusted/missing organism field and retrieve the global pathway list (no `org`) for later reconciliation.\n\nSample sheet rows (use only these values):\n1) sample_id=S01, organism_field=hsa\n2) sample_id=S02, organism_field=HSA\n3) sample_id=S03, organism_field= mmu \n4) sample_id=S04, organism_field=eco\n5) sample_id=S05, organism_field=n/a\n\nFor each row that qualifies, query KEGG `list` for the `pathway` database accordingly and return the plain-text lists.", "answers": "[{\"name\":\"Bio_KEGG_REST_kegg_list\",\"arguments\":{\"database\":\"pathway\",\"org\":\"hsa\"}},{\"name\":\"Bio_KEGG_REST_kegg_list\",\"arguments\":{\"database\":\"pathway\"}},{\"name\":\"Bio_KEGG_REST_kegg_list\",\"arguments\":{\"database\":\"pathway\"}},{\"name\":\"Bio_KEGG_REST_kegg_list\",\"arguments\":{\"database\":\"pathway\",\"org\":\"eco\"}},{\"name\":\"Bio_KEGG_REST_kegg_list\",\"arguments\":{\"database\":\"pathway\"}}]"}
{"func_name": "Bio_Nexus_Nexus_get_start_end", "func_desc": "Return the zero-based start and end indices of the first and last characters\n    in a sequence that are not members of a skiplist. This function is used in\n    Biopython's NEXUS/alignment parsing (Bio.Nexus.Nexus) to locate the ungapped\n    region of an aligned molecular sequence (for example, to trim leading and\n    trailing gap ('-') or unknown ('?') symbols before downstream processing such\n    as consensus building, distance calculations, or writing trimmed alignments).", "tools": [{"function": {"description": "Return the zero-based start and end indices of the first and last characters\nin a sequence that are not members of a skiplist. This function is used in\nBiopython's NEXUS/alignment parsing (Bio.Nexus.Nexus) to locate the ungapped\nregion of an aligned molecular sequence (for example, to trim leading and\ntrailing gap ('-') or unknown ('?') symbols before downstream processing such\nas consensus building, distance calculations, or writing trimmed alignments).", "name": "Bio_Nexus_Nexus_get_start_end", "parameters": {"properties": {"sequence": {"type": "string", "description": "The input sequence to inspect. This should be a Python\nstring representing a biological sequence (for example a DNA, RNA or\nprotein alignment row) that may contain gap or unknown symbols. The\nfunction computes len(sequence) and indexes into sequence, so passing a\nnon-string or a non-indexable object will raise a TypeError. An empty\nstring (length 0) is treated specially and yields (None, None).", "default": ""}, "skiplist": {"type": "any", "description": "A tuple of characters to ignore when scanning from the\nends of sequence. By convention in NEXUS and other alignment contexts,\nthis defaults to ('-', '?') to represent gap and unknown characters.\nElements of skiplist are compared against individual characters of\nsequence using membership (sequence[i] in skiplist). For typical use,\nprovide single-character string elements that match the gap/unknown\nsymbols present in your alignment.", "default": ["-", "?"]}}, "required": ["sequence", "skiplist"], "type": "any"}}, "type": "function"}], "query": "We are validating replicate NEXUS-style aligned DNA rows prior to consensus and distance calculations, but the lab used mixed missing-data conventions across runs. Treat a row as trimmable only if it has clear evidence of alignment padding on both ends: specifically, the sequence must begin with at least 3 consecutive '-' characters and must also contain at least one '?' somewhere in the row (indicating NEXUS missing-data encoding is present). For sequences that meet those criteria, locate the ungapped/known core by returning the zero-based start and end indices of the first and last characters that are not members of the skip set. Use the skiplist ['-', '?'] exactly. Apply this rule to the following replicate rows: (1) '---??ACGTN??--', (2) '---??ATGCCGT---??', (3) '---??ATGCCCTGA--N??'.", "answers": "[{\"name\":\"Bio_Nexus_Nexus_get_start_end\",\"arguments\":{\"sequence\":\"---??ACGTN??--\",\"skiplist\":[\"-\",\"?\"]}},{\"name\":\"Bio_Nexus_Nexus_get_start_end\",\"arguments\":{\"sequence\":\"---??ATGCCGT---??\",\"skiplist\":[\"-\",\"?\"]}},{\"name\":\"Bio_Nexus_Nexus_get_start_end\",\"arguments\":{\"sequence\":\"---??ATGCCCTGA--N??\",\"skiplist\":[\"-\",\"?\"]}}]"}
{"func_name": "Bio_PDB_DSSP_ss_to_index", "func_desc": "Bio.PDB.DSSP.ss_to_index converts a single-letter DSSP secondary structure symbol into a small integer index used by Bio.PDB.DSSP and other Biopython code that needs numeric labels for protein secondary structure. In the context of Biopython (tools for computational molecular biology and bioinformatics), this mapping is used to convert DSSP output symbols into compact integer codes for array indexing, statistical summaries, or machine learning feature labels.", "tools": [{"function": {"description": "Bio.PDB.DSSP.ss_to_index converts a single-letter DSSP secondary structure symbol into a small integer index used by Bio.PDB.DSSP and other Biopython code that needs numeric labels for protein secondary structure. In the context of Biopython (tools for computational molecular biology and bioinformatics), this mapping is used to convert DSSP output symbols into compact integer codes for array indexing, statistical summaries, or machine learning feature labels.\n", "name": "Bio_PDB_DSSP_ss_to_index", "parameters": {"properties": {"ss": {"type": "string", "description": "A single-character DSSP secondary structure symbol. This function expects the exact, case-sensitive symbols produced by DSSP: \"H\" for alpha-helix, \"E\" for beta-strand (extended), and \"C\" for coil or other non-regular secondary structure. The argument is compared directly to these literal strings (no trimming or case conversion is performed).", "default": ""}}, "required": ["ss"], "type": "any"}}, "type": "function"}], "query": "We’re integrating DSSP output from a mixed-quality structural cohort into a machine-learning featurization step. Start by calibrating the numeric label space using a reference panel of DSSP symbols that represent the dominant structural classes we expect to see in well-resolved regions: alpha-helix (\"H\"), beta-strand (\"E\"), and coil/other (\"C\"). Then process a short per-residue annotation stream from the same run, but only convert annotations that are already in canonical DSSP one-letter form (i.e., a single uppercase letter) and appear in the calibration panel. The stream, in acquisition order, is: residue 57 annotated as \"E\", then two replicate checkpoint annotations \"H\" and \"H\". Return the integer index for each conversion, preserving the conversion order (calibration first, then the valid stream entries).", "answers": "[{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"H\"}},{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"E\"}},{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"C\"}},{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"E\"}},{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"H\"}},{\"name\":\"Bio_PDB_DSSP_ss_to_index\",\"arguments\":{\"ss\":\"H\"}}]"}
{"func_name": "Bio_PDB_Polypeptide_index_to_one", "func_desc": "Index to corresponding one-letter amino acid name used by Bio.PDB.Polypeptide.\n    \n    Convert an integer index into the corresponding one-letter amino acid code as used by the Bio.PDB.Polypeptide utilities in Biopython. This function is intended for translating numeric indices (for example, positions in an internal numeric mapping of the 20 standard amino acids) into the single-character amino acid codes commonly used in sequence representation and structural analyses. The implementation performs a direct dictionary lookup against the module-level mapping dindex_to_1. Typical usage is when building or comparing polypeptide sequences extracted from PDB structures where a compact one-letter code is required (for example, assembling a sequence string from residue indices for sequence alignment or annotation). The original doctest examples demonstrate the mapping: index_to_one(0) returns 'A' and index_to_one(19) returns 'Y'.", "tools": [{"function": {"description": "Index to corresponding one-letter amino acid name used by Bio.PDB.Polypeptide.\n\nConvert an integer index into the corresponding one-letter amino acid code as used by the Bio.PDB.Polypeptide utilities in Biopython. This function is intended for translating numeric indices (for example, positions in an internal numeric mapping of the 20 standard amino acids) into the single-character amino acid codes commonly used in sequence representation and structural analyses. The implementation performs a direct dictionary lookup against the module-level mapping dindex_to_1. Typical usage is when building or comparing polypeptide sequences extracted from PDB structures where a compact one-letter code is required (for example, assembling a sequence string from residue indices for sequence alignment or annotation). The original doctest examples demonstrate the mapping: index_to_one(0) returns 'A' and index_to_one(19) returns 'Y'.", "name": "Bio_PDB_Polypeptide_index_to_one", "parameters": {"properties": {"index": {"type": "integer", "description": "Integer index to convert to a one-letter amino acid code. In the context of Bio.PDB.Polypeptide this index corresponds to the position/key used by the module-level mapping dindex_to_1 that maps numeric indices to the 20 standard amino acids' one-letter codes. The argument must be an integer; passing non-integer types will not match keys in the mapping and will typically result in a lookup failure. Providing an index not present in dindex_to_1 (for example, out of the mapping's defined range) will raise a KeyError.", "default": ""}}, "required": ["index"], "type": "any"}}, "type": "function"}], "query": "During QC of a PDB-derived polypeptide reconstruction, we found that some residue-index annotations coming from an upstream join step may include placeholders or partially-parsed tokens. From the following raw residue-index annotations: [12, \"07\", null, \"UNK\", 7, -1, \" 12 \", 19.0], translate into one-letter amino-acid codes using the Bio.PDB.Polypeptide 20-AA index mapping only those entries that represent a valid integer index in the closed interval [0, 19] after canonicalization (i.e., trimming whitespace and allowing numeric strings / integer-valued floats). Preserve the original observation order among the retained indices and return the resulting one-letter codes for sequence verification.", "answers": "[{\"name\":\"Bio_PDB_Polypeptide_index_to_one\",\"arguments\":{\"index\":12}},{\"name\":\"Bio_PDB_Polypeptide_index_to_one\",\"arguments\":{\"index\":7}},{\"name\":\"Bio_PDB_Polypeptide_index_to_one\",\"arguments\":{\"index\":7}},{\"name\":\"Bio_PDB_Polypeptide_index_to_one\",\"arguments\":{\"index\":12}},{\"name\":\"Bio_PDB_Polypeptide_index_to_one\",\"arguments\":{\"index\":19}}]"}
{"func_name": "Bio_PDB_Polypeptide_index_to_three", "func_desc": "Bio.PDB.Polypeptide.index_to_three maps an integer index to the corresponding three-letter amino acid residue name used in PDB-style representations and by the Bio.PDB.Polypeptide utilities for sequence and structure handling. This function is used in the Biopython PDB module to convert numeric residue indices (for example, indices in the range 0–19 for the 20 standard amino acids) into the conventional three-letter residue codes (e.g., 0 -> 'ALA', 19 -> 'TYR') that appear in PDB files and are required by downstream PDB/structure-processing routines.", "tools": [{"function": {"description": "Bio.PDB.Polypeptide.index_to_three maps an integer index to the corresponding three-letter amino acid residue name used in PDB-style representations and by the Bio.PDB.Polypeptide utilities for sequence and structure handling. This function is used in the Biopython PDB module to convert numeric residue indices (for example, indices in the range 0–19 for the 20 standard amino acids) into the conventional three-letter residue codes (e.g., 0 -> 'ALA', 19 -> 'TYR') that appear in PDB files and are required by downstream PDB/structure-processing routines.\n", "name": "Bio_PDB_Polypeptide_index_to_three", "parameters": {"properties": {"i": {"type": "integer", "description": "Integer index representing an amino acid. In the context of Bio.PDB.Polypeptide, this integer is used as a lookup key into the module-level mapping dindex_to_3 to retrieve the canonical three-letter, uppercase residue name. The value should correspond to an index present in that mapping; typical use is with indices for the twenty standard amino acids.", "default": ""}}, "required": ["i"], "type": "any"}}, "type": "function"}], "query": "In our structure-generation pipeline, residue identities arrive as heterogeneous numeric codes from multiple upstream stages. We have a mixed batch of candidate residue indices: [5, 19, 19, -1, 20, 7, 3, 42]. Before emitting PDB ATOM records, standardize only the indices that correspond to the 20 canonical amino acids used by Bio.PDB.Polypeptide (i.e., those that fall within the standard mapping domain) by converting them to three-letter PDB residue codes via Bio.PDB.Polypeptide.index_to_three. Treat any out-of-domain codes as non-canonical artifacts and leave them unconverted for downstream quarantine.", "answers": "[{\"name\":\"Bio_PDB_Polypeptide_index_to_three\",\"arguments\":{\"i\":5}},{\"name\":\"Bio_PDB_Polypeptide_index_to_three\",\"arguments\":{\"i\":19}},{\"name\":\"Bio_PDB_Polypeptide_index_to_three\",\"arguments\":{\"i\":19}},{\"name\":\"Bio_PDB_Polypeptide_index_to_three\",\"arguments\":{\"i\":7}},{\"name\":\"Bio_PDB_Polypeptide_index_to_three\",\"arguments\":{\"i\":3}}]"}
{"func_name": "Bio_PDB_alphafold_db_get_predictions", "func_desc": "Get all AlphaFold predictions for a UniProt accession.\n    \n    This function is part of Bio.PDB.alphafold_db and is used by Biopython users and tools to fetch AlphaFold prediction records for a given UniProt accession (for example, \"P00520\") from the AlphaFold public API. It performs an HTTPS request to the AlphaFold endpoint for the supplied qualifier, decodes the JSON response, and yields each prediction record as a Python dictionary. This enables downstream code in computational molecular biology and structural bioinformatics workflows to iterate over prediction records for integration with PDB parsing, annotation, or analysis.", "tools": [{"function": {"description": "Get all AlphaFold predictions for a UniProt accession.\n\nThis function is part of Bio.PDB.alphafold_db and is used by Biopython users and tools to fetch AlphaFold prediction records for a given UniProt accession (for example, \"P00520\") from the AlphaFold public API. It performs an HTTPS request to the AlphaFold endpoint for the supplied qualifier, decodes the JSON response, and yields each prediction record as a Python dictionary. This enables downstream code in computational molecular biology and structural bioinformatics workflows to iterate over prediction records for integration with PDB parsing, annotation, or analysis.", "name": "Bio_PDB_alphafold_db_get_predictions", "parameters": {"properties": {"qualifier": {"type": "string", "description": "A UniProt accession string used to identify the protein for which AlphaFold predictions are requested, e.g. \"P00520\". The function does not validate the accession beyond inserting it into the request URL; callers should provide a valid UniProt accession as required by the AlphaFold API.", "default": ""}}, "required": ["qualifier"], "type": "any"}}, "type": "function"}], "query": "We’re doing a provenance-aware AlphaFold-DB ingestion pass for a mixed UniProt list coming out of our LIMS export: [\"P04637\", \"P04637-2\", \"Q9Y2X3\", \"Q9Y2X3-1\", \"P04637 \", \"q9y2x3\"]. For each entry, first canonicalize the identifier to the UniProt *accession* used by AlphaFold-DB (trim whitespace, normalize case, and remove any isoform suffix after a hyphen). Then apply a branching retrieval protocol: for accessions that occur more than once after canonicalization (technical duplicates from multi-lab submissions), fetch AlphaFold predictions once per occurrence (so we can compare API payload consistency across repeated pulls). For accessions that occur only once after canonicalization, fetch AlphaFold predictions just once. Return the sequence of API retrievals that should be executed in order.", "answers": "[{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"P04637\"}},{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"P04637\"}},{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"P04637\"}},{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"Q9Y2X3\"}},{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"Q9Y2X3\"}},{\"name\":\"Bio_PDB_alphafold_db_get_predictions\",\"arguments\":{\"qualifier\":\"Q9Y2X3\"}}]"}
{"func_name": "Bio_PDB_internal_coords_set_accuracy_95", "func_desc": "Reduce floating point accuracy to \"9.5\" (format xxxx.xxxxx) by rounding to five decimal places and returning a float.\n    \n    This helper is used by the IC_Residue class in Bio.PDB.internal_coords when writing PIC and SCAD files. In that domain, limiting numeric precision to five digits after the decimal reduces file size, improves human readability of coordinate files, and ensures consistent numeric formatting required by downstream tools that consume PIC/SCAD output.\n    \n    The implementation formats the input using Python string formatting with a fixed field width and five decimal places, then converts the formatted string back to float (float(f\"{num:9.5f}\")). This yields a numeric value rounded to five decimal places; the specified field width (9) ensures consistent formatting when producing text files, although any leading spaces are removed by the float conversion. A previously used alternative, round(num, 5), was commented out in the source because it was measured to be slower in this code path.", "tools": [{"function": {"description": "Reduce floating point accuracy to \"9.5\" (format xxxx.xxxxx) by rounding to five decimal places and returning a float.\n\nThis helper is used by the IC_Residue class in Bio.PDB.internal_coords when writing PIC and SCAD files. In that domain, limiting numeric precision to five digits after the decimal reduces file size, improves human readability of coordinate files, and ensures consistent numeric formatting required by downstream tools that consume PIC/SCAD output.\n\nThe implementation formats the input using Python string formatting with a fixed field width and five decimal places, then converts the formatted string back to float (float(f\"{num:9.5f}\")). This yields a numeric value rounded to five decimal places; the specified field width (9) ensures consistent formatting when producing text files, although any leading spaces are removed by the float conversion. A previously used alternative, round(num, 5), was commented out in the source because it was measured to be slower in this code path.", "name": "Bio_PDB_internal_coords_set_accuracy_95", "parameters": {"properties": {"num": {"type": "float", "description": "Input floating-point number representing a coordinate or other numeric value used in internal coordinate calculations. This function expects a Python float as documented in the function signature; passing a non-float that cannot be formatted as a floating-point number may raise a TypeError or ValueError at format-time. Special IEEE float values such as NaN or infinity are propagated by the formatting and returned as their corresponding float values.", "default": ""}}, "required": ["num"], "type": "any"}}, "type": "function"}], "query": "During PIC/SCAD export for an internal-coordinate torsion scan, I’m merging replicate dihedral readouts that come from a mixed pipeline (some values are in degrees, some are accidentally reported in radians). Use the Bio.PDB.internal_coords-style 9.5 numeric-accuracy helper (format to five decimals, then cast back to float) to normalize only measurements that are plausibly already in degrees, defined as having absolute value within the closed interval [0, 360]. Treat anything outside that range as a unit/mapping artifact and skip it. Apply this sieve to the following raw torsion measurements: -123.987654321, 123.456789, and 123.45678912, and output the normalized (rounded) floats for the measurements that pass the plausibility check.", "answers": "[{\"name\":\"Bio_PDB_internal_coords_set_accuracy_95\",\"arguments\":{\"num\":-123.987654321}},{\"name\":\"Bio_PDB_internal_coords_set_accuracy_95\",\"arguments\":{\"num\":123.456789}},{\"name\":\"Bio_PDB_internal_coords_set_accuracy_95\",\"arguments\":{\"num\":123.45678912}}]"}
{"func_name": "Bio_PDB_vectors_multi_coord_space", "func_desc": "Bio.PDB.vectors.multi_coord_space generates 4x4 homogeneous transformation matrices that map sets of three Cartesian points (atoms) into a local hedron coordinate space used in PDB geometric computations.\n    \n    This function is used in Biopython's PDB/vector routines to construct a local coordinate system from three atoms (a triad) so that downstream code can compute internal coordinates, align fragments, or transform points into a canonical hedron frame. The new coordinate space produced by the forward transform has the following conventions: the second atom (index 1) is placed at the origin, the third atom (index 2) lies on the positive Z axis, and the first atom (index 0) is constrained to the XZ plane. The implementation builds a translation to move atom 1 to the origin, then applies rotations about Z and Y to align atom 2 with +Z, and finally rotates about Z to move atom 0 into the XZ plane. If rev is True, the function also constructs the reverse transforms that map coordinates from the hedron space back to the original coordinate system.", "tools": [{"function": {"description": "Bio.PDB.vectors.multi_coord_space generates 4x4 homogeneous transformation matrices that map sets of three Cartesian points (atoms) into a local hedron coordinate space used in PDB geometric computations.\n\nThis function is used in Biopython's PDB/vector routines to construct a local coordinate system from three atoms (a triad) so that downstream code can compute internal coordinates, align fragments, or transform points into a canonical hedron frame. The new coordinate space produced by the forward transform has the following conventions: the second atom (index 1) is placed at the origin, the third atom (index 2) lies on the positive Z axis, and the first atom (index 0) is constrained to the XZ plane. The implementation builds a translation to move atom 1 to the origin, then applies rotations about Z and Y to align atom 2 with +Z, and finally rotates about Z to move atom 0 into the XZ plane. If rev is True, the function also constructs the reverse transforms that map coordinates from the hedron space back to the original coordinate system.", "name": "Bio_PDB_vectors_multi_coord_space", "parameters": {"properties": {"a3": {"type": "array", "items": {"type": "float"}, "description": "Array containing the input atom coordinates. Expected to contain dLen entries of three points each, provided as homogeneous 4-component vectors (x, y, z, 1). The code uses a3 with indexing a3[:, i, 0:3] and a3[:, i].reshape(-1, 4, 1), so the practical expected shape is (dLen, 3, 4) where the second axis indexes the three atoms (atom 0, atom 1, atom 2) and the last axis holds the homogeneous coordinate (x, y, z, w). Each row corresponds to one hedron instance to transform. The homogeneous w component is typically 1 for Cartesian points. Supplying arrays that do not match the expected third dimension will lead to shape or indexing errors.", "default": ""}, "dLen": {"type": "integer", "description": "Number of entries (hedrons) for which to build transformation matrices. This is used to allocate and shape internal 4x4 matrix arrays (tm initialized with shape (dLen, 4, 4)). For correct behavior, dLen should equal a3.shape[0] (the number of hedron entries). If dLen does not match a3.shape[0], NumPy broadcasting or indexing can produce incorrect results or raise exceptions.", "default": ""}, "rev": {"type": "boolean", "description": "If False (default), return only the forward transforms that map original coordinates into the hedron coordinate space. If True, also return the reverse transforms that map coordinates from hedron space back to the original coordinate system. When rev=True, the function returns both forward and reverse arrays together (see Returns). There are no side effects controlled by this flag beyond the additional computation and larger returned array.", "default": false}}, "required": ["a3", "dLen", "rev"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed-quality backbone-triad dataset prior to fragment superposition. Each record is an N–CA–C triad in homogeneous Cartesian form (x, y, z, 1) intended for hedron-frame construction. Treat each triad as valid only if it forms a well-conditioned local frame: the CA→C bond vector must be non-degenerate (length > 0.5 Å), and the three atoms must not be near-colinear (the area proxy ‖(N−CA)×(C−CA)‖ must be > 0.2 Å²). From the full dataset below, build 4×4 forward transforms into canonical hedron space and also generate the reverse 4×4 transforms, but only for triads that satisfy the conditioning criteria. Dataset (unordered): T1 N=(0.0,1.2,0.5,1), CA=(1.5,0.8,0.3,1), C=(2.8,1.0,-0.2,1); T2 N=(3.1,1.4,-0.5,1), CA=(4.4,1.1,-0.7,1), C=(5.6,1.3,-1.1,1); T3 N=(12.411,8.233,5.102,1), CA=(13.052,9.512,5.601,1), C=(14.511,9.201,5.973,1); T4 N=(3.200,-1.050,0.500,1), CA=(4.300,-0.200,0.900,1), C=(5.100,0.400,2.000,1); T5 N=(0.0,0.0,0.0,1), CA=(0.0,0.0,0.0,1), C=(1.0,0.0,0.0,1); T6 N=(1.0,0.0,0.0,1), CA=(2.0,0.0,0.0,1), C=(3.0,0.0,0.0,1).", "answers": "[{\"name\":\"Bio_PDB_vectors_multi_coord_space\",\"arguments\":{\"a3\":[[[0.0,1.2,0.5,1.0],[1.5,0.8,0.3,1.0],[2.8,1.0,-0.2,1.0]],[[3.1,1.4,-0.5,1.0],[4.4,1.1,-0.7,1.0],[5.6,1.3,-1.1,1.0]],[[12.411,8.233,5.102,1.0],[13.052,9.512,5.601,1.0],[14.511,9.201,5.973,1.0]],[[3.2,-1.05,0.5,1.0],[4.3,-0.2,0.9,1.0],[5.1,0.4,2.0,1.0]]],\"dLen\":4,\"rev\":true}}]"}
{"func_name": "Bio_PDB_vectors_multi_rot_Y", "func_desc": "Create multiple 4x4 homogeneous rotation matrices for rotations about the Y axis.\n    \n    This function is part of the Bio.PDB.vectors utilities in Biopython and is used to generate a batch of homogeneous transformation matrices that rotate 3D coordinates around the Y axis by given angles. Each returned matrix is a 4x4 affine rotation matrix (no translation) in homogeneous coordinates, suitable for applying to 3D points or coordinate frames in molecular structure manipulations (for example, rotating atom coordinates in a PDB model). The function allocates and returns a new NumPy array and does not modify its input.", "tools": [{"function": {"description": "Create multiple 4x4 homogeneous rotation matrices for rotations about the Y axis.\n\nThis function is part of the Bio.PDB.vectors utilities in Biopython and is used to generate a batch of homogeneous transformation matrices that rotate 3D coordinates around the Y axis by given angles. Each returned matrix is a 4x4 affine rotation matrix (no translation) in homogeneous coordinates, suitable for applying to 3D points or coordinate frames in molecular structure manipulations (for example, rotating atom coordinates in a PDB model). The function allocates and returns a new NumPy array and does not modify its input.", "name": "Bio_PDB_vectors_multi_rot_Y", "parameters": {"properties": {"angle_rads": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional NumPy array of angles in radians, shape (N,). Each element angle_rads[i] is the rotation angle (in radians) for which a corresponding 4x4 homogeneous rotation matrix about the Y axis will be produced. The array must be one-dimensional because the implementation uses angle_rads.shape[0] to determine the number of matrices. Passing a scalar or a zero-dimensional array will raise an IndexError when accessing shape[0]; passing a multi-dimensional array will typically raise a ValueError during assignment because the code expects a length-N 1D array.", "default": ""}}, "required": ["angle_rads"], "type": "any"}}, "type": "function"}], "query": "In a cryo-EM model-cleanup step, we need a Y-axis rotational augmentation library from a mixed-angle manifest produced by two upstream tools. The manifest contains entries in either degrees (annotated with a trailing “deg”), radians (annotated with a trailing “rad”), and occasional sentinel/garbage values from failed parses. Manifest: [\"-30deg\", \"0deg\", \"45deg\", \"90deg\", \"0.0rad\", \"0.5235988rad\", \"1.0471976rad\", \"1.5707963rad\", \"999deg\", \"NaNrad\"]. Build two cohorts of 4x4 homogeneous rotation matrices about Y: (1) a calibration cohort from degree-annotated values that fall within the closed interval [-90deg, +90deg], converted to radians; (2) a trajectory cohort from radian-annotated values that are finite and fall within [0, pi/2]. Return the matrices for each cohort in the manifest order after applying the cohort rules.", "answers": "[{\"name\":\"Bio_PDB_vectors_multi_rot_Y\",\"arguments\":{\"angle_rads\":[-0.5235987756,0.0,0.7853981634,1.5707963268]}},{\"name\":\"Bio_PDB_vectors_multi_rot_Y\",\"arguments\":{\"angle_rads\":[0.0,0.5235988,1.0471976,1.5707963]}}]"}
{"func_name": "Bio_PDB_vectors_multi_rot_Z", "func_desc": "Create a stack of 4x4 homogeneous rotation matrices for rotations about the Z axis.\n    \n    This function is used in the Bio.PDB.vectors context (part of Biopython) to build homogeneous transformation matrices that rotate 3D coordinates around the Z axis by specified angles in radians. In computational molecular biology workflows (for example, rotating atom coordinates in a PDB structure, aligning fragments, or composing rigid-body transforms), these matrices can be applied to homogeneous column vectors [x, y, z, 1] to rotate the x-y components while preserving the z coordinate and translation component.", "tools": [{"function": {"description": "Create a stack of 4x4 homogeneous rotation matrices for rotations about the Z axis.\n\nThis function is used in the Bio.PDB.vectors context (part of Biopython) to build homogeneous transformation matrices that rotate 3D coordinates around the Z axis by specified angles in radians. In computational molecular biology workflows (for example, rotating atom coordinates in a PDB structure, aligning fragments, or composing rigid-body transforms), these matrices can be applied to homogeneous column vectors [x, y, z, 1] to rotate the x-y components while preserving the z coordinate and translation component.", "name": "Bio_PDB_vectors_multi_rot_Z", "parameters": {"properties": {"angle_rads": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of angles in radians. The first dimension length, angle_rads.shape[0], determines the number of rotation matrices produced (referred to in older documentation as \"entries\"). The function reads angle values elementwise and computes elementwise cosine and sine; therefore angle_rads must be a numpy.ndarray-like object with numeric values accessible via its first dimension. If angle_rads.shape[0] is zero the function returns an array with shape (0, 4, 4). The input array is not modified by this function.", "default": ""}}, "required": ["angle_rads"], "type": "any"}}, "type": "function"}], "query": "In our Z-spin rigid-body pre-docking stage, we need two rotation-matrix cohorts, but the upstream angle logs are messy. Build 4x4 homogeneous Z-rotation matrix stacks only for angles that are physically meaningful as unique in-plane orientations: keep angles that are finite and normalize-equivalent into the principal interval (−π, π]; then, within each cohort, collapse any angles that are duplicates after this normalization (e.g., values differing by 2π). Cohort A (alternative conformer sampling) raw angles (radians): [0, π/6, π/2, −π/3, 2π + π/6, 7π/2, NaN]. Cohort B (trial docking sweep) is specified in degrees but comes from a mixed sign convention: [0°, 30°, 60°, 90°, 120°, −240°, 390°]. Convert to radians, apply the same normalization and de-duplication rules, and generate the Z-rotation matrix stacks for the remaining unique orientations in each cohort.", "answers": "[{\"name\":\"Bio_PDB_vectors_multi_rot_Z\",\"arguments\":{\"angle_rads\":[0.0,0.5235987756,1.5707963268,-1.0471975512]}},{\"name\":\"Bio_PDB_vectors_multi_rot_Z\",\"arguments\":{\"angle_rads\":[0.0,0.5235987756,1.0471975512,1.5707963268,2.0943951024]}}]"}
{"func_name": "Bio_PDB_vectors_set_Y_homog_rot_mtx", "func_desc": "Set the elements of an existing homogeneous rotation matrix to represent a rotation\n    about the Y axis by the specified angle (in radians). This function is used in the\n    Bio.PDB.vectors module of Biopython to update the rotation block of a transform\n    matrix so it can be applied to 3D coordinate data (for example, atomic coordinates\n    or intermediate frames) without allocating a new array.", "tools": [{"function": {"description": "Set the elements of an existing homogeneous rotation matrix to represent a rotation\nabout the Y axis by the specified angle (in radians). This function is used in the\nBio.PDB.vectors module of Biopython to update the rotation block of a transform\nmatrix so it can be applied to 3D coordinate data (for example, atomic coordinates\nor intermediate frames) without allocating a new array.", "name": "Bio_PDB_vectors_set_Y_homog_rot_mtx", "parameters": {"properties": {"angle_rads": {"type": "float", "description": "Rotation angle in radians. This numeric parameter is the\nangle by which to rotate about the Y axis. The function computes the\ncosine and sine of this value (via numpy.cos and numpy.sin) and writes\nthose results into the matrix entries that control rotation in the X–Z\nplane. Practically, provide the rotation in radians matching the rest of\na Biopython coordinate/transform pipeline; no conversion from degrees is\nperformed.", "default": ""}, "mtx": {"type": "array", "items": {"type": "float"}, "description": "Mutable NumPy array that will be updated in-place to\ncontain the Y-axis rotation terms. The array must support indexing at\nmtx[0][0], mtx[0][2], mtx[2][0], and mtx[2][2]; in typical usage this\nis a 4x4 homogeneous transform matrix where the upper-left 3x3 block is\nthe rotation matrix. The function sets mtx[0][0] and mtx[2][2] to cos(angle_rads),\nmtx[0][2] to +sin(angle_rads), and mtx[2][0] to -sin(angle_rads). All other\nentries of mtx are left unchanged by this call. Because the update is\ndone in-place, callers relying on the original matrix values must make a\ncopy beforehand if needed.", "default": ""}}, "required": ["angle_rads", "mtx"], "type": "any"}}, "type": "function"}], "query": "We’re running a torsion-scan sanity check on a single protein chain’s coordinate frame before applying transforms to atomic coordinates. Start from the same pre-allocated 4×4 homogeneous transform for every replicate:\n[[1.0, 0.0, 0.0, 12.5],\n [0.0, 1.0, 0.0, -3.2],\n [0.0, 0.0, 1.0,  7.8],\n [0.0, 0.0, 0.0,  1.0]]\nWe have a mixed batch of candidate Y-axis rotations coming from upstream metadata (units may be degrees or radians, and some entries are duplicated for cross-checking):\n- cond_id=scan_helixA: value=30, unit=deg\n- cond_id=scan_helixA_repeat: value=0.5235987756, unit=rad\n- cond_id=orthogonal_stresstest: value=1.5707963267948966, unit=rad\n- cond_id=bad_unit_flag: value=30, unit=rad\nFor each condition that represents a physically plausible Y rotation for a single-step perturbation (|angle| ≤ π), keep the translation column unchanged and overwrite only the 3×3 rotation block in-place with a pure Y-axis rotation using the provided function. Interpret degree-tagged values as degrees (convert to radians).", "answers": "[{\"name\":\"Bio_PDB_vectors_set_Y_homog_rot_mtx\",\"arguments\":{\"angle_rads\":0.5235987755982988,\"mtx\":[[1.0,0.0,0.0,12.5],[0.0,1.0,0.0,-3.2],[0.0,0.0,1.0,7.8],[0.0,0.0,0.0,1.0]]}},{\"name\":\"Bio_PDB_vectors_set_Y_homog_rot_mtx\",\"arguments\":{\"angle_rads\":0.5235987756,\"mtx\":[[1.0,0.0,0.0,12.5],[0.0,1.0,0.0,-3.2],[0.0,0.0,1.0,7.8],[0.0,0.0,0.0,1.0]]}},{\"name\":\"Bio_PDB_vectors_set_Y_homog_rot_mtx\",\"arguments\":{\"angle_rads\":1.5707963267948966,\"mtx\":[[1.0,0.0,0.0,12.5],[0.0,1.0,0.0,-3.2],[0.0,0.0,1.0,7.8],[0.0,0.0,0.0,1.0]]}}]"}
{"func_name": "Bio_PDB_vectors_set_Z_homog_rot_mtx", "func_desc": "Update an existing rotation matrix's Z-axis rotation terms in-place.\n    \n    This function computes the cosine and sine of a rotation angle provided in radians and stores those values into the appropriate entries of an existing NumPy array representing a homogeneous-style rotation matrix. In the Bio.PDB/vectors context (used for coordinate transforms and rigid-body rotations in computational molecular biology, e.g. rotating atom coordinates or coordinate frames when manipulating PDB structures), callers typically maintain mutable transformation matrices and call this function to set or change the rotation about the Z axis without reallocating a new matrix.", "tools": [{"function": {"description": "Update an existing rotation matrix's Z-axis rotation terms in-place.\n\nThis function computes the cosine and sine of a rotation angle provided in radians and stores those values into the appropriate entries of an existing NumPy array representing a homogeneous-style rotation matrix. In the Bio.PDB/vectors context (used for coordinate transforms and rigid-body rotations in computational molecular biology, e.g. rotating atom coordinates or coordinate frames when manipulating PDB structures), callers typically maintain mutable transformation matrices and call this function to set or change the rotation about the Z axis without reallocating a new matrix.", "name": "Bio_PDB_vectors_set_Z_homog_rot_mtx", "parameters": {"properties": {"angle_rads": {"type": "float", "description": "Rotation angle in radians. This numeric value is passed to NumPy's trigonometric functions (np.cos, np.sin) to compute the rotation coefficients. The function treats the value as an angle in radians (not degrees); providing values that are not real numbers will propagate NaN/inf through the computed entries.", "default": ""}, "mtx": {"type": "array", "items": {"type": "float"}, "description": "A mutable NumPy array that will be updated in-place. The function assigns to the indexed elements mtx[0][0], mtx[1][1], mtx[1][0], and mtx[0][1] to set the Z-axis rotation terms (cosine on the diagonal, sine and negative sine on the off-diagonal). Callers must provide an array that supports item assignment at these indices and uses a numeric dtype suitable for floating-point values. The function does not allocate or return a new array; it only overwrites these four entries and leaves all other elements of mtx unchanged.", "default": ""}}, "required": ["angle_rads", "mtx"], "type": "any"}}, "type": "function"}], "query": "During PDB frame-calibration we collected a small set of candidate Z-axis twist angles (in radians) inferred from dihedral drift: [1.5707963267948966, -1.0471975511965976, 0.0, 9.42477796076938]. Update a single reusable 4×4 homogeneous transform (row-major) in-place from an identity starting state for each candidate that represents a physically meaningful proper rotation about Z for our rigid-body alignment step. Treat angles modulo 2π, and only apply updates for candidates that are not effectively a no-op (i.e., the Z-rotation would differ from identity beyond numerical noise). Keep translation and all non-Z-rotation elements unchanged.", "answers": "[{\"name\":\"Bio_PDB_vectors_set_Z_homog_rot_mtx\",\"arguments\":{\"angle_rads\":1.5707963267948966,\"mtx\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]}},{\"name\":\"Bio_PDB_vectors_set_Z_homog_rot_mtx\",\"arguments\":{\"angle_rads\":-1.0471975511965976,\"mtx\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]}}]"}
{"func_name": "Bio_Phylo_Consensus_bootstrap_trees", "func_desc": "Generate bootstrap replicate phylogenetic trees from a multiple sequence alignment for use in assessing clade support in phylogenetic analyses.\n    \n    This function implements the standard non-parametric bootstrap for sequence alignments: it produces \"times\" replicate alignments by sampling alignment columns (sites) with replacement and then builds a tree for each replicate using the provided tree_constructor. It supports Biopython alignment objects (Alignment or MultipleSeqAlignment) and NumPy 2-D arrays (shape: sequences x sites), matching how the source code handles both object types. The function yields each replicate tree in turn rather than returning a collection, allowing streaming of results for downstream consensus or support-value calculations commonly used in Bio.Phylo workflows.", "tools": [{"function": {"description": "Generate bootstrap replicate phylogenetic trees from a multiple sequence alignment for use in assessing clade support in phylogenetic analyses.\n\nThis function implements the standard non-parametric bootstrap for sequence alignments: it produces \"times\" replicate alignments by sampling alignment columns (sites) with replacement and then builds a tree for each replicate using the provided tree_constructor. It supports Biopython alignment objects (Alignment or MultipleSeqAlignment) and NumPy 2-D arrays (shape: sequences x sites), matching how the source code handles both object types. The function yields each replicate tree in turn rather than returning a collection, allowing streaming of results for downstream consensus or support-value calculations commonly used in Bio.Phylo workflows.", "name": "Bio_Phylo_Consensus_bootstrap_trees", "parameters": {"properties": {"alignment": {"type": "array", "items": {"type": "float"}, "description": "multiple sequence alignment to generate replicates from. For Biopython MultipleSeqAlignment or Alignment objects, the code treats each sequence as a sequence record and samples columns by slicing (alignment[:, col:col+1]) and concatenating slices to build the bootstrapped alignment. For a numpy.ndarray, the code expects a 2-D array with shape (n_sequences, n_sites) and samples column indices to form each replicate (alignment[:, cols]). This argument is the primary input representing aligned homologous sequences used in phylogenetic bootstrap analysis.", "default": ""}, "times": {"type": "integer", "description": "number of bootstrap replicates to generate. This must be a non-negative integer; if zero, the generator yields no trees. Each replicate involves sampling the same number of sites as the original alignment (sampling with replacement) and then constructing a tree from that resampled alignment. Large values increase computation time and memory usage proportionally to the cost of building trees for each replicate.", "default": ""}, "tree_constructor": {"type": "string", "description": "object responsible for building a phylogenetic tree from an alignment. The object must implement a build_tree(alignment) method that accepts the same alignment type produced for each replicate and returns a tree object (as used by Bio.Phylo). In practice this is a TreeConstructor implementation from Bio.Phylo (for example one wrapping distance or character-based methods). The tree_constructor controls the tree-building algorithm and therefore directly affects the resulting bootstrap trees used for downstream consensus or support calculations.", "default": ""}}, "required": ["alignment", "times", "tree_constructor"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing clade-support estimation under heterogeneous data quality across three alignment cohorts. For each cohort, first screen for bootstrap-worthiness based on site variability (treat columns as alignment sites): proceed only if the alignment contains at least two polymorphic sites (a site is polymorphic if not all taxa share the same symbol/state at that column). For cohorts that pass, choose the bootstrap depth relative to informativeness: if the fraction of polymorphic sites is at least 1/3 of total sites, generate 50 non-parametric bootstrap replicate trees; otherwise generate 25. Use a Neighbor-Joining constructor chosen by encoding: if the cohort is encoded as nucleotide characters, use NJDistanceTreeConstructor; otherwise (numeric-encoded states) use NeighborJoiningTreeConstructor. Cohort A (5 taxa × 8 sites; numeric states): [[0,1,1,0,2,2,1,0],[0,1,0,0,2,2,1,1],[1,1,0,0,2,1,1,1],[1,0,0,1,2,1,0,1],[1,0,1,1,2,1,0,0]]. Cohort B (4 taxa × 12 sites; character nucleotides): [[\"A\",\"C\",\"G\",\"T\",\"A\",\"A\",\"C\",\"G\",\"T\",\"T\",\"C\",\"A\"],[\"A\",\"C\",\"G\",\"T\",\"A\",\"G\",\"C\",\"G\",\"T\",\"T\",\"C\",\"A\"],[\"A\",\"T\",\"G\",\"T\",\"A\",\"A\",\"C\",\"A\",\"T\",\"T\",\"C\",\"A\"],[\"G\",\"C\",\"G\",\"T\",\"A\",\"A\",\"C\",\"G\",\"C\",\"T\",\"C\",\"A\"]]. Cohort C (4 taxa × 12 sites; numeric states): [[1,0,2,2,3,1,0,0,2,1,3,2],[1,0,2,3,3,1,0,1,2,1,3,2],[0,0,2,2,3,1,0,0,1,1,3,2],[1,1,2,2,3,1,0,0,2,2,3,2]]. Stream each replicate tree for downstream consensus/support calculations.", "answers": "[{\"name\":\"Bio_Phylo_Consensus_bootstrap_trees\",\"arguments\":{\"alignment\":[[0,1,1,0,2,2,1,0],[0,1,0,0,2,2,1,1],[1,1,0,0,2,1,1,1],[1,0,0,1,2,1,0,1],[1,0,1,1,2,1,0,0]],\"times\":50,\"tree_constructor\":\"NeighborJoiningTreeConstructor\"}},{\"name\":\"Bio_Phylo_Consensus_bootstrap_trees\",\"arguments\":{\"alignment\":[[\"A\",\"C\",\"G\",\"T\",\"A\",\"A\",\"C\",\"G\",\"T\",\"T\",\"C\",\"A\"],[\"A\",\"C\",\"G\",\"T\",\"A\",\"G\",\"C\",\"G\",\"T\",\"T\",\"C\",\"A\"],[\"A\",\"T\",\"G\",\"T\",\"A\",\"A\",\"C\",\"A\",\"T\",\"T\",\"C\",\"A\"],[\"G\",\"C\",\"G\",\"T\",\"A\",\"A\",\"C\",\"G\",\"C\",\"T\",\"C\",\"A\"]],\"times\":50,\"tree_constructor\":\"NJDistanceTreeConstructor\"}},{\"name\":\"Bio_Phylo_Consensus_bootstrap_trees\",\"arguments\":{\"alignment\":[[1,0,2,2,3,1,0,0,2,1,3,2],[1,0,2,3,3,1,0,1,2,1,3,2],[0,0,2,2,3,1,0,0,1,1,3,2],[1,1,2,2,3,1,0,0,2,2,3,2]],\"times\":50,\"tree_constructor\":\"NeighborJoiningTreeConstructor\"}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_freqs", "func_desc": "Parse and extract basepair and branch frequency parameters from baseml output lines.", "tools": [{"function": {"description": "Parse and extract basepair and branch frequency parameters from baseml output lines.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_freqs", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "float"}, "description": "A list of text lines (strings) from a baseml/PAML output file. Each element should be a string representing one line of the program output. This function scans these lines to locate numeric frequency information using several baseml-specific headings (for example \"Base frequencies\", \"base frequency parameters\", \"freq:\", and \"(frequency parameters for branches)\"). The function expects that floating point numbers within each line can be extracted (the calling module provides a regex, line_floats_re, to find numeric substrings) and converts those substrings to Python float values for storage.", "default": ""}, "parameters": {"type": "any", "description": "A dictionary (typically initially empty or containing other parsed results) that will be updated in-place with any frequency information found in lines. This dict is both a mutable input and the carrier of results returned by the function; callers should pass the same dict they intend to receive updated.", "default": ""}}, "required": ["lines", "parameters"], "type": "any"}}, "type": "function"}], "query": "We are consolidating BASEML (PAML) outputs from two phylogenetic runs, but the logs are messy. For each cohort, parse only the contiguous block that starts at the first line containing the token \"Base frequencies\" and ends at the last line that still belongs to the branch-frequency section (i.e., after the marker \"(frequency parameters for branches)\", retain only lines that actually contain a frequency vector introduced by the token \"freq:\"; stop collecting when the stream transitions to unrelated commentary/headers). Append the extracted (i) overall nucleotide base frequencies and (ii) any branch-specific frequency parameters into each run’s existing `parameters` dict.\n\nCohort A (start with empty parameters {}), raw lines:\n- \"Base frequencies\"\n- \"  A: 0.296  C: 0.204  G: 0.201  T: 0.299\"\n- \"(frequency parameters for branches)\"\n- \"  branch 1 freq: 0.25 0.25 0.25 0.25\"\n- \"  branch 2 freq: 0.30 0.20 0.20 0.30\"\n\nCohort B (start with parameters = {\"run_id\":\"bm_2026_01_12\",\"model\":\"HKY85\"}), raw lines (may include headers and trailing unrelated text):\n- \"BASEML (in paml 4.10)\"\n- \"Base frequencies: 0.295 0.205 0.215 0.285\"\n- \"(frequency parameters for branches)\"\n- \"freq: 0.30 0.20 0.25 0.25\"\n- \"freq: 0.28 0.22 0.20 0.30\"\n- \"other stuff...\"", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_freqs\",\"arguments\":{\"lines\":[\"Base frequencies\",\"  A: 0.296  C: 0.204  G: 0.201  T: 0.299\",\"(frequency parameters for branches)\",\"  branch 1 freq: 0.25 0.25 0.25 0.25\",\"  branch 2 freq: 0.30 0.20 0.20 0.30\"],\"parameters\":{}}},{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_freqs\",\"arguments\":{\"lines\":[\"Base frequencies: 0.295 0.205 0.215 0.285\",\"(frequency parameters for branches)\",\"freq: 0.30 0.20 0.25 0.25\",\"freq: 0.28 0.22 0.20 0.30\"],\"parameters\":{\"run_id\":\"bm_2026_01_12\",\"model\":\"HKY85\"}}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_parameter_list", "func_desc": "Parse the parameters list found in baseml-formatted text and extract the unlabeled\n    numeric parameter vector and associated standard errors (SEs), storing them in\n    the provided parameters dictionary for downstream use in PAML baseml workflows.", "tools": [{"function": {"description": "Parse the parameters list found in baseml-formatted text and extract the unlabeled\nnumeric parameter vector and associated standard errors (SEs), storing them in\nthe provided parameters dictionary for downstream use in PAML baseml workflows.", "name": "Bio_Phylo_PAML__parse_baseml_parse_parameter_list", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "float"}, "description": "Sequence (list) of lines (strings) representing the text output\nto be parsed from a baseml run or an input file such as in.baseml. Each\nelement is treated as a line of text and inspected for floating point\nnumeric tokens. In the Biopython PAML baseml parser this is typically\nthe file contents split on newline characters.", "default": ""}, "parameters": {"type": "any", "description": "Mutable dictionary used to collect parsed results. This\nfunction mutates and returns this dict. On success it sets the key\n\"parameter list\" to the original line (stripped of surrounding whitespace)\nthat contained exactly num_params floating point values, and, if present,\nsets the key \"SEs\" to the corresponding SEs line (also stripped). This\ndictionary is used by callers to capture starting parameter strings for\nreuse in subsequent baseml runs (for example copying into in.baseml).", "default": ""}, "num_params": {"type": "integer", "description": "The expected number of numeric parameters on the unlabeled\nparameter line. The function looks for the first line that contains\nexactly this many floating point numbers (as determined by the compiled\nregular expression line_floats_re in the surrounding module) and treats\nthat line as the canonical parameter list. This value must match the\nnumber of parameters produced by the baseml configuration being parsed.", "default": ""}}, "required": ["lines", "parameters", "num_params"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating baseml replicate outputs from a mixed-quality batch where some runs include multiple unlabeled numeric vectors (e.g., parameter guesses, intermediate iterates) and only one of them is the final optimization parameter vector suitable for downstream reruns. For each replicate, scan the baseml-formatted line list and extract the unlabeled numeric parameter vector only if it is immediately followed by an SE line prefixed with \"SEs:\" and both lines contain exactly 5 floating-point values. Treat any unlabeled 5-float vector not directly paired with an immediate \"SEs:\" line as non-final and ignore it. Store the extracted 5-parameter vector and its 5 SEs back into the provided parameters/results dictionary for that replicate, preserving any existing metadata keys (e.g., run_id/model). Process this batch of replicates:\n\nReplicate A lines:\n- \"Some header text\"\n- \"kappa = 2.1\"\n- \" 0.12345  0.23456  0.34567  0.45678  0.56789 \"\n- \"SEs: 0.01000 0.02000 0.03000 0.04000 0.05000\"\n- \"footer\"\n(use the existing parameters dict containing run_id=\"baseml_demo_2025-04-18\" and model=\"HKY85\")\n\nReplicate B lines:\n- \"lnL(ntime: 5  np: 6):  -1234.56789\"\n- \"initial guess:\"\n- \"0.11111 0.22222 0.33333 0.44444 0.55555\"\n- \"kappa (ts/tv) = 2.153\"\n- \"0.12034  0.98765  3.14159  0.00012  1.23456\"\n- \"SEs: 0.01000  0.02000  0.30000  0.00001  0.05000\"\n- \"(other text continues...)\"\n(use an initially empty parameters dict).", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_parameter_list\",\"arguments\":{\"lines\":[\"Some header text\",\"kappa = 2.1\",\" 0.12345  0.23456  0.34567  0.45678  0.56789 \",\"SEs: 0.01000 0.02000 0.03000 0.04000 0.05000\",\"footer\"],\"parameters\":{\"run_id\":\"baseml_demo_2025-04-18\",\"model\":\"HKY85\"},\"num_params\":5}},{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_parameter_list\",\"arguments\":{\"lines\":[\"lnL(ntime: 5  np: 6):  -1234.56789\",\"initial guess:\",\"0.11111 0.22222 0.33333 0.44444 0.55555\",\"kappa (ts/tv) = 2.153\",\"0.12034  0.98765  3.14159  0.00012  1.23456\",\"SEs: 0.01000  0.02000  0.30000  0.00001  0.05000\",\"(other text continues...)\"],\"parameters\":{},\"num_params\":5}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_parameters", "func_desc": "Parse and collect model parameter values from lines of a baseml/PAML output file and store them into the provided results dictionary for downstream use by the Bio.Phylo.PAML baseml parser.", "tools": [{"function": {"description": "Parse and collect model parameter values from lines of a baseml/PAML output file and store them into the provided results dictionary for downstream use by the Bio.Phylo.PAML baseml parser.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_parameters", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "float"}, "description": "The raw file content provided as a list of text lines (each item is a str). In the context of Biopython's PAML baseml parser, these lines are expected to be the lines of a baseml output or related parameter block. This argument is read but not modified; helper functions scan these lines to locate and extract parameter definitions such as scalar parameters, kappa values (transition/transversion ratios), substitution rate categories, and nucleotide/amino-acid equilibrium frequencies.", "default": ""}, "results": {"type": "any", "description": "A mutable dictionary used to accumulate parsed sections of the baseml output. This function will add or replace the key \"parameters\" in this dictionary with a mapping of parsed parameter names to their values. callers typically pass a results dict that already contains other parsed sections; parse_parameters appends the parsed parameter block so later code in Bio.Phylo.PAML can access model settings for likelihood calculations, model comparison, or reporting.", "default": ""}, "num_params": {"type": "integer", "description": "The expected number of parameters for the initial parameter list parsing step. This integer is forwarded to the helper parse_parameter_list function and determines how many scalar parameters to read from lines. It controls interpretation of a parameter list in the baseml output (for example, the number of estimated free parameters reported by PAML). If this value does not match the content found in lines, the helper functions may produce fewer/more entries or raise an error.", "default": ""}}, "required": ["lines", "results", "num_params"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our baseml/PAML ingestion against messy multi-replicate logs where different runs emit different parameter block layouts. Below are two raw baseml log snippets (each snippet is a list of lines). For each snippet, parse and accumulate model parameter values into its provided results dictionary for downstream parsing. Apply a format-aware rule: if a snippet contains a line matching the pattern “Parameters in M0 (N parameters):”, treat the immediately following numeric line as the primary scalar-parameter list and set `num_params` to N; otherwise, treat the snippet as a minimal kappa/pi run and set `num_params` to the count of numeric rate values that appear on the same line as “Rate parameters:” (i.e., inline rate block). Preserve any pre-existing metadata keys in the results dict (e.g., an existing logfile field).\n\nSnippet A (fresh results dict): [\"Parameters (kappa, pi):\", \"kappa (ts/tv) = 2.35\", \"pi = (0.30 0.20 0.25 0.25)\", \"Rate parameters: 0.10 0.20 0.15 0.05 0.30 0.20\", \"# end\"]\n\nSnippet B (results dict already initialized with logfile=baseml_out_M0.txt): [\"Parameters (kappa and frequencies)\", \"kappa (ts/tv) = 2.35\", \"Parameters in M0 (7 parameters):\", \"  0.12 0.34 0.56 0.78 1.23 4.56 7.89\", \"Rate parameters:\", \"  r0 = 0.9  r1 = 1.1  r2 = 1.3\", \"Base frequencies:\", \"  pi(A) = 0.30  pi(C) = 0.20  pi(G) = 0.25  pi(T) = 0.25\"]", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_parameters\",\"arguments\":{\"lines\":[\"Parameters (kappa, pi):\",\"kappa (ts/tv) = 2.35\",\"pi = (0.30 0.20 0.25 0.25)\",\"Rate parameters: 0.10 0.20 0.15 0.05 0.30 0.20\",\"# end\"],\"results\":{},\"num_params\":6}},{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_parameters\",\"arguments\":{\"lines\":[\"Parameters (kappa and frequencies)\",\"kappa (ts/tv) = 2.35\",\"Parameters in M0 (7 parameters):\",\"  0.12 0.34 0.56 0.78 1.23 4.56 7.89\",\"Rate parameters:\",\"  r0 = 0.9  r1 = 1.1  r2 = 1.3\",\"Base frequencies:\",\"  pi(A) = 0.30  pi(C) = 0.20  pi(G) = 0.25  pi(T) = 0.25\"],\"results\":{\"logfile\":\"baseml_out_M0.txt\"},\"num_params\":7}}]"}
{"func_name": "Bio_Phylo_PAML__parse_baseml_parse_rates", "func_desc": "Bio.Phylo.PAML._parse_baseml.parse_rates parses rate-related lines from baseml/PAML output and extracts numeric rate parameters into a Python dictionary used by the Bio.Phylo.PAML baseml parser. This function is used when reading baseml program output (phylogenetic substitution model results) to collect rate parameters, per-category rates, the 4x4 nucleotide substitution rate matrix Q (for REV-like models), the gamma shape parameter alpha, the rho parameter for auto-discrete-gamma models, and transition probability arrays. The parsed values are returned in the same dictionary object passed in so downstream code in Bio.Phylo.PAML can access model parameters for tasks such as annotating trees, interpreting substitution rates, or performing further calculations.", "tools": [{"function": {"description": "Bio.Phylo.PAML._parse_baseml.parse_rates parses rate-related lines from baseml/PAML output and extracts numeric rate parameters into a Python dictionary used by the Bio.Phylo.PAML baseml parser. This function is used when reading baseml program output (phylogenetic substitution model results) to collect rate parameters, per-category rates, the 4x4 nucleotide substitution rate matrix Q (for REV-like models), the gamma shape parameter alpha, the rho parameter for auto-discrete-gamma models, and transition probability arrays. The parsed values are returned in the same dictionary object passed in so downstream code in Bio.Phylo.PAML can access model parameters for tasks such as annotating trees, interpreting substitution rates, or performing further calculations.\n", "name": "Bio_Phylo_PAML__parse_baseml_parse_rates", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "float"}, "description": "Lines of text (typically a list of str) from a baseml/PAML output file. Each element is scanned in order for substrings that indicate rate information. The function looks for the exact substrings \"Rate parameters:\", \"rate: \", \"matrix Q\", \"alpha\", \"rho\", and \"transition probabilities\" (case-sensitive) and then extracts floating-point numbers from those lines and subsequent expected lines. Practical significance: these lines normally appear in baseml output to report estimated substitution rates, relative rates per site-class, a 4x4 nucleotide rate matrix, and transition probability matrices; providing the raw output lines allows this function to populate structured parameters for use by the Bio.Phylo.PAML baseml parser.", "default": ""}, "parameters": {"type": "any", "description": "A dictionary (typically empty or partially populated) that the function will update in-place with parsed numeric values. The function will add or overwrite keys described below so that downstream code in Bio.Phylo.PAML can read standardized fields. Practical significance: callers should pass a dict to collect parsed model parameters; the same dict is returned for convenience.", "default": ""}}, "required": ["lines", "parameters"], "type": "any"}}, "type": "function"}], "query": "We’re doing QC-aware ingestion of baseml/PAML REV-family outputs coming from a mixed compute environment where stdout can contain partial/duplicate blocks and occasional non-REV artifacts. Treat the three provided line-batches as three independent runs.\n\nProtocol:\n1) Only run the rate-parameter extraction stage on runs that exhibit an explicit REV-like substitution signal: a “matrix Q” block must be present AND it must contain a full 4×4 numeric grid immediately following the header (allow the header to include annotations like “(relative rates)”).\n2) For any run that passes (1), select the segment(s) of lines to parse as follows:\n   - Start at the first line that declares rate parameters (any of: “Rate parameters:” with named fields, unlabeled numeric vector, or labeled r(A-*) fields).\n   - Include all subsequent lines through the end of the last transition-probability matrix block present (recognize headers with or without time annotations).\n   - If a run contains multiple “transition probabilities” headers, keep only the final transition-probability block (to mimic restarts/resumes), but keep the most recent preceding alpha/rho declarations that occur before that final block.\n3) Populate a reusable parameters dictionary for each run using parse_rates, preserving any pre-seeded keys.\n   - Run 1 starts from an empty dict.\n   - Run 2 starts from {model: \"REV\", K: 4, note: \"QC parse: retain existing metadata; populate rates/Q/alpha/rho/P(t)\"}.\n   - Run 3 starts from {run_id: \"baseml_demo_001\", model: \"REV+Gamma+rho\"}.\n\nInput line-batches (use exactly these):\nRun 1 lines: [\"lnL(ntime: 15  np: 19):  -12345.678   +0.000000\", \"Rate parameters:  kappa = 2.345  omega = 0.789\", \"rate:  0.2500  0.7500\", \"matrix Q (relative rates)\", \"   -1.234   0.456   0.389   0.389\", \"    0.567  -1.890   0.678   0.645\", \"    0.432   0.654  -1.765   0.679\", \"    0.321   0.543   0.876  -1.740\", \"alpha (gamma shape parameter) = 0.9876\", \"rho (auto-discrete-gamma) = 0.1234\", \"transition probabilities (time t = 0.1)\", \"  0.950  0.020  0.015  0.015\", \"  0.018  0.945  0.020  0.017\", \"  0.016  0.019  0.947  0.018\", \"  0.015  0.018  0.017  0.950\"]\n\nRun 2 lines: [\"Rate parameters:   1.23456  0.98765  2.34567  0.76543  1.11111\", \"rate:  0.10 0.25 0.30 0.35\", \"matrix Q\", \"  -1.2345   0.4000   0.5000   0.3345\", \"   0.3000  -1.1000   0.6000   0.2000\", \"   0.2000   0.7000  -1.3000   0.4000\", \"   0.4500   0.1500   0.2500  -0.8500\", \"alpha (gamma, K=4) = 0.5678\", \"rho = 0.1234\", \"transition probabilities\", \"   0.91 0.03 0.04 0.02\", \"   0.02 0.92 0.03 0.03\", \"   0.05 0.02 0.90 0.03\", \"   0.01 0.04 0.02 0.93\"]\n\nRun 3 lines: [\"Rate parameters:  r(A-C)= 1.234  r(A-G)= 2.345  r(A-T)= 0.987\", \"rate: 0.10 0.20 0.30 0.40\", \"matrix Q\", \" 0.000000 -1.111111  0.222222  0.333333\", \" 1.444444  0.000000 -1.555555  0.666666\", \" 0.777777  0.888888  0.000000 -1.999999\", \" 1.000000  1.111111  1.222222  0.000000\", \"alpha = 0.765\", \"rho = 0.123\", \"transition probabilities\", \" 0.90 0.05 0.03 0.02\", \" 0.10 0.80 0.05 0.05\", \" 0.20 0.10 0.60 0.10\", \" 0.25 0.25 0.25 0.25\"]\n\nReturn the populated parameters dicts for runs that pass the REV-like substitution signal gate (1).", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_rates\",\"arguments\":{\"lines\":[\"Rate parameters:  kappa = 2.345  omega = 0.789\",\"rate:  0.2500  0.7500\",\"matrix Q (relative rates)\",\"   -1.234   0.456   0.389   0.389\",\"    0.567  -1.890   0.678   0.645\",\"    0.432   0.654  -1.765   0.679\",\"    0.321   0.543   0.876  -1.740\",\"alpha (gamma shape parameter) = 0.9876\",\"rho (auto-discrete-gamma) = 0.1234\",\"transition probabilities (time t = 0.1)\",\"  0.950  0.020  0.015  0.015\",\"  0.018  0.945  0.020  0.017\",\"  0.016  0.019  0.947  0.018\",\"  0.015  0.018  0.017  0.950\"],\"parameters\":{}}},{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_rates\",\"arguments\":{\"lines\":[\"Rate parameters:   1.23456  0.98765  2.34567  0.76543  1.11111\",\"rate:  0.10 0.25 0.30 0.35\",\"matrix Q\",\"  -1.2345   0.4000   0.5000   0.3345\",\"   0.3000  -1.1000   0.6000   0.2000\",\"   0.2000   0.7000  -1.3000   0.4000\",\"   0.4500   0.1500   0.2500  -0.8500\",\"alpha (gamma, K=4) = 0.5678\",\"rho = 0.1234\",\"transition probabilities\",\"   0.91 0.03 0.04 0.02\",\"   0.02 0.92 0.03 0.03\",\"   0.05 0.02 0.90 0.03\",\"   0.01 0.04 0.02 0.93\"],\"parameters\":{\"model\":\"REV\",\"K\":4,\"note\":\"QC parse: retain existing metadata; populate rates/Q/alpha/rho/P(t)\"}}},{\"name\":\"Bio_Phylo_PAML__parse_baseml_parse_rates\",\"arguments\":{\"lines\":[\"Rate parameters:  r(A-C)= 1.234  r(A-G)= 2.345  r(A-T)= 0.987\",\"rate: 0.10 0.20 0.30 0.40\",\"matrix Q\",\" 0.000000 -1.111111  0.222222  0.333333\",\" 1.444444  0.000000 -1.555555  0.666666\",\" 0.777777  0.888888  0.000000 -1.999999\",\" 1.000000  1.111111  1.222222  0.000000\",\"alpha = 0.765\",\"rho = 0.123\",\"transition probabilities\",\" 0.90 0.05 0.03 0.02\",\" 0.10 0.80 0.05 0.05\",\" 0.20 0.10 0.60 0.10\",\" 0.25 0.25 0.25 0.25\"],\"parameters\":{\"run_id\":\"baseml_demo_001\",\"model\":\"REV+Gamma+rho\"}}}]"}
{"func_name": "Bio_Phylo_PAML__parse_yn00_parse_yn00", "func_desc": "Parse the Yang & Nielsen (2000) part of PAML yn00 output and insert\n    pairwise statistics into an existing results dictionary.\n    \n    This function is part of Bio.Phylo.PAML._parse_yn00 and is used to parse\n    the section of PAML's yn00 program output that lists pairwise comparisons\n    between sequences (Yang & Nielsen 2000). Each row in that section is\n    expected to start with two sequence indices (1-based) followed by a set\n    of floating point values corresponding to the quantities S, N, t,\n    kappa, omega, dN, dN SE, dS, and dS SE. The function converts the numeric\n    strings to floats, maps the 1-based indices to sequence names provided\n    via the sequences list, constructs a dictionary of the parsed values\n    under the key \"YN00\" and stores it in the provided results mapping for\n    both orderings of the sequence pair (results[name1][name2] and\n    results[name2][name1]). Typical use is within a PAML output parsing\n    pipeline where an already-initialized results structure must be\n    populated with yn00-derived pairwise evolutionary statistics for downstream\n    analysis (for example, computing dN/dS summaries or annotating trees in\n    Bio.Phylo).", "tools": [{"function": {"description": "Parse the Yang & Nielsen (2000) part of PAML yn00 output and insert\npairwise statistics into an existing results dictionary.\n\nThis function is part of Bio.Phylo.PAML._parse_yn00 and is used to parse\nthe section of PAML's yn00 program output that lists pairwise comparisons\nbetween sequences (Yang & Nielsen 2000). Each row in that section is\nexpected to start with two sequence indices (1-based) followed by a set\nof floating point values corresponding to the quantities S, N, t,\nkappa, omega, dN, dN SE, dS, and dS SE. The function converts the numeric\nstrings to floats, maps the 1-based indices to sequence names provided\nvia the sequences list, constructs a dictionary of the parsed values\nunder the key \"YN00\" and stores it in the provided results mapping for\nboth orderings of the sequence pair (results[name1][name2] and\nresults[name2][name1]). Typical use is within a PAML output parsing\npipeline where an already-initialized results structure must be\npopulated with yn00-derived pairwise evolutionary statistics for downstream\nanalysis (for example, computing dN/dS summaries or annotating trees in\nBio.Phylo).", "name": "Bio_Phylo_PAML__parse_yn00_parse_yn00", "parameters": {"properties": {"lines": {"type": "array", "items": {"type": "float"}, "description": "A list of strings, each string being one line from the\nyn00 program output. The function scans each line for a leading\npair of integers and for floating point values using regular\nexpressions. It expects the nine floats for a table row to be\npresent on the same line as the leading indices; if they are not,\nthe function may raise an IndexError or fail to parse that row.", "default": ""}, "results": {"type": "any", "description": "A nested dictionary mapping sequence name to another\nmapping of sequence name to per-comparison dictionaries. Concretely,\nresults is expected to already contain keys for every sequence\nname in sequences and for each pair results[name1][name2] must be\nan addressable dictionary (for example, created earlier by the\noverall parser). This function will add or overwrite the \"YN00\"\nkey in results[name1][name2] and results[name2][name1] with a\ndictionary containing the parsed numeric fields. Because the\nfunction writes into this structure, the caller should provide a\nmutable dict prepared to receive these entries.", "default": ""}, "sequences": {"type": "array", "items": {"type": "float"}, "description": "A list of sequence names (strings) ordered so that\nthe sequence number labels used in the yn00 output (which are\n1-based integers) correspond to indices in this list (i.e.,\nsequences[0] is the name for sequence number 1). The function\nuses the 1-based indices found in the output lines to look up\nsequence names in this list and will raise an IndexError if an\nindex from the file does not have a corresponding entry in\nsequences.", "default": ""}}, "required": ["lines", "results", "sequences"], "type": "any"}}, "type": "function"}], "query": "We are curating yn00 pairwise output for two 3-taxon cohorts where the yn00 text dump includes a mix of analyzable rows and instrument/format artifacts. For each cohort, ingest the raw Yang & Nielsen (2000) pairwise table lines and populate the pre-initialized nested pairwise results matrix by parsing only the rows that represent biologically meaningful comparisons: keep only rows whose two leading indices are two distinct in-range 1-based sequence IDs for that cohort and whose numeric payload can be parsed to floats with finite values. Map the indices onto the provided sequence-name order, then insert a 'YN00' dictionary containing S, N, t, kappa, omega, dN, dN SE, dS, dS SE into results[name1][name2] and symmetrically into results[name2][name1]. Cohort 1 (species benchmark): sequences [\"Homo_sapiens\", \"Pan_troglodytes\", \"Mus_musculus\"], with raw lines (may include duplicates and malformed artifacts):\n- \"1 2  300.0  900.0  0.12  2.10  0.85  0.034  0.005  0.040  0.006\"\n- \"2 1  300.0  900.0  0.12  2.10  0.85  0.034  0.005  0.040  0.006\"  \n- \"1 3  295.0  910.0  0.20  2.30  0.70  0.028  0.004  0.040  0.007\"\n- \"2 3  298.0  905.0  0.18  2.25  0.90  0.032  0.005  0.036  0.006\"\n- \"0 2  300.0  900.0  0.12  2.10  0.85  0.034  0.005  0.040  0.006\"  \n- \"1 1  999  999  0.00  0.00  0.00  0.0  0.0  0.0  0.0\"\n- \"1 4  295.0  910.0  0.20  2.30  0.70  0.028  0.004  0.040  0.007\"\n\nCohort 2 (internal control replicate): sequences [\"SeqA\", \"SeqB\", \"SeqC\"], with raw lines:\n- \"1 2  120.5  300.2  0.034  2.10  0.15  0.0051  0.0012  0.0340  0.0078\"\n- \"1 3  118.0  305.0  0.040  1.95  0.22  0.0088  0.0020  0.0395  0.0085\"\n- \"2 3  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\"\n- \"3 2  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\"  \n- \"2 2  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\"\n- \"2 X  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\"", "answers": "[{\"name\":\"Bio_Phylo_PAML__parse_yn00_parse_yn00\",\"arguments\":{\"lines\":[\"1 2  300.0  900.0  0.12  2.10  0.85  0.034  0.005  0.040  0.006\",\"2 1  300.0  900.0  0.12  2.10  0.85  0.034  0.005  0.040  0.006\",\"1 3  295.0  910.0  0.20  2.30  0.70  0.028  0.004  0.040  0.007\",\"2 3  298.0  905.0  0.18  2.25  0.90  0.032  0.005  0.036  0.006\"],\"results\":{\"Homo_sapiens\":{\"Homo_sapiens\":{},\"Pan_troglodytes\":{},\"Mus_musculus\":{}},\"Pan_troglodytes\":{\"Homo_sapiens\":{},\"Pan_troglodytes\":{},\"Mus_musculus\":{}},\"Mus_musculus\":{\"Homo_sapiens\":{},\"Pan_troglodytes\":{},\"Mus_musculus\":{}}},\"sequences\":[\"Homo_sapiens\",\"Pan_troglodytes\",\"Mus_musculus\"]}},{\"name\":\"Bio_Phylo_PAML__parse_yn00_parse_yn00\",\"arguments\":{\"lines\":[\"1 2  120.5  300.2  0.034  2.10  0.15  0.0051  0.0012  0.0340  0.0078\",\"1 3  118.0  305.0  0.040  1.95  0.22  0.0088  0.0020  0.0395  0.0085\",\"2 3  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\",\"3 2  121.3  298.7  0.031  2.30  0.10  0.0032  0.0009  0.0321  0.0069\"],\"results\":{\"SeqA\":{\"SeqA\":{},\"SeqB\":{},\"SeqC\":{}},\"SeqB\":{\"SeqA\":{},\"SeqB\":{},\"SeqC\":{}},\"SeqC\":{\"SeqA\":{},\"SeqB\":{},\"SeqC\":{}}},\"sequences\":[\"SeqA\",\"SeqB\",\"SeqC\"]}}]"}
{"func_name": "Bio_Phylo_PAML_chi2_cdf_chi2", "func_desc": "Compute the upper-tail cumulative distribution function (p-value) of the chi-square\n    distribution for a given degrees of freedom and observed test statistic.\n    \n    This function is part of Bio.Phylo.PAML.chi2 and is used in phylogenetics and\n    molecular-evolution analyses (for example when processing PAML output) to convert\n    an observed chi-square test statistic into a p-value. Practically, this is most\n    commonly applied to likelihood ratio test (LRT) statistics (often twice the\n    difference in log-likelihoods) to assess whether a more complex model provides a\n    significantly better fit to sequence or tree data than a simpler model. The\n    implementation computes the upper-tail probability using the relation\n    prob = 1 - I_x(alpha), where x = stat / 2 and alpha = df / 2, and I_x is the\n    regularized lower incomplete gamma function (implemented here as _incomplete_gamma).", "tools": [{"function": {"description": "Compute the upper-tail cumulative distribution function (p-value) of the chi-square\ndistribution for a given degrees of freedom and observed test statistic.\n\nThis function is part of Bio.Phylo.PAML.chi2 and is used in phylogenetics and\nmolecular-evolution analyses (for example when processing PAML output) to convert\nan observed chi-square test statistic into a p-value. Practically, this is most\ncommonly applied to likelihood ratio test (LRT) statistics (often twice the\ndifference in log-likelihoods) to assess whether a more complex model provides a\nsignificantly better fit to sequence or tree data than a simpler model. The\nimplementation computes the upper-tail probability using the relation\nprob = 1 - I_x(alpha), where x = stat / 2 and alpha = df / 2, and I_x is the\nregularized lower incomplete gamma function (implemented here as _incomplete_gamma).", "name": "Bio_Phylo_PAML_chi2_cdf_chi2", "parameters": {"properties": {"df": {"type": "integer", "description": "Degrees of freedom for the chi-square distribution. In the\ncontext of model comparison in PAML/Bio.Phylo, df is the number of\nadditional free parameters in the more complex model relative to the\nsimpler model. Must be an integer greater than or equal to 1. If df < 1,\nthe function raises a ValueError with the message \"df must be at least 1\".", "default": ""}, "stat": {"type": "float", "description": "Observed chi-square test statistic whose upper-tail\nprobability is desired. In phylogenetic applications this commonly is the\nLRT statistic (e.g. 2 * (logL_complex - logL_simple)). Must be a\nnon-negative floating-point value. If stat < 0, the function raises a\nValueError with the message \"The test statistic must be positive\".", "default": ""}}, "required": ["df", "stat"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed set of PAML nested-model LRT outputs from a codon-model benchmarking run. Each record contains an observed LRT statistic (2Δℓ) and the nominal number of additional free parameters in the more complex model. Before converting to chi-square upper-tail p-values, treat the dataset as messy: only evaluate comparisons where the LRT statistic is strictly positive and finite, and where the added-parameter count is an even number (so that df equals that count). For the remaining valid comparisons, compute the chi-square upper-tail p-value using df equal to the added-parameter count. Raw records (label, added_params, LRT): [('codon_model_A_vs_B', 2, 6.72), ('basic_codon_vs_branch_site', 2, 7.85), ('nested_codon_models', 2, 9.21), ('artifact_negative_LRT', 2, -0.40), ('non_nested_mismatch', 3, 8.10), ('instrument_overflow', 2, float('inf'))]. Return p-values for the valid records in the same order they appear after filtering.", "answers": "[{\"name\":\"Bio_Phylo_PAML_chi2_cdf_chi2\",\"arguments\":{\"df\":2,\"stat\":6.72}},{\"name\":\"Bio_Phylo_PAML_chi2_cdf_chi2\",\"arguments\":{\"df\":2,\"stat\":7.85}},{\"name\":\"Bio_Phylo_PAML_chi2_cdf_chi2\",\"arguments\":{\"df\":2,\"stat\":9.21}}]"}
{"func_name": "Bio_PopGen_GenePop_get_indiv", "func_desc": "Bio.PopGen.GenePop.get_indiv extracts an individual's name and genotype marker data from a single line of a GenePop-format record, returning the parsed individual name, a list of allele tuples per locus and the per-allele code length. This function is used in population genetics workflows (as in the GenePop parsers in Bio.PopGen.GenePop) to convert a text line from a GenePop input file into structured Python objects that downstream routines can analyze (e.g., calculating allele frequencies, genotype counts, or performing population genetic tests).", "tools": [{"function": {"description": "Bio.PopGen.GenePop.get_indiv extracts an individual's name and genotype marker data from a single line of a GenePop-format record, returning the parsed individual name, a list of allele tuples per locus and the per-allele code length. This function is used in population genetics workflows (as in the GenePop parsers in Bio.PopGen.GenePop) to convert a text line from a GenePop input file into structured Python objects that downstream routines can analyze (e.g., calculating allele frequencies, genotype counts, or performing population genetic tests).\n", "name": "Bio_PopGen_GenePop_get_indiv", "parameters": {"properties": {"line": {"type": "string", "description": "A single line from a GenePop-style individual record. The expected format is \"individual_name, marker1 marker2 ...\", where the individual's name and the marker string are separated by a single comma. Tabs in the marker region are treated equivalently to spaces. Marker tokens are concatenated allele codes (no explicit separator between alleles) and may contain zeros to represent missing alleles. Typical allele encodings use either 2 or 3 digits per allele (or 4 digits total per locus for two 2-digit alleles). This string is parsed by splitting on the first comma into an individual name and a marker field, normalizing whitespace in the marker field, and then slicing each marker token into per-allele substrings according to the detected allele code length.", "default": ""}}, "required": ["line"], "type": "any"}}, "type": "function"}], "query": "We’re doing a robustness check of a GenePop ingest step where raw individual records come from mixed genotyping platforms and may contain padding artifacts. Given the following single-line GenePop individual records, identify the allele-code width per record from the genotype fields and apply a sieve before parsing: only parse records whose genotype fields (ignoring whitespace and the name prefix up to the comma) can be cleanly tokenized into locus strings where each locus is either an all-zero missing code or an even-length allele-code string consistent with a single per-allele width across that record. Treat tabs/spaces as equivalent separators, preserve the individual identifier exactly as it appears up to the comma (including internal underscores/spaces), and then extract the per-locus allele tuples with missing loci represented by all-zero codes.\n\nRecords:\n1) \"Salmo_17_A1, 0102\\t0000 120120 0310\"\n2) \"Indiv_001 , 101102 000000 205207 150150\"", "answers": "[{\"name\":\"Bio_PopGen_GenePop_get_indiv\",\"arguments\":{\"line\":\"Salmo_17_A1, 0102\\t0000 120120 0310\"}}]"}
{"func_name": "Bio_SCOP_Raf_normalize_letters", "func_desc": "Bio.SCOP.Raf.normalize_letters: Convert RAF one-letter amino acid codes into IUPAC standard codes.\n    \n    This function is part of the Biopython SCOP RAF utilities and is used when parsing or processing SCOP RAF-format residue annotations to ensure amino acid codes conform to the IUPAC one-letter standard expected by downstream Biopython code and common bioinformatics tools. The routine uppercases provided letter codes and maps the RAF-specific unknown/residue placeholder \".\" to the IUPAC convention \"X\".", "tools": [{"function": {"description": "Bio.SCOP.Raf.normalize_letters: Convert RAF one-letter amino acid codes into IUPAC standard codes.\n\nThis function is part of the Biopython SCOP RAF utilities and is used when parsing or processing SCOP RAF-format residue annotations to ensure amino acid codes conform to the IUPAC one-letter standard expected by downstream Biopython code and common bioinformatics tools. The routine uppercases provided letter codes and maps the RAF-specific unknown/residue placeholder \".\" to the IUPAC convention \"X\".", "name": "Bio_SCOP_Raf_normalize_letters", "parameters": {"properties": {"one_letter_code": {"type": "string", "description": "A one-letter amino acid code string as found in RAF-format data. In the SCOP/RAF domain this is typically a single-character string representing an amino acid or the RAF unknown marker \".\", e.g. \".\" or \"a\". The function compares the entire string to \".\"; if equal it returns the IUPAC unknown code \"X\". For any other string value it returns the same string with all characters converted to upper case. Callers should therefore provide a str (preferably a single-character code) that represents an amino acid or the RAF \".\" unknown marker.", "default": ""}}, "required": ["one_letter_code"], "type": "any"}}, "type": "function"}], "query": "In our SCOP RAF ingestion QC, we’re parsing a short residue-code stream extracted from mixed-quality annotation lines: ['.', 'g', '.', 'G', 'x', '-', ' ', 'a']. For downstream compatibility, run Bio.SCOP.Raf.normalize_letters only on tokens that are valid RAF residue-letter fields: a single alphabetic character (any case) or the RAF unknown-residue placeholder '.'. Ignore any token that is not exactly one character long or is not in that residue-field set. Preserve the original order of the tokens that qualify for normalization.", "answers": "[{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\".\"}},{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\"g\"}},{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\".\"}},{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\"G\"}},{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\"x\"}},{\"name\":\"Bio_SCOP_Raf_normalize_letters\",\"arguments\":{\"one_letter_code\":\"a\"}}]"}
{"func_name": "Bio_SearchIO__utils_fullcascade", "func_desc": "Bio.SearchIO._utils.fullcascade returns a Python property that provides a cascading getter and setter for a named attribute on SearchIO container items (for example HSP objects within a Hit or Query container). It is intended for use in Biopython SearchIO code to create container-level attributes that reflect the corresponding attribute on member items: reading the property retrieves the attribute from the first contained item, and writing the property sets that attribute on every contained item.", "tools": [{"function": {"description": "Bio.SearchIO._utils.fullcascade returns a Python property that provides a cascading getter and setter for a named attribute on SearchIO container items (for example HSP objects within a Hit or Query container). It is intended for use in Biopython SearchIO code to create container-level attributes that reflect the corresponding attribute on member items: reading the property retrieves the attribute from the first contained item, and writing the property sets that attribute on every contained item.\n", "name": "Bio_SearchIO__utils_fullcascade", "parameters": {"properties": {"attr": {"type": "string", "description": "The attribute name on the contained items (for example an HSP attribute) to be accessed and modified via the returned property. This string is used with getattr and setattr on items stored in the container (self._items[0] for reads, and iterating over self for writes). In the SearchIO domain, this is typically the name of a per-item attribute you want to expose at the container level (e.g. \"evalue\", \"score\"). The function does not validate that the attribute exists on items at creation time.", "default": ""}, "doc": {"type": "string", "description": "Optional documentation string to assign to the returned property object. Defaults to the empty string \"\". This becomes the property.__doc__ and should describe the purpose of the container-level attribute in the context of SearchIO containers.", "default": ""}}, "required": ["attr", "doc"], "type": "any"}}, "type": "function"}], "query": "We’re hardening a Biopython SearchIO QC workflow where Query/Hit containers may aggregate HSPs from heterogeneous backends, so member HSP objects don’t always use identical attribute names. Create container-level cascading properties only for scoring metrics that (a) are already present on member HSPs under any of these synonymous field names and (b) can be normalized to the canonical SearchIO attribute names.\n\nCanonicalization rules:\n- For bit scores: treat any HSP attribute named exactly `bitscore`, `bit_score`, or `bitScore` as the same metric; expose it at the container level as `bitscore`.\n- For expect values: treat any HSP attribute named exactly `evalue`, `e_value`, `expect`, or `exp` as the same metric; expose it at the container level as `evalue`.\n\nFor each canonical metric that passes the rule above, implement a container-level Python property via Bio.SearchIO._utils.fullcascade that reads from the first HSP (after canonicalization) and writes through to every HSP using the canonical attribute name. Use these exact docstrings:\n- bitscore: \"Cascade bitscore to/from all HSPs in this container.\"\n- evalue: \"Cascade HSP evalue to the container: get from the first HSP, set on all HSPs.\"", "answers": "[{\"name\":\"Bio_SearchIO__utils_fullcascade\",\"arguments\":{\"attr\":\"bitscore\",\"doc\":\"Cascade bitscore to/from all HSPs in this container.\"}},{\"name\":\"Bio_SearchIO__utils_fullcascade\",\"arguments\":{\"attr\":\"evalue\",\"doc\":\"Cascade HSP evalue to the container: get from the first HSP, set on all HSPs.\"}}]"}
{"func_name": "Bio_SearchIO__utils_removesuffix", "func_desc": "Bio.SearchIO._utils.removesuffix: Remove a trailing suffix from a string, providing a small compatibility wrapper for Python 3.8 used in Biopython's SearchIO utilities.\n    \n    This function is used in the Bio.SearchIO._utils module to normalize or clean textual identifiers, filenames, and other string tokens commonly encountered when parsing search results (for example BLAST, HMMER, or other sequence search output). In the Biopython project this helps ensure consistent handling of trailing suffixes across supported Python versions, by delegating to the built-in str.removesuffix on Python versions that provide it and falling back to an equivalent implementation on Python 3.8.", "tools": [{"function": {"description": "Bio.SearchIO._utils.removesuffix: Remove a trailing suffix from a string, providing a small compatibility wrapper for Python 3.8 used in Biopython's SearchIO utilities.\n\nThis function is used in the Bio.SearchIO._utils module to normalize or clean textual identifiers, filenames, and other string tokens commonly encountered when parsing search results (for example BLAST, HMMER, or other sequence search output). In the Biopython project this helps ensure consistent handling of trailing suffixes across supported Python versions, by delegating to the built-in str.removesuffix on Python versions that provide it and falling back to an equivalent implementation on Python 3.8.", "name": "Bio_SearchIO__utils_removesuffix", "parameters": {"properties": {"string": {"type": "string", "description": "The input text from which a trailing suffix may be removed. In the SearchIO parsing context this is typically an identifier, file base name, or other token produced by a parser. The function does not modify the original object (strings are immutable); it returns a new str when modification is required. Passing a non-str value is not supported and will result in a type-related exception.", "default": ""}, "suffix": {"type": "string", "description": "The suffix to remove from the end of string if present. In practical use this can be a file extension (for example \".txt\" or \".fa\"), a known trailing marker applied by a search program, or any other exact suffix to strip. An empty string is treated as \"no suffix\" and the original string is returned unchanged.", "default": ""}}, "required": ["string", "suffix"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed batch of SearchIO-derived tokens (filenames and query identifiers) from multiple instruments, and we need to normalize them into stable base identifiers prior to indexing. For each token, apply suffix stripping only when the token’s trailing segment matches an expected, format-specific trailer: (a) treat HMMER domtblout artifacts as those whose names end with the canonical domtblout extension and strip that extension; (b) treat BLAST XML reports as those whose names end with a single “.xml” trailer and strip only that final trailer (even if other dots remain in the stem); (c) treat database-style query identifiers as those that end with the literal trailer “|ref” and strip it only when it is truly the terminal suffix. Use this raw batch: [\"PF00069.domtblout\", \"PF00069.domtblout.gz\", \"sample_run01.blast.xml\", \"sample_run02.blast.XML\", \"sp|P04637|TP53_HUMAN|ref\", \"sp|P04637|TP53_HUMAN|ref|junk\"].", "answers": "[{\"name\":\"Bio_SearchIO__utils_removesuffix\",\"arguments\":{\"string\":\"PF00069.domtblout\",\"suffix\":\".domtblout\"}},{\"name\":\"Bio_SearchIO__utils_removesuffix\",\"arguments\":{\"string\":\"sample_run01.blast.xml\",\"suffix\":\".xml\"}},{\"name\":\"Bio_SearchIO__utils_removesuffix\",\"arguments\":{\"string\":\"sp|P04637|TP53_HUMAN|ref\",\"suffix\":\"|ref\"}}]"}
{"func_name": "Bio_SeqUtils_CheckSum_seguid", "func_desc": "Bio.SeqUtils.CheckSum.seguid returns the SEGUID (a Sequence Globally Unique IDentifier) for a biological sequence. The function computes a reproducible identifier for a nucleotide or amino-acid sequence (or any string/Seq-like object) by normalizing case, computing the SHA-1 digest of the normalized sequence bytes, base64-encoding the digest, and removing base64 padding and newlines to produce the final SEGUID string. This SEGUID is used in Biopython and computational molecular biology workflows to create compact, comparable identifiers for sequence deduplication, database indexing, or cross-referencing sequences in publications or tools (see http://bioinformatics.anl.gov/seguid/ and https://doi.org/10.1002/pmic.200600032).", "tools": [{"function": {"description": "Bio.SeqUtils.CheckSum.seguid returns the SEGUID (a Sequence Globally Unique IDentifier) for a biological sequence. The function computes a reproducible identifier for a nucleotide or amino-acid sequence (or any string/Seq-like object) by normalizing case, computing the SHA-1 digest of the normalized sequence bytes, base64-encoding the digest, and removing base64 padding and newlines to produce the final SEGUID string. This SEGUID is used in Biopython and computational molecular biology workflows to create compact, comparable identifiers for sequence deduplication, database indexing, or cross-referencing sequences in publications or tools (see http://bioinformatics.anl.gov/seguid/ and https://doi.org/10.1002/pmic.200600032).\n", "name": "Bio_SeqUtils_CheckSum_seguid", "parameters": {"properties": {"seq": {"type": "string", "description": "The input sequence to identify. In practice this is typically a Python string containing a nucleotide or amino-acid sequence (for example, \"ACGTACGT\" or \"MTEYK...\"). The implementation also accepts Biopython Seq objects (the code first attempts bytes(seq)) and, failing that, will call seq.encode() on a Python str. The function normalizes the sequence by converting it to uppercase before hashing, so letter case in the input does not affect the resulting SEGUID. Encoding of Python str objects uses the default str.encode() behavior (UTF-8 by default). If seq cannot be converted to bytes (for example, it is neither a str nor a bytes-convertible object), an exception (TypeError or AttributeError) will be raised.", "default": ""}}, "required": ["seq"], "type": "any"}}, "type": "function"}], "query": "We’re building a weekly cross-database protein index from a messy export where sequences arrive with mixed casing, occasional whitespace/newlines, and sometimes truncated fragments. Treat each raw entry as a candidate protein chain: normalize by stripping all whitespace and then apply SEGUID (which is case-insensitive). Only generate SEGUIDs for entries that look like plausible protein sequences after cleaning (i.e., contain only the 20 standard amino-acid letters A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y). For records that fail this plausibility screen, they should naturally drop out of the indexing step.\n\nRaw export entries:\n1) Full-length TP53 tumor suppressor amino-acid sequence exactly as provided (may contain line breaks as shown):\n\nMEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPLSQETFSDLWKLLPENNV\n\n2) FASTA peptide fragment from a vendor export (mixed case):\n\nmTeYkLvVvgAgGvGksALtiqliqnhfvdeydptiedsyRKQ\n\nReturn the SEGUIDs for the entries that pass the plausibility screen, for use as stable database keys.", "answers": "[{\"name\":\"Bio_SeqUtils_CheckSum_seguid\",\"arguments\":{\"seq\":\"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPLSQETFSDLWKLLPENNV\"}},{\"name\":\"Bio_SeqUtils_CheckSum_seguid\",\"arguments\":{\"seq\":\"mTeYkLvVvgAgGvGksALtiqliqnhfvdeydptiedsyRKQ\"}}]"}
{"func_name": "Bio_SeqUtils_GC_skew", "func_desc": "Calculate GC skew (G-C)/(G+C) for non-overlapping windows along a DNA sequence.\n    \n    This function computes a simple per-window measure of nucleotide composition asymmetry used in computational molecular biology (for example, profiling local GC bias across chromosomes, contigs, or sequencing reads). For each contiguous, non-overlapping window of the input sequence it counts guanine (G/g) and cytosine (C/c) and returns the ratio (G - C) / (G + C). The implementation is case-insensitive for G and C, ignores ambiguous nucleotides (they are treated as neither G nor C), and explicitly handles windows with no G or C by returning 0.0 for that window to avoid division-by-zero errors.", "tools": [{"function": {"description": "Calculate GC skew (G-C)/(G+C) for non-overlapping windows along a DNA sequence.\n\nThis function computes a simple per-window measure of nucleotide composition asymmetry used in computational molecular biology (for example, profiling local GC bias across chromosomes, contigs, or sequencing reads). For each contiguous, non-overlapping window of the input sequence it counts guanine (G/g) and cytosine (C/c) and returns the ratio (G - C) / (G + C). The implementation is case-insensitive for G and C, ignores ambiguous nucleotides (they are treated as neither G nor C), and explicitly handles windows with no G or C by returning 0.0 for that window to avoid division-by-zero errors.", "name": "Bio_SeqUtils_GC_skew", "parameters": {"properties": {"seq": {"type": "string", "description": "The DNA sequence to analyze. Must be a Python str containing nucleotide characters. The function counts only 'G' and 'C' in either uppercase or lowercase; all other characters (including ambiguous nucleotides such as 'N') are not counted toward G or C. In Biopython workflows this parameter is typically a whole-genome, contig, or read sequence supplied by SeqIO or other sequence-handling modules.", "default": ""}, "window": {"type": "integer", "description": "The window size in nucleotides to use for each non-overlapping segment (default 100). The code iterates over seq in steps of this size producing contiguous, non-overlapping windows; the final window may be shorter if len(seq) is not a multiple of window. window is expected to be a positive integer; passing window == 0 will raise a ValueError from range(), and a negative window will result in no windows being produced (an empty result list). The default value of 100 provides a moderate resolution commonly used in exploratory genome-skew analyses but can be adjusted to increase or decrease spatial resolution.", "default": 100}}, "required": ["seq", "window"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed bacterial contig cohort prior to compositional-bias benchmarking. For each candidate DNA sequence below, first assess whether it has enough GC-bearing signal to yield meaningful skew profiles: compute the fraction of ambiguous bases (anything other than A/T/G/C, e.g., N) across the full sequence and only run GC-skew profiling on contigs where the ambiguous fraction is ≤ 25%. Use contiguous, non-overlapping windows. Set the window length per contig using an intrinsic rule: if the contig length is an exact multiple of 10 nt, use 10-nt windows; otherwise use 6-nt windows. Compute GC skew as (G−C)/(G+C), treat G/g and C/c case-insensitively, ignore ambiguous bases as neither G nor C, and for any window with zero total (G+C) report 0.0. Candidate sequences: (i) `ATGCGCGTNNNNNcccgGGTTACGCGCATTGCCGTAAGC`; (ii) `ATGCGCGATNNNNGCATCGCGCGTTAGCGCGCGAATTCGCGC`; (iii) `ACGCGGTTNNccggAAATTTGCC`.", "answers": "[{\"name\":\"Bio_SeqUtils_GC_skew\",\"arguments\":{\"seq\":\"ATGCGCGATNNNNGCATCGCGCGTTAGCGCGCGAATTCGCGC\",\"window\":6}},{\"name\":\"Bio_SeqUtils_GC_skew\",\"arguments\":{\"seq\":\"ACGCGGTTNNccggAAATTTGCC\",\"window\":6}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_Tm_GC", "func_desc": "Return the estimated melting temperature (Tm) in degrees Celsius for a DNA/RNA primer or oligonucleotide using empirical formulas based on percent GC content and optional salt corrections. This function implements a family of simple, commonly cited Tm approximations (see Marmur & Doty, Wetmur, Primer3Plus, von Ahsen, and QuikChange variants) and is used in Biopython for quick Tm estimates when designing primers, checking primer properties, or as a component of higher-level primer design pipelines described in the Biopython documentation.", "tools": [{"function": {"description": "Return the estimated melting temperature (Tm) in degrees Celsius for a DNA/RNA primer or oligonucleotide using empirical formulas based on percent GC content and optional salt corrections. This function implements a family of simple, commonly cited Tm approximations (see Marmur & Doty, Wetmur, Primer3Plus, von Ahsen, and QuikChange variants) and is used in Biopython for quick Tm estimates when designing primers, checking primer properties, or as a component of higher-level primer design pipelines described in the Biopython documentation.\n", "name": "Bio_SeqUtils_MeltingTemp_Tm_GC", "parameters": {"properties": {"seq": {"type": "string", "description": "Nucleotide sequence for which to calculate Tm. The sequence is converted to str() on entry. Ambiguous bases are allowed but handled specially: gc_fraction is computed with the \"weighted\" option (so e.g. \"X\" counts as 0.5 GC for gc_fraction), and if mismatch is True each \"X\" is treated as an actual mismatch and reduces the calculated %GC and final Tm by the corresponding fraction. Sequence length (len(seq)) is used as N in the empirical formulas and in mismatch percentage calculations. The function may call the internal _check(seq, \"Tm_GC\") when check is True to validate/standardize the sequence (see note on check below).", "default": ""}, "check": {"type": "boolean", "description": "If True (default) perform an internal validity check and possible normalization of seq by calling _check(seq, \"Tm_GC\"). This step is intended to catch or normalize invalid characters early (see Biopython SeqUtils internal checks). If False the input is used as-is after conversion to str().", "default": true}, "strict": {"type": "boolean", "description": "If True (default) raise ValueError when the sequence contains any of the ambiguous bases \"K\", \"M\", \"N\", \"R\", \"Y\", \"B\", \"V\", \"D\", \"H\" (exactly those letters). This enforces unambiguous base usage for calculations where ambiguity is not acceptable. If strict is False these letters are permitted and treated according to the weighted gc_fraction routine.", "default": true}, "valueset": {"type": "integer", "description": "Integer selector (default 7) choosing one of several published empirical Tm formula variants. The implementation provides variants 1–8 (see below). The allowed range is 0–8; valueset > 8 raises ValueError. If userset is provided it overrides valueset. Default valueset 7 corresponds to the Primer3Plus style formula used for product Tm.", "default": 7}, "userset": {"type": "any", "nullable": true, "description": "If provided, a tuple of four numeric values (A, B, C, D) overriding the preset values for the empirical formula Tm = A + B*(%GC) - C/N + salt_correction - D*(%mismatch). Userset must be a tuple with exactly four elements; these values are used directly as the constants in the formula. If userset is supplied it takes precedence over valueset. Incorrect length or non-iterable will result in the usual Python unpacking/type error at runtime.", "default": null}, "Na": {"type": "float", "description": "Sodium ion concentration in mM (default 50). Used directly for salt correction when saltcorr specifies a method that uses [Na+]. If any of K, Tris, Mg or dNTPs are non-zero the function will compute a sodium-equivalent concentration (von Ahsen et al., 2001) and use that value for salt correction according to the selected method.", "default": 50}, "K": {"type": "float", "description": "Potassium ion concentration in mM (default 0). If non-zero it is included when computing a sodium-equivalent concentration for salt correction (see Na).", "default": 0}, "Tris": {"type": "float", "description": "Tris buffer concentration in mM (default 0). If non-zero it contributes to the sodium-equivalent concentration used for salt correction.", "default": 0}, "Mg": {"type": "float", "description": "Magnesium ion concentration in mM (default 0). If non-zero it contributes to the sodium-equivalent concentration used for salt correction; the salt correction routine may account for Mg-dNTP interactions when dNTPs is provided.", "default": 0}, "dNTPs": {"type": "float", "description": "Total deoxynucleotide triphosphates concentration in mM (default 0). If non-zero it is used together with Mg to compute effective free Mg2+ (per von Ahsen) when performing salt correction.", "default": 0}, "saltcorr": {"type": "integer", "description": "Integer code selecting the type of salt correction to apply (default 0). saltcorr == 0 or None means no salt correction is applied. Positive integers select among salt correction methods implemented by salt_correction; note that saltcorr == 5 is explicitly not applicable to Tm_GC and will raise ValueError. If saltcorr is non-zero the function will call salt_correction(Na=Na, K=K, Tris=Tris, Mg=Mg, dNTPs=dNTPs, seq=seq, method=saltcorr) and add the returned correction (in degrees Celsius) to the base empirical Tm.", "default": 0}, "mismatch": {"type": "boolean", "description": "If True (default) every 'X' in seq is treated as a mismatch: first gc_fraction is computed with the \"weighted\" option (where X counts as 0.5 GC) and then percent_gc is reduced by seq.count(\"X\") * 50.0 / len(seq) before applying the formula; finally the Tm is decreased by D * (seq.count(\"X\") * 100.0 / len(seq)) to account for the percent mismatch. If mismatch is False the function does not apply the explicit mismatch penalty D*(%mismatch) and leaves the weighted gc_fraction value as-is.", "default": true}}, "required": ["seq", "K", "Tris", "valueset", "Na", "Mg", "userset", "saltcorr", "mismatch", "dNTPs", "check", "strict"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a small PCR primer panel from the optimization cohort where the wet-lab buffer recipe is recorded inconsistently and some candidates include IUPAC ambiguity. Use the same quick GC%-based Tm estimator as in our pipeline (Primer3Plus-style approximation; valueset 7) with salt correction method 1 and sequence validation enabled. Treat any ambiguous base as a mismatch. Process only candidates that are exactly 20 nt long after stripping whitespace and converting to uppercase. Apply a buffer harmonization rule before calculating Tm: if Tris is missing, set Tris to 10 mM; if Tris is provided and is at least 20 mM, keep it as recorded; otherwise keep the recorded Tris value. Compute Tm (°C) for the following raw records:\n1) id=COH_A, seq=' ATGCGTACGTTGACCTGCAA ', Na=50 mM, K=10 mM, Tris missing, Mg=1.5 mM, dNTPs=0.2 mM\n2) id=COH_B, seq='ATGCCGTTGACCTGATCGTA', Na=50 mM, K=10 mM, Tris=20 mM, Mg=1.5 mM, dNTPs=0.2 mM\n3) id=COH_C, seq='ATGCXGTTGACCTGATCGTA', Na=50 mM, K=10 mM, Tris=10 mM, Mg=1.5 mM, dNTPs=0.2 mM", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_GC\",\"arguments\":{\"seq\":\"ATGCGTACGTTGACCTGCAA\",\"check\":true,\"strict\":true,\"valueset\":7,\"userset\":null,\"Na\":50,\"K\":10,\"Tris\":10,\"Mg\":1.5,\"dNTPs\":0.2,\"saltcorr\":1,\"mismatch\":true}},{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_GC\",\"arguments\":{\"seq\":\"ATGCCGTTGACCTGATCGTA\",\"check\":true,\"strict\":true,\"valueset\":7,\"userset\":null,\"Na\":50,\"K\":10,\"Tris\":20,\"Mg\":1.5,\"dNTPs\":0.2,\"saltcorr\":1,\"mismatch\":true}},{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_GC\",\"arguments\":{\"seq\":\"ATGCXGTTGACCTGATCGTA\",\"check\":true,\"strict\":true,\"valueset\":7,\"userset\":null,\"Na\":50,\"K\":10,\"Tris\":10,\"Mg\":1.5,\"dNTPs\":0.2,\"saltcorr\":1,\"mismatch\":true}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_Tm_NN", "func_desc": "Bio.SeqUtils.MeltingTemp.Tm_NN: Calculate the melting temperature (Tm) of a DNA/DNA, RNA/RNA or RNA/DNA duplex using nearest-neighbor (NN) thermodynamic parameters and optional corrections for mismatches, terminal mismatches, dangling ends and salt effects. This function is used in primer and probe design and validation workflows (e.g., PCR primer Tm estimation, oligo/oligo hybridization, RNA/DNA hybridization) by summing enthalpy (ΔH) and entropy (ΔS) contributions from initiation terms, nearest-neighbor stacks, internal mismatches, terminal mismatches and dangling ends, converting concentrations to the equilibrium constant k, applying salt corrections, and returning the predicted Tm in degrees Celsius.", "tools": [{"function": {"description": "Bio.SeqUtils.MeltingTemp.Tm_NN: Calculate the melting temperature (Tm) of a DNA/DNA, RNA/RNA or RNA/DNA duplex using nearest-neighbor (NN) thermodynamic parameters and optional corrections for mismatches, terminal mismatches, dangling ends and salt effects. This function is used in primer and probe design and validation workflows (e.g., PCR primer Tm estimation, oligo/oligo hybridization, RNA/DNA hybridization) by summing enthalpy (ΔH) and entropy (ΔS) contributions from initiation terms, nearest-neighbor stacks, internal mismatches, terminal mismatches and dangling ends, converting concentrations to the equilibrium constant k, applying salt corrections, and returning the predicted Tm in degrees Celsius.\n", "name": "Bio_SeqUtils_MeltingTemp_Tm_NN", "parameters": {"properties": {"seq": {"type": "string", "description": "The primer or probe sequence as a string. For RNA/DNA hybridizations this must be the RNA sequence. The sequence is cast to str internally. If check is True, the sequence is validated by the internal _check() routine (e.g., allowed nucleotide codes); pass check=False to skip validation and any automatic normalization.", "default": ""}, "check": {"type": "boolean", "description": "If True (default), validate seq and c_seq using the module's _check() function before calculation. Validation ensures the sequences conform to expected nucleotide symbols/format used by the nearest-neighbor tables; disabling validation may be useful for pre-validated inputs but risks undefined behavior if invalid characters are present.", "default": true}, "strict": {"type": "boolean", "description": "If True (default), treat missing thermodynamic keys (e.g., an unexpected neighbor pattern, dangling-end key or mismatch key not present in the provided tables) as an error via the internal _key_error() handling, which stops the calculation. If False, missing keys are handled more permissively (the function will attempt to continue and typically issue a warning rather than raising an exception), allowing approximate results when tables are incomplete.", "default": true}, "c_seq": {"type": "string", "nullable": true, "description": "Complementary/template sequence in 3'->5' orientation relative to seq (i.e., the target sequence to which the primer/probe anneals). Default=None, in which case the function computes the perfect Watson–Crick complement of seq using Bio.Seq.Seq(seq).complement(). Providing c_seq is required if you want explicit control for mismatch corrections or dangling-end corrections; when mismatches or dangling ends are present and c_seq is given, the appropriate corrections are applied automatically.", "default": null}, "shift": {"type": "integer", "description": "Integer shift of seq relative to c_seq to indicate alignment offsets and create dangling ends (default=0). Positive shift inserts leading gaps on seq, negative shift inserts leading gaps on c_seq. The function aligns seq and c_seq using shift, pads with dot placeholders (\".\") where needed, trims over-dangling positions, and applies dangling-end thermodynamic corrections for single-base overhangs on either end when present.", "default": 0}, "nn_table": {"type": "any", "nullable": true, "description": "Nearest-neighbor thermodynamic table mapping neighbor keys (e.g., \"AA/TT\") and initiation/symmetry keys to [ΔH, ΔS] values. If None, defaults to DNA_NN3 (Allawi & SantaLucia, 1997) for DNA/DNA hybridizations. Users may pass a custom table constructed with the module's maketable method to use alternative published parameter sets (e.g., Breslauer, Sugimoto, SantaLucia) or RNA-specific tables for RNA/RNA or RNA/DNA hybridizations.", "default": null}, "tmm_table": {"type": "any", "nullable": true, "description": "Thermodynamic values for terminal mismatches mapping terminal-mismatch keys to [ΔH, ΔS]. Default is DNA_TMM1 (SantaLucia & Peyret, 2001). Terminal mismatch terms are applied to the 5'/3' ends when detected after aligning and trimming dangling ends.", "default": null}, "imm_table": {"type": "any", "nullable": true, "description": "Thermodynamic values for internal mismatches (including inosine mismatches where provided), mapping internal-mismatch keys to [ΔH, ΔS]. Default is DNA_IMM1 (Allawi & SantaLucia and others). Internal mismatches are consulted during the nearest-neighbor \"zipping\" step and take precedence over the standard nn_table when present.", "default": null}, "de_table": {"type": "any", "nullable": true, "description": "Thermodynamic values for dangling ends mapping dangling-end keys to [ΔH, ΔS]. Defaults to DNA_DE1 (Bommarito et al., 2000) for DNA; RNA_DE1 (Turner & Mathews, 2010) may be used for RNA. Dangling-end corrections are applied to single-base overhangs at duplex termini identified via shift or length differences and require accurate c_seq to be meaningful.", "default": null}, "dnac1": {"type": "float", "description": "Concentration of the higher-concentration strand in nanomolar (nM). Typically this is the primer or probe concentration in hybridization/PCR experiments. Default=25. This value is used to compute the effective duplex concentration k (see selfcomp).", "default": 25}, "dnac2": {"type": "float", "description": "Concentration of the lower-concentration strand in nanomolar (nM). In PCR applications this often represents the template concentration and may be set to 0 if negligible; in symmetric oligo/oligo hybridizations set equal to dnac1. Default=25. Note: some external tools (e.g., Primer3Plus) use different conventions; to mimic Primer3Plus total-oligo behaviors divide total oligo concentration accordingly and assign values to dnac1 and dnac2 as described in the module documentation.", "default": 25}, "selfcomp": {"type": "boolean", "description": "Whether seq is self-complementary (default=False). If True, the function treats the duplex as self-complementary: the effective concentration for the equilibrium constant k is set to dnac1 (converted to M), and the symmetry correction term nn_table[\"sym\"] (ΔH and ΔS) is added to the totals. For non-self-complementary duplexes k is computed as (dnac1 - dnac2/2) converted from nM to M.", "default": false}, "Na": {"type": "float", "description": "Sodium ion concentration used by the salt correction routine; default=50. See the Tm_GC and salt_correction routines referenced in the module for details about units and the influence of different ionic species. Na, K, Tris, Mg and dNTPs are passed to the salt_correction() method to compute a correction term based on the requested salt correction method (saltcorr).", "default": 50}, "K": {"type": "float", "description": "Potassium ion concentration used by the salt correction routine; default=0.", "default": 0}, "Tris": {"type": "float", "description": "Tris buffer concentration used by the salt correction routine; default=0.", "default": 0}, "Mg": {"type": "float", "description": "Magnesium ion concentration used by the salt correction routine; default=0.", "default": 0}, "dNTPs": {"type": "float", "description": "dNTP concentration used by the salt correction routine; default=0.", "default": 0}, "saltcorr": {"type": "integer", "description": "Integer code selecting the salt-correction method (default=5). Behavior implemented in this function:\n0: no salt correction performed (salt_correction() is not called).\n1,2,3,4: call salt_correction(..., method=saltcorr) and add the returned correction value directly (in degrees C) to the computed melting temperature.\n5: call salt_correction(..., method=5) and add the returned correction (an entropy correction) to the cumulative ΔS before computing Tm (this is the module's default).\n6,7: call salt_correction(..., method=saltcorr) and apply the correction via the reciprocal-temperature formula Tm = 1/(1/(Tm+273.15) + corr) - 273.15.", "default": 5}}, "required": ["seq", "K", "nn_table", "Tris", "selfcomp", "Na", "Mg", "saltcorr", "shift", "tmm_table", "de_table", "dnac2", "dNTPs", "check", "imm_table", "c_seq", "dnac1", "strict"], "type": "any"}}, "type": "function"}], "query": "We’re re-running NN-based duplex stability QC on a mixed primer/probe cohort where sample prep notes indicate heterogeneous chemistries and concentration regimes. Use default NN thermodynamic tables with the standard salt correction method. Enable sequence validation and strict parameter checking throughout; do not provide custom complement strands and do not apply shifts; assume none of the oligos are self-complementary.\n\nRaw replicate records (as received from the LIMS):\n\n1) “DNA PCR primer” sequence: AGTCCGATGCTAGGCTTACG; buffer: 50 mM Na+, 1.5 mM Mg2+, 0.2 mM dNTPs; strand concentrations: primer 200 nM vs template 5 nM.\n\n2) “Candidate qPCR primer” sequence: ATGCGTACCTGAACTG; buffer: 50 mM Na+, 3 mM Mg2+, 0.2 mM dNTPs; strand concentrations: primer 500 nM vs template 5 nM.\n\n3) “RNA oligo for RNA/DNA hybridization” sequence: AUGCGAUCGGAAUCUG; buffer: 150 mM Na+ and 1.5 mM Mg2+; dNTPs 0.2 mM; strand concentrations: 100 nM each.\n\n4) “20-mer DNA PCR primer” sequence: ACGTTGACCTGATCGTACGA; buffer: 50 mM Na+, 1.5 mM Mg2+, 0.2 mM dNTPs; strand concentrations: primer 500 nM with ‘negligible template’ (recorded as 0 nM).\n\nQC rule for this run: compute Tm only for duplex assays where the reported strand concentrations imply a physically meaningful duplex equilibrium input (both strands present at non-zero concentration) and where the oligo alphabet is nucleic-acid valid under strict checking. Return the predicted Tm (°C) for each replicate that passes this rule.", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_NN\",\"arguments\":{\"seq\":\"AGTCCGATGCTAGGCTTACG\",\"check\":true,\"strict\":true,\"c_seq\":null,\"shift\":0,\"nn_table\":null,\"tmm_table\":null,\"imm_table\":null,\"de_table\":null,\"dnac1\":200.0,\"dnac2\":5.0,\"selfcomp\":false,\"Na\":50.0,\"K\":0.0,\"Tris\":0.0,\"Mg\":1.5,\"dNTPs\":0.2,\"saltcorr\":5}},{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_NN\",\"arguments\":{\"seq\":\"ATGCGTACCTGAACTG\",\"check\":true,\"strict\":true,\"c_seq\":null,\"shift\":0,\"nn_table\":null,\"tmm_table\":null,\"imm_table\":null,\"de_table\":null,\"dnac1\":500.0,\"dnac2\":5.0,\"selfcomp\":false,\"Na\":50.0,\"K\":0.0,\"Tris\":0.0,\"Mg\":3.0,\"dNTPs\":0.2,\"saltcorr\":5}},{\"name\":\"Bio_SeqUtils_MeltingTemp_Tm_NN\",\"arguments\":{\"seq\":\"AUGCGAUCGGAAUCUG\",\"check\":true,\"strict\":true,\"c_seq\":null,\"shift\":0,\"nn_table\":null,\"tmm_table\":null,\"imm_table\":null,\"de_table\":null,\"dnac1\":100.0,\"dnac2\":100.0,\"selfcomp\":false,\"Na\":150.0,\"K\":0.0,\"Tris\":0.0,\"Mg\":1.5,\"dNTPs\":0.2,\"saltcorr\":5}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_make_table", "func_desc": "Return a dictionary table of thermodynamic parameters used by DNA melting\n    temperature calculations.\n    \n    This function is used within the Bio.SeqUtils.MeltingTemp module to build or\n    customize the lookup table of nearest-neighbor and initiation thermodynamic\n    parameters (commonly enthalpy and entropy pairs) employed by melting\n    temperature routines. If no existing table is provided, a default table is\n    constructed with a set of standard parameter names initialized to (0, 0).\n    A user can supply a pre-existing table and/or a dictionary of new or updated\n    values to modify that table. This is intended for practical use cases such as\n    replacing initiation parameters from one published dataset (for example,\n    Sugimoto '96, stored in DNA_NN2) with values from another dataset (for\n    example, Allawi & SantaLucia '97), as shown in the original example usage.", "tools": [{"function": {"description": "Return a dictionary table of thermodynamic parameters used by DNA melting\ntemperature calculations.\n\nThis function is used within the Bio.SeqUtils.MeltingTemp module to build or\ncustomize the lookup table of nearest-neighbor and initiation thermodynamic\nparameters (commonly enthalpy and entropy pairs) employed by melting\ntemperature routines. If no existing table is provided, a default table is\nconstructed with a set of standard parameter names initialized to (0, 0).\nA user can supply a pre-existing table and/or a dictionary of new or updated\nvalues to modify that table. This is intended for practical use cases such as\nreplacing initiation parameters from one published dataset (for example,\nSugimoto '96, stored in DNA_NN2) with values from another dataset (for\nexample, Allawi & SantaLucia '97), as shown in the original example usage.", "name": "Bio_SeqUtils_MeltingTemp_make_table", "parameters": {"properties": {"oldtable": {"type": "any", "nullable": true, "description": "An existing dictionary of thermodynamic parameters to use\nas the starting point. The keys are parameter name strings (for\nexample 'AA/TT', 'init_A/T', 'sym') and the values are two-number\ntuples (commonly interpreted as enthalpy and entropy). If None, the\nfunction builds and returns a fresh default table. The default table\ncreated when oldtable is None contains these keys initialized to\n(0, 0): \"init\", \"init_A/T\", \"init_G/C\", \"init_oneG/C\",\n\"init_allA/T\", \"init_5T/A\", \"sym\", \"AA/TT\", \"AT/TA\", \"TA/AT\",\n\"CA/GT\", \"GT/CA\", \"CT/GA\", \"GA/CT\", \"CG/GC\", \"GC/CG\", \"GG/CC\".\nThe function makes a shallow copy of oldtable (via oldtable.copy())\nbefore applying any updates, so passing a mutable mapping will not\nbe mutated by this function (the returned object is a separate\ndictionary). If oldtable is not a mapping with a copy() method, a\nruntime error may be raised when attempting to copy it.", "default": null}, "values": {"type": "any", "nullable": true, "description": "A dictionary of parameter updates to apply to the table.\nKeys should be parameter name strings matching those used by the\nMeltingTemp routines (for example 'init_A/T'). Values should be\ntwo-number tuples consistent with the table's value format (for\nexample (enthalpy, entropy) as floats or ints). If values is None or\nempty, no updates are applied. The function applies updates using\nthe standard dict.update(values) operation; it does not validate keys\nor the numeric nature of tuple elements. If values is not a mapping\nwith an update-compatible interface, a runtime error may be raised.\nNote that because the function only checks truthiness before calling\nupdate (if values: table.update(values)), an empty dict will result\nin no update call but yields the same final table as updating with an\nempty dict would.", "default": null}}, "required": ["oldtable", "values"], "type": "any"}}, "type": "function"}], "query": "We’re standardizing thermodynamic parameter tables for three assay cohorts (A/B/C) from messy lab note exports. For each cohort, build a lookup table using the rule: only incorporate parameter entries that (i) are keyed by canonical DNA NN/initiation/symmetry labels (e.g., init, init_A/T, init_G/C, sym, and dinucleotide-style keys like AA/TT), and (ii) have numeric enthalpy/entropy pairs where both values are finite and entropy is non-positive (physically plausible sign convention for these notes). Any entries that fail these criteria should be ignored rather than forcing them into the table.\n\nCohort A (baseline): start from a fresh default table and apply the cleaned overrides from this raw note block: {\"init_A/T\": (-2.3, -4.1), \"init_G/C\": (-0.1, -2.8), \"sym\": (0.0, -1.4), \"AA/TT\": (-7.9, -22.2), \"comment\": \"hand-entered\", \"init_C/G\": (-0.1, +2.8)}.\n\nCohort B (model-refinement): start from this existing in-memory table: {\"init\": (0.0, 0.0), \"init_A/T\": (-7.5, -20.0), \"init_G/C\": (-8.2, -22.2), \"AA/TT\": (-7.2, -21.0), \"AT/TA\": (-7.2, -20.4)}. Update it using this raw patch list: {\"init_A/T\": (-7.9, -22.2), \"init_G/C\": (-8.0, -19.9), \"AA/TT\": (-7.6, -21.3), \"AT/TA\": \"recheck\", \"sym\": (0.0, -1.4)}.\n\nCohort C (cross-validation): start from a fresh default table and apply initiation/symmetry terms from this export, again only keeping physically plausible numeric pairs: {\"init_A/T\": (-2.3, -4.1), \"init_G/C\": (-0.1, -2.8), \"init\": (0.2, -5.7), \"sym\": (0.0, -1.4), \"run_id\": 4412, \"AA/TT\": (-7.9, -22.2), \"init\": (0.2, -5.7)}.\n\nGenerate the three resulting tables.", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_make_table\",\"arguments\":{\"oldtable\":null,\"values\":{\"init_A/T\":[-2.3,-4.1],\"init_G/C\":[-0.1,-2.8],\"sym\":[0.0,-1.4],\"AA/TT\":[-7.9,-22.2]}}},{\"name\":\"Bio_SeqUtils_MeltingTemp_make_table\",\"arguments\":{\"oldtable\":{\"init\":[0.0,0.0],\"init_A/T\":[-7.5,-20.0],\"init_G/C\":[-8.2,-22.2],\"AA/TT\":[-7.2,-21.0],\"AT/TA\":[-7.2,-20.4]},\"values\":{\"init_A/T\":[-7.9,-22.2],\"init_G/C\":[-8.0,-19.9],\"AA/TT\":[-7.6,-21.3],\"sym\":[0.0,-1.4]}}},{\"name\":\"Bio_SeqUtils_MeltingTemp_make_table\",\"arguments\":{\"oldtable\":null,\"values\":{\"init_A/T\":[-2.3,-4.1],\"init_G/C\":[-0.1,-2.8],\"init\":[0.2,-5.7],\"sym\":[0.0,-1.4],\"AA/TT\":[-7.9,-22.2]}}}]"}
{"func_name": "Bio_SeqUtils_MeltingTemp_salt_correction", "func_desc": "Calculate a term to correct nucleic acid melting temperature (Tm) or entropy\n    for the ionic environment. This function computes a scalar correction term\n    based on supplied millimolar concentrations of common ions (Na+, K+, Tris,\n    Mg2+) and dNTPs, and on a selected empirical method (1-7) drawn from the\n    literature (Schildkraut & Lifson 1965; Wetmur 1991; SantaLucia 1996/1998;\n    Owczarzy 2004/2008, and von Ahsen 2001 for Na-equivalent). The computed\n    correction is intended to be applied to a previously calculated Tm or to\n    deltaS according to the method semantics described below; the function does\n    not itself compute Tm or deltaS, only the ionic correction term.", "tools": [{"function": {"description": "Calculate a term to correct nucleic acid melting temperature (Tm) or entropy\nfor the ionic environment. This function computes a scalar correction term\nbased on supplied millimolar concentrations of common ions (Na+, K+, Tris,\nMg2+) and dNTPs, and on a selected empirical method (1-7) drawn from the\nliterature (Schildkraut & Lifson 1965; Wetmur 1991; SantaLucia 1996/1998;\nOwczarzy 2004/2008, and von Ahsen 2001 for Na-equivalent). The computed\ncorrection is intended to be applied to a previously calculated Tm or to\ndeltaS according to the method semantics described below; the function does\nnot itself compute Tm or deltaS, only the ionic correction term.", "name": "Bio_SeqUtils_MeltingTemp_salt_correction", "parameters": {"properties": {"Na": {"type": "float", "description": "Millimolar concentration of sodium ions [Na+]. This is the\nprimary input for a simple salt correction: pass only Na to apply a\nsodium-only correction. Units are millimolar (mM). Typical practical\nuse: buffer [Na+] expressed in mM derived from experimental conditions.", "default": 0}, "K": {"type": "float", "description": "Millimolar concentration of potassium ions [K+]. When non-zero,\nK contributes to a sodium-equivalent concentration according to the\nvon Ahsen et al. (2001) prescription used here: it is added directly\n(in mM) to the effective monovalent contribution. Defaults to 0.0 mM.", "default": 0}, "Tris": {"type": "float", "description": "Millimolar concentration of Tris base (Tris buffer). In the\nsodium-equivalent calculation Tris contributes half its molar amount\n(Tris/2) because Tris is a weak base that partially provides monovalent\ncations. Units are millimolar (mM). Defaults to 0.0 mM.", "default": 0}, "Mg": {"type": "float", "description": "Millimolar concentration of magnesium ions [Mg2+]. Mg2+ is a\ndivalent cation and affects salt correction differently: it is used\nto compute a sodium-equivalent term 120*sqrt([Mg2+] - [dNTPs]) (mM)\nwhen appropriate (see behavior). Mg is also converted internally to\nmolar units (Mg * 1e-3) for formulae that require molar concentrations.\nDefaults to 0.0 mM.", "default": 0}, "dNTPs": {"type": "float", "description": "Millimolar total concentration of dNTPs (deoxynucleotide\ntriphosphates). dNTPs strongly bind Mg2+ and therefore reduce free\nMg2+. When dNTPs >= Mg the code treats free Mg2+ as negligible in the\nsodium-equivalent calculation. Units are millimolar (mM). Defaults to\n0.0 mM.", "default": 0}, "method": {"type": "integer", "description": "Integer selecting the empirical correction method to apply.\nValid values are 0 and 1-7. Default is 1. Semantic effect of the\nreturned correction depends on method:\n- method == 0: No correction; the function returns 0.0 (useful to\n  disable salt correction programmatically).\n- methods 1-4: The returned correction corr is intended to be added to\n  an existing Tm value: Tm(new) = Tm(old) + corr. These methods use\n  simple log([Na+]) or modified log formulae (units consistent with\n  Tm scales used by the cited authors).\n- method 5: The returned correction is an entropy correction (deltaS).\n  It should be added to an existing deltaS: deltaS(new) = deltaS(old)\n  + corr. Formula is proportional to (N-1)*ln[Na+] where N is sequence\n  length.\n- methods 6 and 7: The returned correction is for the reciprocal of\n  Tm; the corrected Tm is computed as Tm(new) = 1/(1/Tm(old) + corr).\nDetailed definitions and literature sources:\n1: 16.6 * log10([Na+]) (Schildkraut & Lifson 1965)\n2: 16.6 * log10([Na+]/(1 + 0.7*[Na+])) (Wetmur 1991)\n3: 12.5 * log10([Na+]) (SantaLucia et al. 1996)\n4: 11.7 * log10([Na+]) (SantaLucia 1998)\n5: 0.368 * (N - 1) * ln([Na+]) (SantaLucia 1998) — entropy correction\n6: (4.29*%GC - 3.95)*1e-5*ln([Na+]) + 9.40e-6*ln([Na+])^2\n   (Owczarzy et al. 2004) — correction applied to 1/Tm\n7: Complex empirical formula with decision tree and empirical\n   constants, includes explicit Mg2+ correction for dNTP binding and\n   additional regimes based on the ratio sqrt([Mg2+])/[Na_eq]\n   (Owczarzy et al. 2008). Note: method 7 applies a specific Mg2+\n   treatment and a dissociation constant for Mg:dNTP interactions.\nUse this parameter to select which empirical model best matches your\nexperimental protocol or literature source.", "default": 1}, "seq": {"type": "string", "nullable": true, "description": "DNA sequence string used only for methods that require\nsequence-dependent quantities: methods 5, 6 and 7. For method 5 the\nsequence length (len(seq)) is used; for methods 6 and 7 the GC\nfraction (percentage GC) is used via SeqUtils.gc_fraction(seq,\n\"ignore\"). If seq is required by the selected method but is None or\nempty, a ValueError is raised. The function does not validate or\ncanonicalize sequence letters beyond what SeqUtils.gc_fraction\nperforms; pass the nucleotide string you used when computing Tm or\ndeltaS.", "default": null}}, "required": ["K", "method", "Tris", "Na", "Mg", "dNTPs", "seq"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed PCR primer panel where some records are usable and others contain synthesis/annotation artifacts. Use the Owczarzy 2008 mixed-salt correction (method 7) to compute the ionic correction term for each cohort that passes QC. QC/branching rules: (i) only compute corrections for primer sequences composed exclusively of canonical DNA bases (A/C/G/T) and with length between 12 and 30 nt inclusive; (ii) if a cohort’s total monovalent contribution (Na + K + Tris, in mM) is at least 100 mM, treat Tris as part of the monovalent pool as recorded; otherwise, set Tris to 0 mM for the correction step (to mimic low-buffer uncertainty). Apply these rules to the following raw cohort sheet and output the per-cohort scalar salt-correction term using method 7: (1) primer ATGCGTACGTTAGC with 50.0 mM Na+, 10.0 mM K+, 20.0 mM Tris, 2.5 mM Mg2+, 0.8 mM total dNTPs; (2) primer AGCTGCGTACGTTAGC with 50.0 mM Na+, 0.0 mM K+, 0.0 mM Tris, 1.5 mM Mg2+, 0.2 mM total dNTPs; (3) primer ACGTTGACCTGACCGTTAGCTA with 50.0 mM Na+, 70.0 mM K+, 20.0 mM Tris, 2.5 mM Mg2+, 0.2 mM total dNTPs; (4) primer ATGCNTACGTTAGC with 50.0 mM Na+, 10.0 mM K+, 20.0 mM Tris, 2.5 mM Mg2+, 0.8 mM total dNTPs (suspected basecall ambiguity); (5) primer ACGTACGTACGTA with 50.0 mM Na+, 5.0 mM K+, 5.0 mM Tris, 2.0 mM Mg2+, 0.4 mM total dNTPs.", "answers": "[{\"name\":\"Bio_SeqUtils_MeltingTemp_salt_correction\",\"arguments\":{\"Na\":50.0,\"K\":10.0,\"Tris\":0.0,\"Mg\":2.5,\"dNTPs\":0.8,\"method\":7,\"seq\":\"ATGCGTACGTTAGC\"}},{\"name\":\"Bio_SeqUtils_MeltingTemp_salt_correction\",\"arguments\":{\"Na\":50.0,\"K\":0.0,\"Tris\":0.0,\"Mg\":1.5,\"dNTPs\":0.2,\"method\":7,\"seq\":\"AGCTGCGTACGTTAGC\"}},{\"name\":\"Bio_SeqUtils_MeltingTemp_salt_correction\",\"arguments\":{\"Na\":50.0,\"K\":70.0,\"Tris\":20.0,\"Mg\":2.5,\"dNTPs\":0.2,\"method\":7,\"seq\":\"ACGTTGACCTGACCGTTAGCTA\"}},{\"name\":\"Bio_SeqUtils_MeltingTemp_salt_correction\",\"arguments\":{\"Na\":50.0,\"K\":5.0,\"Tris\":0.0,\"Mg\":2.0,\"dNTPs\":0.4,\"method\":7,\"seq\":\"ACGTACGTACGTA\"}}]"}
{"func_name": "Bio_SeqUtils_lcc_lcc_simp", "func_desc": "Bio.SeqUtils.lcc.lcc_simp calculates the Local Composition Complexity (LCC) for a DNA sequence using a normalized Shannon-entropy style measure (log base 4) as described by Konopka (2005). This function is intended for use in computational molecular biology and bioinformatics workflows (for example, when analyzing nucleotide composition bias or segmenting genomic sequences) and returns a single scalar complexity value representing the overall base composition diversity of the provided sequence.", "tools": [{"function": {"description": "Bio.SeqUtils.lcc.lcc_simp calculates the Local Composition Complexity (LCC) for a DNA sequence using a normalized Shannon-entropy style measure (log base 4) as described by Konopka (2005). This function is intended for use in computational molecular biology and bioinformatics workflows (for example, when analyzing nucleotide composition bias or segmenting genomic sequences) and returns a single scalar complexity value representing the overall base composition diversity of the provided sequence.\n", "name": "Bio_SeqUtils_lcc_lcc_simp", "parameters": {"properties": {"seq": {"type": "string", "description": "An unambiguous DNA sequence to analyse. The input may be a Python string (recommended) or a Biopython Seq-like object that supports len(), upper(), and count() semantics. The sequence is treated as nucleotide characters; the function converts the sequence to upper case internally before counting bases. Practical significance: callers should provide the full sequence whose global LCC is required (for instance, a genomic region, gene, or oligonucleotide). Ambiguous or non-ACGT characters are allowed but will affect results (see behaviour and failure modes).", "default": ""}}, "required": ["seq"], "type": "any"}}, "type": "function"}], "query": "We’re running a compositional-complexity QC gate on a small cohort of synthetic promoter constructs prior to motif scanning. Each record may contain synthesis artifacts (mixed case bases, whitespace, delimiter characters, and occasional ambiguous IUPAC calls). Normalize each sequence by uppercasing and removing any non-IUPAC DNA symbols, then compute a single overall LCC (Konopka 2005; log base 4) only for constructs that remain strictly unambiguous after normalization (i.e., composed exclusively of A/C/G/T). Treat the full-length promoter and each short fragment as separate entries. Raw inputs: (1) \"TATAAAGCGCGCTGCGTACGTACGTACGTACGTTTAAACCCGGGTTTAAACCCGGGATATATATATCGCGCGCGCGATGCGTACGTACGTACGTTTAAACCCGGG\" (2) \"GGGG AAAA TTTT CCCC GCGCGC ATATATAT GCGC\" (3) \"AAGGTTCCGGAATTCGATCGATCGATCG\" (4) \"acgtnnNN--ACGT\".", "answers": "[{\"name\":\"Bio_SeqUtils_lcc_lcc_simp\",\"arguments\":{\"seq\":\"TATAAAGCGCGCTGCGTACGTACGTACGTACGTTTAAACCCGGGTTTAAACCCGGGATATATATATCGCGCGCGCGATGCGTACGTACGTACGTTTAAACCCGGG\"}},{\"name\":\"Bio_SeqUtils_lcc_lcc_simp\",\"arguments\":{\"seq\":\"GGGGAAAATTTTCCCCGCGCGCATATATATGCGC\"}},{\"name\":\"Bio_SeqUtils_lcc_lcc_simp\",\"arguments\":{\"seq\":\"AAGGTTCCGGAATTCGATCGATCGATCG\"}}]"}
{"func_name": "Bio_SeqUtils_nt_search", "func_desc": "Search for a DNA subsequence in a DNA sequence string, interpreting IUPAC\n    ambiguous nucleotide codes, and return the regular-expression pattern used\n    followed by the 0-based start positions of each match on the forward strand.\n    \n    This function is part of Biopython's utilities for computational molecular\n    biology and bioinformatics. It is used to locate occurrences of nucleotide\n    subsequences (for example motifs or primer sites) in a larger DNA sequence\n    string while allowing IUPAC ambiguity codes (e.g. N, R, Y) in the query.\n    Ambiguous codes in subseq are expanded using IUPACData.ambiguous_dna_values\n    so that the function builds a Python regular expression pattern (with\n    bracketed character classes like \"[ACGT]\" for ambiguous positions) and then\n    searches only the forward strand of seq using Python's re.search.", "tools": [{"function": {"description": "Search for a DNA subsequence in a DNA sequence string, interpreting IUPAC\nambiguous nucleotide codes, and return the regular-expression pattern used\nfollowed by the 0-based start positions of each match on the forward strand.\n\nThis function is part of Biopython's utilities for computational molecular\nbiology and bioinformatics. It is used to locate occurrences of nucleotide\nsubsequences (for example motifs or primer sites) in a larger DNA sequence\nstring while allowing IUPAC ambiguity codes (e.g. N, R, Y) in the query.\nAmbiguous codes in subseq are expanded using IUPACData.ambiguous_dna_values\nso that the function builds a Python regular expression pattern (with\nbracketed character classes like \"[ACGT]\" for ambiguous positions) and then\nsearches only the forward strand of seq using Python's re.search.", "name": "Bio_SeqUtils_nt_search", "parameters": {"properties": {"seq": {"type": "string", "description": "Target DNA sequence to search. This is the full DNA string in\nwhich occurrences of subseq are sought. The matching performed is\ncase-sensitive because the function constructs a regular expression\nfrom subseq and calls re.search on slices of seq without any\ncase-folding flags; therefore, to reliably match IUPAC uppercase keys\n(as used by IUPACData.ambiguous_dna_values), provide seq in the same\ncase (commonly uppercase). seq is not modified by the function.", "default": ""}, "subseq": {"type": "string", "description": "Query DNA subsequence expressed using IUPAC nucleotide\ncodes (for example A, C, G, T for unambiguous bases and N, R, Y, etc.\nfor ambiguous bases). Each character in subseq is looked up in\nIUPACData.ambiguous_dna_values to determine the set of allowed bases\nat that position; if the mapping yields a single base the literal\ncharacter is used in the generated regex, otherwise a bracketed\ncharacter class is created (e.g. \"N\" -> \"[ACGT]\"). subseq is treated\nas a pattern for the forward strand only; reverse-complement searching\nis not performed by this function.", "default": ""}}, "required": ["seq", "subseq"], "type": "any"}}, "type": "function"}], "query": "I’m doing a contamination-aware motif scan across two mixed-quality DNA cohorts using a forward-strand IUPAC-ambiguous search. Treat any read containing characters outside the unambiguous DNA alphabet (A/C/G/T only) as low-confidence and route it to a permissive “wildcard-tolerant” screen using an all-ambiguous motif of the same length as the cohort’s nominal motif; otherwise use the nominal motif.\n\nCohort A (promoter fragment): sequence `GCTAGCATGACTGGTACCATGAGGATGACTTACGATGCTAGGATGACTGATC`, nominal motif `ATGRYTN`.\nCohort B (test amplicon): sequence `ATGCGTACGTTAGCGTACGTNNCGTA`, nominal motif `CGTR`.\n\nFor each cohort, run the appropriate search and report the exact regex pattern constructed (with bracketed character classes) followed by the 0-based start positions of each forward-strand match.", "answers": "[{\"name\":\"Bio_SeqUtils_nt_search\",\"arguments\":{\"seq\":\"GCTAGCATGACTGGTACCATGAGGATGACTTACGATGCTAGGATGACTGATC\",\"subseq\":\"ATGRYTN\"}},{\"name\":\"Bio_SeqUtils_nt_search\",\"arguments\":{\"seq\":\"ATGCGTACGTTAGCGTACGTNNCGTA\",\"subseq\":\"NNNN\"}}]"}
{"func_name": "Bio_SeqUtils_seq1", "func_desc": "Convert a protein sequence given with three-letter amino acid codes into a string of one-letter amino acid codes.\n    \n    This function is part of Biopython's Bio.SeqUtils utilities used in computational molecular biology workflows to normalize protein sequences for downstream tasks (for example, sequence comparisons, alignments, or database lookups). It maps contiguous three-character tokens from the input sequence to their one-letter IUPAC amino acid codes using Biopython's IUPACData.protein_letters_3to1_extended mapping. The function is case-insensitive for the three-letter codes, supports the IUPAC ambiguous/reserved one-letter codes (B for Asx, J for Xle, X for Xaa, U for Sel, O for Pyl), and by default maps the termination code \"Ter\" to \"*\" (this default can be changed via custom_map). The function performs a simple fixed-width grouping of the input (every three characters), so any trailing characters when the input length is not a multiple of three are ignored.", "tools": [{"function": {"description": "Convert a protein sequence given with three-letter amino acid codes into a string of one-letter amino acid codes.\n\nThis function is part of Biopython's Bio.SeqUtils utilities used in computational molecular biology workflows to normalize protein sequences for downstream tasks (for example, sequence comparisons, alignments, or database lookups). It maps contiguous three-character tokens from the input sequence to their one-letter IUPAC amino acid codes using Biopython's IUPACData.protein_letters_3to1_extended mapping. The function is case-insensitive for the three-letter codes, supports the IUPAC ambiguous/reserved one-letter codes (B for Asx, J for Xle, X for Xaa, U for Sel, O for Pyl), and by default maps the termination code \"Ter\" to \"*\" (this default can be changed via custom_map). The function performs a simple fixed-width grouping of the input (every three characters), so any trailing characters when the input length is not a multiple of three are ignored.", "name": "Bio_SeqUtils_seq1", "parameters": {"properties": {"seq": {"type": "string", "description": "The input protein sequence to convert. In practice this should be a sequence of contiguous three-letter amino acid codes (for example \"MetAlaIle...\"). The original implementation accepts a Python string and has historically been documented to accept Seq or MutableSeq objects from Biopython as well; the code treats the object as indexable and sliceable and will therefore work with objects implementing the same interface. The function is case-insensitive: \"met\", \"Met\", and \"MET\" are equivalent. Whitespace or non-letter characters present in the three-character groups will be treated as part of the group and, if not found in the mapping (after uppercasing), will be replaced by the undef_code. If len(seq) is not divisible by three, characters after the last full three-character group are ignored.", "default": ""}, "custom_map": {"type": "any", "nullable": true, "description": "Optional mapping of three-letter tokens (keys) to one-letter strings (values) to override or extend the built-in mappings. If None (the default), custom_map is set to {\"Ter\": \"*\"} by the function. Keys from custom_map are uppercased before being merged with the internal mapping, making key matching case-insensitive. Values provided in custom_map are inserted verbatim into the output where the corresponding three-letter key is found. Keys in custom_map should be string-like objects providing an upper() method; non-string keys that do not support upper() will raise an AttributeError. Values are not validated for length by the function; providing multi-character values will produce multi-character output at those positions.", "default": null}, "undef_code": {"type": "string", "description": "Single-character string used to represent any three-letter token that is not found in the combined mapping (built-in IUPAC mapping updated with custom_map). Defaults to \"X\". Any unknown or gap-like three-letter groups (including groups containing '-' or other non-alphabetic characters) will be replaced by this undef_code in the returned sequence.", "default": "X"}}, "required": ["seq", "undef_code", "custom_map"], "type": "any"}}, "type": "function"}], "query": "We’re curating a small benchmark of modeling-derived peptides exported as *contiguous* three-letter residue triplets, but the exports are messy: some replicates include explicit termination tokens (Ter) as hard stops, others include Ter only as an internal delimiter that should be preserved as a nonstandard marker, and some contain ambiguous/extended residues typical of low-confidence regions. Normalize the cohort to one-letter IUPAC using the extended mapping with unknown/undefined triplets forced to '?'. Apply this protocol per sequence: (a) If a sequence contains any extended/ambiguous residues from {Xaa, Asx, Xle, Pyl, Sel}, treat any Ter tokens as internal delimiters and remap Ter to '#'. (b) Otherwise, keep the standard termination behavior so Ter maps to '*'. Process this raw cohort: ['MetAlaGlyLysTerSerAsn', 'MetAlaIleGlySerTerXaaPylSelAsxXle', 'MetAlaIleGlySerTerLysPylAsxXaa'].", "answers": "[{\"name\":\"Bio_SeqUtils_seq1\",\"arguments\":{\"seq\":\"MetAlaGlyLysTerSerAsn\",\"custom_map\":{\"Ter\":\"*\"},\"undef_code\":\"?\"}},{\"name\":\"Bio_SeqUtils_seq1\",\"arguments\":{\"seq\":\"MetAlaIleGlySerTerXaaPylSelAsxXle\",\"custom_map\":{\"Ter\":\"#\"},\"undef_code\":\"?\"}},{\"name\":\"Bio_SeqUtils_seq1\",\"arguments\":{\"seq\":\"MetAlaIleGlySerTerLysPylAsxXaa\",\"custom_map\":{\"Ter\":\"#\"},\"undef_code\":\"?\"}}]"}
{"func_name": "Bio_SeqUtils_seq3", "func_desc": "Convert a protein sequence from one-letter amino acid codes to concatenated three-letter codes following the IUPAC convention.\n    \n    This function is used in computational molecular biology workflows (as in Biopython) to translate protein sequences represented with single-letter amino acid codes into their three-letter equivalents for tasks such as human-readable output, legacy format conversion, or interfacing with tools that expect three-letter residue codes. The conversion follows the IUPAC extended mapping (including ambiguous and rare codes B -> Asx, J -> Xle, X -> Xaa, U -> Sel, O -> Pyl) and, by default, maps the stop/termination character '*' to 'Ter'. The implementation builds an internal mapping from the IUPAC standard and an optional user-supplied custom_map without mutating the global IUPAC mapping.", "tools": [{"function": {"description": "Convert a protein sequence from one-letter amino acid codes to concatenated three-letter codes following the IUPAC convention.\n\nThis function is used in computational molecular biology workflows (as in Biopython) to translate protein sequences represented with single-letter amino acid codes into their three-letter equivalents for tasks such as human-readable output, legacy format conversion, or interfacing with tools that expect three-letter residue codes. The conversion follows the IUPAC extended mapping (including ambiguous and rare codes B -> Asx, J -> Xle, X -> Xaa, U -> Sel, O -> Pyl) and, by default, maps the stop/termination character '*' to 'Ter'. The implementation builds an internal mapping from the IUPAC standard and an optional user-supplied custom_map without mutating the global IUPAC mapping.", "name": "Bio_SeqUtils_seq3", "parameters": {"properties": {"seq": {"type": "string", "description": "Input protein sequence to convert. This should be an iterable of one-letter amino acid codes (for example a Python string like \"MAIVMGRWKGAR*\", or a Bio.Seq.Seq / Bio.Seq.MutableSeq object). Each element is looked up as a key in the three-letter mapping. Typical practical use is converting standard protein sequences for display or downstream processing. If seq is not iterable (for example None), a TypeError will be raised by the iteration operation; if seq contains unexpected element types the behaviour depends on whether those elements can be used as dict keys and converted to strings for joining.", "default": ""}, "custom_map": {"type": "any", "nullable": true, "description": "Optional mapping of one-letter residue characters to replacement three-letter strings. If None (the default), custom_map is set to {\"*\": \"Ter\"} so that the termination character '*' maps to \"Ter\". When provided, the function constructs the internal mapping by combining the IUPAC mapping with the items from custom_map appended afterward; entries in custom_map therefore override the IUPAC mapping for matching keys. Keys in custom_map should be single-character strings corresponding to the one-letter codes to override (for example {\"*\": \"***\"} to change the terminator output). Values in custom_map must be strings (three-letter codes or any replacement strings); if they are not strings, the final join will raise a TypeError.", "default": null}, "undef_code": {"type": "string", "description": "String to use for any unknown or undefined one-letter characters that are not present in the IUPAC mapping or custom_map. Defaults to \"Xaa\". This includes common gap characters such as '-' which, by this implementation, will be translated to undef_code unless explicitly provided in custom_map. The value must be a string because it is concatenated into the returned sequence; non-string values will cause a TypeError during the join operation.", "default": "Xaa"}}, "required": ["seq", "undef_code", "custom_map"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed-quality peptide export from a proteomics annotation run, where some records include rare/ambiguous IUPAC residues and termination, and others include alignment artifacts. Given these two raw residue strings: (a) \"MUBJXO*\" and (b) \"ACDEFGHIKLMNPQRSTVWY*\". Convert only sequences that contain at least one extended/ambiguous residue code from {B, J, X, U, O} into concatenated three-letter IUPAC codes for a legacy report, and require the termination symbol to be rendered as \"Stop\" in those exports. Sequences that do not meet the extended/ambiguous criterion should be held back from this export stage.", "answers": "[{\"name\":\"Bio_SeqUtils_seq3\",\"arguments\":{\"seq\":\"MUBJXO*\",\"custom_map\":{\"*\":\"Stop\"}}}]"}
{"func_name": "Bio_TogoWS_search_count", "func_desc": "Bio.TogoWS.search_count — Request the TogoWS search service and return the integer count of records matching a search.\n    \n    This function calls the TogoWS REST search/count endpoint to determine how many records in a given TogoWS database match the provided query. It is intended for use in bioinformatics workflows where you need to estimate the size of a result set (for example to plan batched downloads using offset and limit with Bio.TogoWS.search()). The function caches the list of known searchable database names in the module-level variable _search_db_names by calling _get_fields(_BASE_URL + \"/search\") when needed. If the provided db is not in that cached list, the function issues a runtime warning but still attempts the request. The function performs network I/O by constructing a URL of the form _BASE_URL + \"/search/{db}/{quote(query)}/count\", opening it via _open(), reading the response body, and converting the response to an integer count.", "tools": [{"function": {"description": "Bio.TogoWS.search_count — Request the TogoWS search service and return the integer count of records matching a search.\n\nThis function calls the TogoWS REST search/count endpoint to determine how many records in a given TogoWS database match the provided query. It is intended for use in bioinformatics workflows where you need to estimate the size of a result set (for example to plan batched downloads using offset and limit with Bio.TogoWS.search()). The function caches the list of known searchable database names in the module-level variable _search_db_names by calling _get_fields(_BASE_URL + \"/search\") when needed. If the provided db is not in that cached list, the function issues a runtime warning but still attempts the request. The function performs network I/O by constructing a URL of the form _BASE_URL + \"/search/{db}/{quote(query)}/count\", opening it via _open(), reading the response body, and converting the response to an integer count.", "name": "Bio_TogoWS_search_count", "parameters": {"properties": {"db": {"type": "string", "description": "The TogoWS database name to query. This should be one of the database identifiers exposed by the TogoWS search index (see the TogoWS search listing, e.g. http://togows.dbcls.jp/search). Practically, this parameter tells TogoWS which biological database (such as sequence, taxonomy, or other indexed resources) to apply the search to. If this value is not present in the cached _search_db_names, the function will emit a warning but will still attempt the request.", "default": ""}, "query": {"type": "string", "description": "The search expression passed to the TogoWS search API. This is the text query understood by the specified TogoWS database (for example an identifier, keyword, or more complex query language supported by that DB). The function URL-encodes this value with quote() before including it in the request URL.", "default": ""}}, "required": ["db", "query"], "type": "any"}}, "type": "function"}], "query": "We’re doing a UniProt preflight sizing pass for a heterogeneous human protein cohort assembled from multiple curation sources. Start from this raw list of candidate query fragments (some are artifacts from a spreadsheet export): [\"organism:9606 AND keyword:kinase\", \"TP53 AND organism:9606\", \"tumor suppressor p53\", \"\", \"9606\", \"organism:9606 AND keyword:kinase;DROP TABLE\", \"TP53 AND organism:9606  \", \"tp53 AND ORGANISM:9606\", \"organism:9606 AND keyword:Kinase\"]. Treat a fragment as usable only if it is a non-empty UniProt-style boolean expression that explicitly constrains organism:9606 and contains no punctuation other than spaces, parentheses, colons, and the boolean operators AND/OR/NOT. For usable fragments, normalize case and whitespace; then deduplicate by the normalized query string, but retain exactly two independent technical replicates for the TP53 target definition (i.e., two separate count checks with the same normalized TP53+human query). Use the UniProt search index in TogoWS for all count checks, and return the integer counts in the execution order implied by these rules.", "answers": "[{\"name\":\"Bio_TogoWS_search_count\",\"arguments\":{\"db\":\"uniprot\",\"query\":\"organism:9606 AND keyword:kinase\"}},{\"name\":\"Bio_TogoWS_search_count\",\"arguments\":{\"db\":\"uniprot\",\"query\":\"TP53 AND organism:9606\"}},{\"name\":\"Bio_TogoWS_search_count\",\"arguments\":{\"db\":\"uniprot\",\"query\":\"TP53 AND organism:9606\"}}]"}
{"func_name": "Bio_bgzf_split_virtual_offset", "func_desc": "Split a 64-bit BGZF virtual offset into the BGZF block start and the offset within that block.\n    \n    This function is used in Bio.bgzf and related Biopython code that manipulates BGZF-compressed genomic files (for example BAM/BAI workflows). A BGZF virtual offset encodes a file-position that can be used to seek to a particular record: the high-order bits are the byte offset of the start of a BGZF block in the compressed file, and the low-order 16 bits are the offset inside the uncompressed data of that block. This function reverses that packing: it extracts the block start (the compressed-file byte offset where the BGZF block begins) and the within-block offset (the byte offset inside the uncompressed block where the desired data record begins). The result is commonly used when resolving virtual offsets stored in indexes (such as BAI) to actual file seek positions for random access in large genomic files.", "tools": [{"function": {"description": "Split a 64-bit BGZF virtual offset into the BGZF block start and the offset within that block.\n\nThis function is used in Bio.bgzf and related Biopython code that manipulates BGZF-compressed genomic files (for example BAM/BAI workflows). A BGZF virtual offset encodes a file-position that can be used to seek to a particular record: the high-order bits are the byte offset of the start of a BGZF block in the compressed file, and the low-order 16 bits are the offset inside the uncompressed data of that block. This function reverses that packing: it extracts the block start (the compressed-file byte offset where the BGZF block begins) and the within-block offset (the byte offset inside the uncompressed block where the desired data record begins). The result is commonly used when resolving virtual offsets stored in indexes (such as BAI) to actual file seek positions for random access in large genomic files.", "name": "Bio_bgzf_split_virtual_offset", "parameters": {"properties": {"virtual_offset": {"type": "integer", "description": "A Python int representing the packed 64-bit BGZF virtual offset value. In Biopython usage this value is expected to originate from BGZF-aware indexes or file metadata. The function interprets this integer by taking the high-order bits (shifted right by 16) as the BGZF block start (compressed-file byte offset) and the low-order 16 bits as the within-block offset. Supplying a non-integer will raise a TypeError when Python attempts bitwise/shift operations; supplying negative values or values outside the conventional 64-bit virtual-offset domain may produce results that do not correspond to valid BGZF locations (Python's arbitrary-precision ints will still be processed mathematically).", "default": ""}}, "required": ["virtual_offset"], "type": "any"}}, "type": "function"}], "query": "We’re auditing a BAI-derived set of candidate BGZF virtual offsets before wiring them into our random-seek validator. Given the raw virtual offsets [281479271743489, 3255392257707], decode only those offsets that are structurally consistent with BAM BGZF constraints: the within-block uncompressed offset must fall within a single BGZF block (i.e., < 65536) and should be aligned to an even byte boundary (to mimic 16-bit word alignment in our downstream record parser). For each offset that passes these criteria, split the 64-bit virtual offset into (a) the compressed-file byte offset of the BGZF block start and (b) the within-block uncompressed offset, so we can compare against our seek implementation.", "answers": "[{\"name\":\"Bio_bgzf_split_virtual_offset\",\"arguments\":{\"virtual_offset\":281479271743489}}]"}
{"func_name": "Bio_motifs_jaspar_split_jaspar_id", "func_desc": "Split a JASPAR matrix ID into its base identifier and optional version component.\n    \n    This function is used in the Bio.motifs.jaspar subpackage to parse JASPAR motif matrix identifiers commonly found in JASPAR databases and motif exchange files. A JASPAR ID often encodes a stable matrix identifier (the base ID) and an optional version suffix separated by a single period, for example \"MA0047.2\". The base ID identifies the motif (useful for looking up the canonical matrix or grouping related motifs) and the version component identifies a release or revision of that matrix (useful for tracking changes across database versions).", "tools": [{"function": {"description": "Split a JASPAR matrix ID into its base identifier and optional version component.\n\nThis function is used in the Bio.motifs.jaspar subpackage to parse JASPAR motif matrix identifiers commonly found in JASPAR databases and motif exchange files. A JASPAR ID often encodes a stable matrix identifier (the base ID) and an optional version suffix separated by a single period, for example \"MA0047.2\". The base ID identifies the motif (useful for looking up the canonical matrix or grouping related motifs) and the version component identifies a release or revision of that matrix (useful for tracking changes across database versions).", "name": "Bio_motifs_jaspar_split_jaspar_id", "parameters": {"properties": {"id": {"type": "string", "description": "The JASPAR matrix identifier to split. Must be a Python string containing the identifier as it appears in JASPAR files or related metadata, for example \"MA0047\" or \"MA0047.2\". The function expects this exact parameter name and type; passing a non-string value will result in an AttributeError when the method tries to call the string split operation.", "default": ""}}, "required": ["id"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating motif IDs from a mixed-quality JASPAR ingest where the same upstream tools sometimes append revision suffixes and sometimes emit bare stable IDs. Before grouping motifs, normalize only those identifiers that look like valid JASPAR matrix accessions: they must start with the literal prefix \"MA\" followed by exactly 4 digits, and they may optionally include a single dot followed by an integer revision. Treat anything else as a non-matrix token and ignore it. Apply this rule to the raw token stream exactly in order (keep duplicates when they pass QC): [\"MA0139.1\", \"MA0139.3\", \"MA0139.1\", \"MA0139\", \"MA0139.\", \"MA0139.1.2\", \"ma0139.1\", \"MA139.1\", \"PB0001.1\", \"MA013A.1\", \"MA0139.v3\", \"MA9999.12\", \"MA0000.1\"]. For each token that passes QC, split it into base matrix ID and optional version component using the JASPAR split routine.", "answers": "[{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA0139.1\"}},{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA0139.3\"}},{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA0139.1\"}},{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA0139\"}},{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA9999.12\"}},{\"name\":\"Bio_motifs_jaspar_split_jaspar_id\",\"arguments\":{\"id\":\"MA0000.1\"}}]"}
{"func_name": "Bio_pairwise2_format_alignment", "func_desc": "Format a pairwise alignment (given as two aligned sequences) into a human-readable multi-line string suitable for console display and logging.\n    \n    This function is used in Biopython's pairwise alignment utilities to present an alignment between two sequences (or sequence tokens) in a compact, readable form. It prints a sequence-1 line, a match-line that marks identical matches, mismatches and gaps, and a sequence-2 line, followed by a score line. This is intended for users and developers working with computational molecular biology sequence comparisons, for quick inspection of aligned regions produced by pairwise alignment algorithms.", "tools": [{"function": {"description": "Format a pairwise alignment (given as two aligned sequences) into a human-readable multi-line string suitable for console display and logging.\n\nThis function is used in Biopython's pairwise alignment utilities to present an alignment between two sequences (or sequence tokens) in a compact, readable form. It prints a sequence-1 line, a match-line that marks identical matches, mismatches and gaps, and a sequence-2 line, followed by a score line. This is intended for users and developers working with computational molecular biology sequence comparisons, for quick inspection of aligned regions produced by pairwise alignment algorithms.", "name": "Bio_pairwise2_format_alignment", "parameters": {"properties": {"align1": {"type": "array", "items": {"type": "float"}, "description": "The first aligned sequence provided as a list of string tokens (for example single-character bases/amino acids or multi-character tokens). Each element corresponds to one alignment column. When align1 is a list, elements will be displayed separated by spaces and may be of different lengths; the formatter centers each pairwise column using the maximum element width. The gap symbol used in these lists must be \"-\" (or the list element equal to ['-'] for lists) for correct start-position computation described below.", "default": ""}, "align2": {"type": "array", "items": {"type": "float"}, "description": "The second aligned sequence provided as a list of string tokens parallel to align1. Must have alignment columns aligned to those of align1 (same logical length); the function iterates over align1[begin:end] and align2[begin:end] in parallel. As with align1, gap symbol should be \"-\" for correct start-position computation.", "default": ""}, "score": {"type": "float", "description": "The numerical alignment score to display. It is included on the final output line as \"Score=<value>\" formatted with Python's general format specifier (:g). This value is provided by the alignment algorithm (for example, a dynamic programming routine) and has no effect on the layout beyond being printed.", "default": ""}, "begin": {"type": "integer", "description": "The Python 0-based index into the aligned sequences indicating the first column of the alignment to display. Note: begin/end are indices into the aligned sequences (0-based), not the original un-aligned sequence coordinates. For local alignments (when full_sequences is False) and when begin != 0, the function computes 1-based start positions for the aligned subsequences by counting non-gap tokens before this index.", "default": ""}, "end": {"type": "integer", "description": "The Python 0-based end index (exclusive) into the aligned sequences indicating one-past-the-last column to display. The function slices align1 and align2 using this end index; when full_sequences is False and end != len(align1) only the aligned subrange [begin:end] is shown.", "default": ""}, "full_sequences": {"type": "boolean", "description": "If False (default), and the displayed region does not span the entire aligned sequences (begin != 0 or end != len(align1)), only the aligned subsequence between begin and end is shown and 1-based start positions for the aligned subsequences are printed to the left of the respective sequence lines. If True, the historic behavior is restored: the entire sequences (from index 0 to len(align1)) are displayed, the start-position prefixes are omitted (start markers set to zero width), and non-aligned leading/trailing columns are included; in this case, the match-line shows spaces for columns outside the aligned region. The default is False.", "default": false}}, "required": ["align1", "align2", "score", "begin", "end", "full_sequences"], "type": "any"}}, "type": "function"}], "query": "We’re curating console-ready local-alignment excerpts from a small QC batch of protein-fragment replicate alignments where some records contain edge artifacts (low-information segments dominated by gaps) and where the excerpt window should adapt to the observed aligned core. For each replicate, you are given pre-aligned/gapped tokens (do not perform alignment). First, determine whether the replicate is suitable for logging by requiring that within the requested slice window the fraction of gap characters ('-') across both aligned sequences combined is < 40%. Only suitable replicates should be formatted for the lab log.\n\nFor suitable replicates, format only a local core segment using 0-based indexing with an end-exclusive slice and do not force full-length printing (treat as a local alignment snippet). The slice window is defined per replicate as follows: start at the first position at or after index 2 where both sequences are non-gap, and end at the last position at or before the provided nominal end where both sequences are non-gap, using end-exclusive semantics.\n\nReplicate A (string form): Seq1=ACDEFGHIKLMN-PQR vs Seq2=AC-EYGHIK-MNSPQR, score=42.75, nominal window begin=2, nominal end=15.\nReplicate B (token-list form): seq1=[\"M\",\"K\",\"T\",\"-\",\"A\",\"I\",\"L\",\"V\",\"E\",\"-\",\"G\"] vs seq2=[\"M\",\"K\",\"-\",\"Q\",\"A\",\"I\",\"L\",\"-\",\"E\",\"R\",\"G\"], score=17.5, nominal window begin=2, nominal end=10.\nReplicate C (string form): Seq1=ACDEFGHIKLMN vs Seq2=AC-EYGHIK-MN, score=37.5, nominal window begin=2, nominal end=11.\n\nEach formatted block must include the two sequence lines, the match/mismatch/gap marker line, and the final score line.", "answers": "[{\"name\":\"Bio_pairwise2_format_alignment\",\"arguments\":{\"align1\":[\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"-\",\"P\",\"Q\",\"R\"],\"align2\":[\"A\",\"C\",\"-\",\"E\",\"Y\",\"G\",\"H\",\"I\",\"K\",\"-\",\"M\",\"N\",\"S\",\"P\",\"Q\",\"R\"],\"score\":42.75,\"begin\":3,\"end\":15,\"full_sequences\":false}},{\"name\":\"Bio_pairwise2_format_alignment\",\"arguments\":{\"align1\":[\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\"],\"align2\":[\"A\",\"C\",\"-\",\"E\",\"Y\",\"G\",\"H\",\"I\",\"K\",\"-\",\"M\",\"N\"],\"score\":37.5,\"begin\":3,\"end\":11,\"full_sequences\":false}}]"}
{"func_name": "Bio_pairwise2_print_matrix", "func_desc": "Print a two-dimensional matrix to standard output in a human‑readable, column‑aligned form for debugging pairwise alignment code in Biopython.", "tools": [{"function": {"description": "Print a two-dimensional matrix to standard output in a human‑readable, column‑aligned form for debugging pairwise alignment code in Biopython.\n", "name": "Bio_pairwise2_print_matrix", "parameters": {"properties": {"matrix": {"type": "array", "items": {"type": "float"}, "description": "A two-dimensional matrix represented as a list of row sequences (for example, a list of lists or other indexable row objects) containing the values to print. In the Biopython/pairwise2 context this is typically a dynamic programming scoring or traceback matrix produced during pairwise sequence alignment; each inner row corresponds to one row of the matrix. The function uses the first row (matrix[0]) to determine the number of columns, so matrix must be non-empty and the first row must be indexable. Rows may be shorter than the first row, but no row may be longer than the first row (a longer row will raise IndexError). Individual matrix entries are converted to strings via str(value) and their printed width is computed from that string representation.", "default": ""}}, "required": ["matrix"], "type": "any"}}, "type": "function"}], "query": "We’re triaging DP score-matrix dumps from a pairwise-alignment run where some replicates may be contaminated by re-indexing artifacts. For each replicate matrix, compute the sum of the main diagonal and the sum of the anti-diagonal; print only those matrices where the main-diagonal sum is strictly greater than the anti-diagonal sum. Use the same human-readable, column-aligned debug print as in Biopython. Replicate A (5x5): [[0, -1, -2, -3, -4], [-1, 1, 0, -1, -2], [-2, 0, 2, 1, 0], [-3, -1, 1, 3, 2], [-4, -2, 0, 2, 4]]. Replicate B (4x4): [[0, -2, -4, -6], [-2, 1, -1, -3], [-4, -1, 2, 0], [-6, -3, 0, 3]].", "answers": "[{\"name\":\"Bio_pairwise2_print_matrix\",\"arguments\":{\"matrix\":[[0,-2,-4,-6],[-2,1,-1,-3],[-4,-1,2,0],[-6,-3,0,3]]}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition", "func_desc": "CalculateAADipeptideComposition computes a fixed-length numerical feature vector that encodes the amino acid (AA) composition, dipeptide composition, and 3-mer (tripeptide/spectrum) composition for a given protein primary sequence. In the DeepPurpose toolkit (used for drug-target interaction prediction, protein property/function prediction, PPI, and related molecular modeling tasks), this function is used to transform a raw protein sequence (single-letter amino acid codes) into a deterministic numeric representation (feature vector) suitable for machine learning models, virtual screening, and repurposing workflows.", "tools": [{"function": {"description": "CalculateAADipeptideComposition computes a fixed-length numerical feature vector that encodes the amino acid (AA) composition, dipeptide composition, and 3-mer (tripeptide/spectrum) composition for a given protein primary sequence. In the DeepPurpose toolkit (used for drug-target interaction prediction, protein property/function prediction, PPI, and related molecular modeling tasks), this function is used to transform a raw protein sequence (single-letter amino acid codes) into a deterministic numeric representation (feature vector) suitable for machine learning models, virtual screening, and repurposing workflows.\n", "name": "DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A protein primary sequence provided as a string of single-letter amino acid codes (e.g., \"MSEQ...\"). In the DeepPurpose context, this sequence represents the target protein whose composition-based encoding will be used as input features for models (DTI, PPI, ProteinPred). The function expects a \"pure\" protein sequence; sequences containing non-standard characters, whitespace, or gaps are not guaranteed to be handled and may cause errors or incorrect features. The caller is responsible for providing an appropriately preprocessed sequence (uppercase or lowercase single-letter codes as accepted by the helper composition functions in this module).", "default": ""}}, "required": ["ProteinSequence"], "type": "any"}}, "type": "function"}], "query": "We’re curating a small protein panel for the next DeepPurpose training run, but the cohort must be compositionally well-behaved to avoid featurization artifacts. From the three candidate primary sequences below, compute the AA+dipeptide+3-mer composition feature vector only for sequences that (a) are composed exclusively of the 20 canonical amino acids (single-letter codes) and (b) contain at least one cysteine residue (C), since this cohort is focused on disulfide-capable targets. Candidate sequences: (1) human insulin B-chain: \"FVNQHLCGSHLVEALYLVCGERGFFYTPKT\"; (2) baseline target protein: \"MKWVTFISLLFLFSSAYSRGVFRRDAHKSEVAHRFKDLGE\"; (3) kinase-target sequence: \"MSTAVLENRLGVQGAFGKDWSLQGELGIR\".", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition\",\"arguments\":{\"ProteinSequence\":\"FVNQHLCGSHLVEALYLVCGERGFFYTPKT\"}},{\"name\":\"DeepPurpose_pybiomed_helper_CalculateAADipeptideComposition\",\"arguments\":{\"ProteinSequence\":\"MKWVTFISLLFLFSSAYSRGVFRRDAHKSEVAHRFKDLGE\"}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_CalculateConjointTriad", "func_desc": "CalculateConjointTriad computes the Conjoint Triad (CTriad) encoding for a protein sequence and returns the 343-dimensional count vector used as a fixed-length protein descriptor in DeepPurpose models (for tasks such as drug–target interaction prediction, protein–protein interaction prediction, and protein function prediction). This function maps the input single-letter amino acid sequence to a 1..7 grouped alphabet via the helper function _Str2Num, then counts occurrences of every length-3 contiguous group triad over the grouped alphabet (7^3 = 343 possible triads) and returns those counts in a numpy array in a deterministic order that DeepPurpose expects for downstream encoding/learning pipelines.", "tools": [{"function": {"description": "CalculateConjointTriad computes the Conjoint Triad (CTriad) encoding for a protein sequence and returns the 343-dimensional count vector used as a fixed-length protein descriptor in DeepPurpose models (for tasks such as drug–target interaction prediction, protein–protein interaction prediction, and protein function prediction). This function maps the input single-letter amino acid sequence to a 1..7 grouped alphabet via the helper function _Str2Num, then counts occurrences of every length-3 contiguous group triad over the grouped alphabet (7^3 = 343 possible triads) and returns those counts in a numpy array in a deterministic order that DeepPurpose expects for downstream encoding/learning pipelines.\n", "name": "DeepPurpose_pybiomed_helper_CalculateConjointTriad", "parameters": {"properties": {"proteinsequence": {"type": "string", "description": "A pure protein sequence given as a Python string of single-letter amino acid codes (e.g., \"MSTNPKPQR\"). This argument is the primary input to the Conjoint Triad encoding: it is first converted by the internal helper _Str2Num into a string of digits '1'..'7' representing amino-acid groups, and then all contiguous length-3 group triads are counted. In the DeepPurpose context, this parameter represents the target protein whose sequence-derived descriptor will be used as an input feature vector to DTI, PPI, or protein-function prediction models. The caller must provide a sequence consisting only of standard single-letter amino-acid characters (no whitespace or non-letter characters); providing an empty string is permitted and yields an all-zero feature vector.", "default": ""}}, "required": ["proteinsequence"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mini-batch of N-terminal protein segments for a DeepPurpose drug–target interaction run where low-complexity, cleavage-adjacent fragments can inflate CTriad counts. For each provided segment, first apply a preprocessing rule driven by intrinsic composition: if the segment contains any uninterrupted homopolymer run of length ≥3 (e.g., 'GGG', 'KKK', 'RRR'), trim the sequence to start immediately after the first occurrence of such a run; otherwise keep the segment unchanged. Then, for every resulting (possibly trimmed) segment that is still long enough to yield at least one contiguous length-3 triad, generate the fixed-length 343-dimensional Conjoint Triad (CTriad) count descriptor in the deterministic DeepPurpose order.\n\nSegments (separate entries/replicates):\n1) BRCA1 N-terminus: MDLSALRVEEVQNVINAMQKILECPICLELIKEPVGVRK\n2) Target N-terminus: MSTNPKPQRKTKRNTNRRPQDVKFPGGGQIVGGVLTK", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_CalculateConjointTriad\",\"arguments\":{\"proteinsequence\":\"MDLSALRVEEVQNVINAMQKILECPICLELIKEPVGVRK\"}},{\"name\":\"DeepPurpose_pybiomed_helper_CalculateConjointTriad\",\"arguments\":{\"proteinsequence\":\"QIVGGVLTK\"}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetAPseudoAAC2", "func_desc": "GetAPseudoAAC2 computes the type II pseudo-amino acid composition (PAAC) correlation features for a protein sequence as implemented in DeepPurpose.pybiomed_helper. It returns the \"last lamda\" block of PAAC descriptors (the sequence-order correlation terms based on hydrophobicity and hydrophilicity) that are commonly appended to the 20 conventional amino-acid composition features when encoding proteins for tasks in the DeepPurpose framework (for example, drug-target interaction, protein-protein interaction, and protein function prediction).\n    \n    This function iteratively calls GetSequenceOrderCorrelationFactorForAPAAC for k = 1..lamda to collect two sequence-order correlation factors per k (hydrophobicity- and hydrophilicity-based factors), normalizes them using the standard PAAC normalization term temp = 1 + weight * sum(rightpart), and returns these normalized correlation features scaled to percentages and rounded to three decimal places. The returned descriptors are named \"PAAC21\" through \"PAAC{20+2*lamda}\" (i.e., the PAAC indices immediately following the 20 conventional amino-acid composition descriptors).", "tools": [{"function": {"description": "GetAPseudoAAC2 computes the type II pseudo-amino acid composition (PAAC) correlation features for a protein sequence as implemented in DeepPurpose.pybiomed_helper. It returns the \"last lamda\" block of PAAC descriptors (the sequence-order correlation terms based on hydrophobicity and hydrophilicity) that are commonly appended to the 20 conventional amino-acid composition features when encoding proteins for tasks in the DeepPurpose framework (for example, drug-target interaction, protein-protein interaction, and protein function prediction).\n\nThis function iteratively calls GetSequenceOrderCorrelationFactorForAPAAC for k = 1..lamda to collect two sequence-order correlation factors per k (hydrophobicity- and hydrophilicity-based factors), normalizes them using the standard PAAC normalization term temp = 1 + weight * sum(rightpart), and returns these normalized correlation features scaled to percentages and rounded to three decimal places. The returned descriptors are named \"PAAC21\" through \"PAAC{20+2*lamda}\" (i.e., the PAAC indices immediately following the 20 conventional amino-acid composition descriptors).", "name": "DeepPurpose_pybiomed_helper_GetAPseudoAAC2", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "Protein primary sequence as a single-letter amino-acid string. This is the input sequence to be encoded into PAAC correlation features. In DeepPurpose this sequence is used as the protein input encoding for models that accept APAAC/PAAC-style descriptors. The function delegates sequence-order correlation calculations to GetSequenceOrderCorrelationFactorForAPAAC, so ProteinSequence must be acceptable to that helper (for example, containing recognized amino-acid characters); invalid characters or an incompatible format may cause the helper to raise an exception.", "default": ""}, "lamda": {"type": "integer", "description": "Number of correlation tiers (default 30). For each integer k from 1 to lamda the function obtains two correlation factors, producing 2 * lamda descriptors in total. Practically, lamda controls the maximal sequence separation over which order correlations are computed: larger lamda captures longer-range sequence-order information but increases computation and may be invalid or uninformative for very short sequences. If lamda <= 0, no correlation descriptors are produced and an empty result dictionary is returned. The function does not internally constrain lamda beyond using it as the loop bound; any required validation should be done by the caller or will surface as an error from the underlying helper.", "default": 30}, "weight": {"type": "float", "description": "Weighting factor (default 0.5) applied to the sequence-order correlation terms during normalization. The implementation uses temp = 1 + weight * sum(rightpart) as the normalization denominator and computes each returned value as (weight * correlation_value / temp) * 100, rounded to three decimal places. In practice, weight balances the relative contribution of the sequence-order correlations against the implicit 20 amino-acid composition terms (which are not computed by this function). Passing different weight values adjusts how strongly these correlation features influence downstream models.", "default": 0.5}}, "required": ["ProteinSequence", "lamda", "weight"], "type": "any"}}, "type": "function"}], "query": "I’m curating a small protein QC set for calibrating DeepPurpose’s Type II PAAC sequence-order correlation block (the PAAC indices after the first 20 AAC features). Start from these raw target sequences:\n\n- BRCA1_fragment: MDLSALRVEEVQNVINAMQKILECPICLELIKEPVG\n- secretory_like_peptide: MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\n\nRun GetAPseudoAAC2 only for sequences that look like plausible protein fragments under standard FASTA-style constraints (uppercase one-letter amino-acid codes only, and sequence length sufficient to support lamda=10). Use lamda=10 for all processed sequences.\n\nApply a cohort-dependent weighting rule derived from intrinsic composition: compute the fraction of residues in {D,E,K,R} (charged residues) for each sequence; if the charged fraction is >= 0.30, use weight=0.8 to emphasize sequence-order effects, otherwise use weight=0.4. Return only the PAAC21–PAAC40 block for each processed sequence.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetAPseudoAAC2\",\"arguments\":{\"ProteinSequence\":\"MDLSALRVEEVQNVINAMQKILECPICLELIKEPVG\",\"lamda\":10,\"weight\":0.8}},{\"name\":\"DeepPurpose_pybiomed_helper_GetAPseudoAAC2\",\"arguments\":{\"ProteinSequence\":\"MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\",\"lamda\":10,\"weight\":0.4}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder", "func_desc": "Compute quasi-sequence-order descriptors for a protein sequence following Chou (2000), producing a numeric feature vector used as an input encoding for downstream models in DeepPurpose (e.g., DTI, PPI, protein-function prediction, virtual screening and repurposing workflows).\n    \n    This function implements the quasi-sequence-order (QSO) descriptor family described in:\n    Kuo-Chen Chou. Prediction of Protein Subcellar Locations by Incorporating Quasi-Sequence-Order Effect. Biochemical and Biophysical Research Communications 2000, 278, 477-483.\n    It computes multiple QSO components using two internal distance matrices and four internal procedures (GetQuasiSequenceOrder1SW, GetQuasiSequenceOrder2SW, GetQuasiSequenceOrder1Grant, GetQuasiSequenceOrder2Grant). The resulting 1-D numeric vector encodes sequence-order correlation information (the quasi-sequence-order effect) that augments simple composition features and is intended for use as model input features in machine learning pipelines in DeepPurpose (e.g., for drug-target interaction prediction, protein-protein interaction prediction, or protein function prediction).", "tools": [{"function": {"description": "Compute quasi-sequence-order descriptors for a protein sequence following Chou (2000), producing a numeric feature vector used as an input encoding for downstream models in DeepPurpose (e.g., DTI, PPI, protein-function prediction, virtual screening and repurposing workflows).\n\nThis function implements the quasi-sequence-order (QSO) descriptor family described in:\nKuo-Chen Chou. Prediction of Protein Subcellar Locations by Incorporating Quasi-Sequence-Order Effect. Biochemical and Biophysical Research Communications 2000, 278, 477-483.\nIt computes multiple QSO components using two internal distance matrices and four internal procedures (GetQuasiSequenceOrder1SW, GetQuasiSequenceOrder2SW, GetQuasiSequenceOrder1Grant, GetQuasiSequenceOrder2Grant). The resulting 1-D numeric vector encodes sequence-order correlation information (the quasi-sequence-order effect) that augments simple composition features and is intended for use as model input features in machine learning pipelines in DeepPurpose (e.g., for drug-target interaction prediction, protein-protein interaction prediction, or protein function prediction).", "name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein primary sequence given as a string of standard single-letter amino acid codes (for example, 'MKT...'). This sequence is the biological input to be encoded into quasi-sequence-order descriptors. The sequence must consist of valid amino-acid letters; passing non-standard characters or whitespace may produce incorrect descriptors or cause the internal helper functions to raise exceptions. The method assumes the sequence length is larger than maxlag (see below); very short sequences that do not satisfy that assumption may lead to undefined behavior or errors in the internal computations.", "default": ""}, "maxlag": {"type": "integer", "description": "The maximum lag (non-negative integer) used when computing sequence-order correlation terms. This parameter controls how far apart along the primary sequence pairwise residue correlations are computed: correlations for residue pairs separated by up to maxlag positions are included. The default is 30, matching common choices in the QSO literature and in the original implementation. Practically, choose maxlag relative to typical protein lengths in your dataset; the sequence length should be greater than maxlag for meaningful results.", "default": 30}, "weight": {"type": "float", "description": "A numeric weight factor that scales the relative contribution of sequence-order correlation terms versus composition-like terms in the final descriptor vector. Typical choices from the literature use small positive values (the default is 0.1) — this balances the influence of order information against residue composition. Adjusting this parameter changes feature scaling and can affect downstream model training.", "default": 0.1}}, "required": ["ProteinSequence", "weight", "maxlag"], "type": "any"}}, "type": "function"}], "query": "We’re building a DeepPurpose DTI protein-encoder sanity check from two raw target fragments that may include secretion signals and short/low-information segments. For each provided sequence, compute Chou (2000) quasi-sequence-order (QSO) descriptors only if the sequence is long enough to support the requested correlation depth; set maxlag per target to the largest integer not exceeding one-third of that sequence length, capped at 25. Use a sensitivity-weighting rule tied to sequence complexity: if the sequence contains at least one run of 4 identical residues consecutively, use weight = 0.20; otherwise use weight = 0.08. Sequences: EGFR kinase-domain fragment: MKLPAALGITGATASVVAAE; ACE2 extracellular-domain fragment: MSSSSWLLLSLVAVTAAQSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEETAR. Generate the QSO feature vectors for all sequences meeting the criteria for downstream ingestion.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder\",\"arguments\":{\"ProteinSequence\":\"MSSSSWLLLSLVAVTAAQSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEETAR\",\"maxlag\":21,\"weight\":0.2}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1Grant", "func_desc": "GetQuasiSequenceOrder1Grant computes the first 20 quasi-sequence-order (QSO) descriptors for a protein sequence and returns them as a dictionary suitable for use as protein features in DeepPurpose models (for example DTI, PPI, protein function prediction, or compound–protein interaction feature pipelines). The function combines amino-acid composition with sequence-order coupling information (summed over lags 1..maxlag using a provided amino-acid distance matrix) and normalizes the composition by a factor that depends on the coupling sum and a user-specified weight. The resulting 20 numeric descriptors capture global composition adjusted by sequence-order information and are returned with keys \"QSOgrant1\" .. \"QSOgrant20\" corresponding to the module-level AALetter ordering of the 20 standard amino acids.", "tools": [{"function": {"description": "GetQuasiSequenceOrder1Grant computes the first 20 quasi-sequence-order (QSO) descriptors for a protein sequence and returns them as a dictionary suitable for use as protein features in DeepPurpose models (for example DTI, PPI, protein function prediction, or compound–protein interaction feature pipelines). The function combines amino-acid composition with sequence-order coupling information (summed over lags 1..maxlag using a provided amino-acid distance matrix) and normalizes the composition by a factor that depends on the coupling sum and a user-specified weight. The resulting 20 numeric descriptors capture global composition adjusted by sequence-order information and are returned with keys \"QSOgrant1\" .. \"QSOgrant20\" corresponding to the module-level AALetter ordering of the 20 standard amino acids.\n", "name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1Grant", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "Protein primary sequence represented as a string of one-letter amino-acid codes. This argument is the biological input whose composition and short-range sequence-order statistics are being encoded. The function relies on helper routines GetAAComposition and GetSequenceOrderCouplingNumber that expect standard one-letter amino-acid symbols; nonstandard letters or lowercase input may lead to incorrect counts or downstream lookup errors in the distance matrix.", "default": ""}, "maxlag": {"type": "integer", "description": "Maximum lag (non-negative integer) used when summing sequence-order coupling numbers. The function sums coupling numbers for lags 1 through maxlag (inclusive in effect because the implementation iterates range(maxlag) and calls coupling with lag i+1). Larger maxlag increases the contribution of longer-range sequence-order effects to the normalization term (rightpart) and therefore changes the scaling of the returned QSO descriptors; very large values increase computation and may be biologically less meaningful. The default value is 30.", "default": 30}, "weight": {"type": "float", "description": "Scaling weight applied to the summed sequence-order coupling numbers when forming the normalization denominator temp = 1 + weight * rightpart. This parameter controls the relative influence of sequence-order coupling (rightpart) versus raw amino-acid composition. Typical default is 0.1 (as used in published QSO descriptor formulations); users can increase it to emphasize sequence-order effects or decrease it to reduce their influence.", "default": 0.1}, "distancematrix": {"type": "any", "description": "Pairwise amino-acid distance matrix expressed as a dictionary mapping two-letter keys (concatenated one-letter codes, e.g., \"AG\" for A vs G) to numeric distances. This matrix is used by GetSequenceOrderCouplingNumber to compute coupling between residues separated by a specific lag. The default dictionary supplied to the function provides a precomputed set of distances (as in the module-level _Distance2) for the standard amino acids. The function expects that all residue-pair lookups required by the chosen maxlag and the sequence are present in this dictionary; missing entries will raise a KeyError in the coupling calculation.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "weight", "maxlag"], "type": "any"}}, "type": "function"}], "query": "We’re curating a DTI benchmarking cohort where raw protein FASTA entries may include non-canonical residues from low-confidence regions. Use the Grant-style quasi-sequence-order (QSO) descriptor set (first 20 only; keys QSOgrant1–QSOgrant20; default amino-acid distance matrix) as DeepPurpose-ready features, but only for sequences that are valid single-chain protein strings over the 20 standard amino acids (i.e., contain no ambiguity codes like B/Z/J/X/U/O, no gap/dash characters, and no stop codons such as '*'). For each valid sequence, set weight=0.15 and choose maxlag dynamically as the smaller of 25 and floor(sequence_length/2). Process the following candidate sequences as separate replicates: (a) \"MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\" and (b) \"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAMRDQYMRTGEGFLCVFAINNTKSFEDIHQYREQIKRVKDSDDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPYIETSAKTRQGVEDAFYTLVREIRQHKLRKLNPPDESGPGCMSCKCVLS\". Return the resulting 20 normalized QSO descriptors as dictionaries for downstream ingestion.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1Grant\",\"arguments\":{\"ProteinSequence\":\"MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\",\"maxlag\":20,\"weight\":0.15}},{\"name\":\"DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1Grant\",\"arguments\":{\"ProteinSequence\":\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAMRDQYMRTGEGFLCVFAINNTKSFEDIHQYREQIKRVKDSDDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPYIETSAKTRQGVEDAFYTLVREIRQHKLRKLNPPDESGPGCMSCKCVLS\",\"maxlag\":25,\"weight\":0.15}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1SW", "func_desc": "GetQuasiSequenceOrder1SW computes the first 20 quasi-sequence-order (QSOSW) descriptors for a single protein sequence using a Schneider–Wrede style pairwise amino-acid distance matrix. These descriptors combine normalized amino-acid composition with sequence-order coupling information (summed coupling numbers across lags) and are intended for use as fixed-length protein encodings in DeepPurpose models for tasks such as drug–target interaction (DTI), protein property prediction, PPI, and protein function prediction described in the DeepPurpose README.", "tools": [{"function": {"description": "GetQuasiSequenceOrder1SW computes the first 20 quasi-sequence-order (QSOSW) descriptors for a single protein sequence using a Schneider–Wrede style pairwise amino-acid distance matrix. These descriptors combine normalized amino-acid composition with sequence-order coupling information (summed coupling numbers across lags) and are intended for use as fixed-length protein encodings in DeepPurpose models for tasks such as drug–target interaction (DTI), protein property prediction, PPI, and protein function prediction described in the DeepPurpose README.\n", "name": "DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1SW", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "Protein primary sequence provided as a one-letter amino-acid string (e.g., \"MTEYK...\"). This function uses the sequence to compute amino-acid composition (via GetAAComposition) and to evaluate sequence-order coupling numbers (via GetSequenceOrderCouplingNumber). Practical significance: this argument is the raw biological input representing the target protein whose QSOSW descriptors will be used as features in downstream machine learning models (DTI, virtual screening, repurposing).", "default": ""}, "maxlag": {"type": "integer", "description": "Maximum lag (positive integer) used when summing sequence-order coupling numbers across sequence separations from 1 up to maxlag (the code uses range(maxlag) and passes i+1 to the coupling routine). Default is 30. Role: larger maxlag includes longer-range sequence-order correlations; smaller maxlag focuses on local order. Failure modes: very large values may increase computation time and may produce KeyError or other errors if downstream coupling routines expect sequence length >= lag.", "default": 30}, "weight": {"type": "float", "description": "Weighting factor (default 0.1) that balances the contribution of sequence-order coupling information against raw amino-acid composition. The final amino-acid composition components are normalized by temp = 1 + weight * rightpart, where rightpart is the sum of coupling numbers for all lags. Practical significance: increasing weight emphasizes sequence-order (global/positional) information relative to composition; decreasing weight emphasizes composition. Failure modes: negative or extreme values will change normalization behavior and may produce unexpected encodings; the function does not validate numeric ranges beyond using the value in arithmetic.", "default": 0.1}, "distancematrix": {"type": "any", "description": "Mapping of two-letter amino-acid pair keys (e.g., \"AW\", \"GA\", \"YY\") to float distance values that quantify physicochemical or evolutionary distances between amino-acid types. The function uses this matrix when computing sequence-order coupling numbers (GetSequenceOrderCouplingNumber). The signature default is a pre-defined SW-style distance dictionary (the repository default _Distance1) appropriate for QSOSW calculation. Practical significance: supplying an alternative distance matrix allows using different pairwise metrics; keys must cover all amino-acid pairs present in ProteinSequence. Failure modes: missing keys for any required pair will typically raise a KeyError or propagate errors from GetSequenceOrderCouplingNumber.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "weight", "maxlag"], "type": "any"}}, "type": "function"}], "query": "I’m curating target-protein inputs for a DeepPurpose DTI benchmark and the FASTA headers were lost, so I only have two raw sequences: (A) MGSSHHHHHHSSGLVPRGSHMSTNPKPQRKTKRNTNRRPQDVKFPGG and (B) MKWVTFISLLFLFSSAYS. Before QSOSW encoding, treat sequences as assay-ready only if they look like soluble constructs (i.e., they do not contain an N-terminus consistent with a signal peptide/transmembrane-like hydrophobic stretch). For assay-ready sequences, compute the first 20 Schneider–Wrede QSOSW descriptors using maxlag equal to the smaller of 15 and floor(L/3), and set weight to 0.20 when the sequence contains an N-terminal poly-His tag motif (≥4 consecutive H within the first 12 residues), otherwise use weight 0.15. Return the QSOSW vectors for the sequences that pass these rules.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetQuasiSequenceOrder1SW\",\"arguments\":{\"ProteinSequence\":\"MGSSHHHHHHSSGLVPRGSHMSTNPKPQRKTKRNTNRRPQDVKFPGG\",\"maxlag\":15,\"weight\":0.2}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber", "func_desc": "GetSequenceOrderCouplingNumber computes the d-th rank sequence order coupling number for a protein sequence, producing a single numeric descriptor used in protein encoding for molecular modeling tasks (for example, drug-target interaction prediction, protein–protein interaction prediction, and protein function prediction within the DeepPurpose framework). The function implements the standard sequence order coupling number: it sums the squared pairwise distances between amino acids separated by exactly d positions in the sequence and returns the result rounded to three decimal places. This descriptor captures local sequence-order interactions that are commonly used in pseudo-amino-acid composition and other sequence-based feature sets for QSAR/DTI modeling.", "tools": [{"function": {"description": "GetSequenceOrderCouplingNumber computes the d-th rank sequence order coupling number for a protein sequence, producing a single numeric descriptor used in protein encoding for molecular modeling tasks (for example, drug-target interaction prediction, protein–protein interaction prediction, and protein function prediction within the DeepPurpose framework). The function implements the standard sequence order coupling number: it sums the squared pairwise distances between amino acids separated by exactly d positions in the sequence and returns the result rounded to three decimal places. This descriptor captures local sequence-order interactions that are commonly used in pseudo-amino-acid composition and other sequence-based feature sets for QSAR/DTI modeling.\n", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A protein primary sequence given as a string of single-letter amino acid codes (e.g., \"MTEYK...\"). This sequence is used as the source of residue pairs; each pair is formed by taking a residue at position i and the residue at position i + d for all valid i. Practical significance: the sequence provides the biological input whose local order interactions are quantified for downstream deep learning or statistical models in DeepPurpose.", "default": ""}, "d": {"type": "integer", "description": "The gap (rank) between two amino acids whose pairwise distance is included in the d-th rank coupling number. d should be a positive integer (the code is written assuming d >= 1). Typical use: d = 1 captures immediate neighbor coupling (local interactions), larger d capture longer-range sequence-order correlations. Behavior and defaults: default is 1. If d is greater than or equal to the sequence length, no pairs are summed and the function returns 0.000. Passing d < 1 produces undefined or unintended behavior because negative or zero gaps are not meaningful in the intended biological interpretation.", "default": 1}, "distancematrix": {"type": "any", "description": "A mapping from two-letter amino-acid pair keys (concatenated single-letter codes, e.g., \"AG\", \"YW\") to numeric distance values (floats or ints). This matrix encodes empirical or computed physicochemical distances between residue types and is used to look up the distance for each pair (temp1 + temp2) before squaring and summing. Practical significance: the chosen distance matrix determines what biochemical or structural relationship is being captured by the coupling number. Default: a precomputed dictionary (commonly named _Distance1 in the codebase) that maps all standard residue pairs to numeric distances. Behavior and failure modes: keys must match the exact concatenation of the characters in ProteinSequence (case-sensitive); missing keys will raise KeyError. Values are expected to be numeric; non-numeric values will raise TypeError during arithmetic.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "d", "distancematrix"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our DeepPurpose-compatible protein descriptor pipeline on messy assay inputs. Compute the 3rd-rank sequence order coupling number (d=3) only for peptide records that look like plausible peptides for modeling: uppercase one-letter amino-acid codes only, length at least 9, and no ambiguous/unknown residues (e.g., B, J, O, U, X, Z) or non-letter characters. Use the standard distance convention unless a record is explicitly tagged with a “toy_d3” calibration regime, in which case override the distance lookup with the following minimal custom matrix (only the residue pairs that will actually be queried at d=3 are provided): AE=1.2, CF=0.8, DG=1.5, EH=1.1, FI=0.9, GK=1.3.\n\nRaw cohort (mixed quality):\n- ref_ras_like: MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPT\n- syn_toy_d3: ACDEFGHIK\n- contaminated_1: ACD-EFGHIK\n- ambiguous_2: ACDEXGHIK\n- too_short_3: ACDEFGHI\n\nReturn coupling numbers (rounded to three decimals) for every record that passes QC, applying the appropriate distance convention based on the calibration regime.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber\",\"arguments\":{\"ProteinSequence\":\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPT\",\"d\":3}},{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumber\",\"arguments\":{\"ProteinSequence\":\"ACDEFGHIK\",\"d\":3,\"distancematrix\":{\"AE\":1.2,\"CF\":0.8,\"DG\":1.5,\"EH\":1.1,\"FI\":0.9,\"GK\":1.3}}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberSW", "func_desc": "DeepPurpose.pybiomed_helper.GetSequenceOrderCouplingNumberSW computes sequence-order coupling numbers for a protein sequence using the Schneider–Wrede physicochemical distance matrix and returns them as a dictionary of numeric features. In the DeepPurpose toolkit these coupling numbers are used as handcrafted protein descriptors that capture short-range sequence-order relationships; they are commonly used as input features for downstream models such as drug–target interaction (DTI) predictors, protein property predictors, and other protein-encoding pipelines described in the README.\n    \n    This function iterates lag values from 1 to maxlag and for each lag calls GetSequenceOrderCouplingNumber to compute the Schneider–Wrede coupling number for that lag. The result is a mapping of key names \"tausw1\", \"tausw2\", ..., \"tausw{maxlag}\" to floating-point coupling values that summarize pairwise physicochemical relationships between residues separated by the given lag.", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.GetSequenceOrderCouplingNumberSW computes sequence-order coupling numbers for a protein sequence using the Schneider–Wrede physicochemical distance matrix and returns them as a dictionary of numeric features. In the DeepPurpose toolkit these coupling numbers are used as handcrafted protein descriptors that capture short-range sequence-order relationships; they are commonly used as input features for downstream models such as drug–target interaction (DTI) predictors, protein property predictors, and other protein-encoding pipelines described in the README.\n\nThis function iterates lag values from 1 to maxlag and for each lag calls GetSequenceOrderCouplingNumber to compute the Schneider–Wrede coupling number for that lag. The result is a mapping of key names \"tausw1\", \"tausw2\", ..., \"tausw{maxlag}\" to floating-point coupling values that summarize pairwise physicochemical relationships between residues separated by the given lag.", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberSW", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A single amino-acid sequence string (one-letter codes) representing the protein to be encoded. This sequence is treated as a pure protein sequence (no gaps or non-standard characters). Practical significance: this input is the primary biological sequence from which the Schneider–Wrede sequence-order coupling features are computed for use as model input in DeepPurpose workflows (DTI, protein property prediction, PPI, etc.). The sequence length should be greater than the requested maxlag for meaningful coupling values; otherwise the underlying GetSequenceOrderCouplingNumber call(s) may raise an error.", "default": ""}, "maxlag": {"type": "integer", "description": "Maximum lag (positive integer, default 30) up to which sequence-order coupling numbers are computed. For each integer lag L in 1..maxlag the function computes one coupling number that captures the average physicochemical distance between residues separated by L positions. Practical significance: larger maxlag captures longer-range sequence-order information but increases feature dimensionality and computation cost; set this based on protein length and modeling needs. Default behavior: when omitted, maxlag is 30. Failure modes: non-integer or negative values will result in errors from the implementation or from called helper functions.", "default": 30}, "distancematrix": {"type": "any", "description": "A dictionary implementing the Schneider–Wrede physicochemical distance matrix (default is the module constant _Distance1). Keys are two-letter residue pair strings (for example 'AR', 'GK', etc.) and values are numeric distances (floats). Practical significance: this matrix defines the physicochemical distance measure used to compute coupling numbers; supplying a different valid distance dict allows computing coupling numbers under alternative distance definitions. Failure modes: if required residue-pair keys are missing or values are non-numeric, the computation will fail or raise an exception.", "default": {"GW": 0.923, "GV": 0.464, "GT": 0.272, "GS": 0.158, "GR": 1.0, "GQ": 0.467, "GP": 0.323, "GY": 0.728, "GG": 0.0, "GF": 0.727, "GE": 0.807, "GD": 0.776, "GC": 0.312, "GA": 0.206, "GN": 0.381, "GM": 0.557, "GL": 0.591, "GK": 0.894, "GI": 0.592, "GH": 0.769, "ME": 0.879, "MD": 0.932, "MG": 0.569, "MF": 0.182, "MA": 0.383, "MC": 0.276, "MM": 0.0, "ML": 0.062, "MN": 0.447, "MI": 0.058, "MH": 0.648, "MK": 0.884, "MT": 0.358, "MW": 0.391, "MV": 0.12, "MQ": 0.372, "MP": 0.285, "MS": 0.417, "MR": 1.0, "MY": 0.255, "FP": 0.42, "FQ": 0.459, "FR": 1.0, "FS": 0.548, "FT": 0.499, "FV": 0.252, "FW": 0.207, "FY": 0.179, "FA": 0.508, "FC": 0.405, "FD": 0.977, "FE": 0.918, "FF": 0.0, "FG": 0.69, "FH": 0.663, "FI": 0.128, "FK": 0.903, "FL": 0.131, "FM": 0.169, "FN": 0.541, "SY": 0.615, "SS": 0.0, "SR": 1.0, "SQ": 0.358, "SP": 0.181, "SW": 0.827, "SV": 0.342, "ST": 0.174, "SK": 0.883, "SI": 0.478, "SH": 0.718, "SN": 0.289, "SM": 0.44, "SL": 0.474, "SC": 0.185, "SA": 0.1, "SG": 0.17, "SF": 0.622, "SE": 0.812, "SD": 0.801, "YI": 0.23, "YH": 0.678, "YK": 0.904, "YM": 0.268, "YL": 0.219, "YN": 0.512, "YA": 0.587, "YC": 0.478, "YE": 0.932, "YD": 1.0, "YG": 0.782, "YF": 0.202, "YY": 0.0, "YQ": 0.404, "YP": 0.444, "YS": 0.612, "YR": 0.995, "YT": 0.557, "YW": 0.244, "YV": 0.328, "LF": 0.139, "LG": 0.596, "LD": 0.944, "LE": 0.892, "LC": 0.296, "LA": 0.405, "LN": 0.452, "LL": 0.0, "LM": 0.062, "LK": 0.893, "LH": 0.653, "LI": 0.013, "LV": 0.133, "LW": 0.341, "LT": 0.397, "LR": 1.0, "LS": 0.443, "LP": 0.309, "LQ": 0.376, "LY": 0.205, "RT": 0.808, "RV": 0.914, "RW": 1.0, "RP": 0.796, "RQ": 0.668, "RR": 0.0, "RS": 0.86, "RY": 0.859, "RD": 0.305, "RE": 0.225, "RF": 0.977, "RG": 0.928, "RA": 0.919, "RC": 0.905, "RL": 0.92, "RM": 0.908, "RN": 0.69, "RH": 0.498, "RI": 0.929, "RK": 0.141, "VH": 0.649, "VI": 0.135, "EM": 0.83, "EL": 0.854, "EN": 0.599, "EI": 0.86, "EH": 0.406, "EK": 0.143, "EE": 0.0, "ED": 0.133, "EG": 0.779, "EF": 0.932, "EA": 0.79, "EC": 0.788, "VM": 0.12, "EY": 0.837, "VN": 0.38, "ET": 0.682, "EW": 1.0, "EV": 0.824, "EQ": 0.598, "EP": 0.688, "ES": 0.726, "ER": 0.234, "VP": 0.212, "VQ": 0.339, "VR": 1.0, "VT": 0.305, "VW": 0.472, "KC": 0.871, "KA": 0.889, "KG": 0.9, "KF": 0.957, "KE": 0.149, "KD": 0.279, "KK": 0.0, "KI": 0.899, "KH": 0.438, "KN": 0.667, "KM": 0.871, "KL": 0.892, "KS": 0.825, "KR": 0.154, "KQ": 0.639, "KP": 0.757, "KW": 1.0, "KV": 0.882, "KT": 0.759, "KY": 0.848, "DN": 0.56, "DL": 0.841, "DM": 0.819, "DK": 0.249, "DH": 0.435, "DI": 0.847, "DF": 0.924, "DG": 0.697, "DD": 0.0, "DE": 0.124, "DC": 0.742, "DA": 0.729, "DY": 0.836, "DV": 0.797, "DW": 1.0, "DT": 0.649, "DR": 0.295, "DS": 0.667, "DP": 0.657, "DQ": 0.584, "QQ": 0.0, "QP": 0.272, "QS": 0.461, "QR": 1.0, "QT": 0.389, "QW": 0.831, "QV": 0.464, "QY": 0.522, "QA": 0.512, "QC": 0.462, "QE": 0.861, "QD": 0.903, "QG": 0.648, "QF": 0.671, "QI": 0.532, "QH": 0.765, "QK": 0.881, "QM": 0.505, "QL": 0.518, "QN": 0.181, "WG": 0.829, "WF": 0.196, "WE": 0.931, "WD": 1.0, "WC": 0.56, "WA": 0.658, "WN": 0.631, "WM": 0.344, "WL": 0.304, "WK": 0.892, "WI": 0.305, "WH": 0.678, "WW": 0.0, "WV": 0.418, "WT": 0.638, "WS": 0.689, "WR": 0.968, "WQ": 0.538, "WP": 0.555, "WY": 0.204, "PR": 1.0, "PS": 0.196, "PP": 0.0, "PQ": 0.228, "PV": 0.244, "PW": 0.72, "PT": 0.161, "PY": 0.481, "PC": 0.179, "PA": 0.22, "PF": 0.515, "PG": 0.376, "PD": 0.852, "PE": 0.831, "PK": 0.875, "PH": 0.696, "PI": 0.363, "PN": 0.231, "PL": 0.357, "PM": 0.326, "CK": 0.887, "CI": 0.304, "CH": 0.66, "CN": 0.324, "CM": 0.277, "CL": 0.301, "CC": 0.0, "CA": 0.114, "CG": 0.32, "CF": 0.437, "CE": 0.838, "CD": 0.847, "CY": 0.457, "CS": 0.176, "CR": 1.0, "CQ": 0.341, "CP": 0.157, "CW": 0.639, "CV": 0.167, "CT": 0.233, "IY": 0.213, "VA": 0.275, "VC": 0.165, "VD": 0.9, "VE": 0.867, "VF": 0.269, "VG": 0.471, "IQ": 0.383, "IP": 0.311, "IS": 0.443, "IR": 1.0, "VL": 0.134, "IT": 0.396, "IW": 0.339, "IV": 0.133, "II": 0.0, "IH": 0.652, "IK": 0.892, "VS": 0.322, "IM": 0.057, "IL": 0.013, "VV": 0.0, "IN": 0.457, "IA": 0.403, "VY": 0.31, "IC": 0.296, "IE": 0.891, "ID": 0.942, "IG": 0.592, "IF": 0.134, "HY": 0.821, "HR": 0.697, "HS": 0.865, "HP": 0.777, "HQ": 0.716, "HV": 0.831, "HW": 0.981, "HT": 0.834, "HK": 0.566, "HH": 0.0, "HI": 0.848, "HN": 0.754, "HL": 0.842, "HM": 0.825, "HC": 0.836, "HA": 0.896, "HF": 0.907, "HG": 1.0, "HD": 0.629, "HE": 0.547, "NH": 0.78, "NI": 0.615, "NK": 0.891, "NL": 0.603, "NM": 0.588, "NN": 0.0, "NA": 0.424, "NC": 0.425, "ND": 0.838, "NE": 0.835, "NF": 0.766, "NG": 0.512, "NY": 0.641, "NP": 0.266, "NQ": 0.175, "NR": 1.0, "NS": 0.361, "NT": 0.368, "NV": 0.503, "NW": 0.945, "TY": 0.596, "TV": 0.345, "TW": 0.816, "TT": 0.0, "TR": 1.0, "TS": 0.185, "TP": 0.159, "TQ": 0.322, "TN": 0.315, "TL": 0.453, "TM": 0.403, "TK": 0.866, "TH": 0.737, "TI": 0.455, "TF": 0.604, "TG": 0.312, "TD": 0.83, "TE": 0.812, "TC": 0.261, "TA": 0.251, "AA": 0.0, "AC": 0.112, "AE": 0.827, "AD": 0.819, "AG": 0.208, "AF": 0.54, "AI": 0.407, "AH": 0.696, "AK": 0.891, "AM": 0.379, "AL": 0.406, "AN": 0.318, "AQ": 0.372, "AP": 0.191, "AS": 0.094, "AR": 1.0, "AT": 0.22, "AW": 0.739, "AV": 0.273, "AY": 0.552, "VK": 0.889}}}, "required": ["ProteinSequence", "maxlag"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our DTI/kinase-target protein featurization stage with Schneider–Wrede sequence-order coupling numbers under realistic curation rules. We have three FASTA-like inputs (some are domain fragments). For each protein, first sanitize the sequence by keeping only canonical amino-acid letters (ACDEFGHIKLMNPQRSTVWY) and preserving order. Then decide the coupling horizon per sequence based on its intrinsic length: use a coupling maxlag equal to the smaller of 10 and ⌊L/7⌋, where L is the sanitized sequence length. Additionally, to avoid unstable short-range descriptors, only compute features for sequences whose sanitized length is at least 30 residues. For each qualifying sequence, compute the full Schneider–Wrede coupling dictionary (tausw1…tausw{maxlag}). Inputs: (a) human insulin B-chain: \"FVNQHLCGSHLVEALYLVCGERGFFYTPKT\"; (b) human EGFR kinase domain: \"TKVGSGAFGTVYKGLWIPEGEKVKIPVAIKELREATSPKANKEILDEAYVMASVDNPHVCRLLGICLTSTVQLITQLMPFGCLLDYVREHKDNIGSQYLVVDFGLSRLMEGQWVIYKEVTRMVRDLKPANLVIITEYCCYGDLLNFLRRR\"; (c) EGFR kinase-domain fragment: \"KVLGSGAFGTVYKGLWIPEGEKVKINLKEGDLQVAVKTLKEDTMEVEEKMFLRLAKIASDMHYA\".", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberSW\",\"arguments\":{\"ProteinSequence\":\"FVNQHLCGSHLVEALYLVCGERGFFYTPKT\",\"maxlag\":4}},{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberSW\",\"arguments\":{\"ProteinSequence\":\"TKVGSGAFGTVYKGLWIPEGEKVKIPVAIKELREATSPKANKEILDEAYVMASVDNPHVCRLLGICLTSTVQLITQLMPFGCLLDYVREHKDNIGSQYLVVDFGLSRLMEGQWVIYKEVTRMVRDLKPANLVIITEYCCYGDLLNFLRRR\",\"maxlag\":10}},{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberSW\",\"arguments\":{\"ProteinSequence\":\"KVLGSGAFGTVYKGLWIPEGEKVKINLKEGDLQVAVKTLKEDTMEVEEKMFLRLAKIASDMHYA\",\"maxlag\":9}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberTotal", "func_desc": "GetSequenceOrderCouplingNumberTotal computes sequence-order coupling numbers for a protein sequence by aggregating two established coupling-number computations (Schneider–Wrede style and Grant style) for lags 1..maxlag. In the DeepPurpose toolkit this function produces numeric sequence-order descriptors used as protein encodings for downstream tasks such as drug–target interaction (DTI) prediction, protein–protein interaction (PPI) prediction, and protein function prediction; these descriptors capture correlations of physicochemical properties between residues separated by specific sequence distances (lags) and are intended as features for machine learning models (for example, as part of DeepPurpose model input pipelines).", "tools": [{"function": {"description": "GetSequenceOrderCouplingNumberTotal computes sequence-order coupling numbers for a protein sequence by aggregating two established coupling-number computations (Schneider–Wrede style and Grant style) for lags 1..maxlag. In the DeepPurpose toolkit this function produces numeric sequence-order descriptors used as protein encodings for downstream tasks such as drug–target interaction (DTI) prediction, protein–protein interaction (PPI) prediction, and protein function prediction; these descriptors capture correlations of physicochemical properties between residues separated by specific sequence distances (lags) and are intended as features for machine learning models (for example, as part of DeepPurpose model input pipelines).\n", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberTotal", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein sequence string composed of standard amino acid single-letter codes (e.g., \"ACDEFGHIK...\"). This argument is the primary biological input whose residue order and composition determine the computed coupling numbers. The sequence should not contain non-amino-acid characters (such as digits, whitespace, or punctuation). The sequence length is required to be larger than maxlag for full results; if it is not, the behavior depends on the downstream helper functions and may raise an error or produce only a subset of expected coupling values.", "default": ""}, "maxlag": {"type": "integer", "description": "The maximum lag (positive integer, default 30) for which to compute sequence-order coupling numbers. A lag value L means computing coupling statistics between residues separated by L positions in the linear sequence. The default of 30 is chosen to capture medium-range sequence-order correlations commonly used in protein descriptor sets; increasing maxlag will produce more features and increase computation time and memory usage proportionally.", "default": 30}}, "required": ["ProteinSequence", "maxlag"], "type": "any"}}, "type": "function"}], "query": "We’re running a QC-aware DeepPurpose protein-encoding pass on two candidate target sequences coming from different assays (a short disulfide-rich peptide vs. a longer helical fragment). Generate total sequence-order coupling number descriptors (Schneider–Wrede + Grant aggregated) only for sequences that contain exclusively the 20 standard amino-acid letters (no ambiguous residues). For each eligible sequence, set maxlag dynamically to the smaller of: (a) 15, or (b) floor(sequence length / 2). Apply this rule to the following raw inputs: (1) FVNQHLCGSHLVEALYLVCGERGFFYTPKT, (2) VAIKVLKDLKNNLKEKLAQLQKSN. Return the descriptor sets for the eligible sequences.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberTotal\",\"arguments\":{\"ProteinSequence\":\"FVNQHLCGSHLVEALYLVCGERGFFYTPKT\",\"maxlag\":15}},{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberTotal\",\"arguments\":{\"ProteinSequence\":\"VAIKVLKDLKNNLKEKLAQLQKSN\",\"maxlag\":11}}]"}
{"func_name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberp", "func_desc": "DeepPurpose.pybiomed_helper.GetSequenceOrderCouplingNumberp computes sequence order coupling numbers tau1..tauN for a protein sequence using a user-supplied amino-acid property distance matrix. These coupling numbers quantify correlations between residues separated by specific sequence lags and are used as sequence-order features in DeepPurpose workflows for drug-target interaction (DTI), protein–protein interaction (PPI), protein function prediction, virtual screening, and other protein-encoding tasks.", "tools": [{"function": {"description": "DeepPurpose.pybiomed_helper.GetSequenceOrderCouplingNumberp computes sequence order coupling numbers tau1..tauN for a protein sequence using a user-supplied amino-acid property distance matrix. These coupling numbers quantify correlations between residues separated by specific sequence lags and are used as sequence-order features in DeepPurpose workflows for drug-target interaction (DTI), protein–protein interaction (PPI), protein function prediction, virtual screening, and other protein-encoding tasks.\n", "name": "DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberp", "parameters": {"properties": {"ProteinSequence": {"type": "string", "description": "A pure protein primary sequence string composed of amino-acid single-letter codes. This is the input sequence whose sequence-order coupling numbers will be computed. The function assumes standard amino-acid codes; nonstandard characters or whitespace may cause downstream lookup or indexing errors when accessing the distancematrix.", "default": ""}, "maxlag": {"type": "integer", "description": "The maximum lag (positive integer) to compute. The function computes coupling numbers for all integer lags from 1 up to maxlag inclusive and returns maxlag entries named \"tau1\" through \"tau{maxlag}\". Default is 30. Practically, maxlag should be smaller than the sequence length (see failure modes) and should be chosen to capture the desired-range residue correlations for downstream machine-learning feature sets.", "default": 30}, "distancematrix": {"type": "any", "description": "A dictionary containing pairwise property distances for amino-acid residue pairs used to define the coupling between two positions. In typical use this dict holds 400 numeric entries corresponding to pairwise distances for the 20 standard amino acids (20 x 20 = 400). The function uses these values to compute the property-based correlation at each lag by delegating to GetSequenceOrderCouplingNumber for each lag. If the distancematrix is empty or missing required pair keys, lookups during computation will raise KeyError; therefore supply a complete pairwise distance mapping when using property-based coupling.", "default": {}}}, "required": ["ProteinSequence", "maxlag", "distancematrix"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a DeepPurpose protein-encoding submodule on a messy target manifest assembled from mixed sources (soluble domains, TM fragments, and low-quality predicted peptides). Each entry is a putative amino-acid sequence and may contain ambiguous residues (B, Z, J, U, O, X), stop symbols (*), or alignment gaps (-). For sequence-order coupling (SOCN) benchmarking, do the following:\n\n1) Quality sieve: only compute SOCN for sequences that contain exclusively the 20 canonical amino acids (ACDEFGHIKLMNPQRSTVWY) and have length at least 12.\n\n2) Lag selection rule: set maxlag = 5 for sequences with length >= 30; otherwise set maxlag = 3.\n\n3) Distance-matrix branching: use the integer-valued 0/1 identity/difference matrix when the fraction of hydrophobic residues (A,V,I,L,M,F,W,Y) is >= 0.40; otherwise use the float-valued 0.0/1.0 identity/difference matrix.\n\nApply this to the following raw cohort manifest (treat each as a separate sample):\n- MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\n- MKVLYNLVGLANFVAATATATK\n- M--KWVTFISLLFLFSSAYSRG\n- MKVLYN*LVGLANFVAATATATK\n- ACDXEFGHIKLMNPQRSTVWY\n- GGGGGGGGGGG\n\nReturn tau1..tauN for each retained sequence using its computed maxlag and the appropriate distance matrix per the rule above so we can plug these into downstream DTI modeling and compare soluble-like vs membrane-like sequence-order effects.", "answers": "[{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberp\",\"arguments\":{\"ProteinSequence\":\"MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\",\"maxlag\":5,\"distancematrix\":{\"AA\":0,\"AC\":1,\"AD\":1,\"AE\":1,\"AF\":1,\"AG\":1,\"AH\":1,\"AI\":1,\"AK\":1,\"AL\":1,\"AM\":1,\"AN\":1,\"AP\":1,\"AQ\":1,\"AR\":1,\"AS\":1,\"AT\":1,\"AV\":1,\"AW\":1,\"AY\":1,\"CA\":1,\"CC\":0,\"CD\":1,\"CE\":1,\"CF\":1,\"CG\":1,\"CH\":1,\"CI\":1,\"CK\":1,\"CL\":1,\"CM\":1,\"CN\":1,\"CP\":1,\"CQ\":1,\"CR\":1,\"CS\":1,\"CT\":1,\"CV\":1,\"CW\":1,\"CY\":1,\"DA\":1,\"DC\":1,\"DD\":0,\"DE\":1,\"DF\":1,\"DG\":1,\"DH\":1,\"DI\":1,\"DK\":1,\"DL\":1,\"DM\":1,\"DN\":1,\"DP\":1,\"DQ\":1,\"DR\":1,\"DS\":1,\"DT\":1,\"DV\":1,\"DW\":1,\"DY\":1,\"EA\":1,\"EC\":1,\"ED\":1,\"EE\":0,\"EF\":1,\"EG\":1,\"EH\":1,\"EI\":1,\"EK\":1,\"EL\":1,\"EM\":1,\"EN\":1,\"EP\":1,\"EQ\":1,\"ER\":1,\"ES\":1,\"ET\":1,\"EV\":1,\"EW\":1,\"EY\":1,\"FA\":1,\"FC\":1,\"FD\":1,\"FE\":1,\"FF\":0,\"FG\":1,\"FH\":1,\"FI\":1,\"FK\":1,\"FL\":1,\"FM\":1,\"FN\":1,\"FP\":1,\"FQ\":1,\"FR\":1,\"FS\":1,\"FT\":1,\"FV\":1,\"FW\":1,\"FY\":1,\"GA\":1,\"GC\":1,\"GD\":1,\"GE\":1,\"GF\":1,\"GG\":0,\"GH\":1,\"GI\":1,\"GK\":1,\"GL\":1,\"GM\":1,\"GN\":1,\"GP\":1,\"GQ\":1,\"GR\":1,\"GS\":1,\"GT\":1,\"GV\":1,\"GW\":1,\"GY\":1,\"HA\":1,\"HC\":1,\"HD\":1,\"HE\":1,\"HF\":1,\"HG\":1,\"HH\":0,\"HI\":1,\"HK\":1,\"HL\":1,\"HM\":1,\"HN\":1,\"HP\":1,\"HQ\":1,\"HR\":1,\"HS\":1,\"HT\":1,\"HV\":1,\"HW\":1,\"HY\":1,\"IA\":1,\"IC\":1,\"ID\":1,\"IE\":1,\"IF\":1,\"IG\":1,\"IH\":1,\"II\":0,\"IK\":1,\"IL\":1,\"IM\":1,\"IN\":1,\"IP\":1,\"IQ\":1,\"IR\":1,\"IS\":1,\"IT\":1,\"IV\":1,\"IW\":1,\"IY\":1,\"KA\":1,\"KC\":1,\"KD\":1,\"KE\":1,\"KF\":1,\"KG\":1,\"KH\":1,\"KI\":1,\"KK\":0,\"KL\":1,\"KM\":1,\"KN\":1,\"KP\":1,\"KQ\":1,\"KR\":1,\"KS\":1,\"KT\":1,\"KV\":1,\"KW\":1,\"KY\":1,\"LA\":1,\"LC\":1,\"LD\":1,\"LE\":1,\"LF\":1,\"LG\":1,\"LH\":1,\"LI\":1,\"LK\":1,\"LL\":0,\"LM\":1,\"LN\":1,\"LP\":1,\"LQ\":1,\"LR\":1,\"LS\":1,\"LT\":1,\"LV\":1,\"LW\":1,\"LY\":1,\"MA\":1,\"MC\":1,\"MD\":1,\"ME\":1,\"MF\":1,\"MG\":1,\"MH\":1,\"MI\":1,\"MK\":1,\"ML\":1,\"MM\":0,\"MN\":1,\"MP\":1,\"MQ\":1,\"MR\":1,\"MS\":1,\"MT\":1,\"MV\":1,\"MW\":1,\"MY\":1,\"NA\":1,\"NC\":1,\"ND\":1,\"NE\":1,\"NF\":1,\"NG\":1,\"NH\":1,\"NI\":1,\"NK\":1,\"NL\":1,\"NM\":1,\"NN\":0,\"NP\":1,\"NQ\":1,\"NR\":1,\"NS\":1,\"NT\":1,\"NV\":1,\"NW\":1,\"NY\":1,\"PA\":1,\"PC\":1,\"PD\":1,\"PE\":1,\"PF\":1,\"PG\":1,\"PH\":1,\"PI\":1,\"PK\":1,\"PL\":1,\"PM\":1,\"PN\":1,\"PP\":0,\"PQ\":1,\"PR\":1,\"PS\":1,\"PT\":1,\"PV\":1,\"PW\":1,\"PY\":1,\"QA\":1,\"QC\":1,\"QD\":1,\"QE\":1,\"QF\":1,\"QG\":1,\"QH\":1,\"QI\":1,\"QK\":1,\"QL\":1,\"QM\":1,\"QN\":1,\"QP\":1,\"QQ\":0,\"QR\":1,\"QS\":1,\"QT\":1,\"QV\":1,\"QW\":1,\"QY\":1,\"RA\":1,\"RC\":1,\"RD\":1,\"RE\":1,\"RF\":1,\"RG\":1,\"RH\":1,\"RI\":1,\"RK\":1,\"RL\":1,\"RM\":1,\"RN\":1,\"RP\":1,\"RQ\":1,\"RR\":0,\"RS\":1,\"RT\":1,\"RV\":1,\"RW\":1,\"RY\":1,\"SA\":1,\"SC\":1,\"SD\":1,\"SE\":1,\"SF\":1,\"SG\":1,\"SH\":1,\"SI\":1,\"SK\":1,\"SL\":1,\"SM\":1,\"SN\":1,\"SP\":1,\"SQ\":1,\"SR\":1,\"SS\":0,\"ST\":1,\"SV\":1,\"SW\":1,\"SY\":1,\"TA\":1,\"TC\":1,\"TD\":1,\"TE\":1,\"TF\":1,\"TG\":1,\"TH\":1,\"TI\":1,\"TK\":1,\"TL\":1,\"TM\":1,\"TN\":1,\"TP\":1,\"TQ\":1,\"TR\":1,\"TS\":1,\"TT\":0,\"TV\":1,\"TW\":1,\"TY\":1,\"VA\":1,\"VC\":1,\"VD\":1,\"VE\":1,\"VF\":1,\"VG\":1,\"VH\":1,\"VI\":1,\"VK\":1,\"VL\":1,\"VM\":1,\"VN\":1,\"VP\":1,\"VQ\":1,\"VR\":1,\"VS\":1,\"VT\":1,\"VV\":0,\"VW\":1,\"VY\":1,\"WA\":1,\"WC\":1,\"WD\":1,\"WE\":1,\"WF\":1,\"WG\":1,\"WH\":1,\"WI\":1,\"WK\":1,\"WL\":1,\"WM\":1,\"WN\":1,\"WP\":1,\"WQ\":1,\"WR\":1,\"WS\":1,\"WT\":1,\"WV\":1,\"WW\":0,\"WY\":1,\"YA\":1,\"YC\":1,\"YD\":1,\"YE\":1,\"YF\":1,\"YG\":1,\"YH\":1,\"YI\":1,\"YK\":1,\"YL\":1,\"YM\":1,\"YN\":1,\"YP\":1,\"YQ\":1,\"YR\":1,\"YS\":1,\"YT\":1,\"YV\":1,\"YW\":1,\"YY\":0}}},{\"name\":\"DeepPurpose_pybiomed_helper_GetSequenceOrderCouplingNumberp\",\"arguments\":{\"ProteinSequence\":\"MKVLYNLVGLANFVAATATATK\",\"maxlag\":3,\"distancematrix\":{\"AA\":0,\"AC\":1,\"AD\":1,\"AE\":1,\"AF\":1,\"AG\":1,\"AH\":1,\"AI\":1,\"AK\":1,\"AL\":1,\"AM\":1,\"AN\":1,\"AP\":1,\"AQ\":1,\"AR\":1,\"AS\":1,\"AT\":1,\"AV\":1,\"AW\":1,\"AY\":1,\"CA\":1,\"CC\":0,\"CD\":1,\"CE\":1,\"CF\":1,\"CG\":1,\"CH\":1,\"CI\":1,\"CK\":1,\"CL\":1,\"CM\":1,\"CN\":1,\"CP\":1,\"CQ\":1,\"CR\":1,\"CS\":1,\"CT\":1,\"CV\":1,\"CW\":1,\"CY\":1,\"DA\":1,\"DC\":1,\"DD\":0,\"DE\":1,\"DF\":1,\"DG\":1,\"DH\":1,\"DI\":1,\"DK\":1,\"DL\":1,\"DM\":1,\"DN\":1,\"DP\":1,\"DQ\":1,\"DR\":1,\"DS\":1,\"DT\":1,\"DV\":1,\"DW\":1,\"DY\":1,\"EA\":1,\"EC\":1,\"ED\":1,\"EE\":0,\"EF\":1,\"EG\":1,\"EH\":1,\"EI\":1,\"EK\":1,\"EL\":1,\"EM\":1,\"EN\":1,\"EP\":1,\"EQ\":1,\"ER\":1,\"ES\":1,\"ET\":1,\"EV\":1,\"EW\":1,\"EY\":1,\"FA\":1,\"FC\":1,\"FD\":1,\"FE\":1,\"FF\":0,\"FG\":1,\"FH\":1,\"FI\":1,\"FK\":1,\"FL\":1,\"FM\":1,\"FN\":1,\"FP\":1,\"FQ\":1,\"FR\":1,\"FS\":1,\"FT\":1,\"FV\":1,\"FW\":1,\"FY\":1,\"GA\":1,\"GC\":1,\"GD\":1,\"GE\":1,\"GF\":1,\"GG\":0,\"GH\":1,\"GI\":1,\"GK\":1,\"GL\":1,\"GM\":1,\"GN\":1,\"GP\":1,\"GQ\":1,\"GR\":1,\"GS\":1,\"GT\":1,\"GV\":1,\"GW\":1,\"GY\":1,\"HA\":1,\"HC\":1,\"HD\":1,\"HE\":1,\"HF\":1,\"HG\":1,\"HH\":0,\"HI\":1,\"HK\":1,\"HL\":1,\"HM\":1,\"HN\":1,\"HP\":1,\"HQ\":1,\"HR\":1,\"HS\":1,\"HT\":1,\"HV\":1,\"HW\":1,\"HY\":1,\"IA\":1,\"IC\":1,\"ID\":1,\"IE\":1,\"IF\":1,\"IG\":1,\"IH\":1,\"II\":0,\"IK\":1,\"IL\":1,\"IM\":1,\"IN\":1,\"IP\":1,\"IQ\":1,\"IR\":1,\"IS\":1,\"IT\":1,\"IV\":1,\"IW\":1,\"IY\":1,\"KA\":1,\"KC\":1,\"KD\":1,\"KE\":1,\"KF\":1,\"KG\":1,\"KH\":1,\"KI\":1,\"KK\":0,\"KL\":1,\"KM\":1,\"KN\":1,\"KP\":1,\"KQ\":1,\"KR\":1,\"KS\":1,\"KT\":1,\"KV\":1,\"KW\":1,\"KY\":1,\"LA\":1,\"LC\":1,\"LD\":1,\"LE\":1,\"LF\":1,\"LG\":1,\"LH\":1,\"LI\":1,\"LK\":1,\"LL\":0,\"LM\":1,\"LN\":1,\"LP\":1,\"LQ\":1,\"LR\":1,\"LS\":1,\"LT\":1,\"LV\":1,\"LW\":1,\"LY\":1,\"MA\":1,\"MC\":1,\"MD\":1,\"ME\":1,\"MF\":1,\"MG\":1,\"MH\":1,\"MI\":1,\"MK\":1,\"ML\":1,\"MM\":0,\"MN\":1,\"MP\":1,\"MQ\":1,\"MR\":1,\"MS\":1,\"MT\":1,\"MV\":1,\"MW\":1,\"MY\":1,\"NA\":1,\"NC\":1,\"ND\":1,\"NE\":1,\"NF\":1,\"NG\":1,\"NH\":1,\"NI\":1,\"NK\":1,\"NL\":1,\"NM\":1,\"NN\":0,\"NP\":1,\"NQ\":1,\"NR\":1,\"NS\":1,\"NT\":1,\"NV\":1,\"NW\":1,\"NY\":1,\"PA\":1,\"PC\":1,\"PD\":1,\"PE\":1,\"PF\":1,\"PG\":1,\"PH\":1,\"PI\":1,\"PK\":1,\"PL\":1,\"PM\":1,\"PN\":1,\"PP\":0,\"PQ\":1,\"PR\":1,\"PS\":1,\"PT\":1,\"PV\":1,\"PW\":1,\"PY\":1,\"QA\":1,\"QC\":1,\"QD\":1,\"QE\":1,\"QF\":1,\"QG\":1,\"QH\":1,\"QI\":1,\"QK\":1,\"QL\":1,\"QM\":1,\"QN\":1,\"QP\":1,\"QQ\":0,\"QR\":1,\"QS\":1,\"QT\":1,\"QV\":1,\"QW\":1,\"QY\":1,\"RA\":1,\"RC\":1,\"RD\":1,\"RE\":1,\"RF\":1,\"RG\":1,\"RH\":1,\"RI\":1,\"RK\":1,\"RL\":1,\"RM\":1,\"RN\":1,\"RP\":1,\"RQ\":1,\"RR\":0,\"RS\":1,\"RT\":1,\"RV\":1,\"RW\":1,\"RY\":1,\"SA\":1,\"SC\":1,\"SD\":1,\"SE\":1,\"SF\":1,\"SG\":1,\"SH\":1,\"SI\":1,\"SK\":1,\"SL\":1,\"SM\":1,\"SN\":1,\"SP\":1,\"SQ\":1,\"SR\":1,\"SS\":0,\"ST\":1,\"SV\":1,\"SW\":1,\"SY\":1,\"TA\":1,\"TC\":1,\"TD\":1,\"TE\":1,\"TF\":1,\"TG\":1,\"TH\":1,\"TI\":1,\"TK\":1,\"TL\":1,\"TM\":1,\"TN\":1,\"TP\":1,\"TQ\":1,\"TR\":1,\"TS\":1,\"TT\":0,\"TV\":1,\"TW\":1,\"TY\":1,\"VA\":1,\"VC\":1,\"VD\":1,\"VE\":1,\"VF\":1,\"VG\":1,\"VH\":1,\"VI\":1,\"VK\":1,\"VL\":1,\"VM\":1,\"VN\":1,\"VP\":1,\"VQ\":1,\"VR\":1,\"VS\":1,\"VT\":1,\"VV\":0,\"VW\":1,\"VY\":1,\"WA\":1,\"WC\":1,\"WD\":1,\"WE\":1,\"WF\":1,\"WG\":1,\"WH\":1,\"WI\":1,\"WK\":1,\"WL\":1,\"WM\":1,\"WN\":1,\"WP\":1,\"WQ\":1,\"WR\":1,\"WS\":1,\"WT\":1,\"WV\":1,\"WW\":0,\"WY\":1,\"YA\":1,\"YC\":1,\"YD\":1,\"YE\":1,\"YF\":1,\"YG\":1,\"YH\":1,\"YI\":1,\"YK\":1,\"YL\":1,\"YM\":1,\"YN\":1,\"YP\":1,\"YQ\":1,\"YR\":1,\"YS\":1,\"YT\":1,\"YV\":1,\"YW\":1,\"YY\":0}}}]"}
{"func_name": "DeepPurpose_utils_smiles2mpnnfeature", "func_desc": "DeepPurpose.utils.smiles2mpnnfeature converts a SMILES string into the set of padded torch tensors required as input features for the message-passing neural network (MPNN) encoder used across DeepPurpose (e.g., data_process -> mpnn_collate_func -> mpnn_feature_collate_func -> encoders.MPNN.forward). It parses the molecule, computes per-atom and per-bond feature vectors, builds directed-bond and bond-neighbor adjacency tensors, pads these arrays to the library's fixed maximum sizes (using MAX_ATOM, MAX_BOND, MAX_NB), and returns them together with a shape tensor that records the original (un-padded) atom and bond counts. This function is used in drug/compound encoding for tasks such as drug-target interaction prediction, virtual screening, and other molecular modeling workflows in DeepPurpose.", "tools": [{"function": {"description": "DeepPurpose.utils.smiles2mpnnfeature converts a SMILES string into the set of padded torch tensors required as input features for the message-passing neural network (MPNN) encoder used across DeepPurpose (e.g., data_process -> mpnn_collate_func -> mpnn_feature_collate_func -> encoders.MPNN.forward). It parses the molecule, computes per-atom and per-bond feature vectors, builds directed-bond and bond-neighbor adjacency tensors, pads these arrays to the library's fixed maximum sizes (using MAX_ATOM, MAX_BOND, MAX_NB), and returns them together with a shape tensor that records the original (un-padded) atom and bond counts. This function is used in drug/compound encoding for tasks such as drug-target interaction prediction, virtual screening, and other molecular modeling workflows in DeepPurpose.\n", "name": "DeepPurpose_utils_smiles2mpnnfeature", "parameters": {"properties": {"smiles": {"type": "string", "description": "A single-molecule SMILES string. This is the canonical input representation for this utility: a text string encoding the molecule to be converted into MPNN features. The function uses get_mol(smiles) (RDKit-backed in this repository) to parse the SMILES into a molecular object, then extracts atomic and bond-level descriptors via atom_features(...) and bond_features(...). In the DeepPurpose pipeline, this function is applied element-wise to drug SMILES during data preprocessing (data_process) to produce tensors that downstream MPNN encoders consume.", "default": ""}}, "required": ["smiles"], "type": "any"}}, "type": "function"}], "query": "In our DTI benchmarking pipeline, we need to stress-test the DeepPurpose MPNN compound featurization step against common chemical-registration artifacts. We have two independent replicate SMILES records that may contain inconsistent aromaticity casing and/or extraneous whitespace from upstream exports: (i) a xanthine-like scaffold submitted in aromatic form (potentially with mixed-case aromatic tokens) and (ii) a benzoate/acetylated salicylate submitted in kekulized form. For each record, first canonicalize the string by trimming leading/trailing whitespace; then decide the featurization path based on intrinsic SMILES properties: if the SMILES contains any lowercase aromatic atoms (e.g., 'c', 'n', 'o', 's'), featurize it as-is; if it contains no lowercase aromatic atoms, generate a second replicate by converting only atom symbols to lowercase (leave digits, bond symbols, and parentheses unchanged) and featurize both the original and the lowercased replicate so we can quantify encoder sensitivity to aromaticity representation. Use DeepPurpose.utils.smiles2mpnnfeature to produce the full padded MPNN tensors (atom/bond features, directed-bond and bond-neighbor adjacency, plus shape tensor) for every SMILES that qualifies under this rule. Raw inputs: caffeine: \"  Cn1cnc2n(C)c(=O)n(C)c(=O)c12  \" and aspirin: \"CC(=O)OC1=CC=CC=C1C(=O)O\".", "answers": "[{\"name\":\"DeepPurpose_utils_smiles2mpnnfeature\",\"arguments\":{\"smiles\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\"}},{\"name\":\"DeepPurpose_utils_smiles2mpnnfeature\",\"arguments\":{\"smiles\":\"CC(=O)OC1=CC=CC=C1C(=O)O\"}},{\"name\":\"DeepPurpose_utils_smiles2mpnnfeature\",\"arguments\":{\"smiles\":\"cc(=o)oc1=cc=cc=c1c(=o)o\"}}]"}
{"func_name": "aizynthfinder_utils_math_rectified_linear_unit", "func_desc": "aizynthfinder.utils.math.rectified_linear_unit returns the element-wise Rectified Linear Unit (ReLU) activation of a NumPy array. In the AiZynthFinder codebase this function is used to introduce a simple, computationally efficient non-linearity in neural network computations (for example in expansion policy or filter policy networks that guide the retrosynthetic Monte Carlo tree search). The function maps each input element to itself when it is greater than zero and to zero otherwise, producing a non-negative array that can induce sparsity and improve training/stability of downstream policy networks.", "tools": [{"function": {"description": "aizynthfinder.utils.math.rectified_linear_unit returns the element-wise Rectified Linear Unit (ReLU) activation of a NumPy array. In the AiZynthFinder codebase this function is used to introduce a simple, computationally efficient non-linearity in neural network computations (for example in expansion policy or filter policy networks that guide the retrosynthetic Monte Carlo tree search). The function maps each input element to itself when it is greater than zero and to zero otherwise, producing a non-negative array that can induce sparsity and improve training/stability of downstream policy networks.\n", "name": "aizynthfinder_utils_math_rectified_linear_unit", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input numeric array containing the pre-activation values (for example, outputs from a linear layer of a neural network used by AiZynthFinder policies). The function applies the ReLU activation element-wise. The input shape is preserved and the operation is vectorized over all elements. The array must support comparison with zero (x > 0); passing an array with object dtype or elements that cannot be compared to 0 may raise a TypeError.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "During an AiZynthFinder expansion-policy regression test, we’re comparing two replicate 2D pre-activation tensors coming off the same layer. To mimic real production noise, apply the rectified linear unit only to replicates whose pre-activation field is numerically stable: the entire matrix must be finite (no NaN/Inf) and its largest absolute entry must not exceed 4.0 (otherwise it’s treated as a saturated run and excluded from downstream calibration). Use the following replicates: A = [[-1.2, 0.0, 2.5], [3.1, -0.7, -4.0]] and B = [[-1.2, 0.0, 3.5], [2.3, -0.7, 4.1]]. Return the ReLU-transformed activation tensor(s) for the replicate(s) that pass the stability gate.", "answers": "[{\"name\":\"aizynthfinder_utils_math_rectified_linear_unit\",\"arguments\":{\"x\":[[-1.2,0.0,2.5],[3.1,-0.7,-4.0]]}}]"}
{"func_name": "aizynthfinder_utils_math_softmax", "func_desc": "Compute column-wise softmax of the input scores and return normalized probabilities.\n    \n    This function converts raw scores (logits) into non-negative values normalized per column by applying the exponential function and dividing by the column-wise sum. In the AiZynthFinder retrosynthetic planning workflow, it is typically used to transform the output scores from an expansion policy neural network into probabilities that guide the Monte Carlo tree search when selecting precursor reactions. The implementation performs the operation exp(x) / sum(exp(x), axis=0) using NumPy without additional numerical stabilization.", "tools": [{"function": {"description": "Compute column-wise softmax of the input scores and return normalized probabilities.\n\nThis function converts raw scores (logits) into non-negative values normalized per column by applying the exponential function and dividing by the column-wise sum. In the AiZynthFinder retrosynthetic planning workflow, it is typically used to transform the output scores from an expansion policy neural network into probabilities that guide the Monte Carlo tree search when selecting precursor reactions. The implementation performs the operation exp(x) / sum(exp(x), axis=0) using NumPy without additional numerical stabilization.", "name": "aizynthfinder_utils_math_softmax", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Array of numeric scores (logits). Each column (summation along axis 0) is treated as an independent set of scores to be normalized into a probability distribution. In practice within AiZynthFinder, columns can represent scores for different candidate reactions or actions produced by a policy network for one or more input molecules. The function accepts any ndarray shape but always normalizes along axis 0. The array should contain finite numeric values for stable, meaningful output.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "We are post-processing expansion-policy logits from three replicate target-panels before MCTS action sampling. Each cohort is a candidate-reaction (rows) × target-molecule (columns) logits matrix. Because the downstream sampler is sensitive to extreme dynamic ranges, apply a quality-control rule per cohort: compute the column-wise spread (max−min) of logits, and only cohorts whose maximum column spread is ≤ 5.0 are considered numerically well-behaved and should be converted to column-wise softmax probabilities (exp(x)/column-sum). Use the following raw cohorts: (A) [[2.1, -0.7], [0.3, 1.4], [-1.2, 0.0]]; (B) [[1.2, -0.5, 0.0, 2.3], [0.8, 1.1, -1.4, 0.6], [-0.3, 2.0, 0.9, -0.7]]; (C) [[1.2, -0.4], [0.3, 2.1], [-1.5, 0.0]]. Return probability matrices for the cohorts that pass QC.", "answers": "[{\"name\":\"aizynthfinder_utils_math_softmax\",\"arguments\":{\"x\":[[2.1,-0.7],[0.3,1.4],[-1.2,0.0]]}},{\"name\":\"aizynthfinder_utils_math_softmax\",\"arguments\":{\"x\":[[1.2,-0.5,0.0,2.3],[0.8,1.1,-1.4,0.6],[-0.3,2.0,0.9,-0.7]]}},{\"name\":\"aizynthfinder_utils_math_softmax\",\"arguments\":{\"x\":[[1.2,-0.4],[0.3,2.1],[-1.5,0.0]]}}]"}
{"func_name": "anndata__io_utils_convert_bool", "func_desc": "anndata._io.utils.convert_bool: Determine whether a text token represents a boolean literal and return a normalized boolean indicator and value.\n\nThis helper is used in the anndata I/O utilities when parsing textual metadata or serialized fields (for example, when reading annotation columns from CSV or other plain-text representations of AnnData objects used in single-cell workflows). The function implements a strict, deterministic conversion: it recognizes only the exact, case-sensitive string literals \"True\" and \"False\" and maps them to Python boolean values. Callers (code that constructs or populates AnnData.obs / AnnData.var or other metadata fields from text) should use the first element of the returned tuple to decide whether the input was recognized as a boolean literal before trusting the second element as the boolean value.", "tools": [{"function": {"description": "anndata._io.utils.convert_bool: Determine whether a text token represents a boolean literal and return a normalized boolean indicator and value.\n\nThis helper is used in the anndata I/O utilities when parsing textual metadata or serialized fields (for example, when reading annotation columns from CSV or other plain-text representations of AnnData objects used in single-cell workflows). The function implements a strict, deterministic conversion: it recognizes only the exact, case-sensitive string literals \"True\" and \"False\" and maps them to Python boolean values. Callers (code that constructs or populates AnnData.obs / AnnData.var or other metadata fields from text) should use the first element of the returned tuple to decide whether the input was recognized as a boolean literal before trusting the second element as the boolean value.", "name": "anndata__io_utils_convert_bool", "parameters": {"properties": {"string": {"type": "string", "description": "The input text token to evaluate. In the anndata I/O context this is typically a value read from a textual metadata column or serialized attribute. The function performs exact, case-sensitive equality checks against the two accepted literal values \"True\" and \"False\". The parameter is annotated as str and the intended use is with string values produced by file parsing or serialization routines.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a messy `is_doublet` annotation exported from multiple single-cell QC tools into `AnnData.obs`, and we need to identify which raw tokens can be deterministically trusted as strict boolean literals before any downstream casting. Given this raw token stream sampled from the cohort (including common export artifacts and casing drift):\n\n`[\"True\", \"False\", \"TRUE\", \"false\", \"0\", \"1\", \"\", \"NA\", \"None\", \"nan\", \"False \", \" True\", \"T\", \"F\"]`\n\nRun the strict text-to-boolean check used by the I/O layer and return the normalized (recognized_indicator, value) outputs for every token that is recognized as a boolean literal under the exact, case-sensitive rules. Preserve the original token order when reporting results (i.e., recognized tokens appear in the same relative order as in the raw stream).", "answers": "[{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"True\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"False\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"TRUE\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"false\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"0\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"1\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"NA\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"None\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"nan\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"False \"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\" True\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"T\"}},{\"name\":\"anndata__io_utils_convert_bool\",\"arguments\":{\"string\":\"F\"}}]"}
{"func_name": "anndata_logging_get_logger", "func_desc": "Creates and returns a named child logger that is attached to the anndata library's central logger manager so that messages from library submodules and user code using anndata integrate with the package-wide logging configuration.\n\nThis function is used across the anndata codebase and by downstream packages (for example, Scanpy and other scverse projects) to obtain a logger that delegates to the module-level anndata_logger manager instead of the global logging.root. The practical significance is that callers who obtain a logger with this function will inherit handlers, levels, and formatting configured for anndata, ensuring consistent logging behavior for annotated-data workflows (single-cell omics and related analyses) that rely on anndata for in-memory and on-disk annotated data handling.", "tools": [{"function": {"description": "Creates and returns a named child logger that is attached to the anndata library's central logger manager so that messages from library submodules and user code using anndata integrate with the package-wide logging configuration.\n\nThis function is used across the anndata codebase and by downstream packages (for example, Scanpy and other scverse projects) to obtain a logger that delegates to the module-level anndata_logger manager instead of the global logging.root. The practical significance is that callers who obtain a logger with this function will inherit handlers, levels, and formatting configured for anndata, ensuring consistent logging behavior for annotated-data workflows (single-cell omics and related analyses) that rely on anndata for in-memory and on-disk annotated data handling.", "name": "anndata_logging_get_logger", "parameters": {"properties": {"name": {"type": "string", "description": "The name of the logger to fetch or create. This string identifies the logger (commonly a module-level __name__) and determines the logger hierarchy used by the logging.Manager owned by the module-level anndata_logger. Calling this function with the same name repeatedly returns the same logging.Logger instance managed by anndata_logger.manager. The parameter must be a Python string; passing a non-string value may result in unexpected behavior or a TypeError raised by the underlying logging.Manager.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re integrating AnnData-based logging into a heterogeneous single-cell pipeline where stage identifiers come from a messy run-manifest and must be normalized into a consistent logger namespace. Given these raw stage tags:\n\n- `my_sc_pipeline.qc`\n- `my_sc_pipeline.preprocessing`\n- `my_sc_pipeline.visualization`\n- `scanpy_ext.qc.doublet_detection`\n- `my_sc_pipeline.qc `   \n- `my_sc_pipeline..preprocessing`\n- `my_sc_pipeline.preprocessing.v1`\n- `SCANPY_EXT.QC.DOUBLET_DETECTION`\n- `my_sc_pipeline.visualization/plots`\n\nCreate/get AnnData-attached child loggers only for stage tags that can be canonically mapped to a valid Python dotted-module path under either `my_sc_pipeline` or `scanpy_ext` after normalization (trim surrounding whitespace, convert to lowercase, replace `/` with `.`, and collapse repeated dots). For versioned suffixes like `.vN`, canonicalize them to the parent stage logger (drop the version suffix). Ensure each resulting canonical logger name is created only once even if multiple raw tags map to it.", "answers": "[{\"name\":\"anndata_logging_get_logger\",\"arguments\":{\"name\":\"my_sc_pipeline.qc\"}},{\"name\":\"anndata_logging_get_logger\",\"arguments\":{\"name\":\"my_sc_pipeline.preprocessing\"}},{\"name\":\"anndata_logging_get_logger\",\"arguments\":{\"name\":\"my_sc_pipeline.visualization\"}},{\"name\":\"anndata_logging_get_logger\",\"arguments\":{\"name\":\"scanpy_ext.qc.doublet_detection\"}}]"}
{"func_name": "batchgenerators_augmentations_crop_a__ns_get_lbs_for_center_crop", "func_desc": "batchgenerators.augmentations.crop_and_pad_augmentations.get_lbs_for_center_crop computes the per-axis lower-bound indices for performing a center crop on spatial data; this is used by the crop-and-pad augmentations in batchgenerators (useful for 2D and 3D medical image augmentation pipelines where data arrays have batch and channel leading dimensions).", "tools": [{"function": {"description": "batchgenerators.augmentations.crop_and_pad_augmentations.get_lbs_for_center_crop computes the per-axis lower-bound indices for performing a center crop on spatial data; this is used by the crop-and-pad augmentations in batchgenerators (useful for 2D and 3D medical image augmentation pipelines where data arrays have batch and channel leading dimensions).\n", "name": "batchgenerators_augmentations_crop_a__ns_get_lbs_for_center_crop", "parameters": {"properties": {"crop_size": {"type": "any", "description": "Tuple of integers specifying the size of the desired center crop along each spatial axis (e.g., (x_crop, y_crop) for 2D or (x_crop, y_crop, z_crop) for 3D). In the context of the README and the crop/pad augmentations, this tuple represents how many voxels/pixels should be kept along each spatial dimension when extracting a centered sub-volume from the input. The function expects the order of elements in crop_size to match the spatial axes order in data_shape starting after the batch and channel dimensions.", "default": ""}, "data_shape": {"type": "any", "description": "Tuple describing the full shape of the array to be cropped, including batch and channel dimensions as the first two entries (for example, (b, c, x, y) for 2D or (b, c, x, y, z) for 3D, as described in the README). This value is used to determine the spatial extent of the input along each axis and therefore to compute the center-based lower-bound index for cropping for each spatial axis.", "default": ""}}, "required": ["crop_size", "data_shape"], "type": "any"}}, "type": "function"}], "query": "We’re calibrating center-crop bounds for a mixed 3D brain MRI cohort where each tensor is batch-first with leading (b, c) dims and spatial dims in (x, y, z). For each replicate, derive the target crop size from acquisition anisotropy: keep in-plane resolution standardized to 128×128, and set the through-plane crop to the largest multiple of 32 that does not exceed half of that replicate’s z-extent. Then compute the per-spatial-axis lower-bound indices needed to extract a centered crop from each replicate.\n\nReplicate A data_shape=(2, 1, 192, 192, 160)\nReplicate B data_shape=(2, 1, 160, 192, 128)\n\nReturn lower-bound indices in x/y/z order for each replicate.", "answers": "[{\"name\":\"batchgenerators_augmentations_crop_a__ns_get_lbs_for_center_crop\",\"arguments\":{\"crop_size\":[128,128,64],\"data_shape\":[2,1,192,192,160]}},{\"name\":\"batchgenerators_augmentations_crop_a__ns_get_lbs_for_center_crop\",\"arguments\":{\"crop_size\":[128,128,64],\"data_shape\":[2,1,160,192,128]}}]"}
{"func_name": "batchgenerators_augmentations_crop_an__ions_pad_nd_image_and_seg", "func_desc": "Pads a data array and its corresponding segmentation array to a target minimum spatial size and/or to sizes that are divisible by given factors. This function is used in the batchgenerators data augmentation pipeline (medical image augmentation for 2D and 3D data) to ensure that images and optional segmentation maps meet minimum size requirements (new_shape) and architectural constraints (must_be_divisible_by, e.g., for UNet downsampling stages). Padding is performed per spatial dimension while preserving batch and channel dimensions and returns new arrays without modifying the inputs in place.", "tools": [{"function": {"description": "Pads a data array and its corresponding segmentation array to a target minimum spatial size and/or to sizes that are divisible by given factors. This function is used in the batchgenerators data augmentation pipeline (medical image augmentation for 2D and 3D data) to ensure that images and optional segmentation maps meet minimum size requirements (new_shape) and architectural constraints (must_be_divisible_by, e.g., for UNet downsampling stages). Padding is performed per spatial dimension while preserving batch and channel dimensions and returns new arrays without modifying the inputs in place.\n", "name": "batchgenerators_augmentations_crop_an__ions_pad_nd_image_and_seg", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "Input image data to be padded. In the batchgenerators context this is expected to follow the internal data layout (for example 2D: (b, c, x, y); 3D: (b, c, x, y, z)). Padding is applied only to spatial dimensions (x, y, z). This array is not modified in place; a padded copy is returned. The function forwards padding behavior to numpy.pad via pad_mode_data and np_pad_kwargs_data.", "default": ""}, "seg": {"type": "array", "items": {"type": "float"}, "description": "Optional segmentation array corresponding to data. Shape conventions match data (batch and channel first). If seg is None, no segmentation padding is performed and the returned segmentation value will be None. If provided, padding is applied in the same spatial layout as for data so that spatial alignment between data and seg is preserved.", "default": ""}, "new_shape": {"type": "any", "nullable": true, "description": "Minimum target shape for the spatial dimensions. Interpreted as a per-spatial-dimension minimum size (min_shape). If any spatial dimension of data or seg is smaller than the corresponding entry in new_shape, that dimension is increased by padding. If data/seg is already larger along a dimension, that dimension is left unchanged. If new_shape is None, no minimum size is enforced and only must_be_divisible_by constraints (if any) are applied. When provided, new_shape length should match the number of spatial dimensions.", "default": null}, "must_be_divisible_by": {"type": "any", "nullable": true, "description": "If provided, the resulting spatial shape (after applying new_shape) will be increased if necessary so that each spatial dimension is divisible by the corresponding integer in this list. This is used for network architectures that require inputs to be divisible by a factor (for example, UNet with multiple downsampling operations). must_be_divisible_by should be a list of int with the same length as new_shape (or the number of spatial dimensions when new_shape is None). Values are treated as divisibility factors and dimensions are increased (never decreased) to satisfy divisibility.", "default": null}, "pad_mode_data": {"type": "string", "description": "Padding mode name passed to numpy.pad for the data array. Behaves like the mode argument of numpy.pad (for example 'constant', 'edge', 'reflect', etc.). Choose a mode appropriate for image data; default 'constant' pads with a constant value (see np.pad documentation for exact semantics).", "default": "constant"}, "np_pad_kwargs_data": {"type": "any", "nullable": true, "description": "Additional keyword arguments forwarded to numpy.pad when padding data (for example {'constant_values': 0}). If None, default numpy.pad behavior for the chosen pad_mode_data is used. These kwargs allow control over exact padding values and behavior.", "default": null}, "pad_mode_seg": {"type": "string", "description": "Padding mode name passed to numpy.pad for the segmentation array. Often different choices (for example 'edge' or 'constant') are appropriate for segmentations to avoid introducing artificial labels; the mode must be one accepted by numpy.pad. Default is 'constant'.", "default": "constant"}, "np_pad_kwargs_seg": {"type": "any", "nullable": true, "description": "Additional keyword arguments forwarded to numpy.pad when padding seg. If None, default numpy.pad behavior for pad_mode_seg is used. Use this to set constant_values or other numpy.pad options specific to segmentation padding.", "default": null}}, "required": ["data", "seg", "must_be_divisible_by", "np_pad_kwargs_data", "pad_mode_data", "pad_mode_seg", "np_pad_kwargs_seg", "new_shape"], "type": "any"}}, "type": "function"}], "query": "I’m consolidating three 2D U-Net training cohorts (b=1, c=1) coming from different scanners into a single augmentation stream. For each cohort, pad both the image tensor and its aligned segmentation map using constant padding, but apply a cohort-adaptive minimum target size driven by the label geometry: compute the tight bounding box of all non-background labels (labels > 0) in the segmentation; if that bounding box is at least 3×3 pixels, enforce a minimum padded spatial size of 8×8, otherwise enforce 6×6. In all cases, the final spatial dimensions must be divisible by (4,4) to satisfy the network’s downsampling constraints. Use cohort-specific padding fill conventions: if the image contains any negative intensity values, use -1024.0 for data padding and 0 for seg padding; otherwise use 0 for data padding and use -1 for seg padding when the segmentation contains any label value 2, else use 255 for seg padding. Apply this to the following raw cohorts:\n\nCohort A: data shape (1,1,3,5) with data=[[[[1.2,0.8,0.5,0.1,0.0],[1.0,0.7,0.4,0.2,0.1],[0.9,0.6,0.3,0.1,0.0]]]] and seg=[[[[0,0,1,1,0],[0,2,2,1,0],[0,2,2,0,0]]]]\n\nCohort B: data shape (1,1,3,4) with data=[[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0]]]] and seg=[[[[0,0,1,1],[0,2,2,1],[0,2,2,0]]]]\n\nCohort C: data shape (1,1,5,6) with data=[[[[0.0,1.0,2.0,3.0,4.0,5.0],[6.0,7.0,8.0,9.0,10.0,11.0],[12.0,13.0,14.0,15.0,16.0,17.0],[18.0,19.0,20.0,21.0,22.0,23.0],[24.0,25.0,26.0,27.0,28.0,29.0]]]] and seg=[[[[0,0,0,0,0,0],[0,1,1,1,0,0],[0,1,2,2,0,0],[0,1,2,2,0,0],[0,0,0,0,0,0]]]]", "answers": "[{\"name\":\"batchgenerators_augmentations_crop_an__ions_pad_nd_image_and_seg\",\"arguments\":{\"data\":[[[[1.2,0.8,0.5,0.1,0.0],[1.0,0.7,0.4,0.2,0.1],[0.9,0.6,0.3,0.1,0.0]]]],\"seg\":[[[[0,0,1,1,0],[0,2,2,1,0],[0,2,2,0,0]]]],\"new_shape\":[8,8],\"must_be_divisible_by\":[4,4],\"pad_mode_data\":\"constant\",\"np_pad_kwargs_data\":{\"constant_values\":0},\"pad_mode_seg\":\"constant\",\"np_pad_kwargs_seg\":{\"constant_values\":-1}}},{\"name\":\"batchgenerators_augmentations_crop_an__ions_pad_nd_image_and_seg\",\"arguments\":{\"data\":[[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0]]]],\"seg\":[[[[0,0,1,1],[0,2,2,1],[0,2,2,0]]]],\"new_shape\":[8,8],\"must_be_divisible_by\":[4,4],\"pad_mode_data\":\"constant\",\"np_pad_kwargs_data\":{\"constant_values\":0},\"pad_mode_seg\":\"constant\",\"np_pad_kwargs_seg\":{\"constant_values\":-1}}},{\"name\":\"batchgenerators_augmentations_crop_an__ions_pad_nd_image_and_seg\",\"arguments\":{\"data\":[[[[0.0,1.0,2.0,3.0,4.0,5.0],[6.0,7.0,8.0,9.0,10.0,11.0],[12.0,13.0,14.0,15.0,16.0,17.0],[18.0,19.0,20.0,21.0,22.0,23.0],[24.0,25.0,26.0,27.0,28.0,29.0]]]],\"seg\":[[[[0,0,0,0,0,0],[0,1,1,1,0,0],[0,1,2,2,0,0],[0,1,2,2,0,0],[0,0,0,0,0,0]]]],\"new_shape\":[8,8],\"must_be_divisible_by\":[4,4],\"pad_mode_data\":\"constant\",\"np_pad_kwargs_data\":{\"constant_values\":0},\"pad_mode_seg\":\"constant\",\"np_pad_kwargs_seg\":{\"constant_values\":-1}}}]"}
{"func_name": "batchgenerators_augmentations_spatia__nsformations_augment_rot90", "func_desc": "Augment a data sample and its segmentation by rotating the spatial axes by a multiple of 90 degrees.\n    \n    This function is used in the spatial augmentation pipeline of batchgenerators (a medical image data augmentation library) to increase training variability for 2D and 3D image data. It selects a rotation amount (an integer count of 90-degree steps) randomly from num_rot and selects two spatial axes randomly from axes to define the rotation plane. The chosen rotation is applied to sample_data and, if present, to sample_seg using numpy.rot90 so that both image data and corresponding segmentation remain spatially aligned. This transform is appropriate for per-sample spatial augmentation of batches that follow the batchgenerators data layout (see README: 'data' should have shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).", "tools": [{"function": {"description": "Augment a data sample and its segmentation by rotating the spatial axes by a multiple of 90 degrees.\n\nThis function is used in the spatial augmentation pipeline of batchgenerators (a medical image data augmentation library) to increase training variability for 2D and 3D image data. It selects a rotation amount (an integer count of 90-degree steps) randomly from num_rot and selects two spatial axes randomly from axes to define the rotation plane. The chosen rotation is applied to sample_data and, if present, to sample_seg using numpy.rot90 so that both image data and corresponding segmentation remain spatially aligned. This transform is appropriate for per-sample spatial augmentation of batches that follow the batchgenerators data layout (see README: 'data' should have shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).", "name": "batchgenerators_augmentations_spatia__nsformations_augment_rot90", "parameters": {"properties": {"sample_data": {"type": "array", "items": {"type": "float"}, "description": "The image data array to rotate. In the batchgenerators context this is expected to follow the internal data layout: batch as first dimension and channel as second (shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D). The spatial axes referenced by the axes parameter correspond to the x, y, (z) spatial dimensions and the function adds an internal offset of +1 to these axis indices to account for the batch and channel dimensions before calling numpy.rot90. The function does not modify the input array in-place; it returns a rotated array.", "default": ""}, "sample_seg": {"type": "array", "items": {"type": "float"}, "description": "The segmentation array corresponding to sample_data that should receive the identical spatial rotation. This must use the same layout as sample_data (batch and channel as first two dimensions). If sample_seg is None, no segmentation rotation is performed and None is returned in its place. If provided, the same rotation parameters (k and axes) are applied so labels remain properly aligned with the image.", "default": ""}, "num_rot": {"type": "any", "description": "Tuple of integers indicating how many times to rotate by 90 degrees. At runtime one element k is chosen uniformly at random from this tuple and numpy.rot90 is called with k (i.e., image is rotated by k * 90 degrees counter-clockwise in the chosen plane). Use a single-element tuple (e.g., (1,)) to enforce a deterministic rotation amount; control randomness via numpy's random seed if reproducibility is required.", "default": [1, 2, 3]}, "axes": {"type": "any", "description": "Tuple of integers indexing spatial axes in the order (x, y, z) relative to the spatial dimensions (0->x, 1->y, 2->z). Two distinct values are chosen at random (without replacement) from this tuple to define the plane of rotation. Internally these indices are incremented by 1 to map to the full array dimensions (to skip the batch and channel axes) before calling numpy.rot90. The user must ensure the provided axes are compatible with the spatial dimensionality of sample_data (e.g., for strictly 2D data only axes drawn from (0, 1) are valid); otherwise numpy.rot90 will raise an IndexError.", "default": [0, 1, 2]}}, "required": ["sample_data", "sample_seg", "num_rot", "axes"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing rot90-based spatial augmentation under realistic preprocessing noise where samples differ in dimensionality and label integrity.\n\nYou are given four acquisitions (two 3D MR volumes in one batch and two 2D US slices as separate mini-batches). Apply rot90 augmentation only to acquisitions that look numerically valid for intensity images (finite values and non-negative everywhere). For every acquisition that is processed, rotate its segmentation with the identical rotation so alignment is preserved.\n\nBranching protocol for rotation settings:\n- For 3D volumes: determine whether the through-plane depth is odd or even. If depth is odd, allow rotations from {1,2,3,4} and allow any rotation plane drawn from the three spatial axes {0,1,2}. If depth is even, restrict rotations to {1,3} and restrict the rotation plane to in-plane axes only {0,1}.\n- For 2D slices: decide whether the mask contains any foreground (any value > 0). If it has foreground, allow rotations from {1,2,3} with in-plane axes {0,1}. If it is all-background, only allow rotations from {2,4} with in-plane axes {0,1}.\n\nRaw inputs:\n1) 3D MRI batch (b=2, c=1, x=4, y=4, z=3)\n3D MRI sample_data = [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]], [[1.3, 1.4, 1.5], [1.6, 1.7, 1.8], [1.9, 2.0, 2.1], [2.2, 2.3, 2.4]], [[2.5, 2.6, 2.7], [2.8, 2.9, 3.0], [3.1, 3.2, 3.3], [3.4, 3.5, 3.6]], [[3.7, 3.8, 3.9], [4.0, 4.1, 4.2], [4.3, 4.4, 4.5], [4.6, 4.7, 4.8]]], [[[5.1, 5.2, 5.3], [5.4, 5.5, 5.6], [5.7, 5.8, 5.9], [6.0, 6.1, 6.2]], [[6.3, 6.4, 6.5], [6.6, 6.7, 6.8], [6.9, 7.0, 7.1], [7.2, 7.3, 7.4]], [[7.5, 7.6, 7.7], [7.8, 7.9, 8.0], [8.1, 8.2, 8.3], [8.4, 8.5, 8.6]], [[8.7, 8.8, 8.9], [9.0, 9.1, 9.2], [9.3, 9.4, 9.5], [9.6, 9.7, 9.8]]]]\n3D MRI sample_seg  = [[[[0, 0, 1], [1, 1, 2], [2, 2, 3], [3, 3, 4]], [[4, 4, 5], [5, 5, 6], [6, 6, 7], [7, 7, 8]], [[8, 8, 9], [9, 9, 1], [1, 1, 2], [2, 2, 3]], [[3, 3, 4], [4, 4, 5], [5, 5, 6], [6, 6, 7]]], [[[1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]], [[9, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]], [[8, 9, 1], [1, 2, 3], [3, 4, 5], [5, 6, 7]], [[7, 8, 9], [9, 1, 2], [2, 3, 4], [4, 5, 6]]]]\n\n2) 2D ultrasound slice batch (b=1, c=1, x=3, y=4)\n2D ultrasound image = [[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]]]\n2D ultrasound mask  = [[[[0, 0, 1, 1], [0, 1, 1, 0], [0, 0, 0, 1]]]]\n\nExecute the augmentation calls required by the above rules.", "answers": "[{\"name\":\"batchgenerators_augmentations_spatia__nsformations_augment_rot90\",\"arguments\":{\"sample_data\":[[[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2]],[[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4]],[[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6]],[[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]]],[[[5.1,5.2,5.3],[5.4,5.5,5.6],[5.7,5.8,5.9],[6.0,6.1,6.2]],[[6.3,6.4,6.5],[6.6,6.7,6.8],[6.9,7.0,7.1],[7.2,7.3,7.4]],[[7.5,7.6,7.7],[7.8,7.9,8.0],[8.1,8.2,8.3],[8.4,8.5,8.6]],[[8.7,8.8,8.9],[9.0,9.1,9.2],[9.3,9.4,9.5],[9.6,9.7,9.8]]]],\"sample_seg\":[[[[0,0,1],[1,1,2],[2,2,3],[3,3,4]],[[4,4,5],[5,5,6],[6,6,7],[7,7,8]],[[8,8,9],[9,9,1],[1,1,2],[2,2,3]],[[3,3,4],[4,4,5],[5,5,6],[6,6,7]]],[[[1,2,3],[3,4,5],[5,6,7],[7,8,9]],[[9,1,2],[2,3,4],[4,5,6],[6,7,8]],[[8,9,1],[1,2,3],[3,4,5],[5,6,7]],[[7,8,9],[9,1,2],[2,3,4],[4,5,6]]]],\"num_rot\":[1,2,3,4],\"axes\":[0,1,2]}},{\"name\":\"batchgenerators_augmentations_spatia__nsformations_augment_rot90\",\"arguments\":{\"sample_data\":[[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]],\"sample_seg\":[[[[0,0,1,1],[0,1,1,0],[0,0,0,1]]]],\"num_rot\":[1,2,3],\"axes\":[0,1]}}]"}
{"func_name": "batchgenerators_augmentations_util__eg_image_to_one_hot_encoding", "func_desc": "Convert a segmentation label map to a one-hot encoded array with the class/channel axis first.\n    \n    This function is used in the batchgenerators data-augmentation and preprocessing pipeline to convert a single-sample segmentation label map (a spatial label image) into a one-hot representation where each output channel corresponds to a semantic class. In the context of the README and the medical image augmentation use cases, this is typically applied to a per-sample segmentation (for example a 2D label map of shape (x, y) or a 3D label map of shape (x, y, z)) prior to spatial or intensity augmentations or before feeding the labels into a model that expects channel-first one-hot targets. If classes is None, the set and order of channels is determined from the unique values present in image using numpy.unique. The function performs elementwise equality comparisons (image == c) to generate each class mask; therefore exact value matching semantics apply (this is important for floating-point label maps).", "tools": [{"function": {"description": "Convert a segmentation label map to a one-hot encoded array with the class/channel axis first.\n\nThis function is used in the batchgenerators data-augmentation and preprocessing pipeline to convert a single-sample segmentation label map (a spatial label image) into a one-hot representation where each output channel corresponds to a semantic class. In the context of the README and the medical image augmentation use cases, this is typically applied to a per-sample segmentation (for example a 2D label map of shape (x, y) or a 3D label map of shape (x, y, z)) prior to spatial or intensity augmentations or before feeding the labels into a model that expects channel-first one-hot targets. If classes is None, the set and order of channels is determined from the unique values present in image using numpy.unique. The function performs elementwise equality comparisons (image == c) to generate each class mask; therefore exact value matching semantics apply (this is important for floating-point label maps).", "name": "batchgenerators_augmentations_util__eg_image_to_one_hot_encoding", "parameters": {"properties": {"image": {"type": "array", "items": {"type": "float"}, "description": "Input segmentation label map. This is an N-dimensional numpy array representing spatial labels for a single sample (typical shapes in this repository are 2D: (x, y) or 3D: (x, y, z)). The array stores discrete label values for each spatial location. The function treats every axis of image as spatial (no implicit batch or channel axis is handled); when using batched data or data with an explicit channel axis, call this function per sample or reshape accordingly. The data type of image is preserved in the output array and is used for the one-hot values (0 and 1 stored with image.dtype).", "default": ""}, "classes": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Array of label values that define the class channels and their order in the output. If provided, classes must be a numpy.ndarray (or an array-like that can be interpreted as such) containing the distinct label values to encode. The output will contain len(classes) channels in the same order as classes. If classes is None (default), the function computes classes = numpy.unique(image) and uses that sorted set of unique labels from the input; in that case the channel order follows numpy.unique's output. If classes contains values that do not appear in image, the corresponding output channels will be all zeros.", "default": null}}, "required": ["image", "classes"], "type": "any"}}, "type": "function"}], "query": "We’re doing pre-augmentation QC on a mixed 2D segmentation cohort where label semantics differ by study, and some slices may be unusable due to annotation contamination. For each candidate label map below, convert it to a channel-first one-hot target using an explicit class list that depends on the intrinsic label content: (a) if the slice contains any label value 3, treat it as a 4-class tissue/CSF study and use class order [0,1,2,3]; (b) otherwise, if the slice contains label value 2, treat it as a 3-class tumor study and use class order [0,1,2]; (c) only run the conversion for slices whose label set is a subset of {0,1,2,3} and includes background (0). Use exact-value equality semantics per class. Candidate slices: S1 (4x5)=[[0,0,1,1,2],[0,1,1,2,2],[0,0,0,2,2],[1,1,0,0,2]]; S2 (4x5)=[[0,1,1,2,2],[0,0,1,2,3],[0,1,1,3,3],[0,0,2,2,3]]; S3 (3x4)=[[0,0,1,1],[0,2,2,1],[0,0,1,0]].", "answers": "[{\"name\":\"batchgenerators_augmentations_util__eg_image_to_one_hot_encoding\",\"arguments\":{\"image\":[[0,0,1,1,2],[0,1,1,2,2],[0,0,0,2,2],[1,1,0,0,2]],\"classes\":[0,1,2]}},{\"name\":\"batchgenerators_augmentations_util__eg_image_to_one_hot_encoding\",\"arguments\":{\"image\":[[0,1,1,2,2],[0,0,1,2,3],[0,1,1,3,3],[0,0,2,2,3]],\"classes\":[0,1,2,3]}},{\"name\":\"batchgenerators_augmentations_util__eg_image_to_one_hot_encoding\",\"arguments\":{\"image\":[[0,0,1,1],[0,2,2,1],[0,0,1,0]],\"classes\":[0,1,2]}}]"}
{"func_name": "batchgenerators_augmentations_utils_elastic_deform_coordinates_2", "func_desc": "Elastic deformation of a coordinate grid using smoothed random fields in the Fourier domain, intended for spatial augmentations in the batchgenerators pipeline (e.g., to simulate soft-tissue or other realistic deformations of image sampling grids). The function generates per-dimension random noise, applies a Fourier-domain Gaussian low-pass filter (via fourier_gaussian and FFT/IFFT), scales the resulting deformation fields by the provided magnitudes, normalizes them to the supplied magnitude range, and returns coordinates displaced by these deformation offsets. This is used downstream to map image sampling coordinates for elastic spatial augmentation in 2D/3D data augmentation workflows described in the README.", "tools": [{"function": {"description": "Elastic deformation of a coordinate grid using smoothed random fields in the Fourier domain, intended for spatial augmentations in the batchgenerators pipeline (e.g., to simulate soft-tissue or other realistic deformations of image sampling grids). The function generates per-dimension random noise, applies a Fourier-domain Gaussian low-pass filter (via fourier_gaussian and FFT/IFFT), scales the resulting deformation fields by the provided magnitudes, normalizes them to the supplied magnitude range, and returns coordinates displaced by these deformation offsets. This is used downstream to map image sampling coordinates for elastic spatial augmentation in 2D/3D data augmentation workflows described in the README.\n", "name": "batchgenerators_augmentations_utils_elastic_deform_coordinates_2", "parameters": {"properties": {"coordinates": {"type": "array", "items": {"type": "float"}, "description": "Input coordinate grid to be deformed. The first axis indexes coordinate dimensions (n_dim) and the remaining axes define the spatial grid shape; for example a 2D grid could have shape (2, x, y). Each entry is interpreted as the coordinate values along that dimension. The function computes offsets with the same spatial shape and returns coordinates + offsets. The array is treated as floating point during processing and the returned array will be floating point.", "default": ""}, "sigmas": {"type": "array", "items": {"type": "float"}, "description": "Controls the amount of Gaussian smoothing applied in the Fourier domain for each deformation component. Each element corresponds to the standard deviation (in frequency-space units used by fourier_gaussian) applied to the random field for one coordinate dimension. Per the implementation, if sigmas is not a tuple or list it will be broadcast into a list of length (len(coordinates) - 1); if a list is provided its length is expected to match the per-dimension requirements of the code (see Failure modes). Smoothing reduces high-frequency components of the random field and therefore controls the spatial scale of the deformation.", "default": ""}, "magnitudes": {"type": "array", "items": {"type": "float"}, "description": "Scaling factors for the final deformation amplitude for each coordinate dimension. Each element determines how strongly the normalized deformation field for that dimension is scaled before being added to coordinates. If magnitudes is not a tuple or list it will be broadcast into a list of length (len(coordinates) - 1) per the implementation. The code normalizes each sampled deformation field by its maximum absolute value and then scales it by the corresponding magnitude (numerical stability uses a +1e-8 term to avoid division by zero).", "default": ""}}, "required": ["coordinates", "sigmas", "magnitudes"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our 2D elastic augmentation stage on a small mixed-quality batch of coordinate grids coming from different scanners. Each entry is a 2-channel grid (channel0=x, channel1=y), but some are malformed (non-square, inconsistent axis stepping, or contain NaNs from upstream interpolation). Use the following raw cohorts:\n\n1) Replicate R1 (4×4): x=[[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]], y=[[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]].\n2) Replicate R2 (5×5): x=[[0.0,0.0,0.0,0.0,0.0],[1.0,1.0,1.0,1.0,1.0],[2.0,2.0,2.0,2.0,2.0],[3.0,3.0,3.0,3.0,3.0],[4.0,4.0,4.0,4.0,4.0]], y=[[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0]].\n3) Replicate R3 (4×4) with an interpolation artifact: x=[[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]], y=[[0,1,2,3],[0,1,2,3],[0,1,null,3],[0,1,2,3]].\n4) Replicate R4 (5×4) non-square: x=[[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]], y=[[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]].\n\nProcess only grids that are square, contain finite numeric coordinates in both channels, and have monotonic unit stepping along x and y (consistent with a regular sampling lattice). For each valid grid, apply anisotropic deformation magnitudes of 2.5 px in x and 1.5 px in y. Set smoothness (sigmas) based on grid size: use sigma=10.0 in both dimensions when the grid is 4×4, and sigma=8.0 in both dimensions when the grid is 5×5. Return the displaced coordinates for all valid replicates in the same order they appear in the raw batch (dropping any that fail the criteria by virtue of the rules).", "answers": "[{\"name\":\"batchgenerators_augmentations_utils_elastic_deform_coordinates_2\",\"arguments\":{\"coordinates\":[[[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]],[[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]]],\"sigmas\":[10.0,10.0],\"magnitudes\":[2.5,1.5]}},{\"name\":\"batchgenerators_augmentations_utils_elastic_deform_coordinates_2\",\"arguments\":{\"coordinates\":[[[0.0,0.0,0.0,0.0,0.0],[1.0,1.0,1.0,1.0,1.0],[2.0,2.0,2.0,2.0,2.0],[3.0,3.0,3.0,3.0,3.0],[4.0,4.0,4.0,4.0,4.0]],[[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0],[0.0,1.0,2.0,3.0,4.0]]],\"sigmas\":[8.0,8.0],\"magnitudes\":[2.5,1.5]}}]"}
{"func_name": "batchgenerators_augmentations_utils_get_organ_gradient_field", "func_desc": "Calculates a 3D gradient vector field around a binary organ segmentation to support anatomy-informed data augmentation (used, for example, to simulate soft-tissue deformations in medical image augmentation pipelines).", "tools": [{"function": {"description": "Calculates a 3D gradient vector field around a binary organ segmentation to support anatomy-informed data augmentation (used, for example, to simulate soft-tissue deformations in medical image augmentation pipelines).\n", "name": "batchgenerators_augmentations_utils_get_organ_gradient_field", "parameters": {"properties": {"organ": {"type": "array", "items": {"type": "float"}, "description": "Binary organ segmentation volume. This is the input mask from which the spatial gradient is computed. The array is converted to floating point internally (organ.astype(float)) so the original array is not modified. The function expects a 3D volume with three spatial axes; if a 2D array is provided, numpy.gradient will return only two gradient components and unpacking into three outputs will raise an error.", "default": ""}, "spacing_ratio": {"type": "float", "description": "Ratio of the axial spacing to the in-plane slice thickness (axial spacing / slice thickness). This scalar is required to correctly scale the gradient along the axial axis so that the resulting vector field reflects the physical anisotropy of voxel spacing in typical medical image volumes. The default equals 0.3125/3.0 (approximately 0.1041667) as used in the original implementation. Values <= 0 are invalid and will produce incorrect gradients or may cause the gaussian filter to behave unexpectedly.", "default": 0.10416666666666667}, "blur": {"type": "integer", "description": "Kernel constant that controls the amount of Gaussian smoothing applied to the binary segmentation before computing gradients. Internally, gaussian_filter is called with sigma=(blur * spacing_ratio, blur, blur). Larger values increase the smoothing radius, producing a smoother, lower-magnitude gradient field concentrated over a wider boundary region; smaller values preserve sharper edges. Must be a positive integer; non-positive or non-integer values may lead to unexpected results.", "default": 32}}, "required": ["organ", "spacing_ratio", "blur"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an anatomy-informed deformation-prep step for a QA augmentation pipeline where masks may contain padding/blank slices and the z-extent can vary. Given two candidate binary organ segmentations (z,y,x) below, generate 3D gradient vector fields only for segmentations that contain organ voxels in at least 60% of their z-slices (i.e., most slices are non-empty). For each processed segmentation, set `spacing_ratio` from the provided acquisition metadata as (axial_spacing_mm / in_plane_spacing_mm). Choose the Gaussian `blur` adaptively from morphology: use blur=3 when the organ occupies at least 25% of voxels in its most-populated z-slice; otherwise use blur=2.\n\nMasks + metadata:\n(A) organ mask (4×4×4): three identical organ-containing z-slices followed by one all-background slice:\n[\n  [[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],\n  [[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],\n  [[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],\n  [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n]\nAcquisition: axial_spacing_mm=0.5, in_plane_spacing_mm=1.0\n\n(B) kidney mask (3×4×4): first two z-slices contain the 2×2 organ block, last slice all zeros:\n[\n  [[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],\n  [[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],\n  [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n]\nAcquisition: axial_spacing_mm=1.25, in_plane_spacing_mm=3.0\n\nReturn the gradient vector field(s) for the qualifying segmentation(s) in this run.", "answers": "[{\"name\":\"batchgenerators_augmentations_utils_get_organ_gradient_field\",\"arguments\":{\"organ\":[[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]],\"spacing_ratio\":0.5,\"blur\":3}}]"}
{"func_name": "batchgenerators_augmentations_utils_resize_multichannel_image", "func_desc": "Resize a multichannel image by resizing each channel independently and\n    recombining the resized channels into a multichannel array. This function is\n    used in the batchgenerators augmentation pipeline to resample per-sample\n    channel data (for example modalities in a multi-modal medical image) when a\n    new spatial resolution or shape is required. It preserves the input array's\n    channel ordering and numeric dtype while performing interpolation on a\n    floating-point representation.", "tools": [{"function": {"description": "Resize a multichannel image by resizing each channel independently and\nrecombining the resized channels into a multichannel array. This function is\nused in the batchgenerators augmentation pipeline to resample per-sample\nchannel data (for example modalities in a multi-modal medical image) when a\nnew spatial resolution or shape is required. It preserves the input array's\nchannel ordering and numeric dtype while performing interpolation on a\nfloating-point representation.", "name": "batchgenerators_augmentations_utils_resize_multichannel_image", "parameters": {"properties": {"multichannel_image": {"type": "array", "items": {"type": "float"}, "description": "Input image with channels as the first\naxis. Expected shape is (c, X, Y) for 2D data or (c, X, Y, Z) for 3D\ndata, where c is the number of channels/modalities and X, Y, (Z) are\nspatial dimensions. This is the per-sample \"data\" representation used\nby batchgenerators (the README describes batch-level data as (b, c, ...),\nthis function operates on the per-sample slice with shape beginning\nwith c). Each channel will be resized independently and then stacked\nback into the same channel-first layout. The function does not modify\nthe input array in place.", "default": ""}, "new_shape": {"type": "any", "description": "Target spatial shape for each channel given as a tuple\nof integers (X_new, Y_new) for 2D or (X_new, Y_new, Z_new) for 3D. The\nreturned array will have shape (c, ) + new_shape. The length of this\ntuple must match the spatial dimensionality of multichannel_image\n(i.e., 2 entries for 2D input, 3 entries for 3D input). If this does\nnot hold, the underlying resize implementation will raise an error.", "default": ""}, "order": {"type": "integer", "description": "Interpolation order passed through to the underlying\nresize implementation (default: 3). In practice this controls the\ninterpolation kernel (e.g., 0=nearest, 1=linear, 3=cubic). The value\nmust be an integer supported by the resize routine; invalid values\nwill cause that routine to raise an exception. The default (3) gives\ncubic interpolation commonly used for image resampling.", "default": 3}}, "required": ["multichannel_image", "new_shape", "order"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing a mixed-cohort batch of channel-first medical images for a downstream model that expects consistent in-plane spacing, but we need a simple QC-aware branching rule during resampling. Treat each provided tensor as an independent sample and keep channel ordering. Apply linear interpolation (order=1) only when the sample shows an intensity dynamic range > 1.0 across all voxels (max-min over the entire multichannel array). If the dynamic range is ≤ 1.0 (typical of near-flat or already-normalized calibration patches), resample with nearest-neighbor (order=0) to avoid introducing interpolation-driven gradients. Use the target shape rule: for 2D samples, resample from the given 3×4 plane to 6×8; for the 3D request, resample to (128,128,64). Inputs: (1) 2-channel MRI slice (c,X,Y) with Channel 0 = [[0,1,2,3],[4,5,6,7],[8,9,10,11]] and Channel 1 = [[10,9,8,7],[6,5,4,3],[2,1,0,-1]]. (2) 2-channel T1/T2 slice stored channel-first (2,3,4) with Channel 0 = [[10,12,14,16],[18,20,22,24],[26,28,30,32]] and Channel 1 = [[5,6,7,8],[9,10,11,12],[13,14,15,16]]. (3) 2-channel multichannel input [[[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]],[[1.0,1.1,1.2,1.3],[1.4,1.5,1.6,1.7]]] that must be resized to (128,128,64) with the same QC-aware interpolation rule.", "answers": "[{\"name\":\"batchgenerators_augmentations_utils_resize_multichannel_image\",\"arguments\":{\"multichannel_image\":[[[0,1,2,3],[4,5,6,7],[8,9,10,11]],[[10,9,8,7],[6,5,4,3],[2,1,0,-1]]],\"new_shape\":[6,8],\"order\":1}},{\"name\":\"batchgenerators_augmentations_utils_resize_multichannel_image\",\"arguments\":{\"multichannel_image\":[[[10,12,14,16],[18,20,22,24],[26,28,30,32]],[[5,6,7,8],[9,10,11,12],[13,14,15,16]]],\"new_shape\":[6,8],\"order\":1}},{\"name\":\"batchgenerators_augmentations_utils_resize_multichannel_image\",\"arguments\":{\"multichannel_image\":[[[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]],[[1.0,1.1,1.2,1.3],[1.4,1.5,1.6,1.7]]],\"new_shape\":[128,128,64],\"order\":1}}]"}
{"func_name": "batchgenerators_augmentations_utils_uniform", "func_desc": "Concise wrapper around numpy.random.uniform that ensures well-defined output when the lower and upper bounds are identical.", "tools": [{"function": {"description": "Concise wrapper around numpy.random.uniform that ensures well-defined output when the lower and upper bounds are identical.\n", "name": "batchgenerators_augmentations_utils_uniform", "parameters": {"properties": {"low": {"type": "float", "description": "Lower bound of the interval from which to draw samples. In the batchgenerators augmentation context this represents the minimum value for a random augmentation parameter (for example the smallest possible rotation, scale, or intensity change). If low is equal to high this function treats the sampling as degenerate and returns that constant value instead of delegating to numpy.", "default": ""}, "high": {"type": "float", "description": "Upper bound of the interval from which to draw samples. In the batchgenerators augmentation context this represents the maximum value for a random augmentation parameter. When high == low the function returns the constant value low (or an array filled with low) rather than attempting to sample.", "default": ""}, "size": {"type": "any", "nullable": true, "description": "Shape of the output array to produce when sampling. This matches the semantics of numpy.random.uniform's size argument. Default: None. In the augmentation pipeline, pass a size to obtain per-element independent random values (e.g., one parameter per image or per voxel); leave as None to obtain a scalar.", "default": null}}, "required": ["low", "high", "size"], "type": "any"}}, "type": "function"}], "query": "We’re tuning augmentation parameters for a multi-site medical imaging study where each acquisition block has its own QC flags. For each block, draw parameters using the uniform sampler with rules that depend on intrinsic metadata: (1) Gamma calibration: for each acquisition block, if its measured exposure drift magnitude is within 1% of baseline, treat gamma as deterministic and draw one gamma factor per image by setting identical lower/upper bounds to 0.85; otherwise, allow mild stochasticity by sampling gamma uniformly over [0.80, 0.90]. Apply this to three blocks with (n_images=4, drift=0.0%), (n_images=12, drift=3.4%), and (n_images=8, drift=0.5%). (2) Slice-wise in-plane rotation: for 2D slices, set the rotation range based on anisotropy; if through-plane/ in-plane spacing ratio <= 2.0, sample per-slice angles over [-5.0, 5.0] degrees; if the ratio is larger, constrain it to [-2.0, 2.0]. Apply this to two series: 16 slices with ratio=1.3 and 10 slices with ratio=3.1, returning one angle per slice for each series.", "answers": "[{\"name\":\"batchgenerators_augmentations_utils_uniform\",\"arguments\":{\"low\":0.85,\"high\":0.85,\"size\":[4]}},{\"name\":\"batchgenerators_augmentations_utils_uniform\",\"arguments\":{\"low\":0.8,\"high\":0.9,\"size\":[12]}},{\"name\":\"batchgenerators_augmentations_utils_uniform\",\"arguments\":{\"low\":0.85,\"high\":0.85,\"size\":[8]}},{\"name\":\"batchgenerators_augmentations_utils_uniform\",\"arguments\":{\"low\":-5.0,\"high\":5.0,\"size\":[16]}},{\"name\":\"batchgenerators_augmentations_utils_uniform\",\"arguments\":{\"low\":-2.0,\"high\":2.0,\"size\":[10]}}]"}
{"func_name": "batchgenerators_dataloading_data_loader_default_collate", "func_desc": "Default collate function used by batchgenerators to assemble a list of samples into a batched structure.\n    This function is heavily inspired by torch.utils.data.default_collate and is intended for use inside the\n    batchgenerators data loading / augmentation pipeline (for example in MultiThreadedAugmenter and custom\n    DataLoaderBase implementations). It converts a list of per-sample objects (the argument `batch`) into a single\n    batched object that can be passed to downstream augmentations or model training. In the context of batchgenerators\n    this is typically used to form arrays with a leading batch dimension consistent with the README data convention\n    (e.g., data/seg arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).\n    \n    The function handles a small set of concrete element types (numpy arrays, Python numeric types, numpy numeric\n    scalars, dict/OrderedDict, tuple/list and strings) and applies type-specific collating rules described below.\n    Behavior is recursive for nested containers (dicts of tuples of arrays, etc.). The function enforces consistent\n    structure across samples: for dict/OrderedDict inputs every sample must contain the same keys; for arrays the\n    per-sample array shapes must be compatible for stacking along a new leading batch axis.", "tools": [{"function": {"description": "Default collate function used by batchgenerators to assemble a list of samples into a batched structure.\nThis function is heavily inspired by torch.utils.data.default_collate and is intended for use inside the\nbatchgenerators data loading / augmentation pipeline (for example in MultiThreadedAugmenter and custom\nDataLoaderBase implementations). It converts a list of per-sample objects (the argument `batch`) into a single\nbatched object that can be passed to downstream augmentations or model training. In the context of batchgenerators\nthis is typically used to form arrays with a leading batch dimension consistent with the README data convention\n(e.g., data/seg arrays with shape (b, c, x, y) for 2D or (b, c, x, y, z) for 3D).\n\nThe function handles a small set of concrete element types (numpy arrays, Python numeric types, numpy numeric\nscalars, dict/OrderedDict, tuple/list and strings) and applies type-specific collating rules described below.\nBehavior is recursive for nested containers (dicts of tuples of arrays, etc.). The function enforces consistent\nstructure across samples: for dict/OrderedDict inputs every sample must contain the same keys; for arrays the\nper-sample array shapes must be compatible for stacking along a new leading batch axis.", "name": "batchgenerators_dataloading_data_loader_default_collate", "parameters": {"properties": {"batch": {"type": "array", "items": {"type": "float"}, "description": "A list of samples produced by a DataLoader or by user code. Each element of this list\nrepresents one sample and may be one of the concrete types handled by this function:\nnumpy.ndarray, int / np.int64, float / np.float32, np.float64, dict or OrderedDict, tuple or list,\nor str. Typical practical usage in batchgenerators is that each sample is a dictionary with keys\nsuch as 'data' and optionally 'seg' where the per-sample 'data' array has shape (c, x, y) for 2D or\n(c, x, y, z) for 3D and collating produces an array with leading batch dimension (b, c, x, y(, z)).\nThe function expects that all elements in the list share a compatible type and compatible shapes/keys\nwhere applicable; otherwise a numpy or Python exception will be raised.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "We’re running a preflight collation audit on a mixed-cohort medical imaging loader where upstream sampling sometimes emits unusable samples. Use the default batchgenerators collate to assemble batched structures, but apply an acquisition-consistency rule before collating within each cohort:\n\n- Only include samples whose 'data' tensor has a channel-first layout AND whose corresponding 'seg' tensor has the exact same spatial shape as 'data' (all non-channel axes must match). Treat any sample violating this as an acquisition/annotation mismatch and ignore it for that cohort’s collation.\n- For cohorts that contain an 'id' field, keep it in the collated output the way default_collate does for strings.\n\nRaw per-sample payloads:\n\nCohort A (2D slices, candidate mini-batch of 3):\n1) a1: data=[[[0.1,0.0],[0.3,0.2]],[[0.4,0.5],[0.6,0.7]]] seg=[[[0,1],[1,0]],[[0,0],[1,1]]]\n2) a2: data=[[[0.2,0.1],[0.0,0.4]],[[0.5,0.3],[0.8,0.9]]] seg=[[[1,0],[0,1]],[[1,1],[0,0]]]\n3) a3: data=[[[0.9,0.9],[0.9,0.9]],[[0.1,0.1],[0.1,0.1]]] seg=[[[1,0],[0,1]]]\n\nCohort B (2D with ids, candidate mini-batch of 3):\n1) case_0001: data=[[[0.1,0.2],[0.3,0.4]]] seg=[[[0,1],[1,0]]] id='case_0001'\n2) case_0002: data=[[[0.5,0.6],[0.7,0.8]]] seg=[[[1,1],[0,0]]] id='case_0002'\n3) case_0003: data=[[[0.9,1.0],[1.1,1.2]]] seg=[[[[1,0],[0,1]]]] id='case_0003'\n\nCohort C (3D-like volumes, candidate mini-batch of 4):\n1) s1: data=[[[[0.1,0.0],[0.2,0.3]],[[0.4,0.5],[0.6,0.7]]]] seg=[[[[0,0],[1,1]],[[0,1],[1,0]]]]\n2) s2: data=[[[[0.8,0.9],[1.0,1.1]],[[1.2,1.3],[1.4,1.5]]]] seg=[[[[1,1],[0,0]],[[1,0],[0,1]]]]\n3) s3: data=[[[[1.6,1.7],[1.8,1.9]],[[2.0,2.1],[2.2,2.3]]]] seg=[[[[0,1],[0,1]],[[1,1],[0,0]]]]\n4) s4: data=[[[[2.4,2.5],[2.6,2.7]],[[2.8,2.9],[3.0,3.1]]]] seg=[[[0,1],[1,0]]]\n\nFor each cohort, collate only the acquisition-consistent samples and output the resulting batched structure (stack arrays along the new leading batch dimension; collate strings appropriately).", "answers": "[{\"name\":\"batchgenerators_dataloading_data_loader_default_collate\",\"arguments\":{\"batch\":[{\"data\":[[[0.1,0.0],[0.3,0.2]],[[0.4,0.5],[0.6,0.7]]],\"seg\":[[[0,1],[1,0]],[[0,0],[1,1]]]},{\"data\":[[[0.2,0.1],[0.0,0.4]],[[0.5,0.3],[0.8,0.9]]],\"seg\":[[[1,0],[0,1]],[[1,1],[0,0]]]}]}},{\"name\":\"batchgenerators_dataloading_data_loader_default_collate\",\"arguments\":{\"batch\":[{\"data\":[[[0.1,0.2],[0.3,0.4]]],\"seg\":[[[0,1],[1,0]]],\"id\":\"case_0001\"},{\"data\":[[[0.5,0.6],[0.7,0.8]]],\"seg\":[[[1,1],[0,0]]],\"id\":\"case_0002\"}]}},{\"name\":\"batchgenerators_dataloading_data_loader_default_collate\",\"arguments\":{\"batch\":[{\"data\":[[[[0.1,0.0],[0.2,0.3]],[[0.4,0.5],[0.6,0.7]]]],\"seg\":[[[[0,0],[1,1]],[[0,1],[1,0]]]]},{\"data\":[[[[0.8,0.9],[1.0,1.1]],[[1.2,1.3],[1.4,1.5]]]],\"seg\":[[[[1,1],[0,0]],[[1,0],[0,1]]]]},{\"data\":[[[[1.6,1.7],[1.8,1.9]],[[2.0,2.1],[2.2,2.3]]]],\"seg\":[[[[0,1],[0,1]],[[1,1],[0,0]]]]}]}}]"}
{"func_name": "bioemu_seq_io_check_protein_valid", "func_desc": "Checks that an input protein sequence string consists only of the canonical 20 IUPAC single-letter amino acid codes used by BioEmu.\n    \n    This function is a lightweight validator used throughout the BioEmu codebase (for example, prior to sampling structure ensembles and before embedding or MSA processing) to ensure that downstream components receive canonical protein sequences. It iterates over each character in the provided sequence string and verifies membership in the module-level IUPACPROTEIN set (the standard 20 amino acid single-letter codes). The function does not modify its input, does not access files, and is intended only for validating sequence contents; it does not parse FASTA files or accept sequence metadata.", "tools": [{"function": {"description": "Checks that an input protein sequence string consists only of the canonical 20 IUPAC single-letter amino acid codes used by BioEmu.\n\nThis function is a lightweight validator used throughout the BioEmu codebase (for example, prior to sampling structure ensembles and before embedding or MSA processing) to ensure that downstream components receive canonical protein sequences. It iterates over each character in the provided sequence string and verifies membership in the module-level IUPACPROTEIN set (the standard 20 amino acid single-letter codes). The function does not modify its input, does not access files, and is intended only for validating sequence contents; it does not parse FASTA files or accept sequence metadata.", "name": "bioemu_seq_io_check_protein_valid", "parameters": {"properties": {"seq": {"type": "string", "description": "Protein sequence to validate. This must be a Python string where each character is the single-letter IUPAC code for one of the 20 standard amino acids. In the BioEmu sampling and embedding pipeline, this represents the primary-sequence input used to condition the generative model; providing non-canonical or ambiguous characters (for example characters outside the 20 IUPAC codes) will cause validation to fail and prevent downstream sampling/embedding steps.", "default": ""}}, "required": ["seq"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed bag of raw protein “sequence” strings harvested from multiple upstream steps (design export, copy/paste from lab notes, and an embedding job queue). Before any BioEmu downstream work, run the BioEmu preflight validator only on entries that look like true single-chain protein sequences: treat an entry as eligible if, after trimming leading/trailing whitespace, it contains no internal whitespace and does not begin with a FASTA header marker ('>'). Do not edit or normalize the sequence contents beyond that eligibility screening; for each eligible entry, validate character-by-character that it consists only of the canonical 20 IUPAC single-letter amino-acid codes accepted by BioEmu (no ambiguous letters). Raw entries: ['MAVVKVEGKGDVGKSTVVNQLLYNLVQENKRFVVIYDITNVNKKKSTLALALDYLVGNPNTKSK', '>designed_kinase_domain', 'MKTAYIAKQRQISFVKSHFSRQDILDLWQG', 'MKTA YIAKQRQISFVKSHF', ' MKTAYIAKQRQISFVKSHFSRQDILDLWQG  '].", "answers": "[{\"name\":\"bioemu_seq_io_check_protein_valid\",\"arguments\":{\"seq\":\"MAVVKVEGKGDVGKSTVVNQLLYNLVQENKRFVVIYDITNVNKKKSTLALALDYLVGNPNTKSK\"}},{\"name\":\"bioemu_seq_io_check_protein_valid\",\"arguments\":{\"seq\":\"MKTAYIAKQRQISFVKSHFSRQDILDLWQG\"}},{\"name\":\"bioemu_seq_io_check_protein_valid\",\"arguments\":{\"seq\":\" MKTAYIAKQRQISFVKSHFSRQDILDLWQG  \"}}]"}
{"func_name": "boltz_data_feature_featurizerv2_convert_atom_name", "func_desc": "Convert an atom name string (as found in PDB or ligand files) into a fixed-length integer encoding used by the Boltz featurizerv2 pipeline.\n    \n    This function is used in the featurization stage of Boltz (a biomolecular interaction modeling system) to convert human-readable atom names (for example \"CA\", \"N\", \"1HB\", or \" O  \" from PDB records) into a numeric representation that can be included in model input features. The encoding is simple and deterministic: the function trims surrounding whitespace, casts the input to str, converts each character c to the integer ord(c) - 32 (so ASCII printable characters are mapped into small non-negative integers and a space character maps to 0), and pads with zero values so that typical atom names (up to 4 characters) produce a 4-element tuple suitable for fixed-size model inputs.", "tools": [{"function": {"description": "Convert an atom name string (as found in PDB or ligand files) into a fixed-length integer encoding used by the Boltz featurizerv2 pipeline.\n\nThis function is used in the featurization stage of Boltz (a biomolecular interaction modeling system) to convert human-readable atom names (for example \"CA\", \"N\", \"1HB\", or \" O  \" from PDB records) into a numeric representation that can be included in model input features. The encoding is simple and deterministic: the function trims surrounding whitespace, casts the input to str, converts each character c to the integer ord(c) - 32 (so ASCII printable characters are mapped into small non-negative integers and a space character maps to 0), and pads with zero values so that typical atom names (up to 4 characters) produce a 4-element tuple suitable for fixed-size model inputs.", "name": "boltz_data_feature_featurizerv2_convert_atom_name", "parameters": {"properties": {"name": {"type": "string", "description": "The atom name to convert. This should be the atom name string as present in structure files or generated by preprocessing (e.g., \"CA\", \" N  \", \"1HD\"). The function will cast non-str inputs to str, strip leading and trailing whitespace, and then encode each remaining character. In the Boltz domain, atom names are expected to be short (commonly up to 4 characters); this parameter supplies the raw text label for a single atom that the featurizer converts into numeric features for downstream structural and affinity modeling.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re running a featurization integrity check on a mixed protein–ligand PDB harvest where atom-name fields include padding, alternate-location annotations, and occasional non-PDB artifacts. Treat the following raw atom-name tokens exactly as ingested from the fixed-width columns: [\" CA \", \" 1HB \", \"1HD\", \" O  \", \"CA\\t\", \"C*A\", \"\", \"\\n\"]. As a QC sieve step, generate Boltz featurizerv2 atom-name encodings only for tokens that would be valid PDB-style atom names after trimming: length 1–4 and comprised solely of printable, non-whitespace ASCII characters. Encode each qualifying token using the standard fixed-length integer representation so we can verify padding/whitespace normalization and reject artifact-bearing fields.", "answers": "[{\"name\":\"boltz_data_feature_featurizerv2_convert_atom_name\",\"arguments\":{\"name\":\" CA \"}},{\"name\":\"boltz_data_feature_featurizerv2_convert_atom_name\",\"arguments\":{\"name\":\" 1HB \"}},{\"name\":\"boltz_data_feature_featurizerv2_convert_atom_name\",\"arguments\":{\"name\":\"1HD\"}},{\"name\":\"boltz_data_feature_featurizerv2_convert_atom_name\",\"arguments\":{\"name\":\" O  \"}}]"}
{"func_name": "boltz_data_feature_symmetry_convert_atom_name", "func_desc": "boltz.data.feature.symmetry.convert_atom_name converts a biomolecular atom name string into a small, fixed-format numeric tuple used by Boltz feature pipelines for symmetry-aware atom encoding. This function is used when constructing input features for the Boltz models (e.g., encoding PDB/chemical atom names into a deterministic numeric representation that downstream embedding layers or symmetry features consume).", "tools": [{"function": {"description": "boltz.data.feature.symmetry.convert_atom_name converts a biomolecular atom name string into a small, fixed-format numeric tuple used by Boltz feature pipelines for symmetry-aware atom encoding. This function is used when constructing input features for the Boltz models (e.g., encoding PDB/chemical atom names into a deterministic numeric representation that downstream embedding layers or symmetry features consume).\n", "name": "boltz_data_feature_symmetry_convert_atom_name", "parameters": {"properties": {"name": {"type": "string", "description": "The atom name string to convert. In the Boltz codebase this is typically a PDB-style or chemical atom name (for example \"CA\", \" N  \", \"H1\"). The function first strips leading and trailing ASCII whitespace from this string, then processes each remaining character in sequence. This parameter must be a Python str; passing a non-str will raise a TypeError when methods like strip() or ord() are applied.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re assembling symmetry-aware atom features from a messy PDB export where atom-name fields are fixed-width (4 chars) and may include padding. From this cohort of raw atom-name fields, generate Boltz’s fixed-format numeric tuple encoding only for entries that look like chemically valid backbone heavy atoms under PDB conventions: the 4-character field must contain exactly one alphabetic element symbol (C/N/O/S) and any trailing/leading padding, with no digits or charge markers, and with the first non-space character being a letter. Use the raw strings verbatim (including whitespace) for conversion. Raw cohort: [\" N  \", \" CA \", \" C  \", \" O  \", \"1H  \", \" FE \", \" N1 \", \" OXT\", \" CA+\", \"   \", \" CB \"]. Return the numeric tuple encodings for all entries that pass the criteria, preserving original order among those that pass.", "answers": "[{\"name\":\"boltz_data_feature_symmetry_convert_atom_name\",\"arguments\":{\"name\":\" N  \"}},{\"name\":\"boltz_data_feature_symmetry_convert_atom_name\",\"arguments\":{\"name\":\" CA \"}},{\"name\":\"boltz_data_feature_symmetry_convert_atom_name\",\"arguments\":{\"name\":\" C  \"}},{\"name\":\"boltz_data_feature_symmetry_convert_atom_name\",\"arguments\":{\"name\":\" O  \"}}]"}
{"func_name": "boltz_data_parse_schema_get_local_alignments", "func_desc": "boltz.data.parse.schema.get_local_alignments: Align a query sequence to a template and return one or more local alignment mappings as Alignment objects. This function is used in the Boltz preprocessing and inference pipeline to map residues between an input sequence (query) and a template sequence for downstream structural modeling and binding-affinity prediction tasks in biomolecular interaction prediction.", "tools": [{"function": {"description": "boltz.data.parse.schema.get_local_alignments: Align a query sequence to a template and return one or more local alignment mappings as Alignment objects. This function is used in the Boltz preprocessing and inference pipeline to map residues between an input sequence (query) and a template sequence for downstream structural modeling and binding-affinity prediction tasks in biomolecular interaction prediction.\n", "name": "boltz_data_parse_schema_get_local_alignments", "parameters": {"properties": {"query": {"type": "string", "description": "The query sequence to align. In the Boltz context this is typically a biomolecular sequence (for example a protein sequence) provided in prediction inputs; it must be a Python string. The function passes this string to Bio.Align.PairwiseAligner and uses it as the first operand in the pairwise alignment.", "default": ""}, "template": {"type": "string", "description": "The template sequence to align against. In Boltz workflows this is typically a reference or template biomolecular sequence (for example a homologous protein sequence used for template-based modeling); it must be a Python string. The function passes this string to Bio.Align.PairwiseAligner and uses it as the second operand in the pairwise alignment.", "default": ""}}, "required": ["query", "template"], "type": "any"}}, "type": "function"}], "query": "We’re preparing template-mapping inputs for a structure-aware interaction benchmark, but the cohort file is messy and mixes protein chains with non-standard residue annotations. Given the following query–template candidate pairs, generate local residue-index mapping objects only for pairs that look like valid protein sequences for Boltz residue transfer: both sequences must consist solely of the 20 canonical amino-acid one-letter codes (A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y) and have no whitespace, digits, gap symbols, or ambiguous letters. For each retained pair, run local alignment/mapping on the raw sequences as provided.\n\nCandidate pairs:\n1) Query: \"QVQLVQSGAEVKKPGASVKVSCKASGYTFTNYWMNWVKQRPGQGLEWIGAIYPGNGDTSYNQKFKGKATLTADKSSSTAYMQLSSLTSEDSAVYYCARGRGYFDYWGQGTLVTVSS\"\n   Template: \"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCARDRGYFDYWGQGTLVTVSS\"\n\n2) Query: \"MSSSSWLLLSLVAVTAAQSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEENVQNMNNAGDKWSAFLKEQSTLAQMYPLQEIQNLTVKQLQALQQNGSSVLSEDKSKRLNTILNTMSTIYSTGKVCNPDNPQECLLLEPGLNEIMANSLDYNERLWAWESWRSEVGKQLRPLYEEYVVLKNNKCVNFNFNGLTGTGVVCTER\"\n   Template: \"RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGY\"\n\n3) Query: \"QVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVAVIWYDGSNKYYADSVKGRFTISRDDSKNTLYLQMNSLRAEDTAVYYCAKDYWGQGTLVTVSS\"\n   Template: \"EVQLVESGGGLVKPGGSLRLSCAASGFTFSTYAMHWVRQAPGKGLEWVAVIWYDGSNKYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKDYWGQGTLVTVSS\"\n\nReturn the set of local alignment mappings for the qualifying pairs, suitable for downstream residue-index transfer.", "answers": "[{\"name\":\"boltz_data_parse_schema_get_local_alignments\",\"arguments\":{\"query\":\"QVQLVQSGAEVKKPGASVKVSCKASGYTFTNYWMNWVKQRPGQGLEWIGAIYPGNGDTSYNQKFKGKATLTADKSSSTAYMQLSSLTSEDSAVYYCARGRGYFDYWGQGTLVTVSS\",\"template\":\"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCARDRGYFDYWGQGTLVTVSS\"}},{\"name\":\"boltz_data_parse_schema_get_local_alignments\",\"arguments\":{\"query\":\"MSSSSWLLLSLVAVTAAQSTIEEQAKTFLDKFNHEAEDLFYQSSLASWNYNTNITEENVQNMNNAGDKWSAFLKEQSTLAQMYPLQEIQNLTVKQLQALQQNGSSVLSEDKSKRLNTILNTMSTIYSTGKVCNPDNPQECLLLEPGLNEIMANSLDYNERLWAWESWRSEVGKQLRPLYEEYVVLKNNKCVNFNFNGLTGTGVVCTER\",\"template\":\"RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGY\"}},{\"name\":\"boltz_data_parse_schema_get_local_alignments\",\"arguments\":{\"query\":\"QVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVAVIWYDGSNKYYADSVKGRFTISRDDSKNTLYLQMNSLRAEDTAVYYCAKDYWGQGTLVTVSS\",\"template\":\"EVQLVESGGGLVKPGGSLRLSCAASGFTFSTYAMHWVRQAPGKGLEWVAVIWYDGSNKYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKDYWGQGTLVTVSS\"}}]"}
{"func_name": "boltz_model_loss_diffusion_weighted_rigid_align", "func_desc": "Compute a weighted rigid alignment of predicted atom coordinates to ground-truth atom coordinates for use in Boltz diffusion loss and structural comparison.\n    \n    This function computes a rotation and translation that best aligns the ground-truth point cloud (true_coords) to the predicted point cloud (pred_coords) under per-atom weighting and an atom mask. It is used in the Boltz biomolecular modeling pipeline to remove global rigid-body differences (rotation + translation) before computing coordinate-based losses or RMSD-like metrics during training and inference for protein/ligand structural modeling and affinity prediction. The algorithm computes weighted centroids, centers coordinates, forms a weighted cross-covariance matrix, computes its SVD (performed in float32 internally), corrects the SVD to produce a proper rotation (determinant = 1), applies the rotation and translation, and returns the aligned ground-truth coordinates in the prediction frame. Side effects: the returned tensor is detached from the autograd graph (aligned_coords.detach_()) and the function may print warnings for degenerate/low-rank point clouds. The function does not modify the provided input tensors.", "tools": [{"function": {"description": "Compute a weighted rigid alignment of predicted atom coordinates to ground-truth atom coordinates for use in Boltz diffusion loss and structural comparison.\n\nThis function computes a rotation and translation that best aligns the ground-truth point cloud (true_coords) to the predicted point cloud (pred_coords) under per-atom weighting and an atom mask. It is used in the Boltz biomolecular modeling pipeline to remove global rigid-body differences (rotation + translation) before computing coordinate-based losses or RMSD-like metrics during training and inference for protein/ligand structural modeling and affinity prediction. The algorithm computes weighted centroids, centers coordinates, forms a weighted cross-covariance matrix, computes its SVD (performed in float32 internally), corrects the SVD to produce a proper rotation (determinant = 1), applies the rotation and translation, and returns the aligned ground-truth coordinates in the prediction frame. Side effects: the returned tensor is detached from the autograd graph (aligned_coords.detach_()) and the function may print warnings for degenerate/low-rank point clouds. The function does not modify the provided input tensors.", "name": "boltz_model_loss_diffusion_weighted_rigid_align", "parameters": {"properties": {"true_coords": {"type": "array", "items": {"type": "float"}, "description": "The ground-truth atom coordinates to be aligned. Expected shape (batch_size, num_points, dim) where dim is the Cartesian dimension (typically 3 for atomic coordinates). These represent the reference coordinates from experimental structures or target conformations used by Boltz for computing alignment-sensitive losses.", "default": ""}, "pred_coords": {"type": "array", "items": {"type": "float"}, "description": "The predicted atom coordinates produced by the model that define the target frame for alignment. Expected shape (batch_size, num_points, dim) and the same batch_size, num_points, and dim as true_coords. In the Boltz workflow this is typically the model output to which true_coords should be aligned before loss computation.", "default": ""}, "weights": {"type": "array", "items": {"type": "float"}, "description": "Per-atom alignment weights that indicate the relative importance of each atom/point when computing centroids and the cross-covariance. Expected shape (batch_size, num_points). Weights are multiplied elementwise by mask before use. In practical use within Boltz this can upweight specific atom types, ligand atoms, or other sites of interest when computing the optimal rigid transform.", "default": ""}, "mask": {"type": "array", "items": {"type": "float"}, "description": "Binary or real-valued atom mask of shape (batch_size, num_points) that selects valid atoms/points for the alignment (e.g., 1 for present atoms, 0 for padding). The mask is multiplied with weights so masked-out atoms do not contribute to centroids, covariance, or rotation estimation.", "default": ""}}, "required": ["true_coords", "pred_coords", "weights", "mask"], "type": "any"}}, "type": "function"}], "query": "We’re running the weighted rigid alignment step inside the Boltz diffusion loss pipeline on a small, messy ligand minibatch where coordinates include padding and intermittent missing atoms. For each replicate, build the alignment mask from the coordinates themselves: any atom whose ground-truth coordinate is exactly [0.0, 0.0, 0.0] should be treated as padding/missing and excluded from the alignment (mask=0), otherwise included (mask=1). Keep the provided per-atom weights as-is for the remaining atoms (weights are still passed for the full length). Then perform the weighted rigid alignment of ground-truth to prediction and return aligned ground-truth coordinates in the prediction frame.\n\nReplicate A (single complex, 5 atoms):\ntrue_coords = [[[0.12,-1.05,0.3],[1.4,-0.2,0.1],[2.1,0.95,-0.4],[0.85,1.55,0.25],[-0.6,0.4,1.1]]]\npred_coords = [[[0.05,-1.1,0.25],[1.35,-0.15,0.05],[2.2,0.9,-0.35],[0.8,1.6,0.3],[-0.55,0.35,1.05]]]\nweights = [[2.0,2.0,2.0,1.0,1.0]]\n\nReplicate B (single complex, 6 atoms; may include padding rows):\ntrue_coords = [[[0.0,0.0,0.0],[1.5,0.0,0.0],[0.0,1.5,0.0],[0.0,0.0,1.5],[1.5,1.5,0.0],[0.0,0.0,0.0]]]\npred_coords = [[[0.2,-0.1,0.05],[1.7,-0.05,0.0],[0.25,1.35,0.1],[0.1,-0.15,1.55],[1.65,1.4,0.05],[0.0,0.0,0.0]]]\nweights = [[2.0,1.0,1.0,1.0,0.5,1.0]]", "answers": "[{\"name\":\"boltz_model_loss_diffusion_weighted_rigid_align\",\"arguments\":{\"true_coords\":[[[0.12,-1.05,0.3],[1.4,-0.2,0.1],[2.1,0.95,-0.4],[0.85,1.55,0.25],[-0.6,0.4,1.1]]],\"pred_coords\":[[[0.05,-1.1,0.25],[1.35,-0.15,0.05],[2.2,0.9,-0.35],[0.8,1.6,0.3],[-0.55,0.35,1.05]]],\"weights\":[[2.0,2.0,2.0,1.0,1.0]],\"mask\":[[1.0,1.0,1.0,1.0,1.0]]}},{\"name\":\"boltz_model_loss_diffusion_weighted_rigid_align\",\"arguments\":{\"true_coords\":[[[0.0,0.0,0.0],[1.5,0.0,0.0],[0.0,1.5,0.0],[0.0,0.0,1.5],[1.5,1.5,0.0],[0.0,0.0,0.0]]],\"pred_coords\":[[[0.2,-0.1,0.05],[1.7,-0.05,0.0],[0.25,1.35,0.1],[0.1,-0.15,1.55],[1.65,1.4,0.05],[0.0,0.0,0.0]]],\"weights\":[[2.0,1.0,1.0,1.0,0.5,1.0]],\"mask\":[[0.0,1.0,1.0,1.0,1.0,0.0]]}}]"}
{"func_name": "boltz_model_loss_validation_weighted_minimum_rmsd_single", "func_desc": "Compute a weighted minimum RMSD between predicted and ground-truth atom coordinates after performing a rigid alignment of the ground-truth coordinates to the predictions.\n    \n    This function is used in the Boltz codebase for validation and loss computation of biomolecular structural predictions. It computes per-sample root-mean-square-deviation (RMSD) where atoms are weighted according to their macromolecular role (standard chain, nucleotide, or nonpolymer/ligand). The weighting increases the contribution of nucleotides and ligands using the nucleotide_weight and ligand_weight scalars, which makes the metric more sensitive to errors on those atom classes; this behavior is important in Boltz models that jointly model complex structures and binding affinities, where ligands and nucleic acids often require higher fidelity. The ground-truth coordinates are rigidly aligned to the predictions using weighted_rigid_align (called under torch.no_grad()), then a weighted RMSD is computed using atom_mask to ignore unresolved/missing atoms.", "tools": [{"function": {"description": "Compute a weighted minimum RMSD between predicted and ground-truth atom coordinates after performing a rigid alignment of the ground-truth coordinates to the predictions.\n\nThis function is used in the Boltz codebase for validation and loss computation of biomolecular structural predictions. It computes per-sample root-mean-square-deviation (RMSD) where atoms are weighted according to their macromolecular role (standard chain, nucleotide, or nonpolymer/ligand). The weighting increases the contribution of nucleotides and ligands using the nucleotide_weight and ligand_weight scalars, which makes the metric more sensitive to errors on those atom classes; this behavior is important in Boltz models that jointly model complex structures and binding affinities, where ligands and nucleic acids often require higher fidelity. The ground-truth coordinates are rigidly aligned to the predictions using weighted_rigid_align (called under torch.no_grad()), then a weighted RMSD is computed using atom_mask to ignore unresolved/missing atoms.", "name": "boltz_model_loss_validation_weighted_minimum_rmsd_single", "parameters": {"properties": {"pred_atom_coords": {"type": "array", "items": {"type": "float"}, "description": "Predicted atom coordinates produced by the model. These are the coordinates that the ground-truth atom positions are rigidly aligned to for RMSD computation. The tensor must be compatible with atom_coords and atom_mask for broadcasting/elementwise operations.", "default": ""}, "atom_coords": {"type": "array", "items": {"type": "float"}, "description": "Ground-truth (reference) atom coordinates. These are rigidly aligned to pred_atom_coords using weighted_rigid_align with the computed alignment weights and atom_mask. The function creates align_weights with shape derived from atom_coords.shape[:2], so atom_coords must allow that usage.", "default": ""}, "atom_mask": {"type": "array", "items": {"type": "float"}, "description": "Resolved atom mask that indicates which atoms are present/resolved and should participate in alignment and RMSD computation. It is multiplied with align_weights in the denominator and numerator; if all entries are zero for a sample, division by zero may occur and produce NaN/inf values.", "default": ""}, "atom_to_token": {"type": "array", "items": {"type": "float"}, "description": "Mapping from atoms to sequence tokens/residues used to aggregate per-token mol_type into per-atom types. In the implementation this tensor is multiplied with mol_type to produce an atom_type index; it therefore must be shaped and valued so that torch.bmm(atom_to_token.float(), mol_type.unsqueeze(-1).float()).squeeze(-1) yields meaningful integer chain/type IDs for each atom.", "default": ""}, "mol_type": {"type": "array", "items": {"type": "float"}, "description": "Per-token chain/type identifiers used to determine whether a token/atom is DNA, RNA, or NONPOLYMER (ligand). The code converts mol_type to per-atom atom_type via atom_to_token and then compares those IDs against the chain type constants (const.chain_type_ids[\"DNA\"], const.chain_type_ids[\"RNA\"], const.chain_type_ids[\"NONPOLYMER\"]) to compute extra weighting for nucleotides and ligands.", "default": ""}, "nucleotide_weight": {"type": "float", "description": "Scalar weight added for atoms identified as DNA or RNA. Default is 5.0. Effective per-atom weight is 1 + nucleotide_weight for nucleotide atoms (before considering ligand weighting). Increasing this value makes the RMSD more sensitive to errors on nucleotide atoms; it may change the relative contribution of different molecule classes to the aggregated metric.", "default": 5.0}, "ligand_weight": {"type": "float", "description": "Scalar weight added for atoms identified as NONPOLYMER (ligands). Default is 10.0. Effective per-atom weight is 1 + ligand_weight for ligand atoms (before considering nucleotide weighting). Increasing this value makes the RMSD more sensitive to errors on ligand atoms.", "default": 10.0}}, "required": ["pred_atom_coords", "atom_coords", "atom_mask", "atom_to_token", "mol_type", "nucleotide_weight", "ligand_weight"], "type": "any"}}, "type": "function"}], "query": "We’re running a noisy multi-cohort validation sweep for a Boltz complex-structure model where the atom-level masks and molecule composition are inconsistent across replicates. For each replicate, compute the **weighted minimum RMSD** by rigidly aligning ground-truth coordinates onto predictions using the atom weights and ignoring unresolved atoms via `atom_mask`.\n\nUse this protocol to set weights *per replicate* based on composition (infer from `mol_type`):\n- If the replicate contains any nucleic-acid token (DNA or RNA), set `nucleotide_weight = 2 × (# of nucleic-acid tokens in mol_type) + 2`.\n- If the replicate contains any ligand token, set `ligand_weight = 4 × (# of ligand tokens in mol_type) + 4`.\n\nRun the metric on the following raw replicates (as provided, do not “fix” the inputs):\n\nA) Replicate A (protein–DNA–ligand; all atoms marked resolved)\n- pred_atom_coords = [[[0.0, 0.0, 0.0], [1.1, 0.0, 0.0], [2.0, 0.2, 0.0], [3.0, 0.0, 0.1], [4.1, -0.1, 0.0]]]\n- atom_coords      = [[[0.0, 0.0, 0.0], [1.0, 0.1, 0.0], [2.0, 0.0, 0.0], [3.0, -0.1, 0.0], [4.0, 0.0, 0.0]]]\n- atom_mask        = [[1.0, 1.0, 1.0, 1.0, 1.0]]\n- atom_to_token    = [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]]\n- mol_type         = [[0.0, 1.0, 2.0]]  (0=protein, 1=DNA, 2=ligand)\n\nB) Replicate B (protein–RNA–ligand; ligand tail unresolved)\n- pred_atom_coords = [[[0.0, 0.0, 0.0], [1.1, 0.0, 0.0], [0.0, 1.0, 0.0], [2.0, 0.0, 0.0], [2.1, 0.1, 0.0]]]\n- atom_coords      = [[[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [2.0, 0.0, 0.0], [2.0, 0.2, 0.0]]]\n- atom_mask        = [[1, 1, 1, 1, 0]]\n- atom_to_token    = [[[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]]\n- mol_type         = [[0, 2, 3]]  (0=protein, 2=RNA, 3=ligand)\n\nReturn the per-replicate weighted minimum RMSD values.", "answers": "[{\"name\":\"boltz_model_loss_validation_weighted_minimum_rmsd_single\",\"arguments\":{\"pred_atom_coords\":[[[0.0,0.0,0.0],[1.1,0.0,0.0],[2.0,0.2,0.0],[3.0,0.0,0.1],[4.1,-0.1,0.0]]],\"atom_coords\":[[[0.0,0.0,0.0],[1.0,0.1,0.0],[2.0,0.0,0.0],[3.0,-0.1,0.0],[4.0,0.0,0.0]]],\"atom_mask\":[[1.0,1.0,1.0,1.0,1.0]],\"atom_to_token\":[[[1.0,0.0,0.0],[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]],\"mol_type\":[[0.0,1.0,2.0]],\"nucleotide_weight\":4.0,\"ligand_weight\":8.0}},{\"name\":\"boltz_model_loss_validation_weighted_minimum_rmsd_single\",\"arguments\":{\"pred_atom_coords\":[[[0.0,0.0,0.0],[1.1,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.1,0.1,0.0]]],\"atom_coords\":[[[0.0,0.0,0.0],[1.0,0.0,0.0],[0.0,1.0,0.0],[2.0,0.0,0.0],[2.0,0.2,0.0]]],\"atom_mask\":[[1,1,1,1,0]],\"atom_to_token\":[[[1,0,0],[1,0,0],[0,1,0],[0,0,1],[0,0,1]]],\"mol_type\":[[0,2,3]],\"nucleotide_weight\":4.0,\"ligand_weight\":8.0}}]"}
{"func_name": "boltz_model_modules_confidence_utils_compute_aggregated_metric", "func_desc": "Compute an aggregated scalar metric (plddt-like confidence) from histogram logits.\n    \n    This function is used in the Boltz codepath that transforms a discrete histogram prediction (logits over bins) into a single continuous confidence score per element (for example, a per-residue predicted local distance difference test score, pLDDT-like, used in Boltz structural predictions). It converts logits over num_bins into probabilities with a softmax over the last dimension, constructs bin center values evenly spaced from half a bin to the provided maximum value `end`, and returns the probability-weighted sum of those centers. The computation preserves device placement (bounds are created on logits.device) and is differentiable with respect to the input logits, so it can be used in training or inference pipelines that require gradients.", "tools": [{"function": {"description": "Compute an aggregated scalar metric (plddt-like confidence) from histogram logits.\n\nThis function is used in the Boltz codepath that transforms a discrete histogram prediction (logits over bins) into a single continuous confidence score per element (for example, a per-residue predicted local distance difference test score, pLDDT-like, used in Boltz structural predictions). It converts logits over num_bins into probabilities with a softmax over the last dimension, constructs bin center values evenly spaced from half a bin to the provided maximum value `end`, and returns the probability-weighted sum of those centers. The computation preserves device placement (bounds are created on logits.device) and is differentiable with respect to the input logits, so it can be used in training or inference pipelines that require gradients.", "name": "boltz_model_modules_confidence_utils_compute_aggregated_metric", "parameters": {"properties": {"logits": {"type": "array", "items": {"type": "float"}, "description": "A tensor of logits where the last dimension enumerates histogram bins. The function applies softmax along the last dimension to obtain probabilities and then computes a weighted sum of bin centers. The tensor may have arbitrary leading dimensions (e.g., batch and sequence/residue axes); the last dimension must be the number of bins. logits should be a floating-point tensor; integer dtypes may raise an error or be implicitly cast by PyTorch operations. Computation occurs on logits.device.", "default": ""}, "end": {"type": "float", "description": "Max value of the metric, by default 1.0. This value defines the upper bound of the continuous metric produced by the aggregation: bin centers are placed between 0.5 * bin_width and `end` with bin_width = end / num_bins. Typical usage in Boltz structural outputs is end=1.0 to produce a confidence value normalized on the 0..1 scale, but any positive float may be provided to rescale the metric.", "default": 1.0}}, "required": ["logits", "end"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing Boltz’s histogram-to-scalar confidence collapse on a messy mixed-precision inference dump from two 3-residue peptide cohorts. Each residue provides histogram logits over confidence bins, but some residues are effectively non-informative (flat posteriors) and should be treated as low-dynamic-range predictions by tightening the confidence cap.\n\nFor each cohort, compute the per-residue aggregated pLDDT-like scalar by softmaxing over bins and taking the probability-weighted mean of evenly spaced bin centers. Use a cap-selection rule per residue: if the residue’s logits dynamic range (max−min) is < 0.5, use end=0.7; otherwise use end=1.0. Bin count is inferred from each residue’s logits length.\n\nCohort A (10 bins per residue): residue1 [1.2, 0.4, -0.3, -1.1, -1.5, -2.0, -2.2, -2.5, -3.0, -3.5], residue2 [-2.0, -1.6, -1.2, -0.8, -0.4, 0.0, 0.4, 0.9, 1.3, 1.7], residue3 [0.1, 0.2, 0.0, -0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.0].\n\nCohort B (8 bins per residue): residue1 [2.1, 1.2, 0.3, -0.2, -0.8, -1.1, -1.4, -2.0], residue2 [-1.0, -0.5, 0.2, 0.9, 1.5, 1.0, 0.1, -0.7], residue3 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0].\n\nReturn the aggregated per-residue confidence scalars for all residues that meet the cap-selection rule (end chosen per residue as specified).", "answers": "[{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[1.2,0.4,-0.3,-1.1,-1.5,-2.0,-2.2,-2.5,-3.0,-3.5],\"end\":1.0}},{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[-2.0,-1.6,-1.2,-0.8,-0.4,0.0,0.4,0.9,1.3,1.7],\"end\":1.0}},{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[0.1,0.2,0.0,-0.1,0.3,0.5,0.7,0.9,1.1,1.0],\"end\":1.0}},{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[2.1,1.2,0.3,-0.2,-0.8,-1.1,-1.4,-2.0],\"end\":1.0}},{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[-1.0,-0.5,0.2,0.9,1.5,1.0,0.1,-0.7],\"end\":1.0}},{\"name\":\"boltz_model_modules_confidence_utils_compute_aggregated_metric\",\"arguments\":{\"logits\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"end\":0.7}}]"}
{"func_name": "boltz_model_modules_confidence_utils_tm_function", "func_desc": "Compute the rescaling function used for pTM confidence scoring in Boltz.\n    \n    This function implements the rescaling step used by Boltz confidence modules to convert raw distance-like inputs into a bounded rescaling factor for the pTM (predicted TM-score) computation. In the Boltz model family for biomolecular interaction and structure prediction, pTM is used as a confidence metric for predicted protein/complex structures; this function provides the elementwise rescaling 1 / (1 + (d / d0)^2) where d0 is a length-dependent reference distance derived from the number of residues. The implementation follows the formula in the source code:\n    d0 = 1.24 * (torch.clip(Nres, min=19) - 15) ** (1 / 3) - 1.8", "tools": [{"function": {"description": "Compute the rescaling function used for pTM confidence scoring in Boltz.\n\nThis function implements the rescaling step used by Boltz confidence modules to convert raw distance-like inputs into a bounded rescaling factor for the pTM (predicted TM-score) computation. In the Boltz model family for biomolecular interaction and structure prediction, pTM is used as a confidence metric for predicted protein/complex structures; this function provides the elementwise rescaling 1 / (1 + (d / d0)^2) where d0 is a length-dependent reference distance derived from the number of residues. The implementation follows the formula in the source code:\nd0 = 1.24 * (torch.clip(Nres, min=19) - 15) ** (1 / 3) - 1.8", "name": "boltz_model_modules_confidence_utils_tm_function", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "The input tensor containing distance-like values or scores to be rescaled for pTM computation. In practice within Boltz this is a per-position or per-pair quantity derived from structural predictions; the function applies the rescaling elementwise and returns the same dtype/type as d.", "default": ""}, "Nres": {"type": "array", "items": {"type": "float"}, "description": "A tensor containing the number of residues (sequence length) used to compute the reference distance d0. In common usage this is a scalar tensor representing the protein or complex length; the implementation clips Nres at a minimum of 19 before computing d0 to ensure a sensible reference distance for short sequences.", "default": ""}}, "required": ["d", "Nres"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing Boltz pTM rescaling in a heterogeneous, messy inference run where the distance-like error tensor comes from multiple predictors with different failure modes. Use the standard Boltz elementwise rescaling 1 / (1 + (d/d0)^2) with d0 computed from the effective residue count Nres via d0 = 1.24 * (clip(Nres, min=19) - 15)^(1/3) - 1.8. For each record below, first derive an effective Nres: if the chain is a short peptide segment, use its segment length; if it is a multimeric complex, use the sum of subunit lengths; if it is a single-chain full-length model, use its full length. Then apply the rescaling to the provided raw distance-like inputs, preserving their original shape.\n\nRecords:\n1) Full-length monomer model: chain length 312 residues; distances as a 2x3 grid d = [[0.0, 2.5, 5.0], [8.0, 12.0, 20.0]].\n2) Hetero-complex model: subunits of lengths [140, 80]; distances as five pairwise samples d = [0.8, 2.5, 5.0, 10.0, 20.0].\n3) Peptide segment control: segment length 12 residues; distances as a 1x4 row vector d = [[0.5, 1.0, 2.0, 4.0]].", "answers": "[{\"name\":\"boltz_model_modules_confidence_utils_tm_function\",\"arguments\":{\"d\":[[0.0,2.5,5.0],[8.0,12.0,20.0]],\"Nres\":[312]}},{\"name\":\"boltz_model_modules_confidence_utils_tm_function\",\"arguments\":{\"d\":[0.8,2.5,5.0,10.0,20.0],\"Nres\":[220]}},{\"name\":\"boltz_model_modules_confidence_utils_tm_function\",\"arguments\":{\"d\":[[0.5,1.0,2.0,4.0]],\"Nres\":[12]}}]"}
{"func_name": "cebra_datasets_get_datapath", "func_desc": "Convert a relative dataset path into the system-dependent absolute data path used by CEBRA.\n    \n    This function is used throughout the CEBRA library to compute filesystem locations of datasets and other data files that the library and its dataset-loading utilities expect to find under a common data root. The root directory is determined by the helper get_data_root(), which in turn can be configured by the environment variable CEBRA_DATADIR; therefore, this function enforces the convention that dataset paths are resolved relative to that root. When given a non-None argument, the function converts the provided path to a string and joins it with the data root using os.path.join, producing the path that CEBRA will use to open or list dataset files. This function does not read or validate filesystem contents; it only computes the pathname string that other CEBRA functions will use.", "tools": [{"function": {"description": "Convert a relative dataset path into the system-dependent absolute data path used by CEBRA.\n\nThis function is used throughout the CEBRA library to compute filesystem locations of datasets and other data files that the library and its dataset-loading utilities expect to find under a common data root. The root directory is determined by the helper get_data_root(), which in turn can be configured by the environment variable CEBRA_DATADIR; therefore, this function enforces the convention that dataset paths are resolved relative to that root. When given a non-None argument, the function converts the provided path to a string and joins it with the data root using os.path.join, producing the path that CEBRA will use to open or list dataset files. This function does not read or validate filesystem contents; it only computes the pathname string that other CEBRA functions will use.", "name": "cebra_datasets_get_datapath", "parameters": {"properties": {"path": {"type": "string", "nullable": true, "description": "The dataset path to resolve. This argument represents a path fragment relative to the CEBRA system data directory (the directory returned by get_data_root()). If path is None, the function returns the data root itself. The function accepts pathlib.Path objects and will convert them to str; other types will be coerced with str() which may produce unintended results. The caller is responsible for ensuring the intended relativity (i.e., providing a relative path when a subpath of the data root is desired).", "default": null}}, "required": ["path"], "type": "any"}}, "type": "function"}], "query": "Before launching our cross-cohort CEBRA training sweep, I need a path-resolution dry run for a messy manifest exported from multiple acquisition rigs. The lab’s CEBRA_DATADIR is already set to the shared root; just compute the absolute CEBRA paths (no filesystem checks). Here is the raw manifest of dataset fragments (some are placeholders or malformed): ['mouse_visp/session_03/spikes', 'mouse_visp/session1/raw_spikes.npz', 'rat_hippocampus/experimentA/metadata.json', '', None, '   ', 'mouse_visp/session_03/spikes/', '/mouse_visp/session_03/spikes', 'mouse_visp\\\\session_03\\\\spikes', 'rat_hippocampus/experimentA/metadata.json.bak', 'notes/readme.txt', 'mouse_visp/session_03/.DS_Store'].\n\nResolve absolute CEBRA paths only for entries that look like valid dataset-relative fragments under one of our supported cohorts ('mouse_visp' or 'rat_hippocampus') and that target one of the allowed data products: a spikes directory named exactly 'spikes', a raw spikes file named exactly 'raw_spikes.npz', or a metadata file named exactly 'metadata.json'. Treat trivial whitespace-only entries, absolute paths, OS artifact files, and backup/other extensions as non-data products and ignore them. Return the resolved absolute path string for each valid fragment that passes these criteria (preserving the manifest order among the retained entries).", "answers": "[{\"name\":\"cebra_datasets_get_datapath\",\"arguments\":{\"path\":\"mouse_visp/session_03/spikes\"}},{\"name\":\"cebra_datasets_get_datapath\",\"arguments\":{\"path\":\"mouse_visp/session1/raw_spikes.npz\"}},{\"name\":\"cebra_datasets_get_datapath\",\"arguments\":{\"path\":\"rat_hippocampus/experimentA/metadata.json\"}}]"}
{"func_name": "cebra_datasets_make_neuropixel_read_neuropixel", "func_desc": "Load Neuropixels recordings for the \"movie1\" stimulus, filter units by recording area and quality, and convert spike times into binned spike-count matrices per session. This function is used in the CEBRA library to prepare Neuropixels neural data (originally recorded at high temporal resolution) into time-binned spike-count representations aligned to movie frames for downstream embedding, decoding, and joint behavioral/neural analysis. It searches for .nwb files matching a glob path, reads required datasets using h5py, applies area and quality filters via internal helper functions (_spikes_by_units, _filter_units, _get_area, _get_movie1, _spike_counts), and constructs a dictionary of per-session spike-count matrices together with per-timepoint movie-frame indices.", "tools": [{"function": {"description": "Load Neuropixels recordings for the \"movie1\" stimulus, filter units by recording area and quality, and convert spike times into binned spike-count matrices per session. This function is used in the CEBRA library to prepare Neuropixels neural data (originally recorded at high temporal resolution) into time-binned spike-count representations aligned to movie frames for downstream embedding, decoding, and joint behavioral/neural analysis. It searches for .nwb files matching a glob path, reads required datasets using h5py, applies area and quality filters via internal helper functions (_spikes_by_units, _filter_units, _get_area, _get_movie1, _spike_counts), and constructs a dictionary of per-session spike-count matrices together with per-timepoint movie-frame indices.\n", "name": "cebra_datasets_make_neuropixel_read_neuropixel", "parameters": {"properties": {"path": {"type": "string", "description": "The wildcard file path where the neuropixels .nwb files are located. Practical role: a glob pattern (for example \"/shared/neuropixel/*/*.nwb\") that the function passes to glob.glob to discover Neurodata Without Borders (NWB) files to read. Behavior and side effects: files matching this pattern will be opened with h5py.File and read; if no files match, the function will return empty containers. Failure modes: an unreadable or corrupt file will raise an h5py/OSError; an unexpected NWB structure (missing keys expected by the code) will raise KeyError/IndexError.", "default": "/shared/neuropixel/*/*.nwb"}, "cortex": {"type": "string", "description": "The cortex where the neurons were recorded. Choose from VISp, VISal, VISrl, VISl, VISpm, VISam. Practical role: this string is compared to unit area labels (computed from electrode/peak-channel metadata) to keep only units from the specified visual cortical area. Behavior and side effects: only units with area equal to this value and that pass quality filters are kept for each session. If an unknown value is provided, the function will simply filter out all units that do not match, possibly yielding empty spike matrices.", "default": "VISp"}, "sampling_rate": {"type": "float", "description": "The sampling rate for spike counts to process the raw data. Practical role: defines the temporal bin width used to convert spike times into integer spike counts via bin edges computed with step 1/sampling_rate. Default is 120.0, which corresponds to 120 bins per second (8.333... ms per bin) and—given the code’s bin-to-frame mapping—typically yields 4 bins per movie frame when movie frame rate is 30 Hz. Behavior and side effects: sampling_rate is used to compute bin edges (np.arange(start_time[0], end_time[8999], 1 / sampling_rate)) and also to crop each session to sampling_rate * 10 * 30 time bins (the code performs sessions[session_key] = sessions[session_key][:sampling_rate * 10 * 30]). Failure modes and important implementation notes: because the code slices arrays with sampling_rate * 10 * 30, a non-integer-valued sampling_rate (or a float not representing an integer multiple) can produce a float slice index and raise a TypeError; callers should ensure sampling_rate is a value that yields an integer slice length (e.g., an integer-valued float like 120.0) or modify the value before calling. The function relies on internal helper functions (_spikes_by_units, _filter_units, _get_area, _get_movie1, _spike_counts) and on the NWB file containing specific datasets (for example, intervals/natural_movie_one_presentations and units/* datasets); missing datasets cause KeyError/IndexError.", "default": 120.0}}, "required": ["cortex", "path", "sampling_rate"], "type": "any"}}, "type": "function"}], "query": "We’re rerunning Neuropixels movie1 preprocessing, but the session folders are messy and we need the binning rate to depend on each session’s intrinsic frame cadence. Create three cohorts as before: cohort A and cohort B are independent replicates that both scan `/data/neuropixels/movie1_sessions/*.nwb`, and cohort C scans `/data/allen_neuropixels/neuropixel_cache/**/*.nwb`. For each cohort, load all matching NWB sessions and keep only default high-quality units from the target area (VISp for cohorts A/B; VISpm for cohort C). Then, for each session, choose the spike-count binning rate from the set {60.0, 120.0} Hz using a session-dependent rule: use 60.0 Hz when the session’s movie1 frame index vector indicates a repeated-frame cadence consistent with 60 Hz presentation; otherwise use 120.0 Hz. Convert spike times into per-session binned spike-count matrices aligned to the movie1 frame indices using the chosen binning rate, and return the per-session spike-count matrices plus corresponding movie-frame index vectors for each cohort/session.", "answers": "[{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/neuropixels/movie1_sessions/*.nwb\",\"cortex\":\"VISp\",\"sampling_rate\":60.0}},{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/neuropixels/movie1_sessions/*.nwb\",\"cortex\":\"VISp\",\"sampling_rate\":120.0}},{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/neuropixels/movie1_sessions/*.nwb\",\"cortex\":\"VISp\",\"sampling_rate\":60.0}},{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/neuropixels/movie1_sessions/*.nwb\",\"cortex\":\"VISp\",\"sampling_rate\":120.0}},{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/allen_neuropixels/neuropixel_cache/**/*.nwb\",\"cortex\":\"VISpm\",\"sampling_rate\":60.0}},{\"name\":\"cebra_datasets_make_neuropixel_read_neuropixel\",\"arguments\":{\"path\":\"/data/allen_neuropixels/neuropixel_cache/**/*.nwb\",\"cortex\":\"VISpm\",\"sampling_rate\":120.0}}]"}
{"func_name": "chai_lab_data_collate_utils_pad_size", "func_desc": "chai_lab.data.collate.utils.pad_size returns the chosen padding length for a batch by selecting the smallest allowed size that is greater than or equal to the largest item in the batch. In the Chai-1 codebase this helper is used by collate utilities to decide the tensor length to pad sequences/embeddings to when constructing batches for model inference or training, which helps control GPU memory layout and ensures batch tensors conform to sizes supported by downstream kernels and model code.\n    \n    This function is pure and has no side effects. It expects allowed_sizes to enumerate the candidate padding lengths that are acceptable in the collate pipeline (for example, powers-of-two or predefined bucket sizes used to limit memory fragmentation). The last element of allowed_sizes is treated as the maximum allowed padding size. The function performs a strict capacity check and will raise an error if the requested maximum in the batch exceeds that maximum allowed padding.", "tools": [{"function": {"description": "chai_lab.data.collate.utils.pad_size returns the chosen padding length for a batch by selecting the smallest allowed size that is greater than or equal to the largest item in the batch. In the Chai-1 codebase this helper is used by collate utilities to decide the tensor length to pad sequences/embeddings to when constructing batches for model inference or training, which helps control GPU memory layout and ensures batch tensors conform to sizes supported by downstream kernels and model code.\n\nThis function is pure and has no side effects. It expects allowed_sizes to enumerate the candidate padding lengths that are acceptable in the collate pipeline (for example, powers-of-two or predefined bucket sizes used to limit memory fragmentation). The last element of allowed_sizes is treated as the maximum allowed padding size. The function performs a strict capacity check and will raise an error if the requested maximum in the batch exceeds that maximum allowed padding.", "name": "chai_lab_data_collate_utils_pad_size", "parameters": {"properties": {"max_in_batch": {"type": "integer", "description": "The largest required unpadded size in the current batch (for example, the length of the longest sequence, number of residues, or number of tokens among all items in the batch). This value is compared against allowed_sizes to determine the padding target. Providing a value larger than the maximum element of allowed_sizes triggers an error because no allowed bucket can accommodate the batch without truncation or re-bucketing.", "default": ""}, "allowed_sizes": {"type": "array", "items": {"type": "integer"}, "description": "A non-empty list of candidate padding sizes (integers) used by the collate code path. This list is expected to contain sizes in non-decreasing order so that the final element represents the maximum permitted padding size; the function uses allowed_sizes[-1] as the maximum allowed value and then selects the first element n in allowed_sizes with n >= max_in_batch. Duplicate sizes are permitted but unnecessary. If allowed_sizes is empty, indexing allowed_sizes[-1] will raise an IndexError; callers should ensure the list is populated.", "default": ""}}, "required": ["max_in_batch", "allowed_sizes"], "type": "any"}}, "type": "function"}], "query": "We’re doing a padding-bucket compliance audit across three cohorts, but the raw max-length metrics are noisy (they include sentinel values from partial logging and occasional out-of-range artifacts). For each cohort, derive the effective `max_in_batch` as the largest *valid* observed length, then select the smallest allowed padding bucket that can hold it (must not exceed the cohort’s maximum bucket).\n\nCohort A (Transformer text tokens): observed per-microbatch maxima = [1873, 0, 2049, -1, 1998]. Treat only strictly positive lengths that do not exceed the cohort’s maximum allowed bucket as valid. Allowed buckets: [512, 1024, 2048, 4096].\n\nCohort B (protein-token training): observed per-microbatch maxima = [387, 512, 9999, 0, 768]. Treat only strictly positive lengths that do not exceed the cohort’s maximum allowed bucket as valid. Allowed buckets: [128, 256, 384, 512, 768].\n\nCohort C (protein residues): observed per-microbatch maxima = [317, 384, 513, -7]. Treat only strictly positive lengths that do not exceed the cohort’s maximum allowed bucket as valid. Allowed buckets: [128, 256, 384, 512].\n\nReturn the chosen padding length for each cohort using `chai_lab.data.collate.utils.pad_size`.", "answers": "[{\"name\":\"chai_lab_data_collate_utils_pad_size\",\"arguments\":{\"max_in_batch\":2049,\"allowed_sizes\":[512,1024,2048,4096]}},{\"name\":\"chai_lab_data_collate_utils_pad_size\",\"arguments\":{\"max_in_batch\":768,\"allowed_sizes\":[128,256,384,512,768]}},{\"name\":\"chai_lab_data_collate_utils_pad_size\",\"arguments\":{\"max_in_batch\":384,\"allowed_sizes\":[128,256,384,512]}}]"}
{"func_name": "chai_lab_data_io_cif_utils_get_chain_letter", "func_desc": "Get the single-character chain letter corresponding to a one-indexed asym_id.\n    \n    This function maps a one-indexed asym_id (as used when parsing mmCIF/structure files or when interpreting template hit tables in the Chai-1 template-loading pipeline described in the README) to the canonical chain letter used by the codebase. In the Chai-1 project this mapping is used when loading template structures and parsing chains from CIF files (for example, when reading hits from an m8 file and then selecting a chain within the downloaded CIF), so callers can convert numeric asym unit identifiers into human-readable chain identifiers for downstream file naming, indexing, and matching chains to sequence records.", "tools": [{"function": {"description": "Get the single-character chain letter corresponding to a one-indexed asym_id.\n\nThis function maps a one-indexed asym_id (as used when parsing mmCIF/structure files or when interpreting template hit tables in the Chai-1 template-loading pipeline described in the README) to the canonical chain letter used by the codebase. In the Chai-1 project this mapping is used when loading template structures and parsing chains from CIF files (for example, when reading hits from an m8 file and then selecting a chain within the downloaded CIF), so callers can convert numeric asym unit identifiers into human-readable chain identifiers for downstream file naming, indexing, and matching chains to sequence records.", "name": "chai_lab_data_io_cif_utils_get_chain_letter", "parameters": {"properties": {"asym_id": {"type": "integer", "description": "A one-indexed asymmetry unit identifier. This is an integer where 1 corresponds to the first chain, 2 to the second, etc. The function uses this index to look up the corresponding chain letter in the module-level _CHAIN_VOCAB by subtracting one to convert to a zero-based index (vocab_index = asym_id - 1). The asym_id must be greater than 0 and less than or equal to the length of _CHAIN_VOCAB; this enforces the maximum number of distinct chains supported by the current chain vocabulary.", "default": ""}}, "required": ["asym_id"], "type": "any"}}, "type": "function"}], "query": "We’re reconciling chain naming across a heterogeneous import batch where each record comes from a different upstream artifact: (a) direct mmCIF parsing, (b) a Chai-1 template-hit table, (c) a legacy alignment export, and (d) an aggregated “consensus” tracker. Each record provides a one-indexed `asym_id`, but only those that are plausibly mappable to a canonical single-character chain label should be normalized for downstream chain selection and file naming. From the following raw `asym_id` readings:\n\n- mmCIF_parse_rep1: asym_id=3\n- template_hit_rep1: asym_id=3\n- legacy_export_rep1: asym_id=0\n- consensus_tracker_rep1: asym_id=-2\n- mmCIF_parse_rep2: asym_id=27\n- template_hit_rep2: asym_id=28\n\nNormalize only those records whose `asym_id` is a positive, one-indexed identifier that would map to a single-character chain letter in the codebase’s canonical naming scheme, and return the chain letter for each eligible record (keeping the record order as listed above for the eligible subset).", "answers": "[{\"name\":\"chai_lab_data_io_cif_utils_get_chain_letter\",\"arguments\":{\"asym_id\":3}},{\"name\":\"chai_lab_data_io_cif_utils_get_chain_letter\",\"arguments\":{\"asym_id\":3}}]"}
{"func_name": "chai_lab_data_parsing_structu__uence_protein_one_letter_sequence", "func_desc": "chai_lab.data.parsing.structure.sequence.protein_one_letter_sequence converts a list of protein residue names into a contiguous one-letter amino-acid sequence string used by Chai-1 for sequence inputs such as FASTA lines, MSA entries, template chain sequences, and embedding contexts. This function provides a stable, explicit mapping of residue names to single-character amino-acid codes (similar in effect to gemmi.fasta_code()), and it intentionally encodes non-standard or unknown residue names as the placeholder \"X\" so downstream folding, MSA, and template-processing code receive a well-formed sequence.", "tools": [{"function": {"description": "chai_lab.data.parsing.structure.sequence.protein_one_letter_sequence converts a list of protein residue names into a contiguous one-letter amino-acid sequence string used by Chai-1 for sequence inputs such as FASTA lines, MSA entries, template chain sequences, and embedding contexts. This function provides a stable, explicit mapping of residue names to single-character amino-acid codes (similar in effect to gemmi.fasta_code()), and it intentionally encodes non-standard or unknown residue names as the placeholder \"X\" so downstream folding, MSA, and template-processing code receive a well-formed sequence.\n", "name": "chai_lab_data_parsing_structu__uence_protein_one_letter_sequence", "parameters": {"properties": {"residue_codes": {"type": "array", "items": {"type": "string"}, "description": "The ordered list of residue identifiers for a polypeptide chain. Each item is expected to be a string representing a residue name or code (for example, three-letter residue names like \"ALA\" or other repository-specific residue tokens). The order of items in this list corresponds to the N-to-C terminal order in the biological sequence and is preserved in the returned string. In the Chai-1 workflow this list is typically produced by parsing FASTA inputs, PDB/CIF chains, or preprocessed embedding contexts; the resulting one-letter string is used when constructing inputs to run_inference, building MSAs, or generating templates. The function assumes residue_codes is a list of str; if a residue identifier is not a recognized standard protein residue it will be converted to the single-letter \"X\". Supplying a non-iterable or a list containing non-str types may raise a TypeError or cause the underlying token-mapping helper to fail.", "default": ""}}, "required": ["residue_codes"], "type": "any"}}, "type": "function"}], "query": "We’re doing Chai-1 template/sequence ingestion QC on a mixed-quality PDB-derived two-chain extract where terminal disorder and ligand/unknown residue annotations are common. For each chain, first sanitize the residue-name stream by trimming away low-confidence termini: drop any leading/trailing run of residues whose 3-letter codes are not part of the 20 canonical amino acids set (these typically come from unresolved density, caps, or modified residues). After trimming, convert the remaining ordered 3-letter residue names into the contiguous FASTA-style one-letter sequence using the standard mapping; any non-standard/unknown residue names that still occur internally after trimming must be coerced to 'X' to preserve sequence length for downstream folding/MSA/template steps. \n\nChain A residue names (N→C): [\"MSE\", \"UNK\", \"MET\", \"GLY\", \"ALA\", \"LYS\", \"SER\", \"MSE\", \"THR\", \"UNK\"]\nChain B residue names (N→C): [\"UNK\", \"ALA\", \"CYS\", \"ASP\", \"GLU\", \"PHE\", \"GLY\", \"HIS\", \"ILE\", \"LYS\", \"MSE\", \"SER\", \"THR\", \"UNK\", \"MSE\"]", "answers": "[{\"name\":\"chai_lab_data_parsing_structu__uence_protein_one_letter_sequence\",\"arguments\":{\"residue_codes\":[\"MET\",\"GLY\",\"ALA\",\"LYS\",\"SER\",\"MSE\",\"THR\"]}},{\"name\":\"chai_lab_data_parsing_structu__uence_protein_one_letter_sequence\",\"arguments\":{\"residue_codes\":[\"ALA\",\"CYS\",\"ASP\",\"GLU\",\"PHE\",\"GLY\",\"HIS\",\"ILE\",\"LYS\",\"MSE\",\"SER\",\"THR\"]}}]"}
{"func_name": "chai_lab_data_parsing_structure__n_one_letter_sequence_with_mods", "func_desc": "Convert a list of protein residue codes into a single one-letter amino-acid sequence string, emitting non-standard or modified residues in bracketed form so the sequence preserves modification identity for downstream processing in Chai-1.\n    \n    This function is used by Chai-1 input parsing and folding utilities to produce a compact, human- and machine-readable linear sequence from per-residue annotations (for example, sequences parsed from FASTA files or PDB residue lists) that may contain chemically modified residues. Non-standard residues are preserved as bracketed tokens so the model input retains modification information required for accurate folding of modified proteins and complexes. The implementation delegates token mapping to the internal helper _get_protein_only_residue_token with mods_in_brackets=True, producing a deterministic, side-effect-free transformation.", "tools": [{"function": {"description": "Convert a list of protein residue codes into a single one-letter amino-acid sequence string, emitting non-standard or modified residues in bracketed form so the sequence preserves modification identity for downstream processing in Chai-1.\n\nThis function is used by Chai-1 input parsing and folding utilities to produce a compact, human- and machine-readable linear sequence from per-residue annotations (for example, sequences parsed from FASTA files or PDB residue lists) that may contain chemically modified residues. Non-standard residues are preserved as bracketed tokens so the model input retains modification information required for accurate folding of modified proteins and complexes. The implementation delegates token mapping to the internal helper _get_protein_only_residue_token with mods_in_brackets=True, producing a deterministic, side-effect-free transformation.", "name": "chai_lab_data_parsing_structure__n_one_letter_sequence_with_mods", "parameters": {"properties": {"residue_codes": {"type": "array", "items": {"type": "string"}, "description": "A list of residue code strings, in sequence order, representing each residue of the protein chain. Each element should be the residue identifier as produced by upstream parsers (for example conventional three-letter amino-acid codes like \"ALA\" or modified-residue codes like \"HIP\"). The function will map recognized canonical residues to their single-letter amino-acid codes and will convert non-standard or otherwise unmapped residue codes into bracketed tokens of the form \"[CODE]\" where CODE is the original residue code string. An empty list yields an empty string.", "default": ""}}, "required": ["residue_codes"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a Chai-1 folding benchmark from three PDB-derived chain replicates, but only chains that show evidence of post-translational chemistry should be passed into the modification-aware sequence linearizer. For each replicate, scan the residue-code list and, if it contains at least one residue code that is not among the 20 canonical amino acids (3-letter codes), convert the entire replicate’s per-residue 3-letter codes into a compact one-letter sequence while preserving any non-standard/modified residues as bracketed tokens. Apply this rule to the following raw residue-code lists: (Replicate A) [\"MET\",\"ALA\",\"MSE\",\"LYS\",\"HIP\",\"SER\",\"SEP\",\"TYR\",\"TPO\"]; (Replicate B) [\"MET\",\"GLU\",\"CYS\",\"MSE\",\"GLY\",\"LYS\",\"THR\"]; (Replicate C) [\"MET\",\"ALA\",\"MSE\",\"HIP\",\"SER\",\"SEP\",\"LYS\",\"TPO\",\"GLY\"]. Output the resulting sequence string(s) in replicate order for those that qualify under the rule.", "answers": "[{\"name\":\"chai_lab_data_parsing_structure__n_one_letter_sequence_with_mods\",\"arguments\":{\"residue_codes\":[\"MET\",\"ALA\",\"MSE\",\"LYS\",\"HIP\",\"SER\",\"SEP\",\"TYR\",\"TPO\"]}},{\"name\":\"chai_lab_data_parsing_structure__n_one_letter_sequence_with_mods\",\"arguments\":{\"residue_codes\":[\"MET\",\"GLU\",\"CYS\",\"MSE\",\"GLY\",\"LYS\",\"THR\"]}},{\"name\":\"chai_lab_data_parsing_structure__n_one_letter_sequence_with_mods\",\"arguments\":{\"residue_codes\":[\"MET\",\"ALA\",\"MSE\",\"HIP\",\"SER\",\"SEP\",\"LYS\",\"TPO\",\"GLY\"]}}]"}
{"func_name": "chembl_downloader_queries_get_assay_sql", "func_desc": "Get the SQL query string to retrieve molecular structures and standardized activity measurements for a single ChEMBL assay.", "tools": [{"function": {"description": "Get the SQL query string to retrieve molecular structures and standardized activity measurements for a single ChEMBL assay.\n", "name": "chembl_downloader_queries_get_assay_sql", "parameters": {"properties": {"assay_chembl_id": {"type": "string", "description": "The ChEMBL assay identifier to query (the value stored in the ASSAYS.chembl_id column of the ChEMBL SQLite dump). In the domain of chembl_downloader this is the canonical assay id used to select all activities measured in that assay. This function interpolates this string directly into the SQL WHERE clause to restrict results to the specified assay.", "default": ""}}, "required": ["assay_chembl_id"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a reproducible extraction step for a ChEMBL assay activity benchmark where the input assay identifiers come from two upstream sources (a primary screen manifest and a dashboard config) and may contain mixed formatting from manual curation. Here is the raw cohort list:\n\n- primary_screen_manifest: [\" CHEMBL1000101 \", \"chembl123456\", \"CHEMBL000000\", \"CHEMBL1000101\"]\n- dashboard_config: [\"CHEMBL123456\", \"CHEMBL123456 \", \"CHEMBL-123456\", \"CHEMBL1000101\"]\n\nGenerate the exact SQL query strings (one per unique assay) to retrieve molecular structures plus standardized activity measurements, but only for assay identifiers that can be normalized to the canonical ChEMBL assay format (uppercase, prefix \"CHEMBL\" followed immediately by digits only). Treat each assay independently (no pooling) and deduplicate after normalization so we don’t issue redundant queries.", "answers": "[{\"name\":\"chembl_downloader_queries_get_assay_sql\",\"arguments\":{\"assay_chembl_id\":\"CHEMBL1000101\"}},{\"name\":\"chembl_downloader_queries_get_assay_sql\",\"arguments\":{\"assay_chembl_id\":\"CHEMBL123456\"}}]"}
{"func_name": "chembl_downloader_queries_get_document_molecule_sql", "func_desc": "Get the SQL query string that returns all molecules mentioned in a ChEMBL document.\n    \n    This function constructs and returns a SQL SELECT statement (as a Python str) designed for use against the ChEMBL SQLite dump managed by this package. The returned SQL joins the DOCS, COMPOUND_RECORDS, MOLECULE_DICTIONARY, and COMPOUND_STRUCTURES tables to produce one row per distinct molecule mentioned in the specified document. The query selects the ChEMBL molecule identifier (MOLECULE_DICTIONARY.chembl_id), the human-readable compound name (COMPOUND_RECORDS.compound_name), and the machine-readable canonical SMILES string (COMPOUND_STRUCTURES.canonical_smiles). Typical downstream uses (as shown in the project README) include passing the returned string to chembl_downloader.query() or executing it via a connection/cursor to load results into a pandas.DataFrame or to process with RDKit for cheminformatics workflows (e.g., fingerprinting, substructure search).\n    \n    Behavior and important details:\n    - The query uses SELECT DISTINCT to avoid duplicate molecule rows when the same molecule appears multiple times in the document.\n    - The WHERE clause compares DOCS.chembl_id to the provided document_chembl_id value; the value is interpolated directly into the returned SQL and is placed inside single quotes in the WHERE clause.\n    - This function does not execute the SQL; it only returns the SQL string. There are no side effects such as network or filesystem access.\n    - The function assumes the standard ChEMBL SQLite schema with the tables DOCS, COMPOUND_RECORDS, MOLECULE_DICTIONARY, and COMPOUND_STRUCTURES present. The package README notes that most ChEMBL versions are compatible but that very early SQLite dumps may have caveats; callers should ensure they are querying against a compatible ChEMBL SQLite file.\n    - Because the document identifier is interpolated into the SQL string, callers should ensure the provided document_chembl_id is a valid ChEMBL document identifier (a str matching the DOCS.chembl_id values in the database, e.g., \"CHEMBLXXXX\") and comes from a trusted source or is properly sanitized. If an identifier contains single quotes or other special characters, the produced SQL may be syntactically invalid or may behave unexpectedly; this also implies a risk of SQL injection if untrusted input is provided. For untrusted input, prefer using parameterized queries at execution time rather than relying on this helper to perform escaping.\n    - If the specified document_chembl_id does not exist in the database, executing the returned query will produce an empty result set (zero rows) rather than raising an error.", "tools": [{"function": {"description": "Get the SQL query string that returns all molecules mentioned in a ChEMBL document.\n\nThis function constructs and returns a SQL SELECT statement (as a Python str) designed for use against the ChEMBL SQLite dump managed by this package. The returned SQL joins the DOCS, COMPOUND_RECORDS, MOLECULE_DICTIONARY, and COMPOUND_STRUCTURES tables to produce one row per distinct molecule mentioned in the specified document. The query selects the ChEMBL molecule identifier (MOLECULE_DICTIONARY.chembl_id), the human-readable compound name (COMPOUND_RECORDS.compound_name), and the machine-readable canonical SMILES string (COMPOUND_STRUCTURES.canonical_smiles). Typical downstream uses (as shown in the project README) include passing the returned string to chembl_downloader.query() or executing it via a connection/cursor to load results into a pandas.DataFrame or to process with RDKit for cheminformatics workflows (e.g., fingerprinting, substructure search).\n\nBehavior and important details:\n- The query uses SELECT DISTINCT to avoid duplicate molecule rows when the same molecule appears multiple times in the document.\n- The WHERE clause compares DOCS.chembl_id to the provided document_chembl_id value; the value is interpolated directly into the returned SQL and is placed inside single quotes in the WHERE clause.\n- This function does not execute the SQL; it only returns the SQL string. There are no side effects such as network or filesystem access.\n- The function assumes the standard ChEMBL SQLite schema with the tables DOCS, COMPOUND_RECORDS, MOLECULE_DICTIONARY, and COMPOUND_STRUCTURES present. The package README notes that most ChEMBL versions are compatible but that very early SQLite dumps may have caveats; callers should ensure they are querying against a compatible ChEMBL SQLite file.\n- Because the document identifier is interpolated into the SQL string, callers should ensure the provided document_chembl_id is a valid ChEMBL document identifier (a str matching the DOCS.chembl_id values in the database, e.g., \"CHEMBLXXXX\") and comes from a trusted source or is properly sanitized. If an identifier contains single quotes or other special characters, the produced SQL may be syntactically invalid or may behave unexpectedly; this also implies a risk of SQL injection if untrusted input is provided. For untrusted input, prefer using parameterized queries at execution time rather than relying on this helper to perform escaping.\n- If the specified document_chembl_id does not exist in the database, executing the returned query will produce an empty result set (zero rows) rather than raising an error.", "name": "chembl_downloader_queries_get_document_molecule_sql", "parameters": {"properties": {"document_chembl_id": {"type": "string", "description": "The ChEMBL document identifier to filter by. This is the value compared to DOCS.chembl_id in the SQLite dump and identifies which document's referenced molecules will be returned. It must be supplied as a Python str and should match the identifiers used in the target ChEMBL SQLite file.", "default": ""}}, "required": ["document_chembl_id"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a noisy export of candidate ChEMBL DOCS identifiers coming from an OCR+NER pass over PDFs before running our literature-to-structure extraction. Given the following raw identifiers:\n\n- \"CHEMBL2150863\"\n- \" CHEMBL112233 \"\n- \"CHEMBL0000123\"\n- \"CHEMBL12O345\" \n- \"CHEMBL9876543210\"\n- \"chembl456789\"\n- \"CHEMBL\"\n- \"CHEMBL112233\" (duplicate)\n- \"CHEMBL214' OR '1'='1\" (contains a quote)\n\nGenerate (do not execute) the SQL SELECT statements to retrieve one row per DISTINCT molecule mentioned in each document, but only for identifiers that are valid ChEMBL document IDs under our schema rules: after trimming whitespace, the ID must be uppercase, start with \"CHEMBL\", and have 1–7 trailing digits only. For each qualifying document ID, return the raw SQL that joins DOCS, COMPOUND_RECORDS, MOLECULE_DICTIONARY, and COMPOUND_STRUCTURES and outputs exactly: MOLECULE_DICTIONARY.chembl_id, COMPOUND_RECORDS.compound_name, COMPOUND_STRUCTURES.canonical_smiles, using SELECT DISTINCT and a WHERE filter on DOCS.chembl_id equal to that cleaned ID.", "answers": "[{\"name\":\"chembl_downloader_queries_get_document_molecule_sql\",\"arguments\":{\"document_chembl_id\":\"CHEMBL2150863\"}},{\"name\":\"chembl_downloader_queries_get_document_molecule_sql\",\"arguments\":{\"document_chembl_id\":\"CHEMBL112233\"}},{\"name\":\"chembl_downloader_queries_get_document_molecule_sql\",\"arguments\":{\"document_chembl_id\":\"CHEMBL0000123\"}}]"}
{"func_name": "chempy__util_intdiv", "func_desc": "Integer division which rounds toward zero.\n    \n    Performs integer division of two integers p (numerator) and q (denominator) and returns the integer quotient with truncation toward zero. This differs from Python's floor division operator (//), which rounds toward negative infinity for negative operands; intdiv corrects that behavior so that results are the mathematical truncation of the exact quotient. In the ChemPy codebase this function is useful in contexts that require integer arithmetic with truncation semantics, for example when scaling or normalizing integer stoichiometric coefficients, distributing discrete counts, or computing signed integer indices where rounding toward zero is the intended domain behavior. The function has no side effects and its result is deterministic and constant-time for typical Python integer operations.", "tools": [{"function": {"description": "Integer division which rounds toward zero.\n\nPerforms integer division of two integers p (numerator) and q (denominator) and returns the integer quotient with truncation toward zero. This differs from Python's floor division operator (//), which rounds toward negative infinity for negative operands; intdiv corrects that behavior so that results are the mathematical truncation of the exact quotient. In the ChemPy codebase this function is useful in contexts that require integer arithmetic with truncation semantics, for example when scaling or normalizing integer stoichiometric coefficients, distributing discrete counts, or computing signed integer indices where rounding toward zero is the intended domain behavior. The function has no side effects and its result is deterministic and constant-time for typical Python integer operations.", "name": "chempy__util_intdiv", "parameters": {"properties": {"p": {"type": "integer", "description": "The integer numerator to be divided. In chemical computing contexts this often represents an integer count, coefficient, or signed difference that must be divided by an integer factor.", "default": ""}, "q": {"type": "integer", "description": "The integer denominator by which p is divided. In chemical contexts this can represent a scaling factor, group size, or divisor for normalizing coefficients. Must be non-zero; passing zero will cause a ZeroDivisionError.", "default": ""}}, "required": ["p", "q"], "type": "any"}}, "type": "function"}], "query": "We’re reconciling a small set of redox/stoichiometric correction logs from three replicates. Each replicate reports a signed electron/charge delta (p) and the count of symmetry-equivalent sites available for distributing that delta (q). Use integer division with truncation toward zero to compute the per-site integer quotient, but only for replicates where the site count is physically meaningful (>0) and the delta is nonzero. For deltas whose magnitude exceeds 20 (|p| > 20), treat the distribution as occurring over the full site count as reported. For smaller-magnitude deltas (|p| ≤ 20), treat the effective site count as one fewer than reported (q_eff = q − 1) to model a single inactive site during that run.\n\nRaw replicate logs:\n1) delta = −37 electrons, sites = 5\n2) delta = −17 units, sites = 4\n3) delta = −17 units, sites = 4", "answers": "[{\"name\":\"chempy__util_intdiv\",\"arguments\":{\"p\":-37,\"q\":5}},{\"name\":\"chempy__util_intdiv\",\"arguments\":{\"p\":-17,\"q\":3}},{\"name\":\"chempy__util_intdiv\",\"arguments\":{\"p\":-17,\"q\":3}}]"}
{"func_name": "chempy_chemistry_equilibrium_quotient", "func_desc": "chempy.chemistry.equilibrium_quotient: Calculate the equilibrium quotient Q for a chemical equilibrium from per-substance concentrations and stoichiometric coefficients. In chemical equilibrium modeling (see ChemPy README examples for equilibria and pH calculations), the equilibrium quotient is the product of each species concentration raised to its stoichiometric coefficient; this function computes that product for a single concentration vector or for multiple concentration sets (batched) and is therefore used when evaluating reaction quotients, comparing to equilibrium constants K, or computing pH and speciation.\n    \n    This function accepts a numpy.ndarray of concentrations or any object with a 1-D semantics (no ndim attribute or ndim == 1) representing a single set of per-substance concentrations. If a 2-D numpy.ndarray is provided, it is interpreted as a collection of independent concentration sets with shape (n_sets, n_substances) and the function returns a numpy.ndarray of length n_sets with the quotient for each set. The stoichiometric coefficients are applied elementwise: each concentration is raised to the power given by the corresponding stoichiometric coefficient and all terms are multiplied together. No in-place modification of inputs is performed.", "tools": [{"function": {"description": "chempy.chemistry.equilibrium_quotient: Calculate the equilibrium quotient Q for a chemical equilibrium from per-substance concentrations and stoichiometric coefficients. In chemical equilibrium modeling (see ChemPy README examples for equilibria and pH calculations), the equilibrium quotient is the product of each species concentration raised to its stoichiometric coefficient; this function computes that product for a single concentration vector or for multiple concentration sets (batched) and is therefore used when evaluating reaction quotients, comparing to equilibrium constants K, or computing pH and speciation.\n\nThis function accepts a numpy.ndarray of concentrations or any object with a 1-D semantics (no ndim attribute or ndim == 1) representing a single set of per-substance concentrations. If a 2-D numpy.ndarray is provided, it is interpreted as a collection of independent concentration sets with shape (n_sets, n_substances) and the function returns a numpy.ndarray of length n_sets with the quotient for each set. The stoichiometric coefficients are applied elementwise: each concentration is raised to the power given by the corresponding stoichiometric coefficient and all terms are multiplied together. No in-place modification of inputs is performed.", "name": "chempy_chemistry_equilibrium_quotient", "parameters": {"properties": {"concs": {"type": "array", "items": {"type": "float"}, "description": "Per-substance concentration data used to form the equilibrium quotient. For a single equilibrium calculation, provide a 1-D array-like object of length N where N is the number of chemical species. For multiple independent calculations (batched evaluation), provide a 2-D numpy.ndarray with shape (n_sets, N) where each row is a set of concentrations for the N species; the implementation transposes such a 2-D array internally and returns one quotient per row. Objects without an ndim attribute are treated as 1-D. Values are interpreted as numeric concentrations (e.g., molar), and typical usage in ChemPy is to compare the returned quotient to an equilibrium constant K when solving equilibria or predicting pH/speciation.", "default": ""}, "stoich": {"type": "array", "items": {"type": "float"}, "description": "Iterable of stoichiometric coefficients (integers) of length N, one coefficient per species, describing the exponent applied to each species concentration in the quotient. Positive coefficients correspond to species in the product side of a reaction and negative coefficients to reactants (consistent with ChemPy Equilibrium conventions). The function uses these coefficients directly as exponents in concentration**stoich_element.", "default": ""}}, "required": ["concs", "stoich"], "type": "any"}}, "type": "function"}], "query": "We’re doing a QC-driven reactor/speciation checkpoint for the Haber–Bosch gas-phase equilibrium N2 + 3 H2 ⇌ 2 NH3. Use the consistent species order [N2, H2, NH3] and stoichiometric exponents [-1, -3, 2] (reactants negative, product positive), treating the supplied values as activities for Q.\n\nRaw snapshot log (some entries may be nonphysical due to sensor dropout/blanking):\n- S1 @400 K: [0.80, 2.40, 0.10]\n- S2 @400 K: [0.00, 2.40, 0.10]\n- S3 @400 K: [0.50, 1.50, 0.60]\n- S4 @400 K: [-0.10, 1.00, 0.20]\n- S5 @400 K: [0.20, 0.60, 0.10]\n- S6 @400 K: [1.10, 0.90, 0.05]\n\nCompute equilibrium quotients only for snapshots where every activity is strictly positive.\n\nAdditionally, for the subset of valid snapshots that are NH3-rich relative to N2 (NH3/N2 ≥ 1), evaluate Q again after applying an in-silico dilution that halves every activity (same stoichiometry and ordering). Return Q for each evaluated condition (baseline valid snapshots, plus any NH3-rich diluted re-evaluations).", "answers": "[{\"name\":\"chempy_chemistry_equilibrium_quotient\",\"arguments\":{\"concs\":[0.8,2.4,0.1],\"stoich\":[-1,-3,2]}},{\"name\":\"chempy_chemistry_equilibrium_quotient\",\"arguments\":{\"concs\":[0.5,1.5,0.6],\"stoich\":[-1,-3,2]}},{\"name\":\"chempy_chemistry_equilibrium_quotient\",\"arguments\":{\"concs\":[0.2,0.6,0.1],\"stoich\":[-1,-3,2]}},{\"name\":\"chempy_chemistry_equilibrium_quotient\",\"arguments\":{\"concs\":[1.1,0.9,0.05],\"stoich\":[-1,-3,2]}},{\"name\":\"chempy_chemistry_equilibrium_quotient\",\"arguments\":{\"concs\":[0.25,0.75,0.3],\"stoich\":[-1,-3,2]}}]"}
{"func_name": "chempy_kinetics_integrated_binary_irrev", "func_desc": "chempy.kinetics.integrated.binary_irrev: Analytic product transient for an irreversible 2-to-1 bimolecular reaction.\n    \n    Computes the time-dependent concentration of the product for an irreversible second-order reaction where two reactant molecules (one major and one minor species) form a single product. This function implements the closed-form integrated rate expression used in the chempy kinetics utilities and integrated-rate fitting routines. The implementation uses a backend arithmetic library obtained via get_backend(backend) (defaulting to the numpy backend when backend is None) so the result can be numeric (array-like or scalar) or symbolic (e.g. when using a SymPy backend) depending on the inputs and backend.", "tools": [{"function": {"description": "chempy.kinetics.integrated.binary_irrev: Analytic product transient for an irreversible 2-to-1 bimolecular reaction.\n\nComputes the time-dependent concentration of the product for an irreversible second-order reaction where two reactant molecules (one major and one minor species) form a single product. This function implements the closed-form integrated rate expression used in the chempy kinetics utilities and integrated-rate fitting routines. The implementation uses a backend arithmetic library obtained via get_backend(backend) (defaulting to the numpy backend when backend is None) so the result can be numeric (array-like or scalar) or symbolic (e.g. when using a SymPy backend) depending on the inputs and backend.", "name": "chempy_kinetics_integrated_binary_irrev", "parameters": {"properties": {"t": {"type": "array", "items": {"type": "float"}, "description": "Time variable at which the product concentration is evaluated. For numeric backends this may be a scalar or an array-like sequence of time points; for symbolic backends this may be a symbolic time symbol, enabling algebraic manipulation of the analytic solution. The returned value has the same kind (scalar, array-like or symbolic) as permitted by the chosen backend and inputs.", "default": ""}, "kf": {"type": "float", "description": "Forward (bimolecular) rate constant for the irreversible reaction. In typical kinetic modelling this is a non-negative numeric rate constant; when supplied as a Symbol with a symbolic backend the returned expression will contain this symbol.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the product/complex present at t = 0. This term is added to the analytic transient and permits modelling systems that already contain some product initially.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant at t = 0. This parameter appears in the closed-form expression that determines the time scale and asymptotic behaviour of product formation.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant at t = 0. The difference (major - minor) appears multiplicatively in the exponent of the analytic expression; when minor equals zero the formula involves a division by zero and is therefore singular.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Backend module or backend identifier used to perform arithmetic and exponentiation. If None (the default) the function uses the numpy backend; passing a string such as 'sympy' or a module object selects a symbolic backend (e.g. SymPy) which yields symbolic expressions when inputs are symbolic. The backend must provide an exp implementation and standard arithmetic semantics; otherwise a TypeError or AttributeError may be raised.", "default": null}}, "required": ["t", "kf", "prod", "major", "minor", "backend"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed kinetics panel for an irreversible A + B → product assay where the metadata are messy and conditions vary by cohort. Use the analytic integrated expression to compute product concentration vs time, but only for runs that are physically admissible: nonnegative concentrations, kf > 0, and major ≥ minor at t=0 (major/minor defined by the initial amounts, not the labels). For each admissible run, also choose the sampling grid adaptively: if the characteristic depletion timescale (1/(kf·major)) is < 0.1 s, use the fast grid t = [0, 0.1, 0.5, 1.0, 2.0] s; otherwise use the slow grid t = [0, 5, 10, 20, 50] s. Use the default numeric backend.\n\nRaw run sheet (some entries include artifacts):\n- Run SF-01 (stopped-flow): kf=0.08 L·mol^-1·s^-1, product0=0.0 M, A0=0.020 M, B0=0.005 M\n- Run SOL-02 (solution): kf=0.25, product0=0.002 M, A0=0.10 M, B0=0.04 M\n- Run BR-03 (batch reactor, 298 K): kf=450.0, product0=0.005 M, A0=0.10 M, B0=0.02 M\n- Run QC-ERR (suspected sensor glitch): kf=-0.12, product0=0.0, A0=0.05, B0=0.01\n- Run SWAP-CHK (labeling ambiguity): kf=0.30, product0=0.0, A0=0.005, B0=0.020\n\nReturn product(t) for each admissible run using the grid determined by its timescale rule above.", "answers": "[{\"name\":\"chempy_kinetics_integrated_binary_irrev\",\"arguments\":{\"t\":[0,5,10,20,50],\"kf\":0.08,\"prod\":0.0,\"major\":0.02,\"minor\":0.005,\"backend\":null}},{\"name\":\"chempy_kinetics_integrated_binary_irrev\",\"arguments\":{\"t\":[0,5,10,20,50],\"kf\":0.25,\"prod\":0.002,\"major\":0.1,\"minor\":0.04,\"backend\":null}},{\"name\":\"chempy_kinetics_integrated_binary_irrev\",\"arguments\":{\"t\":[0,0.1,0.5,1.0,2.0],\"kf\":450.0,\"prod\":0.005,\"major\":0.1,\"minor\":0.02,\"backend\":null}}]"}
{"func_name": "chempy_kinetics_integrated_binary_irrev_cstr", "func_desc": "Analytic solution for the transient concentrations in a continuous stirred-tank reactor (CSTR)\n    undergoing the irreversible bimolecular reaction 2 A -> n B.\n    \n    This function evaluates a closed-form (analytic) solution of the ordinary differential\n    equations that describe a CSTR with a feed and perfect mixing for the reaction 2 A -> n B.\n    The solution is useful when you need rapid, direct evaluation of the time-dependent\n    concentrations of the reactant A and product B without performing numerical integration.\n    The implementation follows the symbolic derivation (see comments in source) and returns\n    concentrations evaluated at the supplied times using the chosen numeric/symbolic backend.", "tools": [{"function": {"description": "Analytic solution for the transient concentrations in a continuous stirred-tank reactor (CSTR)\nundergoing the irreversible bimolecular reaction 2 A -> n B.\n\nThis function evaluates a closed-form (analytic) solution of the ordinary differential\nequations that describe a CSTR with a feed and perfect mixing for the reaction 2 A -> n B.\nThe solution is useful when you need rapid, direct evaluation of the time-dependent\nconcentrations of the reactant A and product B without performing numerical integration.\nThe implementation follows the symbolic derivation (see comments in source) and returns\nconcentrations evaluated at the supplied times using the chosen numeric/symbolic backend.", "name": "chempy_kinetics_integrated_binary_irrev_cstr", "parameters": {"properties": {"t": {"type": "array", "items": {"type": "float"}, "description": "Time points at which to evaluate the solution. May be a scalar or\nan array-like sequence; units (for example seconds) must be consistent with the\nrate constant k and the feed-rate-to-volume ratio fv. The returned concentration\nvalues correspond to these same time points.", "default": ""}, "k": {"type": "float", "description": "Reaction rate constant for the irreversible bimolecular step 2 A -> n B.\nFor a reaction 2 A -> n B the physical units are typically concentration^-1 time^-1\n(e.g., M^-1 s^-1). This parameter controls the speed of conversion from reactant A\nto product B and appears in denominators in the analytic expressions (division by k\nwill occur), so k must not be zero for a finite numeric result.", "default": ""}, "r": {"type": "float", "description": "Initial concentration of reactant A in the reactor at t = 0. This value\nsets the starting concentration profile for the analytic solution and must be given\nin the same concentration units as fr, fp and p.", "default": ""}, "p": {"type": "float", "description": "Initial concentration of product B in the reactor at t = 0. This is the\nstarting concentration of B and must use the same units as r, fr and fp.", "default": ""}, "fr": {"type": "float", "description": "Concentration of reactant A in the feed stream. This models the inlet\nconcentration of A supplied continuously to the CSTR and is used together with fv\n(feed rate / tank volume) to set the driving term for the inlet flow in the ODEs.", "default": ""}, "fp": {"type": "float", "description": "Concentration of product B in the feed stream. This models any B that\nmay be present in the inlet and is combined with fv to determine the inlet contribution\nto the reactor concentration of B.", "default": ""}, "fv": {"type": "float", "description": "Feed rate divided by reactor volume (flow-per-volume ratio). Physically\nthis is the volumetric flow rate divided by tank volume and has units of inverse time\n(e.g., s^-1). It controls the residence time and dilution in the CSTR and must be\nprovided in the same time units as those used for t and k.", "default": ""}, "n": {"type": "integer", "description": "Stoichiometric coefficient for product B produced per reaction event (default 1).\nIn the chemical context this is the integer number of B formed by the reaction 2 A -> n B.\nThe implementation accepts an integer and uses it algebraically in the analytic formulae.", "default": 1}, "backend": {"type": "string", "nullable": true, "description": "Backend providing the numeric or symbolic primitives used to\nevaluate the closed-form expressions. Default is 'numpy' (numeric evaluation). Other\noptions include a symbolic backend such as 'sympy' or a module that provides functions\nused internally (sqrt, exp, tanh, atanh/arctanh, cos, etc.). If a string is given,\nit is resolved to a backend module (see source usage of get_backend). The choice of\nbackend determines whether the outputs are numeric arrays (numpy) or symbolic expressions\n(sympy). If backend is None the function will behave as with the default backend.", "default": null}}, "required": ["t", "k", "r", "p", "fr", "fp", "fv", "backend", "n"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a transient CSTR start-up panel for the irreversible bimolecular system 2 A -> n B (perfect mixing, analytic closed-form evaluation with a NumPy backend). The raw design sheet mixes seconds- and minutes-scale trials and includes a few instrument/scheduling artifacts. Use n=3 throughout.\n\nRaw cohort records (each record has units embedded in its label):\n1) id=\"S1\" (seconds): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.10 mol/L; fv=0.20 s^-1; k=0.05 L/(mol·s); requested t=[0, 5, 10, 20, 40, 60] s.\n2) id=\"S2\" (seconds): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.0 mol/L; fv=0.00 s^-1; k=0.05 L/(mol·s); requested t=[0, 5, 10, 20, 40, 60] s.\n3) id=\"S3\" (seconds): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.5 mol/L; fv=-0.10 s^-1; k=0.05 L/(mol·s); requested t=[0, 5, 10, 20, 40, 60] s.\n4) id=\"M1\" (minutes): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.0 mol/L; fv=0.05 min^-1; k=0.15 L/(mol·min); requested t=[0, 1, 2, 3, 4, 5, 10, 15, 20, 25, 30] min.\n5) id=\"M2\" (minutes): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.2 mol/L; fv=0.05 min^-1; k=0.00 L/(mol·min); requested t=[0, 1, 2, 3, 4, 5, 10, 15, 20, 25, 30] min.\n6) id=\"M3\" (minutes): r0=0.8 mol/L, p0=0.0 mol/L; fr=1.0 mol/L, fp=0.0 mol/L; fv=0.05 min^-1; k=0.15 L/(mol·min); requested t=[-1, 0, 1, 2, 3, 4, 5, 10, 15, 20, 25, 30] min.\n\nPipeline rules:\n- Treat a record as physically runnable only if fv and k are strictly positive and all evaluation times are non-negative.\n- To emulate product-inhibition/measurement carryover handling: if fp>0, use an effective rate constant of 0.5*k; otherwise use k unchanged.\n- For the runnable records, evaluate concentrations at their provided time grids (keep the original units/scales as given per record).\n\nReturn the function-call batch needed to evaluate r(t) and p(t) for all runnable records under these rules (backend=\"numpy\").", "answers": "[{\"name\":\"chempy_kinetics_integrated_binary_irrev_cstr\",\"arguments\":{\"t\":[0,5,10,20,40,60],\"k\":0.025,\"r\":0.8,\"p\":0.0,\"fr\":1.0,\"fp\":0.1,\"fv\":0.2,\"n\":3,\"backend\":\"numpy\"}},{\"name\":\"chempy_kinetics_integrated_binary_irrev_cstr\",\"arguments\":{\"t\":[0,1,2,3,4,5,10,15,20,25,30],\"k\":0.15,\"r\":0.8,\"p\":0.0,\"fr\":1.0,\"fp\":0.0,\"fv\":0.05,\"n\":3,\"backend\":\"numpy\"}}]"}
{"func_name": "chempy_kinetics_integrated_binary_rev", "func_desc": "chempy.kinetics.integrated.binary_rev computes the analytic time-dependent concentration of the product (complex) for a reversible 2-to-1 reaction (A + B <-> AB) using the closed-form solution derived for second-order forward (bimolecular) association and first-order backward (unimolecular) dissociation kinetics.\n    \n    This function is used in chemical kinetics modeling and data analysis (for example when comparing to transient experimental measurements or when supplying an analytic integrated rate expression to fitting routines). It evaluates the same symbolic expression used for derivations in the project (see _integrated.ipynb) but dispatches arithmetic to a numeric or symbolic backend via get_backend(backend), enabling both numeric evaluation (e.g. with NumPy) and symbolic algebra (e.g. with SymPy).", "tools": [{"function": {"description": "chempy.kinetics.integrated.binary_rev computes the analytic time-dependent concentration of the product (complex) for a reversible 2-to-1 reaction (A + B <-> AB) using the closed-form solution derived for second-order forward (bimolecular) association and first-order backward (unimolecular) dissociation kinetics.\n\nThis function is used in chemical kinetics modeling and data analysis (for example when comparing to transient experimental measurements or when supplying an analytic integrated rate expression to fitting routines). It evaluates the same symbolic expression used for derivations in the project (see _integrated.ipynb) but dispatches arithmetic to a numeric or symbolic backend via get_backend(backend), enabling both numeric evaluation (e.g. with NumPy) and symbolic algebra (e.g. with SymPy).", "name": "chempy_kinetics_integrated_binary_rev", "parameters": {"properties": {"t": {"type": "float", "description": "Time at which to evaluate the product concentration. In a physical chemistry context this represents elapsed time with units compatible with the rate constants; when using numeric backends t may be a scalar or an array of times to produce a time series of concentrations. For symbolic backends t may be a Symbol to obtain an algebraic expression.", "default": ""}, "kf": {"type": "float", "description": "Forward (bimolecular) rate constant for the association A + B -> AB. Physically this has units consistent with concentration^-1 time^-1 (e.g. M^-1 s^-1) and must be expressed in units compatible with t, prod, major, and minor. If provided as a Symbol, a symbolic expression for the transient is returned.", "default": ""}, "kb": {"type": "float", "description": "Backward (unimolecular) rate constant for the dissociation AB -> A + B. Physically this has units of time^-1 (e.g. s^-1). Must be in consistent units with t and kf.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the complex AB at time zero. This value sets the starting amount of product (complex) and has the same concentration units as major and minor. For typical kinetic experiments prod is often zero but the analytic expression supports nonzero initial complex.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant (the \"major\" reactant) at time zero. This parameter represents one reactant pool in the bimolecular forward step; the analytic form assumes two distinct reactant pools of different initial abundances with this being the larger.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant (the \"minor\" reactant) at time zero. This parameter represents the second reactant pool in the bimolecular forward step; the analytic form assumes a major/minor labeling so the solution is applicable when initial reactant concentrations are unequal.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Optional. Backend to use for arithmetic and special functions. Default behavior is to use the NumPy-based backend for numeric evaluation; passing a symbolic backend such as SymPy (module or the string 'sympy') yields a symbolic expression. The backend must support the operations used (addition, multiplication, sqrt, exp). If None is passed, the function uses the default numeric backend (NumPy) as documented in the project.", "default": null}}, "required": ["t", "kf", "kb", "prod", "major", "minor", "backend"], "type": "any"}}, "type": "function"}], "query": "We’re reconciling three reversible protein–ligand binding cohorts (A + B ⇌ AB) where the raw metadata mixes true association/dissociation transients with runs that were effectively at equilibrium or instrument-limited. For each cohort below, compute the time-dependent complex concentration [AB](t) using chempy.kinetics.integrated.binary_rev, but only for cohorts that are kinetically resolvable over the reported observation window (t must be at least 10× the dissociation time constant, i.e., t·kb ≥ 10). For the cohorts that qualify, set the initial product concentration prod depending on mixing regime: if the major:minor ratio is ≥ 20 treat it as a pseudo-first-order pre-equilibrated setup and set prod to 10% of the limiting reagent (0.1·minor); otherwise use prod = 0. Evaluate numerically with the default backend.\n\nCohorts (t, kf, kb, major, minor):\n1) Incubation: t = 120 s, kf = 1.5e5 M^-1 s^-1, kb = 0.2 s^-1, major = 5e-6 M, minor = 1e-6 M.\n2) Transient A: t = 12.0 s, kf = 1.25e5 M^-1 s^-1, kb = 0.08 s^-1, major = 2.0e-6 M, minor = 5.0e-7 M.\n3) Transient B: t = 12.5 s, kf = 1.8e4 M^-1 s^-1, kb = 0.35 s^-1, major = 5e-5 M, minor = 2e-5 M.", "answers": "[{\"name\":\"chempy_kinetics_integrated_binary_rev\",\"arguments\":{\"t\":120,\"kf\":150000.0,\"kb\":0.2,\"prod\":0.0,\"major\":5e-06,\"minor\":1e-06,\"backend\":null}}]"}
{"func_name": "chempy_kinetics_integrated_pseudo_irrev", "func_desc": "chempy.kinetics.integrated.pseudo_irrev: Analytic product transient for an irreversible pseudo first-order reaction used in ChemPy's integrated kinetics utilities.\n    \n    Computes the time-dependent product concentration for an irreversible, bimolecular → unimolecular reaction treated under the pseudo-first-order approximation. In this approximation the more abundant reactant (major) is assumed to remain effectively constant, so the bimolecular forward reaction with rate constant kf reduces to first-order kinetics with an effective rate k_eff = major * kf. The function returns the analytic expression prod + minor * (1 - exp(-major * kf * t)), which represents the product (or complex) concentration at time t given the initial concentrations and rate constant. This closed-form expression is useful in kinetics modeling, parameter estimation and fitting of integrated rate laws as described in the ChemPy README's kinetics examples.", "tools": [{"function": {"description": "chempy.kinetics.integrated.pseudo_irrev: Analytic product transient for an irreversible pseudo first-order reaction used in ChemPy's integrated kinetics utilities.\n\nComputes the time-dependent product concentration for an irreversible, bimolecular → unimolecular reaction treated under the pseudo-first-order approximation. In this approximation the more abundant reactant (major) is assumed to remain effectively constant, so the bimolecular forward reaction with rate constant kf reduces to first-order kinetics with an effective rate k_eff = major * kf. The function returns the analytic expression prod + minor * (1 - exp(-major * kf * t)), which represents the product (or complex) concentration at time t given the initial concentrations and rate constant. This closed-form expression is useful in kinetics modeling, parameter estimation and fitting of integrated rate laws as described in the ChemPy README's kinetics examples.", "name": "chempy_kinetics_integrated_pseudo_irrev", "parameters": {"properties": {"t": {"type": "float", "description": "Time at which to evaluate the product concentration. In practical use this may be a scalar float for a single time point, a sequence/array of time points for waveform evaluation (numpy arrays are supported by the default backend), or a symbolic time variable (Symbol) when constructing analytic expressions with e.g. sympy. Units are not interpreted by this function; pass unitless numeric values or use compatible unit handling externally (see ChemPy units utilities).", "default": ""}, "kf": {"type": "float", "description": "Forward bimolecular rate constant for the reaction. In the pseudo-first-order reduction this combines with the major reactant concentration to form an effective first-order rate. Typical units are (concentration^-1 time^-1) for bimolecular kf; when combined with major (concentration) the product major * kf has units of time^-1. kf may be a numeric value for numerical evaluation or a Symbol for symbolic algebra.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the product/complex (the species whose transient is reported) at time t = 0. This is the baseline concentration to which the generated product from the minor reactant adds. Use a numeric value for numeric evaluation or a Symbol for symbolic manipulation.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant. In the pseudo-first-order approximation this concentration is treated as effectively constant and multiplies kf to give the effective first-order rate constant (major * kf). This parameter should be positive in physical models; the function does not enforce sign constraints.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant that is consumed to form product. The term minor * (1 - exp(-major * kf * t)) represents the fraction of this initial minor pool converted to product over time. Use a numeric value for numeric evaluation or a Symbol for symbolic manipulation.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Backend providing basic mathematical operations used to evaluate the expression. Default is 'numpy' when backend is None (i.e., get_backend(backend) will return the numpy-backed API), but can be e.g. the sympy module when constructing symbolic expressions. The backend must provide an exp function (called as backend.exp) and support arithmetic between the provided inputs; an incompatible backend or one lacking exp will raise an AttributeError or similar when called.", "default": null}}, "required": ["t", "kf", "prod", "major", "minor"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a small stopped‑flow plate of irreversible association runs (A + B → complex) using the pseudo‑first‑order integrated transient. Each run reports (A0, B0, kf, initial complex baseline, readout time) but units are mixed.\n\nRaw runs:\n- R1: A0 = 0.15 M, B0 = 0.0025 M, kf = 12.0 M^-1 s^-1, baseline complex = 0.0001 M, t = 8.0 s\n- R2: A0 = 0.80 mM, B0 = 25 mM, kf = 120 mM^-1 s^-1, baseline complex = 0.05 mM, t = 0.015 s\n- R3: A0 = 0 M, B0 = 10 mM, kf = 50 mM^-1 s^-1, baseline complex = 0 mM, t = 0.1 s\n- R4: A0 = 1.2 mM, B0 = 1.2 mM, kf = 8.0 mM^-1 s^-1, baseline complex = 0.0 mM, t = 2.0 s\n\nPipeline rules:\n1) Standardize concentrations and baselines to mM.\n2) Identify the pseudo‑first‑order “major” reactant as the species whose initial concentration is at least 20× the other; the other species is the “minor”.\n3) Only runs meeting the pseudo‑first‑order criterion in (2) are eligible for prediction.\n4) For eligible runs, compute the predicted complex concentration at the provided time using the analytic pseudo‑first‑order irreversible product transient (with k_eff = major·kf) and the provided baseline as the initial product/complex level.", "answers": "[{\"name\":\"chempy_kinetics_integrated_pseudo_irrev\",\"arguments\":{\"t\":8.0,\"kf\":0.012,\"prod\":0.1,\"major\":150.0,\"minor\":2.5}},{\"name\":\"chempy_kinetics_integrated_pseudo_irrev\",\"arguments\":{\"t\":0.015,\"kf\":120.0,\"prod\":0.05,\"major\":25.0,\"minor\":0.8}}]"}
{"func_name": "chempy_kinetics_integrated_pseudo_rev", "func_desc": "Analytic product transient for a reversible pseudo-first-order reaction.\n    \n    Computes the time-dependent concentration of the product/complex for a reversible\n    bimolecular forward and unimolecular backward reaction under the pseudo-first-order\n    assumption (one reactant, \"major\", is present in large excess and treated as constant).\n    This routine is used in ChemPy's kinetics/integrated rate expressions to produce an\n    explicit analytic expression that can be evaluated for numeric time arrays (e.g. for\n    plotting or fitting) or as a symbolic expression when a symbolic backend is selected.\n    The expression returned corresponds to the solution of A + B <-> C with forward\n    rate kf (bimolecular) and backward rate kb (unimolecular) when [A] ~ major (constant).", "tools": [{"function": {"description": "Analytic product transient for a reversible pseudo-first-order reaction.\n\nComputes the time-dependent concentration of the product/complex for a reversible\nbimolecular forward and unimolecular backward reaction under the pseudo-first-order\nassumption (one reactant, \"major\", is present in large excess and treated as constant).\nThis routine is used in ChemPy's kinetics/integrated rate expressions to produce an\nexplicit analytic expression that can be evaluated for numeric time arrays (e.g. for\nplotting or fitting) or as a symbolic expression when a symbolic backend is selected.\nThe expression returned corresponds to the solution of A + B <-> C with forward\nrate kf (bimolecular) and backward rate kb (unimolecular) when [A] ~ major (constant).", "name": "chempy_kinetics_integrated_pseudo_rev", "parameters": {"properties": {"t": {"type": "array", "items": {"type": "float"}, "description": "Time points at which to evaluate the product concentration.\nIn practice this is typically a one-dimensional array of monotonically increasing\ntime values used for transient simulation or fitting to experimental kinetic data.\nThe units of t must be consistent with the units of the rate constants (kf, kb).", "default": ""}, "kf": {"type": "float", "description": "Forward (bimolecular) rate constant. This parameter represents the\nbimolecular rate coefficient for formation of the complex (C) from reactants (A + B).\nIts units are typically concentration^-1 time^-1 (e.g. M^-1 s^-1) so that kf * major\nhas units of time^-1 under the pseudo-first-order assumption. kf directly controls\nthe effective forward first-order rate kf * major used in the analytic solution.", "default": ""}, "kb": {"type": "float", "description": "Backward (unimolecular) rate constant. This parameter represents the\nfirst-order dissociation rate of the complex back to reactants (C -> A + B),\nwith units of time^-1 (e.g. s^-1). kb competes with the effective forward rate\nkf * major to determine the approach to equilibrium and the transient time scale.", "default": ""}, "prod": {"type": "float", "description": "Initial concentration of the complex (product) at t = 0. This is the\nstarting concentration of C used in the analytic transient expression. The units\nmust match those of major and minor (e.g. mol/L) so that returned concentrations\nare in the same concentration units.", "default": ""}, "major": {"type": "float", "description": "Initial concentration of the more abundant reactant (the one assumed\nto be in large excess and treated as constant). Under the pseudo-first-order\nassumption the forward term is approximated as kf * major (first-order with respect\nto the complex). major sets the effective forward first-order rate and thereby\nstrongly influences the transient kinetics and equilibrium position.", "default": ""}, "minor": {"type": "float", "description": "Initial concentration of the less abundant reactant. In the analytic\nexpression this initial minor concentration appears in the product of kf * major * minor\nwhich determines the equilibrium concentration term when t -> infinity.\nThe units must be consistent with prod and major.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Optional string selecting the numerical or symbolic backend used to\nevaluate the analytic expression. Default is None, which selects the default backend\n(typically 'numpy' via get_backend). Common values include 'numpy' for numeric\nevaluation of arrays and 'sympy' for symbolic expressions; the choice controls which\nexponential and arithmetic routines are used and whether a symbolic expression may be\nreturned. If backend is a string naming a supported backend, the function obtains\nthe backend via get_backend(backend).", "default": null}}, "required": ["t", "kf", "kb", "prod", "major", "minor", "backend"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed batch of reversible complexation shots for A + B <-> C under pseudo-first-order conditions (A in excess, treated constant) and want NumPy-evaluable analytic C(t) transients, all starting from zero complex (prod=0.0). The raw run sheet contains three candidate cohorts:\n\n1) fast, sub-second sampling: major=1.5 M, minor=0.2 M, kf=120.0 M^-1 s^-1, kb=0.8 s^-1, t=[0, 0.01, 0.02, 0.05, 0.1] s\n2) high-affinity, minute-scale: major=2.0 mM, minor=0.10 mM, kf=1.2e5 M^-1 s^-1, kb=0.15 s^-1, t=0..60 s in 5 s steps\n3) slow kinetics control: major=0.01 M, minor=5e-4 M, kf=1.5 M^-1 s^-1, kb=0.2 s^-1, t=[0, 1, 2, 5, 10, 20] s\n\nApply a concentration hygiene rule before computation: interpret any concentration reported in mM as millimolar and convert it to molar units; treat concentrations already reported in M as molar. Then, to avoid pseudo-first-order breakdown artifacts, only generate C(t) for cohorts where the excess condition is strong enough that major/minor >= 10. For each retained cohort, evaluate C(t) on the provided time grid using the NumPy backend and return the resulting arrays cohort-by-cohort.", "answers": "[{\"name\":\"chempy_kinetics_integrated_pseudo_rev\",\"arguments\":{\"t\":[0,0.01,0.02,0.05,0.1],\"kf\":120.0,\"kb\":0.8,\"prod\":0.0,\"major\":1.5,\"minor\":0.2,\"backend\":\"numpy\"}},{\"name\":\"chempy_kinetics_integrated_pseudo_rev\",\"arguments\":{\"t\":[0,5,10,15,20,25,30,35,40,45,50,55,60],\"kf\":120000.0,\"kb\":0.15,\"prod\":0.0,\"major\":0.002,\"minor\":0.0001,\"backend\":\"numpy\"}},{\"name\":\"chempy_kinetics_integrated_pseudo_rev\",\"arguments\":{\"t\":[0,1,2,5,10,20],\"kf\":1.5,\"kb\":0.2,\"prod\":0.0,\"major\":0.01,\"minor\":0.0005,\"backend\":\"numpy\"}}]"}
{"func_name": "chempy_kinetics_integrated_unary_irrev_cstr", "func_desc": "chempy.kinetics.integrated.unary_irrev_cstr: Analytic solution for a first-order irreversible reaction A -> B in a continuously stirred tank reactor (CSTR).\n    \n    This function returns the time-dependent, closed-form concentrations for the reactant (A) and product (B) for the first-order irreversible reaction A -> B in a CSTR. It is intended for chemical kinetics modelling, analytical validation of ODE solver results, parameter estimation or integrated rate expression evaluation described in the ChemPy kinetics utilities. The implementation uses a small set of algebraic combinations and exponentials so that the result can be produced either numerically (default numpy backend) or symbolically (e.g., sympy) by supplying a compatible backend.", "tools": [{"function": {"description": "chempy.kinetics.integrated.unary_irrev_cstr: Analytic solution for a first-order irreversible reaction A -> B in a continuously stirred tank reactor (CSTR).\n\nThis function returns the time-dependent, closed-form concentrations for the reactant (A) and product (B) for the first-order irreversible reaction A -> B in a CSTR. It is intended for chemical kinetics modelling, analytical validation of ODE solver results, parameter estimation or integrated rate expression evaluation described in the ChemPy kinetics utilities. The implementation uses a small set of algebraic combinations and exponentials so that the result can be produced either numerically (default numpy backend) or symbolically (e.g., sympy) by supplying a compatible backend.", "name": "chempy_kinetics_integrated_unary_irrev_cstr", "parameters": {"properties": {"t": {"type": "array", "items": {"type": "float"}, "description": "Time point or array of time points at which to evaluate the analytic solution. Typical use is a sequence of times returned by an ODE integrator or a grid for plotting. The backend exponential function is applied to t; the returned concentration arrays or expressions will have the same shape as t (or will broadcast accordingly). t = 0 yields the initial concentrations r and p.", "default": ""}, "k": {"type": "float", "description": "First-order rate constant for the irreversible reaction A -> B. Physically this has units of inverse time and competes with the dilution rate fv in determining dynamics and steady state. Must be numeric (or backend-compatible symbolic) and used directly in exponentials and the algebraic steady-state expressions.", "default": ""}, "r": {"type": "float", "description": "Initial concentration of the reactant A at time t = 0. Used as the starting condition for the analytic expression; the function returns r for the reactant component when t equals zero.", "default": ""}, "p": {"type": "float", "description": "Initial concentration of the product B at time t = 0. Used as the starting condition for the analytic expression; the function returns p for the product component when t equals zero.", "default": ""}, "fr": {"type": "float", "description": "Concentration of reactant A in the feed stream entering the CSTR. Together with fv this determines the feed-driven steady-state contribution to the reactant concentration (the steady state reactant concentration equals fr*fv/(fv + k) when exponential transients have decayed).", "default": ""}, "fp": {"type": "float", "description": "Concentration of product B in the feed stream entering the CSTR. Contributes to the long-time (steady-state) product concentration through dilution and reaction terms.", "default": ""}, "fv": {"type": "float", "description": "Feed rate divided by reactor volume (often called dilution rate); has units of inverse time. It appears additively with k (fv + k) and therefore fv + k must not be zero (see Failure modes). Physically, fv governs how quickly feed concentrations replace reactor contents and thus influences both transient decay rates and steady-state values.", "default": ""}, "backend": {"type": "string", "nullable": true, "description": "Backend providing numeric/symbolic operations (notably exp). Default is 'numpy' (the function selects numpy by default). Alternatively pass a module like sympy or a string recognized by the library backend selector; when a symbolic backend is used the returned values are symbolic expressions rather than numeric arrays.", "default": null}}, "required": ["t", "k", "r", "p", "fr", "fp", "fv"], "type": "any"}}, "type": "function"}], "query": "We’re validating the analytic transient solution for a first‑order irreversible A → B in a CSTR against a messy pilot run log where some “snapshots” were recorded in mixed time units. For each snapshot, compute time‑dependent concentrations using the default numerical backend, but only for snapshots that represent physically admissible reactor states: dilution rate fv must be > 0, rate constant k must be > 0, and all stated concentrations (r, p, fr, fp) must be ≥ 0. Interpret each snapshot’s time vector rule as follows: if the window is given in minutes, convert to seconds and evaluate at the listed grid; if the window is given as an evenly spaced replicate series, generate the specified count of equally spaced time points including endpoints and then convert to seconds if needed.\n\nRaw snapshot log (each is an isolated parameter set):\n1) Window: 10 min, coarse grid at [0, 1, 2, 5, 10] min; k = 0.015 1/s; initial r = 1.2 mol/L, p = 0.0 mol/L; feed fr = 2.0 mol/L, fp = 0.1 mol/L; fv = 0.02 1/s.\n2) Window: 1 min, 7 evenly spaced replicates including endpoints; k = 0.08 1/s; initial r = 1.0 mol/L, p = 0.0 mol/L; feed fr = 2.0 mol/L, fp = 0.0 mol/L; fv = 0.05 1/s.\n3) Window: 10 min, coarse grid at [0, 60, 120, 300, 600] s; k = 0.02 1/s; initial r = 0.9 mol/L, p = 0.0 mol/L; feed fr = 1.8 mol/L, fp = 0.05 mol/L; fv = 0.0 1/s.\n4) Window: 1 min, 7 evenly spaced replicates including endpoints; k = -0.01 1/s; initial r = 1.1 mol/L, p = 0.0 mol/L; feed fr = 2.2 mol/L, fp = 0.0 mol/L; fv = 0.04 1/s.\n5) Window: 10 min, coarse grid at [0, 1, 2, 5, 10] min; k = 0.03 1/s; initial r = 1.0 mol/L, p = 0.1 mol/L; feed fr = 2.0 mol/L, fp = 0.1 mol/L; fv = 0.02 1/s; (note: recorded p = -0.02 mol/L in the raw sheet).\n\nProcess only the snapshots that pass the admissibility criteria and produce the corresponding model evaluations at the appropriate time vectors (in seconds).", "answers": "[{\"name\":\"chempy_kinetics_integrated_unary_irrev_cstr\",\"arguments\":{\"t\":[0,60,120,300,600],\"k\":0.015,\"r\":1.2,\"p\":0.0,\"fr\":2.0,\"fp\":0.1,\"fv\":0.02}},{\"name\":\"chempy_kinetics_integrated_unary_irrev_cstr\",\"arguments\":{\"t\":[0,10,20,30,40,50,60],\"k\":0.08,\"r\":1.0,\"p\":0.0,\"fr\":2.0,\"fp\":0.0,\"fv\":0.05}}]"}
{"func_name": "chempy_symmetry_representations_print_mulliken", "func_desc": "Print Mulliken symbols of the irreducible representations for a molecular point group.\n    \n    This function is part of ChemPy's symmetry utilities and is used to display the Mulliken notation labels for irreducible representations associated with a point group given in Schoenflies notation (for example, \"C2v\"). Mulliken symbols are the standard labels used in molecular symmetry and group theory to identify irreducible representations; they are commonly used in spectroscopy, vibrational analysis, and quantum-chemical point-group classification to determine mode symmetries and selection rules. The function looks up a module-level mapping named \"mulliken\" keyed by lower-cased Schoenflies strings and prints the symbols in the order stored in that mapping.", "tools": [{"function": {"description": "Print Mulliken symbols of the irreducible representations for a molecular point group.\n\nThis function is part of ChemPy's symmetry utilities and is used to display the Mulliken notation labels for irreducible representations associated with a point group given in Schoenflies notation (for example, \"C2v\"). Mulliken symbols are the standard labels used in molecular symmetry and group theory to identify irreducible representations; they are commonly used in spectroscopy, vibrational analysis, and quantum-chemical point-group classification to determine mode symmetries and selection rules. The function looks up a module-level mapping named \"mulliken\" keyed by lower-cased Schoenflies strings and prints the symbols in the order stored in that mapping.", "name": "chempy_symmetry_representations_print_mulliken", "parameters": {"properties": {"group": {"type": "string", "description": "Point group in Schoenflies notation (e.g., 'C2v'). This argument is case-insensitive: the function calls group.lower() before lookup. The value must be a string corresponding to a key in the module's internal mapping of Mulliken symbols.", "default": ""}}, "required": ["group"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating point-group annotations from mixed upstream sources before assigning vibrational mode symmetries. Given the raw Schoenflies strings reported by different steps: [\" C3V \", \"c2v\", \"C3v\", \"D2h\", \"c1\", \"C2V\"]. For each entry, treat the point-group key in a case-insensitive way after trimming surrounding whitespace, and print Mulliken irreducible-representation symbols only for groups that are commonly used for the NH3/H2O benchmarking set (i.e., C3v and C2v).", "answers": "[{\"name\":\"chempy_symmetry_representations_print_mulliken\",\"arguments\":{\"group\":\"C3V\"}},{\"name\":\"chempy_symmetry_representations_print_mulliken\",\"arguments\":{\"group\":\"c2v\"}},{\"name\":\"chempy_symmetry_representations_print_mulliken\",\"arguments\":{\"group\":\"C3v\"}},{\"name\":\"chempy_symmetry_representations_print_mulliken\",\"arguments\":{\"group\":\"C2V\"}}]"}
{"func_name": "chempy_units_linspace", "func_desc": "chempy.units.linspace generates an array of evenly spaced numeric values between two scalar endpoints while preserving and returning the unit associated with the start value. It is analogous to numpy.linspace but integrated with ChemPy's units subsystem (which wraps the quantities package) so the returned array carries the same physical unit as the start argument. This function is useful in chemical modelling workflows (for example creating time grids, temperature ramps, concentration sequences, or other evenly spaced parameter sweeps where units must be tracked consistently).", "tools": [{"function": {"description": "chempy.units.linspace generates an array of evenly spaced numeric values between two scalar endpoints while preserving and returning the unit associated with the start value. It is analogous to numpy.linspace but integrated with ChemPy's units subsystem (which wraps the quantities package) so the returned array carries the same physical unit as the start argument. This function is useful in chemical modelling workflows (for example creating time grids, temperature ramps, concentration sequences, or other evenly spaced parameter sweeps where units must be tracked consistently).\n", "name": "chempy_units_linspace", "parameters": {"properties": {"start": {"type": "float", "description": "The left endpoint of the interval. In practice this is the numeric value or a quantities-aware scalar carrying a physical unit (for example a concentration, temperature or time). The function queries the unit of start (via unit_of) and uses that unit for the returned array. If start is a plain Python float it is treated as dimensionless.", "default": ""}, "stop": {"type": "float", "description": "The right endpoint of the interval. This is converted to the same unit as start (via to_unitless with the unit derived from start) before generating the sequence. If stop has an incompatible unit (for example meters vs seconds) the underlying unit conversion routine will raise an error (see failure modes below).", "default": ""}, "num": {"type": "integer", "description": "Number of samples to generate. This integer controls the length of the returned sequence and defaults to 50. The value is passed directly to numpy.linspace, so its detailed semantics (including behavior for zero or negative values) follow numpy.linspace.", "default": 50}}, "required": ["start", "stop", "num"], "type": "any"}}, "type": "function"}], "query": "In our automated kinetic ramp generator, we ingest a mixed set of planned setpoint grids coming from different instruments. Each request contains (start_value, stop_value, nominal_points) plus a context tag. Build units-consistent evenly spaced grids using chempy.units.linspace under the following protocol: treat a request as a temperature ramp if both endpoints are in the physiochemically plausible temperature window [250, 600] K and the span is at least 20 K; for temperature ramps, use the nominal_points as provided unless the nominal_points exceeds 30, in which case cap it at 30 to keep acquisition overhead bounded. Treat a request as a kinetic integration time base if it starts at exactly 0 s, stops between 10 s and 10^4 s, and nominal_points is between 10 and 200; for time bases, override nominal_points to exactly 25 to standardize downstream integrator stability. Ignore any request that does not match either protocol. Raw requests: A) (298.15 K → 350.0 K, 11 pts, tag='exp_T'), B) (298.15 K → 350.0 K, 25 pts, tag='sim_T_hi_res'), C) (0 s → 120 s, 64 pts, tag='integration_t'), D) (700 K → 900 K, 25 pts, tag='overheat_probe'), E) (5 s → 120 s, 25 pts, tag='offset_time'). Generate the grids that pass the protocol and ensure each returned array retains the unit of its start value.", "answers": "[{\"name\":\"chempy_units_linspace\",\"arguments\":{\"start\":298.15,\"stop\":350.0,\"num\":11}},{\"name\":\"chempy_units_linspace\",\"arguments\":{\"start\":298.15,\"stop\":350.0,\"num\":25}},{\"name\":\"chempy_units_linspace\",\"arguments\":{\"start\":0,\"stop\":120,\"num\":25}}]"}
{"func_name": "chempy_units_logspace_from_lin", "func_desc": "Logarithmically spaced data points with units preserved.\n    \n    This function is part of chempy.units, which wraps the quantities package for\n    unit-aware numerical work in chemistry (e.g. concentrations, amounts, rates).\n    logspace_from_lin produces an array of values that are evenly spaced in\n    base-2 logarithmic space between two endpoint values. The unit of the\n    returned array is taken from start; stop is converted to that unit before\n    computing the logarithmic spacing. This is useful when sampling a physical\n    quantity (for example, concentration, activity, or pressure) across orders\n    of magnitude while keeping the values' units consistent with the rest of a\n    chempy workflow.", "tools": [{"function": {"description": "Logarithmically spaced data points with units preserved.\n\nThis function is part of chempy.units, which wraps the quantities package for\nunit-aware numerical work in chemistry (e.g. concentrations, amounts, rates).\nlogspace_from_lin produces an array of values that are evenly spaced in\nbase-2 logarithmic space between two endpoint values. The unit of the\nreturned array is taken from start; stop is converted to that unit before\ncomputing the logarithmic spacing. This is useful when sampling a physical\nquantity (for example, concentration, activity, or pressure) across orders\nof magnitude while keeping the values' units consistent with the rest of a\nchempy workflow.", "name": "chempy_units_logspace_from_lin", "parameters": {"properties": {"start": {"type": "float", "description": "Left endpoint of the interval expressed in a unit-aware\nnumeric value. The unit of start determines the unit of the returned\narray. In the context of chempy.units this is typically a quantities\nobject or a numeric value associated with a unit; unit_of(start) is\nused internally to extract the unit. The returned values are spaced\nso that log2(value) is linearly spaced between log2(start) and\nlog2(stop_in_start_unit). start must be finite.", "default": ""}, "stop": {"type": "float", "description": "Right endpoint of the interval expressed in a unit-aware\nnumeric value. stop is converted to the unit of start using\nchempy.units.to_unitless(start_unit) semantics (i.e., the function\ncomputes log2(to_unitless(stop, unit_of(start)))). If stop has\nincompatible units such that conversion to the unit of start fails,\na conversion error (for example ValueError) from the underlying units\nhandling code will be propagated. stop must be finite after\nconversion.", "default": ""}, "num": {"type": "integer", "description": "Number of samples to generate. Defaults to 50. num is passed\ndirectly to numpy.linspace to create num evenly spaced points in the\nexponent (log2) domain and therefore must be a positive integer\naccepted by numpy.linspace (typical use is an integer >= 1).", "default": 50}}, "required": ["start", "stop", "num"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating parameter panels from multiple assay teams into a single kinetics workflow. Each panel is specified by a (start, stop) pair, but the raw export is messy: some teams provided concentration ranges (intended mol/L), others provided rate constants (intended s^-1), and a few panels are clearly invalid because the start/stop are non-positive. Build base-2 log-spaced cohorts (12 points each) only for panels with physically meaningful endpoints (both start and stop > 0). Apply a branching rule to standardize sampling density: if the dynamic range spans at least 2^10 (i.e., stop/start >= 1024), keep 12 points; otherwise downsample to 8 points to avoid over-resolving narrow ranges. Use these raw panels:\n- Panel A: 1e-9 to 1e-3 (concentration)\n- Panel B: 1e-5 to 10 (rate constant)\n- Panel C: 0.25 to 128 (concentration)\n- Panel D: -1e-6 to 1e-2 (concentration; suspected baseline subtraction artifact)\n- Panel E: 5e-4 to 2e-4 (concentration; inverted bounds from spreadsheet sort)\nReturn unit-consistent arrays (unit inherited from start) and treat stop as convertible into start’s unit before spacing.", "answers": "[{\"name\":\"chempy_units_logspace_from_lin\",\"arguments\":{\"start\":1e-09,\"stop\":0.001,\"num\":12}},{\"name\":\"chempy_units_logspace_from_lin\",\"arguments\":{\"start\":1e-05,\"stop\":10.0,\"num\":12}},{\"name\":\"chempy_units_logspace_from_lin\",\"arguments\":{\"start\":0.25,\"stop\":128,\"num\":12}}]"}
{"func_name": "chempy_util__expr_create_Piecewise", "func_desc": "create_Piecewise creates a parameterized piecewise expression factory for use in ChemPy expression trees and symbolic/numeric backends.\n    \n    This function returns an Expr factory (produced by Expr.from_callback) that constructs a piecewise expression which selects one of several sub-expressions based on the value of a single runtime parameter. It is intended for use in ChemPy contexts where expressions depend on a single named parameter (for example 'x' in concentration- or position-dependent expressions used in kinetics, equilibria, or property functions). The factory expects a specific sequence encoding alternating interval bounds and expressions (see behavior below). The implementation supports two execution modes: if the provided backend exposes a Piecewise constructor (e.g. a symbolic backend such as SymPy) a symbolic backend.Piecewise expression is returned (with evaluate=False to avoid eager simplification); otherwise a simple numeric selection is performed by iterating the supplied bounds and returning the matching expression. If the runtime parameter is a quantity, bounds are converted to unitless values using the parameter's unit so comparisons are meaningful in unit-aware contexts.", "tools": [{"function": {"description": "create_Piecewise creates a parameterized piecewise expression factory for use in ChemPy expression trees and symbolic/numeric backends.\n\nThis function returns an Expr factory (produced by Expr.from_callback) that constructs a piecewise expression which selects one of several sub-expressions based on the value of a single runtime parameter. It is intended for use in ChemPy contexts where expressions depend on a single named parameter (for example 'x' in concentration- or position-dependent expressions used in kinetics, equilibria, or property functions). The factory expects a specific sequence encoding alternating interval bounds and expressions (see behavior below). The implementation supports two execution modes: if the provided backend exposes a Piecewise constructor (e.g. a symbolic backend such as SymPy) a symbolic backend.Piecewise expression is returned (with evaluate=False to avoid eager simplification); otherwise a simple numeric selection is performed by iterating the supplied bounds and returning the matching expression. If the runtime parameter is a quantity, bounds are converted to unitless values using the parameter's unit so comparisons are meaningful in unit-aware contexts.", "name": "chempy_util__expr_create_Piecewise", "parameters": {"properties": {"parameter_name": {"type": "string", "description": "Name of the single runtime parameter used to select which branch of the piecewise expression to evaluate. This exact string is passed as the parameter key when creating the Expr via Expr.from_callback, so callers and downstream code must use the same name to provide the parameter value (for example, 'x' to indicate the independent variable in a spatial or concentration-dependent expression). The returned Expr factory therefore produces expressions that expect a mapping or value for this parameter at evaluation time.", "default": ""}, "nan_fallback": {"type": "boolean", "description": "When True, the constructed symbolic Piecewise will include a final fallback branch that returns a backend Symbol called 'NAN' with an unconditional (True) predicate; this ensures the Piecewise is always defined in symbolic backends even when the parameter lies outside all provided intervals. When False (the default), no unconditional fallback is added for the symbolic Piecewise; in numeric/backends-without-Piecewise mode, omission of a matching interval causes a ValueError to be raised. Use nan_fallback=True when you need a defined symbolic placeholder for out-of-range values and are prepared to handle the backend-specific 'NAN' symbol.", "default": false}}, "required": ["parameter_name", "nan_fallback"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a reusable library of ChemPy expression callbacks for a mixed-quality kinetics+transport dataset where different cohorts expose different single-parameter controls. From the following candidate control parameters discovered in metadata: `['x', 'T', 'z', 't', 'pH', 'r', 'x ', 'TEMP', None]`, construct piecewise-expression factories only for parameters that are valid ChemPy symbol names (non-empty strings), match exactly one of the canonical controls used in our pipeline (`x`, `T`, `z`), and require an explicit symbolic NaN fallback to keep symbolic backends stable during parameter sweeps when values fall outside all declared intervals. Return factories for every parameter that satisfies those criteria, using the parameter name as provided by the canonical set and with NaN fallback enabled.", "answers": "[{\"name\":\"chempy_util__expr_create_Piecewise\",\"arguments\":{\"parameter_name\":\"x\",\"nan_fallback\":true}},{\"name\":\"chempy_util__expr_create_Piecewise\",\"arguments\":{\"parameter_name\":\"T\",\"nan_fallback\":true}},{\"name\":\"chempy_util__expr_create_Piecewise\",\"arguments\":{\"parameter_name\":\"z\",\"nan_fallback\":true}}]"}
{"func_name": "chempy_util__expr_create_Poly", "func_desc": "Create a polynomial Expression factory for a single scalar parameter.\n    \n    This function is used in ChemPy to build Expr objects (chempy.util._expr.Expr) that evaluate polynomials in a single named scalar parameter — for example to represent temperature-dependent empirical fits or other scalar-parameter-dependent property approximations that occur throughout the ChemPy codebase (see README examples for temperature-dependent properties and kinetics). The returned Expr implements a callback that, when called with a sequence of arguments and a mapping of parameter values, evaluates the polynomial\n    sum_{n=0}^{N-1} coeff_n * x0^{n},\n    where x0 is either the parameter value or a shifted / reciprocal transform of it as specified by the arguments below. The implementation uses successive multiplication (or division for reciprocal polynomials) to build powers efficiently.", "tools": [{"function": {"description": "Create a polynomial Expression factory for a single scalar parameter.\n\nThis function is used in ChemPy to build Expr objects (chempy.util._expr.Expr) that evaluate polynomials in a single named scalar parameter — for example to represent temperature-dependent empirical fits or other scalar-parameter-dependent property approximations that occur throughout the ChemPy codebase (see README examples for temperature-dependent properties and kinetics). The returned Expr implements a callback that, when called with a sequence of arguments and a mapping of parameter values, evaluates the polynomial\nsum_{n=0}^{N-1} coeff_n * x0^{n},\nwhere x0 is either the parameter value or a shifted / reciprocal transform of it as specified by the arguments below. The implementation uses successive multiplication (or division for reciprocal polynomials) to build powers efficiently.", "name": "chempy_util__expr_create_Poly", "parameters": {"properties": {"parameter_name": {"type": "string", "description": "The name of the scalar parameter the polynomial depends on. This string is used as the single parameter key in the Expr returned by create_Poly. In practice this is a physical variable name such as 'T' for temperature or 'x' for a generic scalar; when evaluating the Expr you provide a mapping (e.g. {'T': 298.15}) that supplies the parameter value.", "default": ""}, "reciprocal": {"type": "boolean", "description": "If False (default), the polynomial is in nonnegative integer powers of the parameter (x0^0, x0^1, x0^2, ...). If True, the polynomial uses successive powers of the reciprocal of x0 (1/x0, 1/x0^2, ...), producing terms coeff_n * (1/x0)^n with the first term still being coeff_0 (multiplied by 1). This option is useful for fits that are naturally expressed as polynomials in 1/T or other reciprocals (examples in code show reciprocal=True for temperature reciprocals). Beware that setting reciprocal=True will raise a division-by-zero error if the evaluated x0 equals zero.", "default": false}, "shift": {"type": "string", "nullable": true, "description": "If None (the default), the polynomial is evaluated directly at the parameter value x (so x0 = x). If a string is provided, that string is used as the name of the first argument that supplies the shift/reference value and the polynomial is evaluated in powers of (x - x_shift) where x_shift is the first argument in the argument list for the Expr. If shift is True, this is a shorthand that is converted to the string 'shift' internally (so the Expr will expect a first argument named 'shift'). Concretely, when shift is not None the callback expects the first element of the provided args to be the shift/reference value and the remaining elements to be the polynomial coefficients. This is useful for representing polynomials expanded about a reference point (for example, expansion about a reference temperature). Note: the function also supports passing the shift value at call-time via the returned Expr's argument mechanisms (see examples in source).", "default": null}, "name": {"type": "string", "nullable": true, "description": "Optional Python function name to assign to the internal callback created for the Expr. If provided, the callback's __name__ will be set to this value; otherwise the default function name is left unchanged. This has no effect on numerical behavior but can make debugging and introspection outputs more readable.", "default": null}}, "required": ["parameter_name", "reciprocal", "name", "shift"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating a mixed set of temperature-dependent empirical submodels into a single ChemPy fitting pipeline. Each candidate submodel record includes a callback label and a note about its intended transform:\n\nrecords = [\n  {\"label\": \"k_fit_invT\", \"domain\": \"kinetics\", \"transform\": \"arrhenius_like\", \"uses\": \"rate\"},\n  {\"label\": \"invT_viscosity_fit\", \"domain\": \"transport\", \"transform\": \"arrhenius_like\", \"uses\": \"viscosity\"},\n  {\"label\": \"rho_linear_T\", \"domain\": \"thermo\", \"transform\": \"linear_T\", \"uses\": \"density\"},\n  {\"label\": \"k_fit_shifted_invT\", \"domain\": \"kinetics\", \"transform\": \"invT_shifted\", \"uses\": \"rate\"},\n  {\"label\": \"bad_missing_label\", \"domain\": \"transport\", \"transform\": \"arrhenius_like\", \"uses\": \"viscosity\", \"label\": \"\"}\n]\n\nBuild ChemPy polynomial Expression factories only for submodels that are intended to be Arrhenius-like in temperature, meaning they should evaluate a polynomial in reciprocal absolute temperature 1/T with no additional shift, using the scalar parameter named \"T\". Use each qualifying record’s label as the callback name so the inverse-temperature models can be tracked during debugging and cross-validation.", "answers": "[{\"name\":\"chempy_util__expr_create_Poly\",\"arguments\":{\"parameter_name\":\"T\",\"reciprocal\":true,\"shift\":null,\"name\":\"k_fit_invT\"}},{\"name\":\"chempy_util__expr_create_Poly\",\"arguments\":{\"parameter_name\":\"T\",\"reciprocal\":true,\"shift\":null,\"name\":\"invT_viscosity_fit\"}}]"}
{"func_name": "chempy_util_stoich_get_coeff_mtx", "func_desc": "chempy.util.stoich.get_coeff_mtx computes the net stoichiometry coefficient matrix for a set of chemical reactions. It converts a sequence of reaction stoichiometries (each given as a pair of reactant and product dictionaries mapping substance keys to stoichiometric coefficients) into a 2-D integer NumPy array where each row corresponds to a substance and each column corresponds to a reaction. This matrix is the standard coefficient matrix used in balancing stoichiometric equations and in forming linear systems for chemical-equilibrium and reaction-network calculations (for example, as an intermediate in balance_stoichiometry and related routines in ChemPy).", "tools": [{"function": {"description": "chempy.util.stoich.get_coeff_mtx computes the net stoichiometry coefficient matrix for a set of chemical reactions. It converts a sequence of reaction stoichiometries (each given as a pair of reactant and product dictionaries mapping substance keys to stoichiometric coefficients) into a 2-D integer NumPy array where each row corresponds to a substance and each column corresponds to a reaction. This matrix is the standard coefficient matrix used in balancing stoichiometric equations and in forming linear systems for chemical-equilibrium and reaction-network calculations (for example, as an intermediate in balance_stoichiometry and related routines in ChemPy).\n", "name": "chempy_util_stoich_get_coeff_mtx", "parameters": {"properties": {"substances": {"type": "array", "items": {"type": "float"}, "description": "Ordered list of substance keys. Each entry identifies a substance (for example a chemical formula string or any hashable key used consistently in the stoichiometry dictionaries). The order of this list defines the row order of the returned matrix: row i corresponds to substances[i]. If a substance does not appear in a particular reaction pair, it is treated as having coefficient zero for that reaction.", "default": ""}, "stoichs": {"type": "array", "items": {"type": "float"}, "description": "List of reaction stoichiometries, one element per reaction, in the same column order as the desired output. Each element must be a pair (reactant_dict, product_dict), where each dict maps the same kind of substance keys (as used in substances) to stoichiometric coefficients (integers are expected). The function iterates over this sequence and uses each pair to compute the net production minus consumption for each substance.", "default": ""}}, "required": ["substances", "stoichs"], "type": "any"}}, "type": "function"}], "query": "We’re validating two reaction-network cohorts for a kinetics/equilibrium pipeline, but the raw mechanism export is messy: some reactions include spectators, non-integer stoichiometry, and species outside the cohort’s tracking list. For each cohort below, generate the net stoichiometric coefficient matrix (rows = substances in the exact fixed order given; columns = reactions in the original listed sequence) using only reactions that (i) have strictly positive integer stoichiometric coefficients on both sides and (ii) reference no substances outside the cohort’s tracking list. \n\nCohort A (tracking order fixed as [\"H2\", \"O2\", \"H2O\", \"CO\", \"CO2\"]):\n- R1: 2 H2 + O2 -> 2 H2O\n- R2: CO + H2O -> CO2 + H2\n- R3: H2 + 0.5 O2 -> H2O\n- R4: 2 H2 + O2 + Ar -> 2 H2O + Ar\n\nCohort B (tracking order fixed as [\"CH4\", \"O2\", \"CO2\", \"H2O\", \"N2\", \"H2\", \"NH3\"]):\n- R1: CH4 + 2 O2 -> CO2 + 2 H2O\n- R2: N2 + 3 H2 -> 2 NH3\n- R3: N2 + H2 -> NH3\n- R4: CH4 + 2 O2 -> CO2 + 2 H2O + NO\n\nReturn one coefficient matrix per cohort for the reactions that pass the criteria.", "answers": "[{\"name\":\"chempy_util_stoich_get_coeff_mtx\",\"arguments\":{\"substances\":[\"H2\",\"O2\",\"H2O\",\"CO\",\"CO2\"],\"stoichs\":[[{\"H2\":2,\"O2\":1},{\"H2O\":2}],[{\"CO\":1,\"H2O\":1},{\"CO2\":1,\"H2\":1}]]}},{\"name\":\"chempy_util_stoich_get_coeff_mtx\",\"arguments\":{\"substances\":[\"CH4\",\"O2\",\"CO2\",\"H2O\",\"N2\",\"H2\",\"NH3\"],\"stoichs\":[[{\"CH4\":1,\"O2\":2},{\"CO2\":1,\"H2O\":2}],[{\"N2\":1,\"H2\":3},{\"NH3\":2}],[{\"N2\":1,\"H2\":1},{\"NH3\":1}]]}}]"}
{"func_name": "chempy_util_terminal_limit_logging", "func_desc": "chempy.util.terminal.limit_logging provides a context manager that temporarily raises the global logging threshold to suppress logging messages up to and including a specified numeric level. It is intended for use in ChemPy workflows where noisy log output from numerical integrators, equilibrium solvers, or optional backend libraries (see README: ODE solver front-end, equilibria solvers, and optional backends) should be silenced for clarity during tests, example runs, or user scripts.", "tools": [{"function": {"description": "chempy.util.terminal.limit_logging provides a context manager that temporarily raises the global logging threshold to suppress logging messages up to and including a specified numeric level. It is intended for use in ChemPy workflows where noisy log output from numerical integrators, equilibrium solvers, or optional backend libraries (see README: ODE solver front-end, equilibria solvers, and optional backends) should be silenced for clarity during tests, example runs, or user scripts.\n", "name": "chempy_util_terminal_limit_logging", "parameters": {"properties": {"max_lvl": {"type": "integer", "description": "Numeric logging threshold to disable. This integer is forwarded to logging.disable and therefore follows the semantics of the standard Python logging levels (for example, logging.CRITICAL == 50). The default value in the function signature is 50, which corresponds to logging.CRITICAL and will suppress all messages at that level and below while the context is active. The parameter controls the global logging state for the process; passing a lower value will allow fewer messages through and a higher value will suppress more.", "default": 50}}, "required": ["max_lvl"], "type": "any"}}, "type": "function"}], "query": "I’m running a mixed ChemPy benchmark with three run blocks that generate very different log characteristics. Each block emits an estimated noisy-line-rate (lines/min) from the solver/integrator stack, and I want logging suppression to be applied only where the noise rate indicates it will drown out the scientific output. Use this rule: for blocks with noise-line-rate in the range [20, 300), suppress messages up to and including WARNING (numeric level 30); for blocks with noise-line-rate >= 300, suppress everything up to and including CRITICAL (numeric level 50). Blocks with noise-line-rate < 20 should run without any logging suppression.\n\nBlocks:\n1) Equilibrium cohort (test replicate): noise-line-rate = 120 lines/min.\n2) Equilibrium cohort (report replicate): noise-line-rate = 900 lines/min.\n3) ODE integration demo replicate: noise-line-rate = 80 lines/min.\n\nSet up the appropriate `chempy.util.terminal.limit_logging` context manager for each block that qualifies under the rule so the console stays readable while preserving low-noise diagnostics.", "answers": "[{\"name\":\"chempy_util_terminal_limit_logging\",\"arguments\":{\"max_lvl\":30}},{\"name\":\"chempy_util_terminal_limit_logging\",\"arguments\":{\"max_lvl\":50}},{\"name\":\"chempy_util_terminal_limit_logging\",\"arguments\":{\"max_lvl\":30}}]"}
{"func_name": "cirpy_construct_api_url", "func_desc": "Construct and return the CIR (Chemical Identifier Resolver) API URL for a desired resolution request.\n    \n    This function is used by CIRpy to build the exact HTTP request URL that will be sent to the CIR web service (the NCI/NIH Chemical Identifier Resolver). Given a chemical identifier (for example, a chemical name or registry number) and the desired output representation (for example, a SMILES string, InChI, or a file format), this function encodes the identifier, selects the appropriate path and query parameters accepted by the CIR API, and returns a single percent-encoded URL string. The function does not perform any network I/O; it only composes the URL. It respects module-level constants such as FILE_FORMATS (which cause a representation to be sent as representation=file with a format=... query parameter) and API_BASE (the CIR endpoint base path).", "tools": [{"function": {"description": "Construct and return the CIR (Chemical Identifier Resolver) API URL for a desired resolution request.\n\nThis function is used by CIRpy to build the exact HTTP request URL that will be sent to the CIR web service (the NCI/NIH Chemical Identifier Resolver). Given a chemical identifier (for example, a chemical name or registry number) and the desired output representation (for example, a SMILES string, InChI, or a file format), this function encodes the identifier, selects the appropriate path and query parameters accepted by the CIR API, and returns a single percent-encoded URL string. The function does not perform any network I/O; it only composes the URL. It respects module-level constants such as FILE_FORMATS (which cause a representation to be sent as representation=file with a format=... query parameter) and API_BASE (the CIR endpoint base path).", "name": "cirpy_construct_api_url", "parameters": {"properties": {"input": {"type": "string", "description": "Chemical identifier to resolve. This is the raw identifier provided by the caller (for example \"Aspirin\" or \"50-78-2\"). The value will be percent-encoded (quoted) for safe inclusion in a URL path; if non-string values are passed, the underlying quote call may raise a TypeError. If tautomers is True, the function will prepend the literal prefix \"tautomers:\" to this string before encoding to instruct CIR to return all tautomers for the identifier.", "default": ""}, "representation": {"type": "string", "description": "Desired output representation requested from CIR. This specifies the CIR resource path segment (for example \"smiles\", \"inchi\", or other representation names). If this value is a member of the module FILE_FORMATS set, the function will instead set the path representation to \"file\" and add a query parameter format=<representation> so CIR returns the requested file format. The returned URL path will include this representation (or \"file\" when a file format is requested).", "default": ""}, "resolvers": {"type": "any", "nullable": true, "description": "Optional ordered list of resolver names to pass to the CIR service. When provided, the list elements are joined with commas and added as the resolver query parameter (resolver=name1,name2,...). The order in this list indicates the order in which CIR should attempt resolution. Elements must be strings; non-string elements will cause a TypeError during the join operation.", "default": null}, "get3d": {"type": "boolean", "description": "Optional flag indicating whether to request 3D coordinates from CIR when applicable. When True, the function adds get3d=True to the query string. Default is False. This flag is meaningful for representations and resolver combinations that can return 3D coordinate data.", "default": false}, "tautomers": {"type": "boolean", "description": "Optional flag indicating whether to request all tautomers of the given identifier. When True, the function prepends the literal prefix \"tautomers:\" to the input identifier (before percent-encoding) which tells the CIR service to return alternate tautomeric forms. Default is False.", "default": false}, "xml": {"type": "boolean", "description": "Optional flag indicating whether to request the CIR XML wrapper for the response. When True (the default), the function appends the literal path segment \"/xml\" to the constructed path so the CIR service returns its full XML response wrapper. When False, the \"/xml\" suffix is omitted and the raw resource endpoint is returned.", "default": true}, "kwargs": {"type": "dict", "additionalProperties": {"type": "any"}, "description": "Additional optional query parameters to include in the URL as a query string. Keys and values are encoded with urllib.parse.urlencode. Common uses include specifying format (for file outputs), page or detail options supported by CIR, or any other query parameters accepted by the CIR API. Values should be types accepted by urlencode (strings or sequences as appropriate); unsupported types may cause an exception during encoding.", "default": ""}}, "required": ["input", "representation", "get3d", "kwargs", "resolvers", "tautomers", "xml"], "type": "any"}}, "type": "function"}], "query": "We’re validating CIR URL construction against a heterogeneous, partially messy identifier intake stream from a metabolomics curation pass: [\"caffeine\", \"50-78-2\"]. Apply a branching protocol driven by identifier semantics and requested modality: treat records that look like a CAS Registry Number (two hyphen separators with an all-digit check digit) as registry identifiers and request a structure file deliverable; all other records are treated as common-name text identifiers and request a line notation.\n\nFor common-name identifiers: request the SMILES representation, force a non-XML response, enable tautomer enumeration, and append the additional query parameters detail=high and page=2.\n\nFor CAS-like identifiers: request an SDF file, force a non-XML response, set resolver preference order to PubChem then ChemSpider, enable 3D coordinates, leave tautomers disabled, and do not add any extra query parameters.", "answers": "[{\"name\":\"cirpy_construct_api_url\",\"arguments\":{\"input\":\"caffeine\",\"representation\":\"smiles\",\"resolvers\":null,\"get3d\":false,\"tautomers\":true,\"xml\":false,\"kwargs\":{\"detail\":\"high\",\"page\":2}}},{\"name\":\"cirpy_construct_api_url\",\"arguments\":{\"input\":\"50-78-2\",\"representation\":\"sdf\",\"resolvers\":[\"pubchem\",\"chemspider\"],\"get3d\":true,\"tautomers\":false,\"xml\":false,\"kwargs\":{}}}]"}
{"func_name": "datamol_data_chembl_samples", "func_desc": "datamol.data.chembl_samples returns a small curated sample of molecules drawn from the ChEMBL database (approximately 2,000 entries). This helper function provides an easy, fast source of real-world molecules for examples, tutorials, quick experiments, and tests within the datamol ecosystem. The sample data is read from the packaged resource file \"chembl_samples.csv\" (originally proposed by Patrick Walters at https://github.com/PatWalters/practical_cheminformatics_posts) using datamol's open_datamol_data_file helper, then loaded with pandas and optionally converted to RDKit Mol objects for downstream cheminformatics processing.", "tools": [{"function": {"description": "datamol.data.chembl_samples returns a small curated sample of molecules drawn from the ChEMBL database (approximately 2,000 entries). This helper function provides an easy, fast source of real-world molecules for examples, tutorials, quick experiments, and tests within the datamol ecosystem. The sample data is read from the packaged resource file \"chembl_samples.csv\" (originally proposed by Patrick Walters at https://github.com/PatWalters/practical_cheminformatics_posts) using datamol's open_datamol_data_file helper, then loaded with pandas and optionally converted to RDKit Mol objects for downstream cheminformatics processing.\n", "name": "datamol_data_chembl_samples", "parameters": {"properties": {"as_df": {"type": "boolean", "description": "If True (default), return the raw data loaded from the packaged CSV as a pandas.core.frame.DataFrame. The DataFrame contains the columns and metadata provided in the packaged \"chembl_samples.csv\" (for example, structural representations and identifiers as present in the source CSV) and is suitable for dataframe-based analysis, filtering, or persisting. If False, the function converts the DataFrame into a Python list of rdkit.Chem.rdchem.Mol objects by calling datamol.from_df; this is useful when the caller intends to perform RDKit-based molecule operations (fingerprinting, sanitization, 2D/3D processing, etc.). Note that conversion to rdkit.Chem.rdchem.Mol objects requires RDKit to be available in the environment; the conversion behavior (e.g., how invalid rows are handled) follows datamol.from_df semantics. The default value is True.", "default": true}}, "required": ["as_df"], "type": "any"}}, "type": "function"}], "query": "I’m setting up a two-stage cheminformatics intake workflow using the packaged ChEMBL sample set (~2k entries). Stage A is a rapid metadata/QC screen: load the dataset as a pandas DataFrame so we can inspect identifier/structure fields and compute simple per-row sanity checks (e.g., presence/emptiness patterns across key columns). Stage B is structure-first processing for downstream RDKit transforms: load the same dataset as RDKit Mol objects, but only after the QC screen is available to define which subset is chemically interpretable (e.g., non-null structure field and successfully parseable into RDKit). Run both stages so the QC output can drive which records proceed to structure-based benchmarking.", "answers": "[{\"name\":\"datamol_data_chembl_samples\",\"arguments\":{\"as_df\":true}},{\"name\":\"datamol_data_chembl_samples\",\"arguments\":{\"as_df\":false}}]"}
{"func_name": "datamol_descriptors_compute_any_rdkit_descriptor", "func_desc": "Return an RDKit descriptor function by its attribute name from the standard RDKit descriptor modules.\nThis helper is used in datamol (a thin layer on top of RDKit) to resolve a descriptor\nby name so callers can compute molecular descriptors/features for rdkit.Chem.Mol objects\nwithout importing RDKit descriptor modules directly. The function implements a lookup\nstrategy: it first attempts to retrieve the attribute from rdkit.Chem.Descriptors and,\nif not found, from rdkit.Chem.rdMolDescriptors. This is useful in datamol pipelines\nthat build descriptor-based feature vectors for machine learning, filtering, or\nchemoinformatics analyses.", "tools": [{"function": {"description": "Return an RDKit descriptor function by its attribute name from the standard RDKit descriptor modules.\nThis helper is used in datamol (a thin layer on top of RDKit) to resolve a descriptor\nby name so callers can compute molecular descriptors/features for rdkit.Chem.Mol objects\nwithout importing RDKit descriptor modules directly. The function implements a lookup\nstrategy: it first attempts to retrieve the attribute from rdkit.Chem.Descriptors and,\nif not found, from rdkit.Chem.rdMolDescriptors. This is useful in datamol pipelines\nthat build descriptor-based feature vectors for machine learning, filtering, or\nchemoinformatics analyses.", "name": "datamol_descriptors_compute_any_rdkit_descriptor", "parameters": {"properties": {"name": {"type": "string", "description": "The exact attribute name of the RDKit descriptor to resolve. This is\ncase-sensitive and must match the attribute name defined in rdkit.Chem.Descriptors\nor rdkit.Chem.rdMolDescriptors (for example, \"MolWt\", \"NumRotatableBonds\",\nor \"CalcTPSA\"). The parameter represents the descriptor identifier that datamol\ncallers provide when they want to compute a specific molecular feature.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a descriptor registry for a mixed-lot small-molecule screen where the requested feature set comes from multiple downstream tasks and may contain duplicates, aliases, and malformed entries from configuration merges. Given this raw descriptor-name manifest:\n\n['CalcTPSA', 'MolWt', 'ExactMolWt', 'MolWt', 'calcTPSA', 'TPSA', '', None, 'MolWt ', 'HeavyAtomMolWt']\n\nBuild the callable resolution list using our standard datamol lookup (rdkit.Chem.Descriptors first, then rdkit.Chem.rdMolDescriptors). Apply these protocol rules:\n- Canonicalize each candidate name by trimming whitespace; accept only names that match an RDKit descriptor attribute exactly after canonicalization.\n- Treat case-variants and common alias tokens as non-canonical and therefore not eligible for resolution.\n- De-duplicate by canonical name while preserving first-seen order.\n\nReturn the sequence of descriptor-callables to be used for the feature vector construction in this run.", "answers": "[{\"name\":\"datamol_descriptors_compute_any_rdkit_descriptor\",\"arguments\":{\"name\":\"CalcTPSA\"}},{\"name\":\"datamol_descriptors_compute_any_rdkit_descriptor\",\"arguments\":{\"name\":\"MolWt\"}},{\"name\":\"datamol_descriptors_compute_any_rdkit_descriptor\",\"arguments\":{\"name\":\"ExactMolWt\"}},{\"name\":\"datamol_descriptors_compute_any_rdkit_descriptor\",\"arguments\":{\"name\":\"HeavyAtomMolWt\"}}]"}
{"func_name": "datamol_reactions__reactions_rxn_from_smarts", "func_desc": "datamol.reactions._reactions.rxn_from_smarts: Create and initialize an RDKit ChemicalReaction from a reaction SMARTS string for use in Datamol reaction processing pipelines.", "tools": [{"function": {"description": "datamol.reactions._reactions.rxn_from_smarts: Create and initialize an RDKit ChemicalReaction from a reaction SMARTS string for use in Datamol reaction processing pipelines.\n", "name": "datamol_reactions__reactions_rxn_from_smarts", "parameters": {"properties": {"rxn_smarts": {"type": "string", "description": "Reaction SMARTS string describing the chemical transformation using RDKit reaction SMARTS syntax. This string is passed verbatim to rdkit.Chem.rdChemReactions.ReactionFromSmarts(SMARTS=...), so it must follow RDKit's SMARTS conventions for reactant and product patterns, atom mapping, and bond specifications. In the Datamol context this SMARTS is used to define transformation rules that can be applied to rdkit.Chem.Mol objects (for example via ChemicalReaction.RunReactants) during reaction enumeration, retrosynthesis workflows, or virtual chemical transformations.", "default": ""}}, "required": ["rxn_smarts"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mechanistic reaction-template library from a noisy export of atom-mapped reaction SMARTS produced by multiple vendors. From the following candidate reaction SMARTS strings, initialize Datamol-compatible RDKit ChemicalReaction objects only for entries that (i) contain exactly one reaction arrow ('>>'), (ii) include at least one atom-mapping label (e.g., ':1'), and (iii) are fully mapped in the sense that every mapped atom index appearing on the product side also appears somewhere on the reactant side (to avoid template leakage/teleportation artifacts). Candidate set:\n1) [C:1](=O)[O:2][H:3].[O:4][C:5]>>[C:1](=O)[O:4][C:5].[O:2][H:3]\n2) [CH3:1][CH2:2][Br:3].[OH-:4]>>[CH3:1][CH2:2][O:4][H].[Br-:3]\n3) CCO.O>>CCO\n4) [CH3:1][CH2:2][Br:3].[OH-:4]>[CH3:1][CH2:2][O:4][H].[Br-:3]\n5) [CH3:1][CH2:2][Br:3]>>[CH3:1][CH2:2][O:4][H].[Br-:3]\nReturn the initialized reactions for the templates that pass QC.", "answers": "[{\"name\":\"datamol_reactions__reactions_rxn_from_smarts\",\"arguments\":{\"rxn_smarts\":\"[C:1](=O)[O:2][H:3].[O:4][C:5]>>[C:1](=O)[O:4][C:5].[O:2][H:3]\"}},{\"name\":\"datamol_reactions__reactions_rxn_from_smarts\",\"arguments\":{\"rxn_smarts\":\"[CH3:1][CH2:2][Br:3].[OH-:4]>>[CH3:1][CH2:2][O:4][H].[Br-:3]\"}}]"}
{"func_name": "deepchem_data_datasets_pad_batch", "func_desc": "Pads a minibatch of examples to exactly batch_size by repeating the provided examples in tiled fashion.\n    \n    This function is used by DeepChem data-loading and training code to ensure each minibatch passed to a model has a fixed size (batch_size) even when the dataset size or the last batch of an epoch is smaller. It takes arrays of features, labels, sample weights, and identifiers that all represent the same short batch (length <= batch_size) and produces new arrays of length batch_size suitable for input to neural network training or evaluation routines in molecular machine learning, drug discovery, materials science, and related life-science applications.", "tools": [{"function": {"description": "Pads a minibatch of examples to exactly batch_size by repeating the provided examples in tiled fashion.\n\nThis function is used by DeepChem data-loading and training code to ensure each minibatch passed to a model has a fixed size (batch_size) even when the dataset size or the last batch of an epoch is smaller. It takes arrays of features, labels, sample weights, and identifiers that all represent the same short batch (length <= batch_size) and produces new arrays of length batch_size suitable for input to neural network training or evaluation routines in molecular machine learning, drug discovery, materials science, and related life-science applications.", "name": "deepchem_data_datasets_pad_batch", "parameters": {"properties": {"batch_size": {"type": "integer", "description": "The target number of datapoints required in the output batch. In training and evaluation loops this is the minibatch dimension expected by model code and loss computations. Must be a positive integer.", "default": ""}, "X_b": {"type": "array", "items": {"type": "float"}, "description": "Array of input features for the current (possibly short) batch. Must have length len(X_b) <= batch_size and must share a consistent feature shape across examples. If X_b.ndim > 1 then the output feature array will have shape (batch_size,) + X_b.shape[1:], preserving the per-example feature shape and the numpy dtype of X_b. If X_b.ndim == 1 then the output feature array will have shape (batch_size,) and the same dtype as X_b. X_b provides the concrete example tensors that will be tiled into X_out to reach batch_size.", "default": ""}, "y_b": {"type": "array", "items": {"type": "float"}, "description": "Array of labels/targets corresponding to X_b; must have length len(y_b) == len(X_b) and len(y_b) <= batch_size. If y_b is None, no label array will be produced (y_out will be None). If y_b is a 1-D array the returned y_out will have shape (batch_size,); if y_b has additional per-example dimensions (y_b.ndim >= 2) the returned y_out will have shape (batch_size,) + y_b.shape[1:], and dtype preserved. y_b supplies supervised targets used by loss functions and evaluation.", "default": ""}, "w_b": {"type": "array", "items": {"type": "float"}, "description": "Array of sample weights corresponding to X_b; must have length len(w_b) == len(X_b) and len(w_b) <= batch_size. If w_b is None, w_out will be None. If w_b is 1-D the returned w_out will have shape (batch_size,); if w_b has extra per-example dimensions (w_b.ndim >= 2) the returned w_out will have shape (batch_size,) + w_b.shape[1:], and dtype preserved. Note: when padding occurs this implementation assigns the original w_b values only to the first len(X_b) positions of w_out (see behavior below); remaining weight entries remain zero.", "default": ""}, "ids_b": {"type": "array", "items": {"type": "float"}, "description": "1-D array of identifiers (e.g., example IDs, indices, or string/object dtype allowed by numpy) for the examples in X_b; must have length len(ids_b) == len(X_b) and len(ids_b) <= batch_size. ids_out will be a numpy array of length batch_size with the same dtype as ids_b; ids from ids_b are copied repeatedly to fill ids_out.", "default": ""}}, "required": ["batch_size", "X_b", "y_b", "w_b", "ids_b"], "type": "any"}}, "type": "function"}], "query": "In our molecular property prediction pipeline we’re stabilizing end-of-epoch minibatches to a fixed batch_size=8, but the dataloader may emit heterogeneous “short batches” that include artifacts.\n\nWe collected two short candidate minibatches (each length 3) from different featurization stacks. Before padding, apply a data-sieve rule consistent with typical assay/training hygiene:\n\n1) Only retain examples whose molecule identifier matches the expected canonical pattern for that featurization stack.\n   - For graph/atom-embedding batches, ids must match the pattern `mol_###` (underscore + exactly 3 digits).\n   - For fixed-length descriptor batches, ids must match the pattern `mol-###` (hyphen + exactly 3 digits).\n\n2) Additionally, only retain examples with strictly positive sample weight (w_b > 0).\n\nAfter filtering, for each surviving short batch, pad to exactly batch_size=8 by repeating/tiling the remaining examples in order, keeping X_b, y_b, w_b, and ids_b aligned.\n\nRaw Batch A (graph/atom-embedding style features):\nX_b = [ [[0.1, 0.2, 0.3, 0.4], [0.0, -0.1, 0.5, 0.7]], [[1.0, 0.9, 0.8, 0.7], [0.2, 0.3, 0.4, 0.5]], [[-0.3, 0.0, 0.1, 0.2], [0.6, 0.7, 0.8, 0.9]] ]\ny_b = [0.85, 0.43, 0.67]\nw_b = [1.0, 0.0, 1.0]\nids_b = [\"mol_001\", \"mol-002\", \"mol_003\"]\n\nRaw Batch B (fixed-length descriptor features):\nX_b = [ [0.12, 1.5, -0.3, 2.2], [0.05, 1.2, -0.1, 2.0], [0.2, 1.8, -0.4, 2.5] ]\ny_b = [7.1, 6.8, 7.5]\nw_b = [1.0, -0.5, 1.0]\nids_b = [\"mol-001\", \"mol_002\", \"mol-003\"]\n\nReturn the padded outputs for each batch that remains after applying these intrinsic validity rules.", "answers": "[{\"name\":\"deepchem_data_datasets_pad_batch\",\"arguments\":{\"batch_size\":8,\"X_b\":[[[0.1,0.2,0.3,0.4],[0.0,-0.1,0.5,0.7]],[[-0.3,0.0,0.1,0.2],[0.6,0.7,0.8,0.9]]],\"y_b\":[0.85,0.67],\"w_b\":[1.0,1.0],\"ids_b\":[\"mol_001\",\"mol_003\"]}},{\"name\":\"deepchem_data_datasets_pad_batch\",\"arguments\":{\"batch_size\":8,\"X_b\":[[0.12,1.5,-0.3,2.2],[0.2,1.8,-0.4,2.5]],\"y_b\":[7.1,7.5],\"w_b\":[1.0,1.0],\"ids_b\":[\"mol-001\",\"mol-003\"]}}]"}
{"func_name": "deepchem_data_datasets_pad_features", "func_desc": "Pads a batch of features to exactly the requested batch size by repeating (tiling) the input examples.\n    \n    This function is used in DeepChem's data pipeline for inference-time query processing when a model or runtime requires a fixed batch size. Given an input array of feature vectors X_b whose length is less than or equal to batch_size, pad_features constructs and returns a new numpy.ndarray of length exactly batch_size by tiling the rows (or elements) of X_b in order until the batch is full. The output preserves the dtype of X_b and the per-sample feature shape: if X_b is 1-D with shape (N,), the result has shape (batch_size,); if X_b is multi-dimensional with shape (N, ...) the result has shape (batch_size, ...). This function performs no operation on labels or weights and is intended for features-only padding (see similar utilities such as pad_batch for label/weight handling).", "tools": [{"function": {"description": "Pads a batch of features to exactly the requested batch size by repeating (tiling) the input examples.\n\nThis function is used in DeepChem's data pipeline for inference-time query processing when a model or runtime requires a fixed batch size. Given an input array of feature vectors X_b whose length is less than or equal to batch_size, pad_features constructs and returns a new numpy.ndarray of length exactly batch_size by tiling the rows (or elements) of X_b in order until the batch is full. The output preserves the dtype of X_b and the per-sample feature shape: if X_b is 1-D with shape (N,), the result has shape (batch_size,); if X_b is multi-dimensional with shape (N, ...) the result has shape (batch_size, ...). This function performs no operation on labels or weights and is intended for features-only padding (see similar utilities such as pad_batch for label/weight handling).", "name": "deepchem_data_datasets_pad_features", "parameters": {"properties": {"batch_size": {"type": "integer", "description": "The target number of datapoints in the returned batch. This controls the first dimension of the returned array. Must be a positive integer and must be greater than or equal to the number of samples in X_b. If batch_size is smaller than the number of input samples, the function will raise a ValueError (see Raises:).", "default": ""}, "X_b": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of feature examples to be padded. X_b must satisfy 1 <= len(X_b) <= batch_size. Each element/row of X_b represents a single example's features in the domain (for example, molecular descriptors or fingerprint vectors in DeepChem workflows). The function preserves X_b.dtype and the per-example feature shape (the trailing dimensions of X_b). X_b is not modified in-place; a new array is returned.", "default": ""}}, "required": ["batch_size", "X_b"], "type": "any"}}, "type": "function"}], "query": "We’re running inference-time serving on three incoming micro-batches of molecular feature vectors, but the runtime requires fixed batch sizes. Before padding, apply a QC sieve: keep only feature rows that are finite and have all components within the physically plausible normalized descriptor range [0, 1]. For any micro-batch that passes QC with at least one remaining row, pad features-only by tiling the remaining rows in-order until the batch is exactly full, preserving dtype and per-sample shape. Use a dynamic batch sizing rule: set batch_size to the smallest power-of-two that is >= the number of QC-passing rows in that micro-batch.\n\nRaw micro-batches:\nA: X_b=[[0.1, 0.0, 0.3, 0.6], [0.5, 0.2, 0.1, 0.2]]\nB: X_b=[[0.1, 0.0, 0.5, 0.2], [0.3, 0.8, 0.0, 0.4], [0.9, 0.2, 0.1, 0.0]]\nC: X_b=[[0.12, -1.5, 3.3, 0.0], [0.08, -1.2, 3.0, 0.5], [0.20, -1.7, 3.6, -0.2]]", "answers": "[{\"name\":\"deepchem_data_datasets_pad_features\",\"arguments\":{\"batch_size\":2,\"X_b\":[[0.1,0.0,0.3,0.6],[0.5,0.2,0.1,0.2]]}},{\"name\":\"deepchem_data_datasets_pad_features\",\"arguments\":{\"batch_size\":4,\"X_b\":[[0.1,0.0,0.5,0.2],[0.3,0.8,0.0,0.4],[0.9,0.2,0.1,0.0]]}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_gaussian_first", "func_desc": "Computes Autodock Vina's first Gaussian interaction term.\n    \n    This function implements the first Gaussian term from the Autodock Vina scoring function (Jain, 1996) that is commonly used in molecular docking to estimate a component of protein–ligand binding affinity. The computation is performed elementwise as exp(-(d / 0.5)**2), where the constant 0.5 is the Gaussian width parameter used by Autodock Vina. In practical workflows within DeepChem, this function converts a matrix of surface distances between ligand and protein atoms or grid points into a matrix of Gaussian-shaped interaction contributions that can be summed or combined with other terms to produce a docking score.", "tools": [{"function": {"description": "Computes Autodock Vina's first Gaussian interaction term.\n\nThis function implements the first Gaussian term from the Autodock Vina scoring function (Jain, 1996) that is commonly used in molecular docking to estimate a component of protein–ligand binding affinity. The computation is performed elementwise as exp(-(d / 0.5)**2), where the constant 0.5 is the Gaussian width parameter used by Autodock Vina. In practical workflows within DeepChem, this function converts a matrix of surface distances between ligand and protein atoms or grid points into a matrix of Gaussian-shaped interaction contributions that can be summed or combined with other terms to produce a docking score.", "name": "deepchem_dock_pose_scoring_vina_gaussian_first", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of shape `(N, M)` containing surface distances as defined in Jain 1996. Each element represents a distance measure used by the Vina scoring model (typically non-negative distances between ligand and protein surface points). The array should have a numeric dtype (e.g., float32 or float64). The function expects a full (N, M) array; providing inputs of a different shape will result in an output whose shape follows numpy's broadcasting rules or may raise an exception if broadcasting is not possible.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "We’re generating Vina-style Gaussian(1) interaction features from two replicate protein–ligand surface-distance grids (nm), but the raw distance exports include a mix of physically meaningful contact distances and non-contact/invalid entries. For each replicate matrix, compute Autodock Vina’s first Gaussian term exp(-(d/0.5)^2) only for entries that represent plausible near-contact surface distances in the closed interval [0.0, 1.5] nm; treat any negative values, NaNs, or values > 1.5 nm as out-of-scope for this feature-generation step and exclude them by filtering prior to calling the transform. Replicate A (3x4): [[0.0, 0.2, 0.5, 1.0],[0.1, 0.3, 0.6, 1.2],[0.05, 0.25, 0.55, 0.9]]. Replicate B (3x3): [[0.0, 0.3, 0.7],[1.0, 1.5, 2.0],[0.25, 0.5, 0.75]].", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_vina_gaussian_first\",\"arguments\":{\"d\":[[0.0,0.2,0.5,1.0],[0.1,0.3,0.6,1.2],[0.05,0.25,0.55,0.9]]}},{\"name\":\"deepchem_dock_pose_scoring_vina_gaussian_first\",\"arguments\":{\"d\":[[0.0,0.3,0.7],[1.0,1.5],[0.25,0.5,0.75]]}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_hbond", "func_desc": "Computes AutoDock Vina's hydrogen bond interaction term for docking pose scoring.\n    \n    This function implements the piecewise linear hydrogen-bond term used by AutoDock Vina (Jain 1996) to convert inter-surface distances into a normalized interaction contribution used in scoring protein-ligand poses. In DeepChem's docking/pose-scoring pipeline this term is one component of the overall Vina-style scoring function and is applied to the set of surface distances computed between interacting atoms or molecular surfaces. The input d is expected to contain the surface distances as defined in the cited reference: negative values indicate overlapping/close contact (favorable for hydrogen bonding), and non-negative values indicate separated surfaces (no hydrogen-bond contribution).\n    \n    Behavior: for each element of d the function returns a value in the interval [0, 1] according to the following piecewise definition implemented in the source code:\n    - If d < -0.7, the interaction term is saturated at 1.\n    - If -0.7 <= d < 0, the interaction term increases linearly as (1.0 / 0.7) * (0 - d) (equivalently -d / 0.7), producing a smooth ramp from 1 at d = -0.7 to 0 at d = 0.\n    - If d >= 0, the interaction term is 0.\n    \n    The function performs no in-place modification of its input and has no side effects beyond returning the computed array.", "tools": [{"function": {"description": "Computes AutoDock Vina's hydrogen bond interaction term for docking pose scoring.\n\nThis function implements the piecewise linear hydrogen-bond term used by AutoDock Vina (Jain 1996) to convert inter-surface distances into a normalized interaction contribution used in scoring protein-ligand poses. In DeepChem's docking/pose-scoring pipeline this term is one component of the overall Vina-style scoring function and is applied to the set of surface distances computed between interacting atoms or molecular surfaces. The input d is expected to contain the surface distances as defined in the cited reference: negative values indicate overlapping/close contact (favorable for hydrogen bonding), and non-negative values indicate separated surfaces (no hydrogen-bond contribution).\n\nBehavior: for each element of d the function returns a value in the interval [0, 1] according to the following piecewise definition implemented in the source code:\n- If d < -0.7, the interaction term is saturated at 1.\n- If -0.7 <= d < 0, the interaction term increases linearly as (1.0 / 0.7) * (0 - d) (equivalently -d / 0.7), producing a smooth ramp from 1 at d = -0.7 to 0 at d = 0.\n- If d >= 0, the interaction term is 0.\n\nThe function performs no in-place modification of its input and has no side effects beyond returning the computed array.", "name": "deepchem_dock_pose_scoring_vina_hbond", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of surface distances with shape (N, M). Each element is a numeric surface distance as used in AutoDock Vina-style scoring (negative values denote close contact/overlap, non-negative values denote separation). The array must be a numpy.ndarray; values should be numeric (float or integer). The function expects the distances arranged so that corresponding rows/columns map to the pairs of interacting entities used elsewhere in the DeepChem docking pipeline.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "We’re triaging hydrogen-bond contact maps from three docking replicates where the distance extractor occasionally emits non-finite artifacts or clearly non-physical magnitudes. For each replicate’s 2D protein–ligand inter-surface distance matrix d, construct a cleaned matrix by retaining only entries that are finite real numbers and whose magnitude is within a physically plausible surface-distance window (|d| <= 2.0 Å); for any entry failing these criteria, replace it with 0.0 (treat as no hydrogen-bond contribution). Then, run the AutoDock Vina hydrogen-bond term conversion on each cleaned matrix using the standard piecewise rule. Replicate A d = [[-0.9, -0.7, -0.35, 0.0], [-0.05, 0.2, -1.2, -0.6], [0.8, -0.1, -0.72, 0.03]]. Replicate B d = [[-1.2, -0.7, -0.35, 0.0, 0.4], [-0.9, -0.5, -0.2, 0.1, 0.8], [-1.0, -0.8, -0.1, 0.0, 0.3]]. Replicate C d = [[-1.2, -0.8, -0.5], [-0.7, -0.35, 0.0], [-0.1, 0.2, 0.9], [-1.0, -0.2, 0.05]]. Additionally, include a fourth QC replicate D that contains typical extractor artifacts: d = [[-0.9, -0.7, null, 0.0], [-0.05, 2.5, -1.2, -0.6], [0.8, -0.1, -0.72, \"NaN\"], [\"inf\", -0.2, 0.05, -3.1]]. Return the hydrogen-bond interaction-term matrices for all replicates after cleaning.", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_vina_hbond\",\"arguments\":{\"d\":[[-0.9,-0.7,-0.35,0.0],[-0.05,0.2,-1.2,-0.6],[0.8,-0.1,-0.72,0.03]]}},{\"name\":\"deepchem_dock_pose_scoring_vina_hbond\",\"arguments\":{\"d\":[[-1.2,-0.7,-0.35,0.0,0.4],[-0.9,-0.5,-0.2,0.1,0.8],[-1.0,-0.8,-0.1,0.0,0.3]]}},{\"name\":\"deepchem_dock_pose_scoring_vina_hbond\",\"arguments\":{\"d\":[[-1.2,-0.8,-0.5],[-0.7,-0.35,0.0],[-0.1,0.2,0.9],[-1.0,-0.2,0.05]]}},{\"name\":\"deepchem_dock_pose_scoring_vina_hbond\",\"arguments\":{\"d\":[[-0.9,-0.7,0.0,0.0],[-0.05,0.0,-1.2,-0.6],[0.8,-0.1,-0.72,0.0],[0.0,-0.2,0.05,0.0]]}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_nonlinearity", "func_desc": "deepchem.dock.pose_scoring.vina_nonlinearity: Compute a Vina-inspired nonlinearity used in docking pose scoring to attenuate per-activation values according to molecular flexibility.\n    \n    This function implements a simple, elementwise nonlinearity used in DeepChem's docking/pose-scoring utilities. It divides every element of an input activation matrix by a scalar factor (1 + w * Nrot). In the context of molecular docking and drug-discovery workflows (see DeepChem README), this nonlinearity is used to penalize or scale pose scoring activations based on the number of rotatable bonds in a ligand: as the number of rotatable bonds increases, the denominator increases (for positive w) and activations are correspondingly reduced, reflecting increased conformational entropy and flexibility.", "tools": [{"function": {"description": "deepchem.dock.pose_scoring.vina_nonlinearity: Compute a Vina-inspired nonlinearity used in docking pose scoring to attenuate per-activation values according to molecular flexibility.\n\nThis function implements a simple, elementwise nonlinearity used in DeepChem's docking/pose-scoring utilities. It divides every element of an input activation matrix by a scalar factor (1 + w * Nrot). In the context of molecular docking and drug-discovery workflows (see DeepChem README), this nonlinearity is used to penalize or scale pose scoring activations based on the number of rotatable bonds in a ligand: as the number of rotatable bonds increases, the denominator increases (for positive w) and activations are correspondingly reduced, reflecting increased conformational entropy and flexibility.", "name": "deepchem_dock_pose_scoring_vina_nonlinearity", "parameters": {"properties": {"c": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of shape (N, M) containing input activations or scores for N examples and M channels/features. Each element is treated as a numeric activation value and will be divided by the computed scalar factor. The input array is not modified in-place; a new array is returned.", "default": ""}, "w": {"type": "float", "description": "Weighting term used to scale the contribution of Nrot in the denominator. The scalar factor applied to c is (1 + w * Nrot). Practically, w controls how strongly the number of rotatable bonds attenuates the activations: larger positive w increases attenuation. The function accepts any Python float; incorrect (non-numeric) values will result in numpy type errors when dividing.", "default": ""}, "Nrot": {"type": "integer", "description": "Number of rotatable bonds in the molecule (an integer count, typically >= 0). This integer is multiplied by w to form the denominator adjustment term. Passing a non-integer value is not recommended and may produce unexpected results or numpy type coercion; the function signature and intended domain semantics expect an integer count.", "default": ""}}, "required": ["c", "w", "Nrot"], "type": "any"}}, "type": "function"}], "query": "We’re curating a docking benchmark where per-pose activation tensors must be attenuated for ligand flexibility, but the raw cohort dump is messy: each cohort may correspond to a different ligand microstate with a different rotatable-bond count (Nrot), and only physically meaningful flexibility regimes should be passed to the Vina-style nonlinearity. For each cohort below, first infer Nrot from the activation matrix itself as the count of strictly negative entries (treating negatives as torsional-strain indicators). Then apply the elementwise Vina-inspired attenuation using a cohort-specific penalty rule: if the inferred Nrot is at least 4, use w = 0.15 (high-flexibility penalty); otherwise use w = 0.05 (moderate penalty). Process the following activation cohorts: (A) 2×4 matrix [[1.2, -0.6, 0.9, 2.1], [0.3, 1.5, -0.2, 0.8]]; (B) 3×4 matrix [[2.5, 1.8, 0.9, 3.2], [1.1, 0.5, 2.0, 1.7], [3.3, 2.2, 1.0, 0.8]]; (C) 3×4 matrix [[2.3, 1.8, -0.5, 0.0], [3.1, 0.9, 1.2, -1.4], [0.7, -0.2, 2.0, 1.5]]. Output the attenuated activation matrices for all cohorts that satisfy the inferred flexibility regime.", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_vina_nonlinearity\",\"arguments\":{\"c\":[[1.2,-0.6,0.9,2.1],[0.3,1.5,-0.2,0.8]],\"w\":0.05,\"Nrot\":2}},{\"name\":\"deepchem_dock_pose_scoring_vina_nonlinearity\",\"arguments\":{\"c\":[[2.5,1.8,0.9,3.2],[1.1,0.5,2.0,1.7],[3.3,2.2,1.0,0.8]],\"w\":0.05,\"Nrot\":0}},{\"name\":\"deepchem_dock_pose_scoring_vina_nonlinearity\",\"arguments\":{\"c\":[[2.3,1.8,-0.5,0.0],[3.1,0.9,1.2,-1.4],[0.7,-0.2,2.0,1.5]],\"w\":0.05,\"Nrot\":3}}]"}
{"func_name": "deepchem_dock_pose_scoring_vina_repulsion", "func_desc": "deepchem.dock.pose_scoring.vina_repulsion computes the Autodock Vina repulsion interaction term used in molecular docking pose scoring. In the docking domain this term penalizes steric overlaps between atoms: negative values in the input indicate overlap (distance deficit) and produce a positive quadratic repulsion penalty, whereas non-overlapping separations produce no repulsion contribution.", "tools": [{"function": {"description": "deepchem.dock.pose_scoring.vina_repulsion computes the Autodock Vina repulsion interaction term used in molecular docking pose scoring. In the docking domain this term penalizes steric overlaps between atoms: negative values in the input indicate overlap (distance deficit) and produce a positive quadratic repulsion penalty, whereas non-overlapping separations produce no repulsion contribution.\n", "name": "deepchem_dock_pose_scoring_vina_repulsion", "parameters": {"properties": {"d": {"type": "array", "items": {"type": "float"}, "description": "A numeric numpy array of shape `(N, M)` containing elementwise distance-like values used by the Vina scoring pipeline. In typical use within DeepChem's pose-scoring routines, each element represents a signed distance measure or deviation for an atom pair or grid point where negative values denote overlap/penetration and non-negative values denote non-overlap. The function performs elementwise operations and therefore expects a numeric dtype; supplying non-numeric data or incompatible objects may raise an exception during computation.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "We’re triaging steric-clash artifacts across docking pose replicates where the signed distance-deficit matrices were exported from two different scoring builds.\n\nReplicate A is a clean 3×4 matrix in Å and can be scored directly.\n\nReplicate B is also 3×4 but comes from a build that sometimes emits sentinel “no-contact” values as large positives and occasionally flips the sign convention for an entire ligand atom row (row-wise) during a coordinate-frame handoff. Before scoring B, apply this protocol:\n1) Treat any entry > +1.0 Å as a no-contact placeholder and clamp it to 0.0 so it contributes no repulsion.\n2) If a ligand-atom row contains at least two overlaps (negative entries after step 1), assume that row’s sign convention was flipped and multiply the entire row by −1.\n\nAfter applying this protocol, compute the Autodock Vina repulsion term for Replicate A (as-is) and for the corrected Replicate B (post-protocol). Use the following raw matrices:\n\nReplicate A:\n[[-0.30, 0.10, -0.05, 0.00],\n [ 0.25,-0.20, 0.15,-0.10],\n [ 0.00, 0.40,-0.35, 0.05]]\n\nReplicate B (raw):\n[[-0.8, -0.2, 0.0, 1.5],\n [-1.1,  0.3,-0.4, 0.0],\n [ 0.6,  0.0, 2.0,-0.05]]", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_vina_repulsion\",\"arguments\":{\"d\":[[-0.3,0.1,-0.05,0.0],[0.25,-0.2,0.15,-0.1],[0.0,0.4,-0.35,0.05]]}},{\"name\":\"deepchem_dock_pose_scoring_vina_repulsion\",\"arguments\":{\"d\":[[0.8,0.2,-0.0,0.0],[1.1,-0.3,0.4,-0.0],[0.6,0.0,0.0,-0.05]]}}]"}
{"func_name": "deepchem_dock_pose_scoring_weighted_linear_sum", "func_desc": "deepchem.dock.pose_scoring.weighted_linear_sum computes a weighted linear sum by contracting a one-dimensional weight vector with the first axis of a three-dimensional feature tensor. In the DeepChem docking/pose-scoring context (drug discovery and molecular docking workflows described in the repository README), this function is used to combine N individual component scores or feature channels (for example, per-term energy contributions or per-feature model outputs) into final per-pose/per-output values that can be used for ranking or downstream evaluation.", "tools": [{"function": {"description": "deepchem.dock.pose_scoring.weighted_linear_sum computes a weighted linear sum by contracting a one-dimensional weight vector with the first axis of a three-dimensional feature tensor. In the DeepChem docking/pose-scoring context (drug discovery and molecular docking workflows described in the repository README), this function is used to combine N individual component scores or feature channels (for example, per-term energy contributions or per-feature model outputs) into final per-pose/per-output values that can be used for ranking or downstream evaluation.\n", "name": "deepchem_dock_pose_scoring_weighted_linear_sum", "parameters": {"properties": {"w": {"type": "array", "items": {"type": "float"}, "description": "A 1-D numpy array of shape (N,) containing the weight for each of the N components or feature channels. In docking pipelines this represents the importance or coefficient applied to each score component. The function expects w to have length N that matches the first dimension of x; if shapes are incompatible a ValueError will be raised by the underlying numpy operation.", "default": ""}, "x": {"type": "array", "items": {"type": "float"}, "description": "A 3-D numpy array of shape (N, M, L) containing N channels of values to be combined across M items (for example, M docking poses or M molecules) and L outputs per item (for example, L different target properties or replicate outputs). Each slice x[i, :, :] corresponds to the values contributed by the i-th component whose weight is given by w[i]. The function assumes this layout and will contract w with the first axis of x.", "default": ""}}, "required": ["w", "x"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating a noisy multi-cohort docking benchmark into final per-pose/per-output scores using DeepChem’s `deepchem.dock.pose_scoring.weighted_linear_sum`, but the aggregation policy differs by cohort based on intrinsic signal characteristics.\n\nRaw cohorts (feature tensors are component-major: shape (N_components, N_poses, N_outputs)):\n\n**Cohort A (4-term physics energy; 3 poses × 2 outputs)**\n- Component order: (van der Waals, electrostatic, solvation, hydrogen bonding)\n- Baseline weights: w_base = [0.5, 1.2, -0.3, 0.8]\n- x (4,3,2):\n  - x[0] = [[-7.2, -6.8], [-5.9, -5.5], [-8.1, -7.6]]\n  - x[1] = [[-10.5, -9.8], [-9.1, -8.7], [-11.3, -10.9]]\n  - x[2] = [[ 1.8,  2.0], [ 2.3,  2.5], [ 1.5,  1.7]]\n  - x[3] = [[-3.2, -3.0], [-2.7, -2.5], [-3.5, -3.3]]\nAggregation rule: use **w_base** if the cohort’s electrostatic channel has a mean magnitude > 9 across all poses/outputs; otherwise, use a **down-weighted electrostatic** vector where only the electrostatic weight is multiplied by 0.5.\n\n**Cohort B (4-term empirical; 3 poses × 2 outputs)**\n- Component order: (van der Waals, electrostatics, desolvation, hydrogen bonding)\n- Baseline weights: w_base = [0.5, -0.2, 0.3, 1.0]\n- x (4,3,2):\n  - x[0] = [[-7.2, -6.8], [-6.5, -6.1], [-8.0, -7.6]]\n  - x[1] = [[-1.0, -0.8], [-0.5, -0.4], [-1.5, -1.3]]\n  - x[2] = [[ 0.8,  0.6], [ 1.0,  0.9], [ 0.5,  0.3]]\n  - x[3] = [[-2.0, -1.7], [-1.8, -1.5], [-2.3, -2.0]]\nAggregation rule: if the desolvation channel’s mean across all poses/outputs is < 0.75, apply a **penalty emphasis** by multiplying only the desolvation weight by 1.5; otherwise keep w_base.\n\n**Cohort C (3-component model-feature blend; 2 poses × 4 outputs)**\n- Component order: (model_score, uncertainty_proxy, interaction_prior)\n- Baseline weights: w_base = [0.5, -1.2, 2.0]\n- x (3,2,4):\n  - x[0] = [[ 1.0, 0.8, 0.5, 0.2], [0.9, 0.7, 0.4, 0.1]]\n  - x[1] = [[ 0.3, 0.1, -0.2, -0.4], [0.2, 0.0, -0.3, -0.5]]\n  - x[2] = [[-0.6, -0.3, 0.0, 0.2], [-0.4, -0.1, 0.1, 0.3]]\nAggregation rule: if any value in the uncertainty_proxy channel is negative, flip the sign of the uncertainty weight (i.e., multiply only that weight by -1) before contraction; otherwise keep w_base.\n\nFor each cohort, compute the final per-pose/per-output score tensor via the weighted linear contraction over the component axis, using the cohort-specific rule above. Return the aggregated score tensors for all cohorts for downstream ranking and cross-cohort comparison.", "answers": "[{\"name\":\"deepchem_dock_pose_scoring_weighted_linear_sum\",\"arguments\":{\"w\":[0.5,1.2,-0.3,0.8],\"x\":[[[-7.2,-6.8],[-5.9,-5.5],[-8.1,-7.6]],[[-10.5,-9.8],[-9.1,-8.7],[-11.3,-10.9]],[[1.8,2.0],[2.3,2.5],[1.5,1.7]],[[-3.2,-3.0],[-2.7,-2.5],[-3.5,-3.3]]]}} ,{\"name\":\"deepchem_dock_pose_scoring_weighted_linear_sum\",\"arguments\":{\"w\":[0.5,-0.2,0.45,1.0],\"x\":[[[-7.2,-6.8],[-6.5,-6.1],[-8.0,-7.6]],[[-1.0,-0.8],[-0.5,-0.4],[-1.5,-1.3]],[[0.8,0.6],[1.0,0.9],[0.5,0.3]],[[-2.0,-1.7],[-1.8,-1.5],[-2.3,-2.0]]]}} ,{\"name\":\"deepchem_dock_pose_scoring_weighted_linear_sum\",\"arguments\":{\"w\":[0.5,1.2,2.0],\"x\":[[[1.0,0.8,0.5,0.2],[0.9,0.7,0.4,0.1]],[[0.3,0.1,-0.2,-0.4],[0.2,0.0,-0.3,-0.5]],[[-0.6,-0.3,0.0,0.2],[-0.4,-0.1,0.1,0.3]]]}}]"}
{"func_name": "deepchem_feat_graph_features_features_to_id", "func_desc": "Convert a list of discrete graph features into a single integer index used by DeepChem graph featurizers.\n    \n    This function is used in DeepChem's graph feature pipeline to map a vector of discrete, typically integer-valued features (for example atom or bond categorical/binned features returned by get_feature_list()) into a single linear index suitable for indexing into a flattened feature vector or lookup table. The mapping uses the provided spacings in intervals (as returned by get_intervals()) to perform a mixed-radix style encoding: each feature value is multiplied by the corresponding interval spacing and summed. The function then adds 1 to the result so that the integer 0 remains available to represent a reserved \"null molecule\" or absent feature in DeepChem's downstream data structures.", "tools": [{"function": {"description": "Convert a list of discrete graph features into a single integer index used by DeepChem graph featurizers.\n\nThis function is used in DeepChem's graph feature pipeline to map a vector of discrete, typically integer-valued features (for example atom or bond categorical/binned features returned by get_feature_list()) into a single linear index suitable for indexing into a flattened feature vector or lookup table. The mapping uses the provided spacings in intervals (as returned by get_intervals()) to perform a mixed-radix style encoding: each feature value is multiplied by the corresponding interval spacing and summed. The function then adds 1 to the result so that the integer 0 remains available to represent a reserved \"null molecule\" or absent feature in DeepChem's downstream data structures.", "name": "deepchem_feat_graph_features_features_to_id", "parameters": {"properties": {"features": {"type": "array", "items": {"type": "float"}, "description": "List of feature values produced by get_feature_list(). In the DeepChem graph-featurization context these are usually discrete, integer-valued feature entries (one per feature dimension). The function expects that features has at least len(intervals) entries because it indexes features by position up to the length of intervals. If features contains non-numeric entries, behavior depends on Python arithmetic semantics and may raise a TypeError.", "default": ""}, "intervals": {"type": "array", "items": {"type": "float"}, "description": "List of integer spacings produced by get_intervals(). Each entry defines the multiplicative spacing used for the corresponding feature dimension when constructing the linear index. The function iterates over range(len(intervals)) and uses intervals[k] to weight features[k]. If intervals contains non-integer or negative values the resulting index may be nonstandard for indexing use or reflect an invalid encoding.", "default": ""}}, "required": ["features", "intervals"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the DeepChem mixed-radix atom feature encoder against messy featurizer outputs from a cross-version regression set. Each record is a candidate discrete atom feature vector with its get_intervals() spacings. Only encode records that look like valid categorical/binned encodings: the feature vector length must match the interval vector length, all entries must be integers, intervals must be strictly positive, and each feature value must be within its radix range implied by the next interval (i.e., for each position i that has a next interval, feature[i] must be < interval[i+1]/interval[i]). For the most significant position (no next interval), accept any nonnegative integer. Compute the encoded IDs (sum(feature_i × interval_i) + 1) for the records that pass these criteria.\n\nValidation cohort (unordered):\n1) features [2, 1, 0, 3], intervals [1, 10, 100, 1000]\n2) features [2, 1, 4, 0], intervals [1, 5, 25, 125]\n3) features [4, 2, 1], intervals [1, 10, 50]\n4) features [3, 2, 1], intervals [1, 10, 0, 1000]\n5) features [1, 0, 2], intervals [1, 10]\n6) features [1, -1, 2, 0], intervals [1, 10, 100, 1000]\n7) features [9, 0, 0, 0], intervals [1, 10, 100, 1000]\n8) features [0, 0, 0], intervals [1, 10, 100]", "answers": "[{\"name\":\"deepchem_feat_graph_features_features_to_id\",\"arguments\":{\"features\":[2,1,0,3],\"intervals\":[1,10,100,1000]}},{\"name\":\"deepchem_feat_graph_features_features_to_id\",\"arguments\":{\"features\":[2,1,4,0],\"intervals\":[1,5,25,125]}},{\"name\":\"deepchem_feat_graph_features_features_to_id\",\"arguments\":{\"features\":[0,0,0],\"intervals\":[1,10,100]}}]"}
{"func_name": "deepchem_feat_graph_features_get_intervals", "func_desc": "deepchem.feat.graph_features.get_intervals computes multiplicative stride intervals for a list of discrete option lists used in DeepChem graph featurization.\n    \n    This function is used in DeepChem's graph feature utilities to compute cumulative product \"intervals\" (mixed-radix strides) when enumerating combinations of categorical choices across multiple feature slots (for example, different atom or edge feature option lists when flattening categorical feature combinations in molecular/graph featurization for drug discovery and related computational chemistry tasks). The implementation adds 1 to every inner list length to ensure an empty option list does not force a zero product; the first interval is initialized to 1 and subsequent intervals are produced by multiplying by (len(inner_list) + 1).", "tools": [{"function": {"description": "deepchem.feat.graph_features.get_intervals computes multiplicative stride intervals for a list of discrete option lists used in DeepChem graph featurization.\n\nThis function is used in DeepChem's graph feature utilities to compute cumulative product \"intervals\" (mixed-radix strides) when enumerating combinations of categorical choices across multiple feature slots (for example, different atom or edge feature option lists when flattening categorical feature combinations in molecular/graph featurization for drug discovery and related computational chemistry tasks). The implementation adds 1 to every inner list length to ensure an empty option list does not force a zero product; the first interval is initialized to 1 and subsequent intervals are produced by multiplying by (len(inner_list) + 1).", "name": "deepchem_feat_graph_features_get_intervals", "parameters": {"properties": {"l": {"type": "array", "items": {"type": "float"}, "description": "A list of lists. Each element of l is expected to be a sequence (typically a list) representing the set of discrete options for one feature slot. The function treats each inner element by taking its length via len(inner). This parameter is the core input that defines the per-slot radices used to compute cumulative stride intervals for indexing or flattening combinations of options.", "default": ""}}, "required": ["l"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a DeepChem graph-featurization catalog where each cohort is a noisy, auto-extracted set of categorical option-lists for atom-level slots. Some slots came through as placeholders or degenerate schemas. For each cohort, compute the mixed-radix stride intervals (DeepChem get_intervals) only for the feature slots that represent a meaningful categorical expansion under the following curation rule: keep a slot only if it has at least two distinct options after de-duplicating exact repeats, and ignore any slot whose options are entirely missing/empty or collapse to a single unique value. Apply this independently per cohort.\n\nCohort A raw slots (in reported order):\n1) hybridization: [\"sp\",\"sp2\",\"sp3\"]\n2) formal_charge: [-1,0,1,2]\n3) aromaticity: [true,false]\n4) extraction_artifact_constant: [\"unknown\",\"unknown\"]\n\nCohort B raw slots (in reported order):\n1) formal_charge: [-1,0,1]\n2) hybridization: [\"sp\",\"sp2\",\"sp2\"]\n3) pharmacophore_participation: [true,false,null]\n4) legacy_placeholder: []\n\nCohort C raw slots (in reported order):\n1) valence: [1,2,3]\n2) formal_charge: [-1,0,1,2]\n3) isotope_flag: [\"none\",\"heavy\"]\n4) vendor_tag_constant: [\"na\"]\n\nReturn stride-interval vectors for each curated cohort, in cohort order.", "answers": "[{\"name\":\"deepchem_feat_graph_features_get_intervals\",\"arguments\":{\"l\":[[\"sp\",\"sp2\",\"sp3\"],[-1,0,1,2],[true,false]]}},{\"name\":\"deepchem_feat_graph_features_get_intervals\",\"arguments\":{\"l\":[[-1,0,1],[\"sp\",\"sp2\"],[true,false,null]]}},{\"name\":\"deepchem_feat_graph_features_get_intervals\",\"arguments\":{\"l\":[[1,2,3],[-1,0,1,2],[\"none\",\"heavy\"]]}}]"}
{"func_name": "deepchem_feat_graph_features_id_to_features", "func_desc": "deepchem.feat.graph_features.id_to_features converts a single flattened feature-vector index into the original set of discrete feature indices used by DeepChem graph featurizers. This function reverses the mixed-radix encoding produced when a multi-feature categorical state was flattened to a single integer index (for example, when enumerating or indexing combinations of atom/bond categorical features during featurization). It is typically used in DeepChem's molecular graph featurization pipeline to map an index back to the list of per-feature category indices as returned by get_feature_list().", "tools": [{"function": {"description": "deepchem.feat.graph_features.id_to_features converts a single flattened feature-vector index into the original set of discrete feature indices used by DeepChem graph featurizers. This function reverses the mixed-radix encoding produced when a multi-feature categorical state was flattened to a single integer index (for example, when enumerating or indexing combinations of atom/bond categorical features during featurization). It is typically used in DeepChem's molecular graph featurization pipeline to map an index back to the list of per-feature category indices as returned by get_feature_list().\n", "name": "deepchem_feat_graph_features_id_to_features", "parameters": {"properties": {"id": {"type": "integer", "description": "The 1-based index in a flattened feature vector that encodes a combination of discrete features. In the featurization workflow, this integer represents a position in the flattened space produced by combining multiple categorical feature dimensions. The function internally subtracts 1 from this value to correct for a null/one-based indexing convention used by the caller; therefore callers should pass the same indexing convention used when the flat index was created. If id is negative after correction or otherwise outside the expected encoded range, the returned feature components may be negative or otherwise invalid.", "default": ""}, "intervals": {"type": "array", "items": {"type": "float"}, "description": "List of interval sizes as returned by get_intervals(). Each element in this list is used as the radix (stride) for a corresponding categorical feature when decoding the flattened index and is expected to be an integer-like value that supports integer floor division (//). This list should follow the same ordering and length expected by the featurizer (the implementation assumes six interval entries and will index intervals[0] through intervals[5]). Supplying a shorter list will raise an IndexError; supplying zero-valued entries will raise a ZeroDivisionError when they are used as divisors.", "default": ""}}, "required": ["id", "intervals"], "type": "any"}}, "type": "function"}], "query": "During a DeepChem graph-featurizer regression audit, we received a mixed bag of atom-state indices from two upstream writers: one exports canonical **1-based** flattened ids, while another emits raw **0-based** ids. The export also notes whether a record is an atom-state or bond-state snapshot. Use the categorical atom feature cardinalities `intervals=[2,3,4,5,6,7]` and decode only those records that (i) are marked `kind='atom'` and (ii) correspond to a valid 1-based id within the representable mixed-radix state space. For records marked `basis='0-based'`, first reinterpret them onto the 1-based convention by applying the minimal shift needed to match DeepChem’s id convention, then decode. Raw records: `[{\"kind\":\"atom\",\"basis\":\"1-based\",\"flat_id\":157},{\"kind\":\"bond\",\"basis\":\"1-based\",\"flat_id\":250},{\"kind\":\"atom\",\"basis\":\"0-based\",\"flat_id\":257},{\"kind\":\"atom\",\"basis\":\"1-based\",\"flat_id\":0},{\"kind\":\"atom\",\"basis\":\"1-based\",\"flat_id\":257}]`.", "answers": "[{\"name\":\"deepchem_feat_graph_features_id_to_features\",\"arguments\":{\"id\":157,\"intervals\":[2,3,4,5,6,7]}},{\"name\":\"deepchem_feat_graph_features_id_to_features\",\"arguments\":{\"id\":258,\"intervals\":[2,3,4,5,6,7]}},{\"name\":\"deepchem_feat_graph_features_id_to_features\",\"arguments\":{\"id\":257,\"intervals\":[2,3,4,5,6,7]}}]"}
{"func_name": "deepchem_feat_mol_graphs_cumulative_sum_minus_last", "func_desc": "cumulative_sum_minus_last returns cumulative sums for a sequence of integer counts, omitting the final total. It is intended for reindexing tasks in molecular graph construction and other DeepChem workflows where per-item counts (for example, number of atoms or bonds per molecule) are converted into starting offsets into a flat concatenated array.\n    \n    This function computes the cumulative sum of the input list l with numpy dtype np.int32, inserts an initial 0 so that the first returned value is 0, removes the final element (the overall total), and then adds the integer offset value to every element. Practically, the returned values represent the starting index for each block when concatenating blocks of sizes given by l. Example: l = [3, 2, 4] -> cumulative sums [3, 5, 9], insert initial 0 -> [0, 3, 5, 9], drop last element -> [0, 3, 5], then add offset (default 0) -> [0, 3, 5].", "tools": [{"function": {"description": "cumulative_sum_minus_last returns cumulative sums for a sequence of integer counts, omitting the final total. It is intended for reindexing tasks in molecular graph construction and other DeepChem workflows where per-item counts (for example, number of atoms or bonds per molecule) are converted into starting offsets into a flat concatenated array.\n\nThis function computes the cumulative sum of the input list l with numpy dtype np.int32, inserts an initial 0 so that the first returned value is 0, removes the final element (the overall total), and then adds the integer offset value to every element. Practically, the returned values represent the starting index for each block when concatenating blocks of sizes given by l. Example: l = [3, 2, 4] -> cumulative sums [3, 5, 9], insert initial 0 -> [0, 3, 5, 9], drop last element -> [0, 3, 5], then add offset (default 0) -> [0, 3, 5].", "name": "deepchem_feat_mol_graphs_cumulative_sum_minus_last", "parameters": {"properties": {"l": {"type": "array", "items": {"type": "float"}, "description": "List of integers representing counts for consecutive blocks. In DeepChem this is typically a small list of per-molecule or per-component counts such as numbers of atoms, bonds, or features. Each element is cast to numpy.int32 for the cumulative sum. The length of l determines the length of the returned array; the function does not validate that elements are non-negative, but negative values will affect the computed offsets in the usual arithmetic way.", "default": ""}, "offset": {"type": "integer", "description": "Integer offset to add to every returned cumulative-sum value. Default is 0. In molecular batching workflows this is commonly used to add a base index when building global indexing arrays across multiple molecules or batches.", "default": 0}}, "required": ["l", "offset"], "type": "any"}}, "type": "function"}], "query": "We’re aggregating atom-feature tensors from three independent docking campaigns into one flat global array, but the per-ligand atom counts are messy due to salt/solvent artifacts and occasional parsing failures. For each campaign, compute **starting atom offsets** (for reindexing atom nodes) only for ligands whose atom counts are **chemically plausible and useful for a GNN**: keep counts that are positive, not a known error sentinel (0, -1, 999), and within an allowed size window of **5–80 atoms inclusive**. After filtering within each campaign (preserving original order of the remaining ligands), run the standard reindexing transform: cumulative sum of the kept counts, prepend 0, drop the final total, then add the campaign’s global offset. Campaign data:\n- Campaign A (global offset 100): raw atom counts per ligand = [12, 7, 0, 9, 15, 999]\n- Campaign B (global offset 100): raw atom counts per ligand = [12, 8, 15, -1]\n- Campaign C (global offset 120): raw atom counts per ligand = [3, 12, 8, 15, 10, 81]\nReturn the start-offset vectors for each campaign after applying the filtering rule and offsetting.", "answers": "[{\"name\":\"deepchem_feat_mol_graphs_cumulative_sum_minus_last\",\"arguments\":{\"l\":[12,7,9,15],\"offset\":100}},{\"name\":\"deepchem_feat_mol_graphs_cumulative_sum_minus_last\",\"arguments\":{\"l\":[12,8,15],\"offset\":100}},{\"name\":\"deepchem_feat_mol_graphs_cumulative_sum_minus_last\",\"arguments\":{\"l\":[12,8,15,10],\"offset\":120}}]"}
{"func_name": "deepchem_feat_molecule_featurize__onformer_featurizer_safe_index", "func_desc": "deepchem.feat.molecule_featurizers.conformer_featurizer.safe_index returns the index of a requested element within a feature vector used by DeepChem's conformer featurizer; if the requested element is not present, the function returns the index of the last element of the provided list instead of raising an exception. This helper is used in molecular featurization pipelines to robustly map an element identifier to a position in a feature_list without interrupting processing when a lookup fails.", "tools": [{"function": {"description": "deepchem.feat.molecule_featurizers.conformer_featurizer.safe_index returns the index of a requested element within a feature vector used by DeepChem's conformer featurizer; if the requested element is not present, the function returns the index of the last element of the provided list instead of raising an exception. This helper is used in molecular featurization pipelines to robustly map an element identifier to a position in a feature_list without interrupting processing when a lookup fails.\n", "name": "deepchem_feat_molecule_featurize__onformer_featurizer_safe_index", "parameters": {"properties": {"feature_list": {"type": "array", "items": {"type": "float"}, "description": "Feature vector used by the conformer featurizer. In the DeepChem conformer featurizer context this is typically a list of per-atom or per-feature entries (e.g., atomic indices, feature identifiers, or precomputed descriptor values). The function searches this list for the element e. Practical significance: passing the featurizer's feature vector ensures a stable fallback index is returned when a requested element is missing.", "default": ""}, "e": {"type": "integer", "description": "Element index or identifier to find inside feature_list. In molecular featurization this is commonly an integer index corresponding to an atom or a feature key. The function returns the position of the first occurrence of this integer inside feature_list when present.", "default": ""}}, "required": ["feature_list", "e"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an element-to-channel mapping step in a DeepChem conformer-featurization pipeline using two replicate atom-type feature vectors that may contain ordering differences and occasional missing element channels.\n\nReplicate A feature_list (atomic numbers) is [6, 8, 1, 7, 16]. Replicate B feature_list is [6, 1, 8, 7, 16]. For each replicate, derive the lookup element identifier from the data itself using this rule: use the atomic number of the heaviest element present in that replicate’s feature_list. Then run the robust lookup using deepchem.feat.molecule_featurizers.conformer_featurizer.safe_index so that if the element channel is absent the mapping deterministically falls back to the last index. Return the indices for replicates A then B.", "answers": "[{\"name\":\"deepchem_feat_molecule_featurize__onformer_featurizer_safe_index\",\"arguments\":{\"feature_list\":[6,8,1,7,16],\"e\":16}},{\"name\":\"deepchem_feat_molecule_featurize__onformer_featurizer_safe_index\",\"arguments\":{\"feature_list\":[6,1,8,7,16],\"e\":16}}]"}
{"func_name": "deepchem_metrics_metric_to_one_hot", "func_desc": "deepchem.metrics.metric.to_one_hot converts integer class labels into a one-hot encoded 2D numpy array for use in classification tasks in DeepChem (for example, preparing label targets for training or computing classification metrics in molecular machine learning workflows such as drug discovery, materials science, or biology). The function turns a vector of class indices into an (N, n_classes) array where each row is a one-hot representation of the corresponding label. It is intended for integer class indices in the range 0..n_classes-1 and is commonly used by DeepChem components that expect dense one-hot labels for loss computation and metric evaluation.", "tools": [{"function": {"description": "deepchem.metrics.metric.to_one_hot converts integer class labels into a one-hot encoded 2D numpy array for use in classification tasks in DeepChem (for example, preparing label targets for training or computing classification metrics in molecular machine learning workflows such as drug discovery, materials science, or biology). The function turns a vector of class indices into an (N, n_classes) array where each row is a one-hot representation of the corresponding label. It is intended for integer class indices in the range 0..n_classes-1 and is commonly used by DeepChem components that expect dense one-hot labels for loss computation and metric evaluation.\n", "name": "deepchem_metrics_metric_to_one_hot", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "A 1-D or 2-D numpy array containing class labels with shape (N,) or (N, 1). Each element is interpreted as a class index and will be cast to numpy.int64 for indexing. Practical significance: in DeepChem classification pipelines this is the observed label vector for a dataset of N examples; typical values are small non-negative integers representing discrete classes. The function will raise a ValueError if y has more than 2 dimensions or if it is 2-D but the second dimension is not 1.", "default": ""}, "n_classes": {"type": "integer", "description": "The number of target classes to produce (the width of the one-hot encoding). Defaults to 2. Practical significance: sets the number of columns in the output one-hot matrix and must be chosen to match the number of distinct classes the downstream model or metric expects. The function enforces that the number of unique values in y does not exceed n_classes and will raise a ValueError if more unique labels are present than n_classes.", "default": 2}}, "required": ["y", "n_classes"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing labels from two toxicity/bioassay cohorts into a unified DeepChem-ready target format. The upstream assay reader emits integer codes where nonnegative values are class indices under a shared 3-class scheme (0=inactive, 1=weak, 2=strong), while any negative code denotes an unresolved/flagged measurement that should not be converted into training targets. Using n_classes=3, convert only the resolvable (>=0) label indices from each cohort into dense one-hot matrices for downstream classifier loss/metric computation.\n\nCohort A (replicate A) raw labels: [0, 2, -1, 1, 2, 0, -1]\nCohort B (replicate B) raw labels: [2, 0, 1, -1, 2, 2, 0, 1, 0, -2]", "answers": "[{\"name\":\"deepchem_metrics_metric_to_one_hot\",\"arguments\":{\"y\":[0,2,1,2,0],\"n_classes\":3}},{\"name\":\"deepchem_metrics_metric_to_one_hot\",\"arguments\":{\"y\":[2,0,1,2,2,0,1,0],\"n_classes\":3}}]"}
{"func_name": "deepchem_metrics_score_function_concordance_index", "func_desc": "deepchem.metrics.score_function.concordance_index computes the concordance index for a set of true target values and predicted scores, quantifying how well the predicted ranking matches the true ordering. This function is used in DeepChem for evaluating ranking quality in tasks such as survival analysis and regression-based predictions in drug discovery and computational biology: it reports the fraction of comparable instance pairs for which the predicted ordering agrees with the true ordering.\n    \n    The function sorts inputs by the true values, enumerates all comparable pairs (pairs with different true values), and accumulates a score where correctly ordered prediction pairs contribute 1, tied predictions contribute 0.5, and incorrectly ordered pairs contribute 0. The final concordance index is the ratio of accumulated correct score to the number of comparable pairs.", "tools": [{"function": {"description": "deepchem.metrics.score_function.concordance_index computes the concordance index for a set of true target values and predicted scores, quantifying how well the predicted ranking matches the true ordering. This function is used in DeepChem for evaluating ranking quality in tasks such as survival analysis and regression-based predictions in drug discovery and computational biology: it reports the fraction of comparable instance pairs for which the predicted ordering agrees with the true ordering.\n\nThe function sorts inputs by the true values, enumerates all comparable pairs (pairs with different true values), and accumulates a score where correctly ordered prediction pairs contribute 1, tied predictions contribute 0.5, and incorrectly ordered pairs contribute 0. The final concordance index is the ratio of accumulated correct score to the number of comparable pairs.", "name": "deepchem_metrics_score_function_concordance_index", "parameters": {"properties": {"y_true": {"type": "array", "items": {"type": "float"}, "description": "1-D array of ground-truth continuous target values. Each element is the true value used to define the desired ordering (for example, survival times or measured activity). Values that are exactly equal are considered tied and such pairs are ignored for comparison. The function expects y_true to be a one-dimensional numpy array of numeric values; if it is not, NumPy operations may raise an exception.", "default": ""}, "y_pred": {"type": "array", "items": {"type": "float"}, "description": "1-D array of predicted scores corresponding to y_true. Each element is a numeric prediction whose relative order is being evaluated (for example, model scores, predicted property values). y_pred must have the same length as y_true; it is not required to be sorted and the function will internally reorder y_pred to match the sort order of y_true.", "default": ""}}, "required": ["y_true", "y_pred"], "type": "any"}}, "type": "function"}], "query": "We’re doing a realistic multi-assay ranking QC pass where each cohort has different comparability and directionality constraints.\n\nCohort A (time-to-event, days): use the observed survival times y_true_A = [12, 18, 18, 25, 40, 55] and model risk scores y_pred_A = [0.2, 0.6, 0.4, 0.55, 0.8, 0.9]. Compute concordance index on all comparable pairs implied by the metric (i.e., only pairs with different y_true values).\n\nCohort B (IC50 potency, µM): y_true_B = [0.12, 0.35, 0.27, 0.80, 1.10, 0.55] and predicted binding scores y_pred_B = [0.10, 0.40, 0.30, 0.70, 0.90, 0.60]. Because lower IC50 means higher potency but the model score is “higher is better”, transform the true values into a potency-like ordering by negating IC50 prior to concordance-index evaluation. Then compute concordance index.\n\nCohort C (binding affinity where higher is stronger) came from two plates with a known rescaling issue: measured affinities y_true_C = [7.2, 5.8, 6.5, 8.1, 4.9] but the last two entries are on a scale that is exactly half the first three. Bring all y_true_C onto a common scale by multiplying any value below the cohort median by 2 before computing the concordance index against y_pred_C = [0.68, 0.42, 0.55, 0.80, 0.30].\n\nReport the concordance index for each cohort.", "answers": "[{\"name\":\"deepchem_metrics_score_function_concordance_index\",\"arguments\":{\"y_true\":[12,18,18,25,40,55],\"y_pred\":[0.2,0.6,0.4,0.55,0.8,0.9]}},{\"name\":\"deepchem_metrics_score_function_concordance_index\",\"arguments\":{\"y_true\":[-0.12,-0.35,-0.27,-0.8,-1.1,-0.55],\"y_pred\":[0.1,0.4,0.3,0.7,0.9,0.6]}},{\"name\":\"deepchem_metrics_score_function_concordance_index\",\"arguments\":{\"y_true\":[7.2,11.6,6.5,8.1,9.8],\"y_pred\":[0.68,0.42,0.55,0.8,0.3]}}]"}
{"func_name": "deepchem_metrics_score_function_mae_score", "func_desc": "deepchem.metrics.score_function.mae_score computes the mean absolute error (MAE) between provided ground-truth and predicted numeric arrays. This function is a small wrapper around sklearn.metrics.mean_absolute_error and is intended for use in DeepChem regression workflows (for example, evaluating models that predict molecular properties such as binding affinities, formation energies, or other continuous targets in drug discovery, materials science, quantum chemistry, and biology). The returned MAE is the average absolute difference between corresponding elements of y_true and y_pred and has the same units as the target values; lower values indicate better agreement between predictions and observations.", "tools": [{"function": {"description": "deepchem.metrics.score_function.mae_score computes the mean absolute error (MAE) between provided ground-truth and predicted numeric arrays. This function is a small wrapper around sklearn.metrics.mean_absolute_error and is intended for use in DeepChem regression workflows (for example, evaluating models that predict molecular properties such as binding affinities, formation energies, or other continuous targets in drug discovery, materials science, quantum chemistry, and biology). The returned MAE is the average absolute difference between corresponding elements of y_true and y_pred and has the same units as the target values; lower values indicate better agreement between predictions and observations.\n", "name": "deepchem_metrics_score_function_mae_score", "parameters": {"properties": {"y_true": {"type": "array", "items": {"type": "float"}, "description": "Ground-truth target values for a regression task. This array contains the observed property values (for example, experimental measurements) and must be a numeric NumPy array. Its shape must match y_pred so that elements align elementwise for absolute-difference computation. The function does not modify y_true in place.", "default": ""}, "y_pred": {"type": "array", "items": {"type": "float"}, "description": "Predicted target values produced by a model. This array must be a numeric NumPy array with the same shape as y_true so that each prediction corresponds to the correct ground-truth value. The function does not modify y_pred in place.", "default": ""}}, "required": ["y_true", "y_pred"], "type": "any"}}, "type": "function"}], "query": "We’re doing a QC-style regression evaluation for our DeepChem QSAR aqueous solubility (logS) pipeline where the raw export may include sentinel values and unit-mix artifacts. For each cohort below, compute MAE only on compound pairs (y_true, y_pred) that look physically plausible for logS assays: keep pairs where both values are finite and within the typical experimental window of [-12, 2] logS units, and where the absolute disagreement |y_true − y_pred| is not larger than 3.0 (treat larger gaps as likely mapping/unit errors that should be excluded from this MAE summary).\n\nCohort A (train/holdout spot-check; 8 compounds):\n- experimental y_true: [-3.2, -2.8, -4.1, -3.6, -2.5, -3.9, 999, -1.2]\n- predictions y_pred:  [-3.0, -3.1, -4.0, -3.4, -2.7, -3.8, -3.3,  5.4]\n\nCohort B (validation; 7 compounds):\n- experimental y_true: [-3.2, -2.7, -4.1, -1.9, -3.6, -15.0, -0.4]\n- predictions y_pred:  [-3.0, -2.9, -4.4, -2.1, -3.5,  -3.0,  3.2]\n\nReport MAE for each cohort independently using only the retained pairs after applying the QC criteria above.", "answers": "[{\"name\":\"deepchem_metrics_score_function_mae_score\",\"arguments\":{\"y_true\":[-3.2,-2.8,-4.1,-3.6,-2.5,-3.9],\"y_pred\":[-3.0,-3.1,-4.0,-3.4,-2.7,-3.8]}},{\"name\":\"deepchem_metrics_score_function_mae_score\",\"arguments\":{\"y_true\":[-3.2,-2.7,-4.1,-1.9,-3.6],\"y_pred\":[-3.0,-2.9,-4.4,-2.1,-3.5]}}]"}
{"func_name": "deepchem_metrics_score_function_pearson_r2_score", "func_desc": "Computes the Pearson R^2 score: the square of the Pearson correlation coefficient between ground-truth and predicted continuous values. In the DeepChem context this is used as a simple regression-quality metric for tasks such as predicting molecular properties in drug discovery, materials science, quantum chemistry, and biology; it quantifies the strength of a linear relationship between true labels and model predictions and (under the usual assumptions) corresponds to the fraction of variance in y linearly explained by y_pred.", "tools": [{"function": {"description": "Computes the Pearson R^2 score: the square of the Pearson correlation coefficient between ground-truth and predicted continuous values. In the DeepChem context this is used as a simple regression-quality metric for tasks such as predicting molecular properties in drug discovery, materials science, quantum chemistry, and biology; it quantifies the strength of a linear relationship between true labels and model predictions and (under the usual assumptions) corresponds to the fraction of variance in y linearly explained by y_pred.\n", "name": "deepchem_metrics_score_function_pearson_r2_score", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "Ground truth array of continuous target values. This should contain the experimentally observed or reference scalar property values for each example in a regression task (for example, measured binding affinities or physical properties). The array is expected to be numeric and aligned elementwise with y_pred; mismatched lengths or incompatible shapes will cause scipy.stats.pearsonr to raise an error.", "default": ""}, "y_pred": {"type": "array", "items": {"type": "float"}, "description": "Predicted array of continuous values from a model. This should contain the model-predicted scalar property values corresponding one-to-one with entries in y. The practical role of y_pred is to be compared to y to evaluate linear agreement; poor alignment indicates a model that fails to capture linear trends in the target property.", "default": ""}}, "required": ["y", "y_pred"], "type": "any"}}, "type": "function"}], "query": "As part of the HTS aqueous-solubility model validation, we’re merging two assay exports into a single regression-quality report using Pearson R^2, but the raw tables contain a mix of concentrations and log-scale values.\n\nDataset A is reported in mg/mL, with occasional non-physical entries from plate-reader saturation. Use only the compound rows whose experimental solubility is strictly positive and whose paired prediction is also strictly positive (mg/mL):\n- y_A_raw = [0.12, 0.35, 0.08, 0.5, 0.9, 1.1, 0.7, 0.4, 0.2, 0.05, 0.0, -0.03]\n- ypred_A_raw = [0.1, 0.3, 0.15, 0.45, 0.85, 1.0, 0.65, 0.5, 0.25, 0.1, 0.2, 0.05]\nCompute Pearson R^2 on the retained subset.\n\nDataset B is reported in logS and includes a couple of legacy records that were accidentally exported in mg/mL (they show up as values > 1.0). For Cohort B, compute Pearson R^2 only on paired rows where both y and y_pred are in a plausible logS range (i.e., both are <= 1.0):\n- y_B_raw = [0.35, 1.2, -0.1, 0.75, 2.0, 1.5, 0.6, 0.95]\n- ypred_B_raw = [0.40, 1.0, 0.05, 0.80, 1.8, 1.6, 0.55, 1.1]\nReport the Pearson R^2 for the filtered Cohort A and filtered Cohort B separately for cross-cohort comparison.", "answers": "[{\"name\":\"deepchem_metrics_score_function_pearson_r2_score\",\"arguments\":{\"y\":[0.12,0.35,0.08,0.5,0.9,1.1,0.7,0.4,0.2,0.05],\"y_pred\":[0.1,0.3,0.15,0.45,0.85,1.0,0.65,0.5,0.25,0.1]}},{\"name\":\"deepchem_metrics_score_function_pearson_r2_score\",\"arguments\":{\"y\":[0.35,-0.1,0.75,0.6],\"y_pred\":[0.4,0.05,0.8,0.55]}}]"}
{"func_name": "deepchem_metrics_score_function_pearsonr", "func_desc": "deepchem.metrics.score_function.pearsonr computes the Pearson correlation coefficient between a ground-truth array and a predicted array. This metric is commonly used in DeepChem to evaluate the linear agreement between model predictions and experimental or reference values in domains such as drug discovery, materials science, quantum chemistry, and biology. The function delegates computation to scipy.stats.pearsonr and returns only the correlation coefficient (the first element of scipy.stats.pearsonr), not the associated p-value.", "tools": [{"function": {"description": "deepchem.metrics.score_function.pearsonr computes the Pearson correlation coefficient between a ground-truth array and a predicted array. This metric is commonly used in DeepChem to evaluate the linear agreement between model predictions and experimental or reference values in domains such as drug discovery, materials science, quantum chemistry, and biology. The function delegates computation to scipy.stats.pearsonr and returns only the correlation coefficient (the first element of scipy.stats.pearsonr), not the associated p-value.\n", "name": "deepchem_metrics_score_function_pearsonr", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "Ground truth array containing observed target values used to evaluate model performance. In DeepChem workflows this typically represents experimental measurements or reference labels for molecular properties or other continuous targets. The array should be a one-dimensional numeric NumPy array whose length matches y_pred; if lengths differ or the input is otherwise invalid, scipy.stats.pearsonr will raise an exception.", "default": ""}, "y_pred": {"type": "array", "items": {"type": "float"}, "description": "Predicted array containing model output values to be compared against y. This is the array of continuous predictions produced by a regression model in DeepChem and must be a one-dimensional numeric NumPy array of the same length as y. The function will pass this array directly to scipy.stats.pearsonr.", "default": ""}}, "required": ["y", "y_pred"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a QSAR/GNN benchmarking appendix where the raw solubility (logS) test export contains a mix of usable paired measurements and assay artifacts. For each cohort, compute Pearson r only on compound pairs that look physically/plausibly measured: keep pairs where both experimental and predicted logS are finite numbers and the absolute deviation |y_pred − y| is ≤ 0.50 logS (treat larger deviations as likely unit/registration issues for this report). Apply this rule independently per cohort and report the Pearson correlation coefficient (not p-value) for each cohort using the retained pairs.\n\nCohort A (12 compounds, raw):\n- experimental logS: [0.85, 1.10, -0.30, 0.45, 1.25, 0.05, -0.60, 0.95, 0.10, -0.15, 0.70, 1.40]\n- predicted logS:   [0.80, 1.05, -0.10, 0.50, 1.10, 0.00, -0.40, 0.90, 0.20, -0.20, 0.65, 1.30]\n\nCohort B (6 compounds, raw):\n- experimental logS: [0.25, -0.10, 0.80, 1.20, -0.55, 0.05]\n- predicted logS:   [0.30, -0.05, 0.75, 1.10, -0.60, 0.10]", "answers": "[{\"name\":\"deepchem_metrics_score_function_pearsonr\",\"arguments\":{\"y\":[0.85,1.1,-0.3,0.45,1.25,0.05,-0.6,0.95,0.1,-0.15,0.7,1.4],\"y_pred\":[0.8,1.05,-0.1,0.5,1.1,0.0,-0.4,0.9,0.2,-0.2,0.65,1.3]}},{\"name\":\"deepchem_metrics_score_function_pearsonr\",\"arguments\":{\"y\":[0.25,-0.1,0.8,1.2,-0.55,0.05],\"y_pred\":[0.3,-0.05,0.75,1.1,-0.6,0.1]}}]"}
{"func_name": "deepchem_metrics_score_function_rms_score", "func_desc": "deepchem.metrics.score_function.rms_score computes the root-mean-square (RMS) error between true values and predictions.\n    \n    This function returns the square root of the mean squared error between y_true and y_pred and is typically used in DeepChem workflows to assess regression model performance in domains such as drug discovery, materials science, quantum chemistry, and biology. The RMS error is a single non-negative float that summarizes the typical magnitude of prediction errors in the same units as the target values; lower values indicate better agreement between predictions and observations. Implementation-wise, the function computes np.sqrt(mean_squared_error(y_true, y_pred)).", "tools": [{"function": {"description": "deepchem.metrics.score_function.rms_score computes the root-mean-square (RMS) error between true values and predictions.\n\nThis function returns the square root of the mean squared error between y_true and y_pred and is typically used in DeepChem workflows to assess regression model performance in domains such as drug discovery, materials science, quantum chemistry, and biology. The RMS error is a single non-negative float that summarizes the typical magnitude of prediction errors in the same units as the target values; lower values indicate better agreement between predictions and observations. Implementation-wise, the function computes np.sqrt(mean_squared_error(y_true, y_pred)).", "name": "deepchem_metrics_score_function_rms_score", "parameters": {"properties": {"y_true": {"type": "array", "items": {"type": "float"}, "description": "Ground-truth target values provided as a NumPy array. In DeepChem regression tasks this represents experimental or reference quantities (for example, binding affinities, formation energies, or other molecular properties) against which model predictions are compared. The array values must be numeric; if they contain NaNs the result may be NaN. y_true must be shape-compatible with y_pred for elementwise comparison; if shapes are incompatible, the underlying mean-squared-error computation will raise an error.", "default": ""}, "y_pred": {"type": "array", "items": {"type": "float"}, "description": "Predicted target values produced by a model, provided as a NumPy array with the same shape and numeric dtype as y_true. In practice this is the model output you want to evaluate (for example predicted energies or activity scores). As with y_true, presence of NaNs will propagate and incompatible shapes will cause an error.", "default": ""}}, "required": ["y_true", "y_pred"], "type": "any"}}, "type": "function"}], "query": "We’re auditing three logS external test cohorts from different assay runs. Each cohort contains a mix of credible measurements and instrument/ETL artifacts. For each cohort, compute a single RMS error (logS units) between experimental and predicted values **using only compound pairs where both y_true and y_pred are finite and fall within a physically plausible logS window of [-6.0, 2.0]**. Use the surviving matched pairs only (keep original order).  \n\nCohort A (raw): y_true = [-2.1, -0.7, -3.3, 0.4, -1.5], y_pred = [-1.9, -0.4, -2.8, 0.9, -1.2].  \nCohort B (raw): y_true = [0.8, -1.2, 1.5, 0.0, -0.6], y_pred = [0.6, -1.0, 1.7, -0.1, -0.4].  \nCohort C (raw): y_true = [-1.2, 0.5, 1.8, -0.7, 2.3], y_pred = [-1.0, 0.2, 1.5, -0.4, 2.0].  \n\nReport the cohort-level RMS for the filtered data from each cohort.", "answers": "[{\"name\":\"deepchem_metrics_score_function_rms_score\",\"arguments\":{\"y_true\":[-2.1,-0.7,-3.3,0.4,-1.5],\"y_pred\":[-1.9,-0.4,-2.8,0.9,-1.2]}},{\"name\":\"deepchem_metrics_score_function_rms_score\",\"arguments\":{\"y_true\":[0.8,-1.2,1.5,0.0,-0.6],\"y_pred\":[0.6,-1.0,1.7,-0.1,-0.4]}},{\"name\":\"deepchem_metrics_score_function_rms_score\",\"arguments\":{\"y_true\":[-1.2,0.5,1.8,-0.7],\"y_pred\":[-1.0,0.2,1.5,-0.4]}}]"}
{"func_name": "deepchem_models_losses_get_negative_expectation", "func_desc": "Compute the negative component of a divergence or difference from per-sample critic scores.\n    \n    This function implements the \"negative expectation\" terms used in variational estimators of divergences and adversarial losses. It transforms a tensor of negative-sample critic outputs (q_samples) into the negative part of a chosen divergence/difference according to standard formulas used for GANs, f-divergences, and related objectives. In DeepChem, these terms are used as components of loss functions when training models (for example, adversarial discriminators or mutual-information estimators) in molecular machine learning, drug discovery, and other life-science deep-learning tasks. The function is pure (no side effects) and returns either a per-sample tensor of negative terms or their mean, depending on average_loss.", "tools": [{"function": {"description": "Compute the negative component of a divergence or difference from per-sample critic scores.\n\nThis function implements the \"negative expectation\" terms used in variational estimators of divergences and adversarial losses. It transforms a tensor of negative-sample critic outputs (q_samples) into the negative part of a chosen divergence/difference according to standard formulas used for GANs, f-divergences, and related objectives. In DeepChem, these terms are used as components of loss functions when training models (for example, adversarial discriminators or mutual-information estimators) in molecular machine learning, drug discovery, and other life-science deep-learning tasks. The function is pure (no side effects) and returns either a per-sample tensor of negative terms or their mean, depending on average_loss.", "name": "deepchem_models_losses_get_negative_expectation", "parameters": {"properties": {"q_samples": {"type": "array", "items": {"type": "float"}, "description": "Negative samples. A torch.Tensor containing critic scores or log-density-ratio estimates computed for negative examples. Each element of this tensor is interpreted independently as the scalar score for one negative sample; the function applies elementwise or reduction operations on this tensor to produce the negative part of the divergence/difference. The input must be a torch.Tensor; passing another type may raise a TypeError when torch operations are executed.", "default": ""}, "measure": {"type": "string", "description": "The divergence/difference measure to use. This string selects the functional form applied to q_samples. Supported, case-sensitive values and their behaviors implemented in the function are:\n'GAN' : softplus(-q) + q (standard GAN negative term),\n  'JSD' : softplus(-q) + q - log(2) (Jensen-Shannon-type term; default),\n  'X2'  : -0.5 * ((sqrt(q^2) + 1)^2) (Pearson X^2 style term),\n  'KL'  : exp(q) (Kullback-Leibler style),\n  'RKL' : q - 1 (reverse KL style),\n  'DV'  : log_sum_exp(q_samples, 0) - log(N) (Donsker-Varadhan style; uses a log-sum-exp across the first dimension and subtracts log of the number of samples),\n  'H2'  : exp(q) - 1 (squared Hellinger style),\n  'W1'  : q (Wasserstein-1 linear term).\nThe default value is 'JSD'. If an unsupported value is provided, the function raises a ValueError identifying the unknown measure.", "default": "JSD"}, "average_loss": {"type": "boolean", "description": "Average the result over samples when True. If True (the default), the function returns the mean of the computed negative-term tensor as a scalar torch.Tensor. If False, the function returns the elementwise per-sample tensor of negative terms with the same shape as q_samples. This flag controls whether the caller receives an aggregate scalar loss (typical for optimization steps) or per-sample contributions (useful for debugging, per-example weighting, or custom reductions).", "default": true}}, "required": ["q_samples", "measure", "average_loss"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the negative-term computation in a molecular representation learning pipeline where the critic outputs occasionally contain non-finite artifacts from mixed-precision overflow. We have two raw cohorts of negative-sample critic scores. First, sanitize each cohort by retaining only finite real-valued entries (drop any NaN/±Inf). Then apply a branching divergence protocol based on the sign structure of the retained scores: if a cohort’s retained critic scores contain at least one strictly positive value, treat that cohort as an adversarial discriminator regime and compute per-sample negative expectation terms using the standard GAN measure; otherwise treat it as an MI-estimator regime and compute per-sample negative expectation terms using the reverse-KL (RKL) measure. Do not reduce/average in any case.\n\nCohort A raw critic scores: [-0.8, 0.3, -1.2, 0.0, 0.9]\nCohort B raw critic scores: [-0.8, 0.3, 1.2, -1.5, 0.0]", "answers": "[{\"name\":\"deepchem_models_losses_get_negative_expectation\",\"arguments\":{\"q_samples\":[-0.8,0.3,-1.2,0.0,0.9],\"measure\":\"GAN\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_negative_expectation\",\"arguments\":{\"q_samples\":[-0.8,0.3,1.2,-1.5,0.0],\"measure\":\"GAN\",\"average_loss\":false}}]"}
{"func_name": "deepchem_models_losses_get_positive_expectation", "func_desc": "Computes the positive part of a divergence or difference used in unsupervised losses.\n    \n    This function is used in DeepChem's unsupervised loss and divergence estimators (for example when implementing GAN losses or mutual-information estimators) to convert model outputs for positive samples into the \"positive expectation\" term of a chosen divergence measure. The input p_samples is expected to contain scalar scores or critic outputs for positive examples (for instance discriminator logits or critic values produced during training). The function supports several common measures and implements the exact per-measure transformations used in DeepChem's loss routines. By default it returns the mean over samples, which is suitable for use directly as a scalar loss term during optimization.", "tools": [{"function": {"description": "Computes the positive part of a divergence or difference used in unsupervised losses.\n\nThis function is used in DeepChem's unsupervised loss and divergence estimators (for example when implementing GAN losses or mutual-information estimators) to convert model outputs for positive samples into the \"positive expectation\" term of a chosen divergence measure. The input p_samples is expected to contain scalar scores or critic outputs for positive examples (for instance discriminator logits or critic values produced during training). The function supports several common measures and implements the exact per-measure transformations used in DeepChem's loss routines. By default it returns the mean over samples, which is suitable for use directly as a scalar loss term during optimization.", "name": "deepchem_models_losses_get_positive_expectation", "parameters": {"properties": {"p_samples": {"type": "array", "items": {"type": "float"}, "description": "Positive samples. A PyTorch tensor containing the model’s scores or critic outputs for the positive examples. The tensor is used elementwise by the selected divergence formula; standard PyTorch broadcasting and dtype/device semantics apply. This argument is required.", "default": ""}, "measure": {"type": "string", "description": "The divergence measure to use for the unsupervised loss. Options are 'GAN', 'JSD', 'KL', 'RKL', 'X2', 'DV', 'H2', or 'W1'. The default is 'JSD'. Each measure maps p_samples to the positive expectation Ep as implemented in the function:\n- 'GAN': Ep = -softplus(-p_samples)\n  - 'JSD': Ep = log(2) - softplus(-p_samples)\n  - 'X2': Ep = p_samples**2\n  - 'KL': Ep = p_samples + 1.\n  - 'RKL': Ep = -exp(-p_samples)\n  - 'DV': Ep = p_samples\n  - 'H2': Ep = 1. - exp(-p_samples)\n  - 'W1': Ep = p_samples\nThe measure parameter selects which of these formulas is applied. If an unknown measure string is provided, the function raises a ValueError.", "default": "JSD"}, "average_loss": {"type": "boolean", "description": "Average the result over samples. If True (the default), the function returns the mean of Ep across all elements in p_samples, producing a scalar tensor suitable as a loss value for optimization. If False, the function returns Ep with the same elementwise shape as p_samples so callers can apply custom reductions or weighting.", "default": true}}, "required": ["p_samples", "measure", "average_loss"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a mutual-information critic under a realistic training batch where the “positive pair” stream contains occasional degenerate pairs (near-zero logits) and rare extreme activations. Use per-sample outputs (no batch averaging) so we can later apply sample-wise weights.\n\n1) Single-replicate positive critic scores (scalar logits): p_samples = [0.8, -0.3, 1.5, 0.0, 2.1]. Compute the positive-expectation term under a branching rule: if a score is strictly positive, treat it as a heavy-tail regime and use the chi-squared (X2) divergence; otherwise treat it as a low-signal regime and use Jensen–Shannon (JSD). Return per-sample terms.\n\n2) Two-replicate matrix of positive logits: p_samples = [[0.7, -1.2, 0.3], [1.5, 0.0, -0.4]]. For each replicate independently, select the divergence measure based on that replicate’s intrinsic activation pattern: if the replicate’s mean logit is positive, use KL; otherwise use JSD. Return per-sample terms for each replicate.\n\n3) Separate single-replicate batch: p_samples = [0.8, -0.3, 1.2, 0.0, 2.5]. Compute per-sample positive-expectation terms under JSD for entries whose magnitude is at most 1, and under KL for entries whose magnitude exceeds 1, to compare behavior across low- vs high-activation positives without aggregating.", "answers": "[{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[0.8,1.5,2.1],\"measure\":\"X2\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[-0.3,0.0],\"measure\":\"JSD\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[0.7,-1.2,0.3],\"measure\":\"JSD\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[1.5,0.0,-0.4],\"measure\":\"KL\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[0.8,-0.3,0.0],\"measure\":\"JSD\",\"average_loss\":false}},{\"name\":\"deepchem_models_losses_get_positive_expectation\",\"arguments\":{\"p_samples\":[1.2,2.5],\"measure\":\"KL\",\"average_loss\":false}}]"}
{"func_name": "deepchem_trans_transformers_get_cdf_values", "func_desc": "get_cdf_values computes per-column empirical cumulative distribution function (CDF) quantile values for a 1D or 2D numeric array. This helper is used in DeepChem transformer code to map raw feature values (rows = examples, columns = features) to discretized CDF-like values by splitting the sorted data into a fixed number of bins and assigning each sample a bin-based quantile. It is intended for preprocessing and normalization steps in molecular machine-learning workflows where consistent, column-wise percentile mapping is required.", "tools": [{"function": {"description": "get_cdf_values computes per-column empirical cumulative distribution function (CDF) quantile values for a 1D or 2D numeric array. This helper is used in DeepChem transformer code to map raw feature values (rows = examples, columns = features) to discretized CDF-like values by splitting the sorted data into a fixed number of bins and assigning each sample a bin-based quantile. It is intended for preprocessing and normalization steps in molecular machine-learning workflows where consistent, column-wise percentile mapping is required.\n", "name": "deepchem_trans_transformers_get_cdf_values", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "float"}, "description": "Input data to be mapped to CDF-like values. Must be either a 1-D array of shape (n_rows,) or a 2-D array of shape (n_rows, n_cols). Rows are interpreted as independent samples (examples) and columns as separate features/channels. If a 1-D array is provided, it is internally reshaped to shape (n_rows, 1) so the output always has two dimensions. Arrays with more than two dimensions are not supported. The function does not modify the input array in place; it returns a new numpy.ndarray.", "default": ""}, "bins": {"type": "integer", "description": "Number of bins to split the data into when computing the quantile mapping. This integer controls the discretization resolution: larger values produce finer-grained quantile steps. The implementation requires bins to be an integer greater than 1 (bins <= 1 will cause division-by-zero or invalid partitioning). If bins is odd the code normalizes by (bins - 1) producing values that can reach 1.0; if bins is even the code normalizes by bins producing values with maximum (bins - 1) / bins (< 1.0). Non-integer or non-positive inputs for this parameter will raise an error (TypeError for non-int types, or runtime errors such as ZeroDivisionError for invalid values).", "default": ""}}, "required": ["array", "bins"], "type": "any"}}, "type": "function"}], "query": "We’re validating a descriptor-normalization (per-feature empirical CDF binning) stage across three independently processed assay cohorts, but the raw descriptor exports are messy. For each cohort matrix below (rows=molecules, columns=continuous descriptors), first screen for numeric integrity and scale sanity: only run CDF mapping for cohorts whose entire matrix is finite and whose descriptor columns are not orders-of-magnitude inconsistent (i.e., each feature column’s max/min ratio is < 1e3 when the column min is positive). Then apply a cohort-specific binning rule based on cohort size: if the cohort has at most 5 molecules, use 4 bins; if it has 6 molecules, use 8 bins; otherwise use 5 bins. Cohort A (pilot descriptor panel, 5x3): [[0.12, 1.5, 10.0], [0.05, 0.7, 8.2], [0.3, 1.1, 9.5], [0.18, 0.9, 11.3], [0.07, 1.8, 7.9]]. Cohort B (logP/TPSA/MW batch, 6x3): [[1.2, 35.0, 250.5], [0.7, 42.1, 310.2], [2.1, 28.4, 180.0], [1.2, 35.0, 400.0], [3.4, 55.2, 290.1], [0.0, 20.0, 210.3]] (note: one export line may contain a zero placeholder). Cohort C (logP/MW mini-set, 5x2): [[1.2, 250.0], [0.8, 300.0], [1.5, 280.0], [0.5, 260.0], [1.0, 310.0]]. Return the per-example, per-feature CDF-mapped values for every cohort that passes the screen, each treated independently.", "answers": "[{\"name\":\"deepchem_trans_transformers_get_cdf_values\",\"arguments\":{\"array\":[[0.12,1.5,10.0],[0.05,0.7,8.2],[0.3,1.1,9.5],[0.18,0.9,11.3],[0.07,1.8,7.9]],\"bins\":4}},{\"name\":\"deepchem_trans_transformers_get_cdf_values\",\"arguments\":{\"array\":[[1.2,250.0],[0.8,300.0],[1.5,280.0],[0.5,260.0],[1.0,310.0]],\"bins\":4}}]"}
{"func_name": "deepchem_utils_batch_utils_batch_coulomb_matrix_features", "func_desc": "Computes per-batch Coulomb-matrix-derived features used by models such as the DTNN (Deep Tensor Neural Network). This helper converts a batch of Coulomb matrices into atomic identifiers and a Gaussian-expanded pairwise distance representation that downstream models consume as input features. The function expects a 3-D NumPy array of Coulomb matrices (a batch), infers the number of atoms per molecule from nonzero entries, recovers approximate atomic numbers from Coulomb-matrix diagonals, computes interatomic distances by inverting the Coulomb interaction entries, and projects those distances onto a fixed set of Gaussian basis functions (distance bins). The outputs encode atom identity, pairwise distance features at specified granularity, per-atom molecule membership, and flattened pair-index mappings required by graph- or tensor-based molecular models.", "tools": [{"function": {"description": "Computes per-batch Coulomb-matrix-derived features used by models such as the DTNN (Deep Tensor Neural Network). This helper converts a batch of Coulomb matrices into atomic identifiers and a Gaussian-expanded pairwise distance representation that downstream models consume as input features. The function expects a 3-D NumPy array of Coulomb matrices (a batch), infers the number of atoms per molecule from nonzero entries, recovers approximate atomic numbers from Coulomb-matrix diagonals, computes interatomic distances by inverting the Coulomb interaction entries, and projects those distances onto a fixed set of Gaussian basis functions (distance bins). The outputs encode atom identity, pairwise distance features at specified granularity, per-atom molecule membership, and flattened pair-index mappings required by graph- or tensor-based molecular models.\n", "name": "deepchem_utils_batch_utils_batch_coulomb_matrix_features", "parameters": {"properties": {"X_b": {"type": "array", "items": {"type": "float"}, "description": "A 3-D NumPy array representing a batch of Coulomb matrices. The expected layout is (batch_size, max_atoms, max_atoms, ...) where the function indexes X_b[:, :, 0] to detect nonzero entries and uses X_b[i, :num_atoms, :num_atoms] as the Coulomb matrix for molecule i. Nonzero entries in X_b[:, :, 0] indicate atom presence and determine per-molecule atom counts. In the molecular machine-learning domain this array is produced by a CoulombMatrix featurizer (for example the CoulombMatrix featurizer used with QM9-like datasets) and its off-diagonal entries are assumed to follow the Coulomb interaction form Z_i * Z_j / r_ij (atomic charges product divided by interatomic distance) and diagonal entries correspond to the diagonal Coulomb-matrix prescription (used to recover approximate atomic number).", "default": ""}, "distance_max": {"type": "float", "description": "Upper bound of the distance range (in Angstrom) used to construct the Gaussian basis. This value is used with distance_min and n_distance to compute the step size and the center positions of the Gaussian distance bins. Practical significance: controls the maximum pairwise distance scale covered by the Gaussian expansion (larger values include longer-range interactions). Default in function signature is -1.0. If distance_max equals distance_min or n_distance <= 0, the computed step size will be zero or invalid and will cause division-by-zero errors in the Gaussian computation; callers must ensure distance_max, distance_min, and n_distance together define a valid nonzero step size.", "default": -1}, "distance_min": {"type": "float", "description": "Lower bound of the distance range (in Angstrom) used to construct the Gaussian basis. This value is used together with distance_max and n_distance to compute the Gaussian centers and the Gaussian width (step size). Practical significance: controls the minimum pairwise distance scale represented in the Gaussian expansion (smaller values allow resolution near-zero distances). Default in function signature is 18.0. If distance_min equals distance_max or n_distance <= 0, the computed step size will be zero or invalid and will cause division-by-zero errors in the Gaussian computation.", "default": 18}, "n_distance": {"type": "integer", "description": "Number of Gaussian bins (granularity) used to expand each pairwise distance into a fixed-length vector. The function computes step_size = (distance_max - distance_min) / n_distance and then defines n_distance centers. Practical significance: each pairwise atom distance will be represented by a length-n_distance vector (a radial basis expansion); larger n_distance increases angular resolution and memory cost. Default in function signature is 100. Must be a positive integer; nonpositive values will produce runtime errors (division by zero).", "default": 100}}, "required": ["X_b", "distance_min", "n_distance", "distance_max"], "type": "any"}}, "type": "function"}], "query": "We’re assembling DTNN-ready features from two raw calibration cohorts of **padded** Coulomb-matrix batches (each batch contains 2 molecules padded to max 3 atoms; infer true atom count from the nonzero structure).\n\nBecause these cohorts were collected on different instruments, the **distance-basis resolution must be chosen from the batch itself**:\n- Compute a robust scale of Coulomb coupling strength for each cohort as the **median of all strictly positive off-diagonal entries** across the entire batch.\n- If that median coupling is **>= 2.0**, treat the cohort as “high-coupling/short-range” and use a **compact distance basis**: distance_min = 0.5 Å, distance_max = 5.0 Å, n_distance = 8.\n- Otherwise, treat it as “low-coupling/broad-range” and use a **broad distance basis**: distance_min = 0.5 Å, distance_max = 10.0 Å, n_distance = 32.\n\nFor each cohort: recover approximate atomic identifiers from Coulomb-matrix diagonals, invert off-diagonal Coulomb terms to estimate interatomic distances, then Gaussian-expand those distances into the selected fixed distance basis to produce DTNN input features (atom IDs, pairwise distance features, molecule-membership indexing, and pair-index mappings).\n\nCohort A batch:\nX_b = [\n  [[0.5, 0.7407, 0.0], [0.7407, 0.5, 0.0], [0.0, 0.0, 0.0]],\n  [[73.5167, 4.2105, 4.2105], [4.2105, 0.5, 0.6623], [4.2105, 0.6623, 0.5]]\n]\n\nCohort B batch:\nX_b = [\n  [[5.0, 3.0, 0.0], [3.0, 6.0, 2.0], [0.0, 2.0, 7.0]],\n  [[4.5, 2.5, 0.0], [2.5, 5.5, 0.0], [0.0, 0.0, 0.0]]\n]", "answers": "[{\"name\":\"deepchem_utils_batch_utils_batch_coulomb_matrix_features\",\"arguments\":{\"X_b\":[[[0.5,0.7407,0.0],[0.7407,0.5,0.0],[0.0,0.0,0.0]],[[73.5167,4.2105,4.2105],[4.2105,0.5,0.6623],[4.2105,0.6623,0.5]]],\"distance_min\":0.5,\"distance_max\":10.0,\"n_distance\":32}},{\"name\":\"deepchem_utils_batch_utils_batch_coulomb_matrix_features\",\"arguments\":{\"X_b\":[[[5.0,3.0,0.0],[3.0,6.0,2.0],[0.0,2.0,7.0]],[[4.5,2.5,0.0],[2.5,5.5,0.0],[0.0,0.0,0.0]]],\"distance_min\":0.5,\"distance_max\":5.0,\"n_distance\":8}}]"}
{"func_name": "deepchem_utils_coordinate_box_utils_get_face_boxes", "func_desc": "Get coordinate-space bounding boxes for each triangular face of the convex hull of a molecular coordinate set.\n    \n    For a macromolecule represented by a set of 3D atomic coordinates, the convex hull procedure identifies exterior triangular faces that describe the outer surface of the structure. For each triangular face, this function computes axis-aligned bounding intervals (x, y, z) that enclose the three vertices of the triangle, expands those intervals by an additive padding value, and returns a CoordinateBox for each face. These per-face boxes serve as crude geometric approximations of local exterior regions of the molecule (for example, to define candidate binding/interacting regions, to generate local grids for featurization, or to filter points by spatial locality). The algorithm uses simple geometry (floor/ceil of vertex coordinates followed by padding) and therefore is fast and interpretable but may be a coarse approximation of the true pocket geometry.", "tools": [{"function": {"description": "Get coordinate-space bounding boxes for each triangular face of the convex hull of a molecular coordinate set.\n\nFor a macromolecule represented by a set of 3D atomic coordinates, the convex hull procedure identifies exterior triangular faces that describe the outer surface of the structure. For each triangular face, this function computes axis-aligned bounding intervals (x, y, z) that enclose the three vertices of the triangle, expands those intervals by an additive padding value, and returns a CoordinateBox for each face. These per-face boxes serve as crude geometric approximations of local exterior regions of the molecule (for example, to define candidate binding/interacting regions, to generate local grids for featurization, or to filter points by spatial locality). The algorithm uses simple geometry (floor/ceil of vertex coordinates followed by padding) and therefore is fast and interpretable but may be a coarse approximation of the true pocket geometry.", "name": "deepchem_utils_coordinate_box_utils_get_face_boxes", "parameters": {"properties": {"coords": {"type": "array", "items": {"type": "float"}, "description": "A numpy array of shape (N, 3) containing the 3D coordinates of a molecule (typically atomic coordinates in angstroms). This array is treated as an N×3 list of (x, y, z) points and is used as input to compute the convex hull. Practical role: provides the spatial points from which exterior triangular faces are extracted. Requirements and failure modes: coords must be two-dimensional with second dimension size 3 and must contain a sufficient number of non-coplanar points for a 3D convex hull (typically at least four non-coplanar points). If coords has an invalid shape or the point set is degenerate, the underlying convex hull routine (e.g., scipy.spatial.ConvexHull) will raise an error.", "default": ""}, "pad": {"type": "float", "description": "The number of angstroms to add outside the min/max extent of each triangular face when forming the coordinate box. Default is 5.0 (angstroms). Behavior detail: for each axis the code first computes the floor of the minimum vertex coordinate and the ceil of the maximum vertex coordinate (casting those to int), then subtracts pad from the floored minimum and adds pad to the ceiled maximum to produce final axis bounds. Because pad is a float and the floor/ceil are cast to int before applying pad, the resulting bounds may be floating-point values. Practical role: controls how much exterior margin is included around each face to capture surrounding interaction volume. Failure modes and notes: extremely large or negative pad values will respectively produce very large boxes or shrink/flip bounds; pad is applied uniformly to all three axes.", "default": 5.0}}, "required": ["coords", "pad"], "type": "any"}}, "type": "function"}], "query": "We’re triaging two peptide-fragment coordinate cohorts from a surface-localization precompute where some runs include solvent/cosolute points and unit-scale issues. For each cohort, first keep only atomic coordinates that fall inside a conservative instrument envelope of -3.0 Å to +3.0 Å on every axis (x,y,z). On the retained point cloud, compute the convex hull and generate per-face axis-aligned CoordinateBox bounds as usual (floor/ceil of each face’s vertices, then additive padding). Use a padding rule that depends on the cohort’s spatial extent: set pad = 2.5 Å if the retained coordinates span no more than 5.0 Å in x (max(x)-min(x) ≤ 5.0), otherwise pad = 4.0 Å. Cohort A raw coords: [[1.2, -0.7, 0.0], [2.8, 0.4, 1.9], [0.1, 1.1, -1.4], [-1.5, 0.3, 0.8], [0.6, -2.2, 1.1], [-0.9, -1.3, -0.6]]. Cohort B raw coords: [[0.5, 1.2, -0.7], [2.1, 0.9, 0.3], [1.4, 2.5, 1.1], [3.0, 1.8, -1.2], [2.7, 3.3, 0.5], [0.8, 2.9, -1.5]]. Return the full set of per-face CoordinateBox outputs for each cohort after this filtering and padding selection.", "answers": "[{\"name\":\"deepchem_utils_coordinate_box_utils_get_face_boxes\",\"arguments\":{\"coords\":[[1.2,-0.7,0.0],[2.8,0.4,1.9],[0.1,1.1,-1.4],[-1.5,0.3,0.8],[0.6,-2.2,1.1],[-0.9,-1.3,-0.6]],\"pad\":2.5}},{\"name\":\"deepchem_utils_coordinate_box_utils_get_face_boxes\",\"arguments\":{\"coords\":[[0.5,1.2,-0.7],[2.1,0.9,0.3],[1.4,2.5,1.1],[3.0,1.8,-1.2],[0.8,2.9,-1.5]],\"pad\":2.5}}]"}
{"func_name": "deepchem_utils_coordinate_box_utils_intersect_interval", "func_desc": "deepchem.utils.coordinate_box_utils.intersect_interval computes the intersection of two one-dimensional closed intervals. This function is used in DeepChem codepaths that manipulate coordinate-aligned bounding boxes or 1D coordinate ranges (for example, computing overlap along a single axis when intersecting molecular or atom-centered boxes in drug discovery, materials science, quantum chemistry, and biology workflows). The function returns a tuple representing the overlapping interval or a sentinel representing an empty intersection.", "tools": [{"function": {"description": "deepchem.utils.coordinate_box_utils.intersect_interval computes the intersection of two one-dimensional closed intervals. This function is used in DeepChem codepaths that manipulate coordinate-aligned bounding boxes or 1D coordinate ranges (for example, computing overlap along a single axis when intersecting molecular or atom-centered boxes in drug discovery, materials science, quantum chemistry, and biology workflows). The function returns a tuple representing the overlapping interval or a sentinel representing an empty intersection.\n", "name": "deepchem_utils_coordinate_box_utils_intersect_interval", "parameters": {"properties": {"interval1": {"type": "tuple", "prefixItems": [{"type": "float"}, {"type": "float"}], "description": "The first interval, specified as (x1_min, x1_max). These floats represent the lower and upper bounds along a single coordinate axis. The caller is expected to supply the bounds in increasing order (min then max). The function uses these values directly to determine overlap; it does not reorder or validate that x1_min <= x1_max.", "default": ""}, "interval2": {"type": "tuple", "prefixItems": [{"type": "float"}, {"type": "float"}], "description": "The second interval, specified as (x2_min, x2_max). These floats represent the lower and upper bounds along the same coordinate axis as interval1. As with interval1, the caller must supply bounds in increasing order. The function treats the intervals as closed (endpoints included) when determining intersection.", "default": ""}}, "required": ["interval1", "interval2"], "type": "any"}}, "type": "function"}], "query": "We’re QC’ing the x-axis overlap stage for a docking/fragment-scan run where the upstream parser may output degenerate or malformed 1D ranges. Each candidate x-range is *intended* to be a closed interval in Å, but some entries arrive with endpoints swapped (min/max inverted) and some collapse to a single coordinate (contact-plane artifacts). For each replicate, first canonicalize each reported range into a valid closed interval by ordering its endpoints (lower, upper). Then run `deepchem.utils.coordinate_box_utils.intersect_interval` only for replicate-pairs whose *canonicalized* intersection would represent a physically meaningful overlap with nonzero width (i.e., the overlap must be strictly longer than a point-contact). Use these three replicates: (1) reaction-coordinate fragments: fragment A raw x-range [1.0, -2.5] vs fragment B raw x-range [-1.0, 3.2]; (2) protein pocket atom-centered boxes: box1 raw [7.8, 3.2] vs box2 raw [10.5, 5.0]; (3) ligand–receptor docking x-overlap: ligand raw [-2.5, 4.0] vs receptor pocket raw [6.5, 1.25]. Report the intersection interval (or the empty-intersection sentinel if none) for all replicate-pairs that pass the nonzero-overlap criterion.", "answers": "[{\"name\":\"deepchem_utils_coordinate_box_utils_intersect_interval\",\"arguments\":{\"interval1\":[-2.5,1.0],\"interval2\":[-1.0,3.2]}},{\"name\":\"deepchem_utils_coordinate_box_utils_intersect_interval\",\"arguments\":{\"interval1\":[3.2,7.8],\"interval2\":[5.0,10.5]}},{\"name\":\"deepchem_utils_coordinate_box_utils_intersect_interval\",\"arguments\":{\"interval1\":[-2.5,4.0],\"interval2\":[1.25,6.5]}}]"}
{"func_name": "deepchem_utils_data_utils_load_pickle_files", "func_desc": "Load objects from a sequence of pickle files and yield them one at a time.\n    \n    This function is used in DeepChem workflows to load serialized Python objects that represent datasets or other artifacts used in molecular machine learning, drug discovery, materials science, quantum chemistry, and biology. Given a list of file paths, it opens each file (supporting plain pickle files and gzipped pickle files like XXXX.pkl.gz), delegates the actual file deserialization to the module-level helper load_pickle_file, and yields the deserialized top-level Python object for each file in the same order as the input list. Because it yields results as an iterator, this function enables lazy loading of potentially large dataset objects so the caller can avoid loading all files into memory at once.", "tools": [{"function": {"description": "Load objects from a sequence of pickle files and yield them one at a time.\n\nThis function is used in DeepChem workflows to load serialized Python objects that represent datasets or other artifacts used in molecular machine learning, drug discovery, materials science, quantum chemistry, and biology. Given a list of file paths, it opens each file (supporting plain pickle files and gzipped pickle files like XXXX.pkl.gz), delegates the actual file deserialization to the module-level helper load_pickle_file, and yields the deserialized top-level Python object for each file in the same order as the input list. Because it yields results as an iterator, this function enables lazy loading of potentially large dataset objects so the caller can avoid loading all files into memory at once.", "name": "deepchem_utils_data_utils_load_pickle_files", "parameters": {"properties": {"input_files": {"type": "array", "items": {"type": "string"}, "description": "A list of filesystem paths to pickle files to load. Each entry must be a string path to a file containing a single pickled Python object (for example, a DeepChem Dataset, a pandas DataFrame, a NumPy array, or a Python dict/list). Paths may point to gzipped pickles using a .gz extension (e.g., \"dataset.pkl.gz\"); such files are handled transparently by the underlying loader. The order of paths in this list determines the order in which objects are yielded.", "default": ""}}, "required": ["input_files"], "type": "any"}}, "type": "function"}], "query": "We’re auditing Tox21 benchmark artifacts coming from three sources (run outputs, curated registry, and processed shards) where the directory listings are messy and contain non-pickle byproducts. Construct a memory-efficient ingestion pass that only streams candidate files that plausibly contain DeepChem-serialized Python objects: accept paths whose basename indicates a dataset-bearing artifact (contains tokens like `dataset`, `shard`, or `validation`) and whose extension is either `.pkl` or `.pkl.gz`. Ignore ancillary run byproducts (e.g., `model_metadata`, feature-stat snapshots, logs) that don’t satisfy the dataset-bearing naming rule. Maintain the original ordering within each cohort after filtering, and lazily iterate each cohort independently so downstream dataset inspection and shard-wise statistics can consume one object at a time.\n\nCohort A (run outputs): /mnt/deepchem_runs/tox21/train_dataset.pkl, /mnt/deepchem_runs/tox21/valid_dataset.pkl.gz, /mnt/deepchem_runs/tox21/model_metadata.pkl\nCohort B (curated artifacts): /mnt/deepchem/artifacts/tox21_train_dataset.pkl.gz, /mnt/deepchem/artifacts/tox21_valid_dataset.pkl.gz, /mnt/deepchem/artifacts/tox21_feature_stats.pkl\nCohort C (processed shards): data/processed/tox21_train_shard1.pkl.gz, data/processed/tox21_train_shard2.pkl.gz, data/processed/tox21_validation.pkl", "answers": "[{\"name\":\"deepchem_utils_data_utils_load_pickle_files\",\"arguments\":{\"input_files\":[\"/mnt/deepchem_runs/tox21/train_dataset.pkl\",\"/mnt/deepchem_runs/tox21/valid_dataset.pkl.gz\"]}},{\"name\":\"deepchem_utils_data_utils_load_pickle_files\",\"arguments\":{\"input_files\":[\"/mnt/deepchem/artifacts/tox21_train_dataset.pkl.gz\",\"/mnt/deepchem/artifacts/tox21_valid_dataset.pkl.gz\"]}},{\"name\":\"deepchem_utils_data_utils_load_pickle_files\",\"arguments\":{\"input_files\":[\"data/processed/tox21_train_shard1.pkl.gz\",\"data/processed/tox21_train_shard2.pkl.gz\",\"data/processed/tox21_validation.pkl\"]}}]"}
{"func_name": "deepchem_utils_differentiation_utils_misc_get_and_pop_keys", "func_desc": "Get and pop keys from a dictionary.\n    \n    This utility extracts the values for the specified keys from the provided dictionary and removes those entries from the original dictionary. It is primarily used in DeepChem's differentiation utilities and related preprocessing code to extract and remove selected configuration entries, intermediate tensors, or auxiliary values from a parameter or metadata dictionary before further processing (for example, removing gradient-related entries prior to a numerical routine). The function returns a new dictionary mapping each requested key to its popped value while mutating the input dictionary in-place.", "tools": [{"function": {"description": "Get and pop keys from a dictionary.\n\nThis utility extracts the values for the specified keys from the provided dictionary and removes those entries from the original dictionary. It is primarily used in DeepChem's differentiation utilities and related preprocessing code to extract and remove selected configuration entries, intermediate tensors, or auxiliary values from a parameter or metadata dictionary before further processing (for example, removing gradient-related entries prior to a numerical routine). The function returns a new dictionary mapping each requested key to its popped value while mutating the input dictionary in-place.", "name": "deepchem_utils_differentiation_utils_misc_get_and_pop_keys", "parameters": {"properties": {"dct": {"type": "any", "description": "Dictionary to pop from. This is the mutable mapping that will be modified by this function: each key listed in keys will be removed from this dictionary and its value returned in the result. In DeepChem code paths this typically represents model inputs, parameter maps, or metadata dictionaries used during differentiation or preprocessing.", "default": ""}, "keys": {"type": "array", "description": "Keys to pop. This is an iterable list of keys that must exist in dct; for each key in this list the function performs dct.pop(key) and stores the popped value in the returned dictionary in the same iteration order as this list. Supplying duplicate keys in this list will cause subsequent attempts to pop an already-removed key to fail.", "default": ""}}, "required": ["dct", "keys"], "type": "any"}}, "type": "function"}], "query": "We’re staging a differentiation-prep “metadata sieve” across two independent cohorts where the incoming configuration dicts are messy and may include optional controls. For each cohort, extract-and-remove only the entries that belong to the differentiation/gradient-tracking instrumentation layer, defined by keys whose names either (a) start with the prefix \"grad_\", or (b) are exactly one of {\"use_grad_clip\", \"grads\", \"tape\", \"aux_loss\"}. Leave all other hyperparameters, logging cadence, and checkpointing settings in-place for downstream numerical routines.\n\nCohort A (baseline training config): {\"learning_rate\": 0.001, \"batch_size\": 64, \"use_grad_clip\": true, \"grad_clip_norm\": 5.0, \"log_interval\": 10, \"save_checkpoints\": true}\n\nCohort B (differentiation step options): {\"learning_rate\": 0.0005, \"optimizer\": \"adam\", \"grads\": [[0.1, -0.2], [0.05, 0.3]], \"tape\": \"persistent\", \"aux_loss\": 0.012, \"max_iter\": 200, \"grad_accum_steps\": 4}\n\nReturn, for each cohort, the popped-key dictionary produced by the cleanup step (while mutating the original dict to its sanitized remainder).", "answers": "[{\"name\":\"deepchem_utils_differentiation_utils_misc_get_and_pop_keys\",\"arguments\":{\"dct\":{\"learning_rate\":0.001,\"batch_size\":64,\"use_grad_clip\":true,\"grad_clip_norm\":5.0,\"log_interval\":10,\"save_checkpoints\":true},\"keys\":[\"use_grad_clip\",\"grad_clip_norm\"]}},{\"name\":\"deepchem_utils_differentiation_utils_misc_get_and_pop_keys\",\"arguments\":{\"dct\":{\"learning_rate\":0.0005,\"optimizer\":\"adam\",\"grads\":[[0.1,-0.2],[0.05,0.3]],\"tape\":\"persistent\",\"aux_loss\":0.012,\"max_iter\":200,\"grad_accum_steps\":4},\"keys\":[\"grads\",\"tape\",\"aux_loss\",\"grad_accum_steps\"]}}]"}
{"func_name": "deepchem_utils_equivariance_utils_so3_generators", "func_desc": "deepchem.utils.equivariance_utils.so3_generators constructs the three matrix generators of the SO(3) Lie algebra for a given representation index k used to represent angular momentum (quantum spin) in rotation-equivariant computations.\n    \n    This function is used in DeepChem's equivariance utilities when constructing rotation-equivariant operators or representations (for example, in physics-informed or molecular models where three-dimensional rotations and angular momentum operators appear). Internally it obtains SU(2) generators for the representation index k via su2_generators(k), obtains the change-of-basis matrix from real to complex spherical harmonics via change_basis_real_to_complex(k), performs the similarity transform X <- conj(Q.T) @ X @ Q to move the SU(2) generators into the SO(3) (complex spherical harmonic) basis, and then returns the real part of the resulting matrices so the final SO(3) generators are purely real-valued.", "tools": [{"function": {"description": "deepchem.utils.equivariance_utils.so3_generators constructs the three matrix generators of the SO(3) Lie algebra for a given representation index k used to represent angular momentum (quantum spin) in rotation-equivariant computations.\n\nThis function is used in DeepChem's equivariance utilities when constructing rotation-equivariant operators or representations (for example, in physics-informed or molecular models where three-dimensional rotations and angular momentum operators appear). Internally it obtains SU(2) generators for the representation index k via su2_generators(k), obtains the change-of-basis matrix from real to complex spherical harmonics via change_basis_real_to_complex(k), performs the similarity transform X <- conj(Q.T) @ X @ Q to move the SU(2) generators into the SO(3) (complex spherical harmonic) basis, and then returns the real part of the resulting matrices so the final SO(3) generators are purely real-valued.", "name": "deepchem_utils_equivariance_utils_so3_generators", "parameters": {"properties": {"k": {"type": "integer", "description": "The representation index that determines the order (dimension) of the angular-momentum representation used to build the generators. In the context of the SU(2)/SO(3) representations used by the helper functions called here, k indexes the representation returned by su2_generators(k) and the corresponding basis change returned by change_basis_real_to_complex(k). Practically, k selects the matrix size used for the angular-momentum operators (the resulting matrices operate on a (2*k + 1)-dimensional representation space). k must be provided as an integer; if k is not an integer or is an invalid index for the underlying helper functions, those functions will raise an error.", "default": ""}}, "required": ["k"], "type": "any"}}, "type": "function"}], "query": "We’re running a quick rotation-equivariant sanity sweep across a mixed set of angular-momentum channels coming out of a molecular model’s tensor-product block. The raw channel list is k=[0, 1, 2, 3, -1, 2]. Treat any entry as a valid SO(3) irrep index only if it is an integer k>=0 and corresponds to a physically relevant orbital channel (k>0). For numerical stability, handle parity as follows: for even k, generate the real-valued SO(3) Lie algebra generators once for the main path and once for an independent cross-check replicate; for odd k, generate them only once (single replicate). Use deepchem.utils.equivariance_utils.so3_generators(k) for each required generator construction.", "answers": "[{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":2}},{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":2}},{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":2}},{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":2}},{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":1}},{\"name\":\"deepchem_utils_equivariance_utils_so3_generators\",\"arguments\":{\"k\":3}}]"}
{"func_name": "deepchem_utils_evaluate_relative_difference", "func_desc": "Compute the elementwise relative difference between two numpy arrays as (x - y) / abs(y).\n    \n    This function is used in DeepChem for numerical comparison of two arrays, for example to measure relative error between model predictions and reference values in drug discovery, materials science, quantum chemistry, and biology workflows described in the project README. The operation is performed elementwise and returns a numpy.ndarray that contains the relative difference for each corresponding element pair from x and y.", "tools": [{"function": {"description": "Compute the elementwise relative difference between two numpy arrays as (x - y) / abs(y).\n\nThis function is used in DeepChem for numerical comparison of two arrays, for example to measure relative error between model predictions and reference values in drug discovery, materials science, quantum chemistry, and biology workflows described in the project README. The operation is performed elementwise and returns a numpy.ndarray that contains the relative difference for each corresponding element pair from x and y.", "name": "deepchem_utils_evaluate_relative_difference", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "First input array. This is typically the numerator array (for example, predicted values from a model) and must have the same shape as y for a direct elementwise comparison. The function does not perform any in-place modification of x.", "default": ""}, "y": {"type": "array", "items": {"type": "float"}, "description": "Second input array. This is used as the denominator (for example, ground-truth or reference values). y must have the same shape as x for a direct elementwise comparison. The function uses elementwise absolute value of y to form the denominator.", "default": ""}}, "required": ["x", "y"], "type": "any"}}, "type": "function"}], "query": "We’re running a numerical-comparison QC pass across three assay cohorts (quantum-chemistry interaction energies, drug-discovery binding affinities, and molecular-property solubilities), but the incoming batch has mixed validity and requires cohort-specific handling.\n\nCompute elementwise relative differences as (x − y) / abs(y) only for comparisons where the reference value is numerically usable for a relative-error metric (i.e., abs(y) > 0). Keep the original shapes within each cohort after applying the cohort’s selection rule.\n\nCohort A — Quantum chemistry (2×3):\n- Predicted energies xA = [[-12.3, -8.7, -5.1], [-3.4, -15.0, -21.8]]\n- Reference energies yA_raw = [[-12.0, -9.0, 0.0], [-3.5, -14.5, -22.0]]\nCompute relative differences only for the interaction columns whose references are non-zero across the cohort (so the resulting matrices retain row structure but may have fewer columns).\n\nCohort B — Binding affinity regression (4 compounds):\n- Predictions xB = [-8.7, -9.3, -7.5, -10.1]\n- References yB_raw = [-9.0, 0.0, -8.0, -10.0]\nCompute relative differences only for compounds with non-zero reference affinities.\n\nCohort C — Solubility (5 compounds):\n- Predictions xC = [0.85, 1.10, 0.60, 0.95, 1.25]\n- References yC_raw = [0.80, 1.00, 0.75, 0.0, 1.20]\nCompute relative differences only for compounds with non-zero reference solubilities.\n\nReturn the three resulting relative-difference arrays (one per cohort) for downstream reporting.", "answers": "[{\"name\":\"deepchem_utils_evaluate_relative_difference\",\"arguments\":{\"x\":[[-12.3,-8.7],[-3.4,-15.0]],\"y\":[[-12.0,-9.0],[-3.5,-14.5]]}},{\"name\":\"deepchem_utils_evaluate_relative_difference\",\"arguments\":{\"x\":[-8.7,-7.5,-10.1],\"y\":[-9.0,-8.0,-10.0]}},{\"name\":\"deepchem_utils_evaluate_relative_difference\",\"arguments\":{\"x\":[0.85,1.1,0.6,1.25],\"y\":[0.8,1.0,0.75,1.2]}}]"}
{"func_name": "deepchem_utils_fake_data_generator_generate_edge_index", "func_desc": "Generate a random edge index array containing source and destination node indices for synthetic graphs used in DeepChem utilities, tests, and examples. The function draws n_nodes * avg_degree random directed edges (as integer node indices) using NumPy and optionally removes self-loops.", "tools": [{"function": {"description": "Generate a random edge index array containing source and destination node indices for synthetic graphs used in DeepChem utilities, tests, and examples. The function draws n_nodes * avg_degree random directed edges (as integer node indices) using NumPy and optionally removes self-loops.\n", "name": "deepchem_utils_fake_data_generator_generate_edge_index", "parameters": {"properties": {"n_nodes": {"type": "integer", "description": "Number of nodes in the graph. This integer is passed as the exclusive upper bound (\"high\") to numpy.random.randint and therefore defines the valid node index range [0, n_nodes - 1] for every source and destination entry. In practice within DeepChem this models the number of atoms or graph vertices in a synthetic molecular or material graph used to exercise graph-based models.", "default": ""}, "avg_degree": {"type": "integer", "description": "Average degree per node. The function computes n_edges = n_nodes * avg_degree and allocates an initial edge index array with shape (2, n_edges). Each of the n_edges columns represents one directed edge as (source_index, destination_index). In practice this parameter controls the total number of sampled edges used to construct a fake graph for testing or benchmarking DeepChem graph algorithms.", "default": ""}, "remove_loops": {"type": "boolean", "description": "If True (default), self-loops (edges where source_index == destination_index) are removed from the returned edge index by calling remove_self_loops(edge_index). If False, self-loops generated by random sampling are retained. This flag lets callers produce either strictly simple directed graphs (no self-loops) or allow self-edges for algorithms that require them.", "default": true}}, "required": ["n_nodes", "avg_degree", "remove_loops"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a DeepChem message-passing pipeline with a mixed-quality batch of candidate synthetic graph specs coming from multiple upstream generators (some are biologically/plausibly sized, some are malformed). Given the following proposed specs, generate directed edge-index replicates only for graphs that are valid under these QC rules: (1) n_nodes must be an integer in [8, 256]; (2) avg_degree must be an integer in [1, 12]; (3) decide self-loop handling based on graph density: if avg_degree/n_nodes >= 0.1, remove self-loops to avoid dense self-connection artifacts; otherwise keep self-loops to preserve residual/self-connection behavior. Proposed specs: S1={n_nodes: 50, avg_degree: 3}, S2={n_nodes: 120, avg_degree: 4}, S3={n_nodes: 12, avg_degree: 3}, S4={n_nodes: 6, avg_degree: 2}, S5={n_nodes: 200, avg_degree: 25}, S6={n_nodes: 64, avg_degree: 8}, S7={n_nodes: 255, avg_degree: 1}. Produce one edge-index replicate per QC-valid spec using the density-based loop rule.", "answers": "[{\"name\":\"deepchem_utils_fake_data_generator_generate_edge_index\",\"arguments\":{\"n_nodes\":50,\"avg_degree\":3,\"remove_loops\":false}},{\"name\":\"deepchem_utils_fake_data_generator_generate_edge_index\",\"arguments\":{\"n_nodes\":120,\"avg_degree\":4,\"remove_loops\":false}},{\"name\":\"deepchem_utils_fake_data_generator_generate_edge_index\",\"arguments\":{\"n_nodes\":12,\"avg_degree\":3,\"remove_loops\":true}},{\"name\":\"deepchem_utils_fake_data_generator_generate_edge_index\",\"arguments\":{\"n_nodes\":64,\"avg_degree\":8,\"remove_loops\":true}},{\"name\":\"deepchem_utils_fake_data_generator_generate_edge_index\",\"arguments\":{\"n_nodes\":255,\"avg_degree\":1,\"remove_loops\":false}}]"}
{"func_name": "deepchem_utils_geometry_utils_is_angle_within_cutoff", "func_desc": "deepchem.utils.geometry_utils.is_angle_within_cutoff: Determine whether two 3-D vectors are within a specified angular deviation (in degrees) of being exactly 180 degrees apart. This function is used in DeepChem geometry utilities to detect anti-parallel or nearly anti-parallel directions (for example, opposing bond directions or opposite normals) by computing the angle between vectors in degrees and testing whether that angle lies strictly within the symmetric interval (180 - angle_cutoff, 180 + angle_cutoff).", "tools": [{"function": {"description": "deepchem.utils.geometry_utils.is_angle_within_cutoff: Determine whether two 3-D vectors are within a specified angular deviation (in degrees) of being exactly 180 degrees apart. This function is used in DeepChem geometry utilities to detect anti-parallel or nearly anti-parallel directions (for example, opposing bond directions or opposite normals) by computing the angle between vectors in degrees and testing whether that angle lies strictly within the symmetric interval (180 - angle_cutoff, 180 + angle_cutoff).\n", "name": "deepchem_utils_geometry_utils_is_angle_within_cutoff", "parameters": {"properties": {"vector_i": {"type": "array", "items": {"type": "float"}, "description": "A 1-D numpy array of shape (3,) representing a 3D vector with components (x, y, z). In molecular and materials contexts (per the DeepChem project), this typically encodes a bond direction, displacement, or normal vector. The function expects a real-valued vector of length 3; providing arrays of other shapes may lead to incorrect results or exceptions.", "default": ""}, "vector_j": {"type": "array", "items": {"type": "float"}, "description": "A 1-D numpy array of shape (3,) representing a second 3D vector with components (x, y, z). As with vector_i, this usually represents a geometric direction in molecular structures. The relative orientation of vector_i and vector_j is tested against the 180-degree criterion.", "default": ""}, "angle_cutoff": {"type": "float", "description": "The allowed deviation from exactly 180 degrees, expressed in degrees. For example, angle_cutoff = 5.0 means the function returns True when the angle between vector_i and vector_j is strictly greater than 175.0 degrees and strictly less than 185.0 degrees. The comparison is strict (uses > and <), so exact boundary values equal to 180 +/- angle_cutoff do not return True.", "default": ""}}, "required": ["vector_i", "vector_j", "angle_cutoff"], "type": "any"}}, "type": "function"}], "query": "I’m running a geometry sanity-screen across two structural cohorts where anti-parallelity is only meaningful for well-conditioned direction vectors. Use deepchem.utils.geometry_utils.is_angle_within_cutoff to evaluate only those vector pairs where both vectors are finite 3D triples and neither vector is near-degenerate (Euclidean norm >= 0.2). Apply a cohort-specific tolerance rule: for docking-pose normals, use a tight cutoff of 3.0° when the exit-direction vector is nearly unit-length (its norm is within 2% of 1.0), otherwise use 6.0°. For MD/bond-direction checks, use 10.0° when the two vectors have comparable magnitudes (their norm ratio lies in [0.8, 1.25]); otherwise use 15.0°.\n\nCohort A (docking poses):\n1) pocket_normal = [0.0, 0.0, 1.0], ligand_exit = [0.05, 0.0, -1.0]\n2) pocket_normal = [0.0, 0.0, 1.0], ligand_exit = [0.0, 0.0, 0.0]\n3) pocket_normal = [0.0, 0.0, 1.0], ligand_exit = [0.0, 0.0, -0.97]\n\nCohort B (MD frames / bond directions):\n1) v(C1→C2) = [1.0, -2.0, 0.5], v(C3→C2) = [-0.9, 1.8, -0.4]\n2) v(C1→C2) = [0.01, 0.01, 0.0], v(C3→C2) = [-1.0, 0.0, 0.0]\n3) v(C1→C2) = [2.0, 0.0, 0.0], v(C3→C2) = [-0.5, 0.0, 0.0]\n\nReturn pass/fail booleans for each evaluated pair (in the same order as they appear after filtering).", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_is_angle_within_cutoff\",\"arguments\":{\"vector_i\":[0.0,0.0,1.0],\"vector_j\":[0.05,0.0,-1.0],\"angle_cutoff\":3.0}},{\"name\":\"deepchem_utils_geometry_utils_is_angle_within_cutoff\",\"arguments\":{\"vector_i\":[0.0,0.0,1.0],\"vector_j\":[0.0,0.0,-0.97],\"angle_cutoff\":3.0}},{\"name\":\"deepchem_utils_geometry_utils_is_angle_within_cutoff\",\"arguments\":{\"vector_i\":[1.0,-2.0,0.5],\"vector_j\":[-0.9,1.8,-0.4],\"angle_cutoff\":10.0}},{\"name\":\"deepchem_utils_geometry_utils_is_angle_within_cutoff\",\"arguments\":{\"vector_i\":[2.0,0.0,0.0],\"vector_j\":[-0.5,0.0,0.0],\"angle_cutoff\":15.0}}]"}
{"func_name": "deepchem_utils_geometry_utils_rotate_molecules", "func_desc": "deepchem.utils.geometry_utils.rotate_molecules rotates a collection of molecular Cartesian coordinate sets by a single random 3D rotation matrix.\n    \n    Rotates each molecule's 3D atomic coordinates by generating one random rotation matrix via generate_random_rotation_matrix() and applying that same rotation to every molecule in mol_coordinates_list. This is typically used in DeepChem workflows for molecular machine learning and data augmentation in drug discovery, materials science, and computational chemistry to produce rotated variants of molecular geometries while preserving interatomic distances and overall molecular shape. The random rotation matrix is drawn so that applying it to a 3-vector uniformly samples orientations on the sphere of radius equal to the vector norm, as implemented by generate_random_rotation_matrix(). The function performs a deep copy of each input coordinate array before applying the rotation, so the input objects are not modified.", "tools": [{"function": {"description": "deepchem.utils.geometry_utils.rotate_molecules rotates a collection of molecular Cartesian coordinate sets by a single random 3D rotation matrix.\n\nRotates each molecule's 3D atomic coordinates by generating one random rotation matrix via generate_random_rotation_matrix() and applying that same rotation to every molecule in mol_coordinates_list. This is typically used in DeepChem workflows for molecular machine learning and data augmentation in drug discovery, materials science, and computational chemistry to produce rotated variants of molecular geometries while preserving interatomic distances and overall molecular shape. The random rotation matrix is drawn so that applying it to a 3-vector uniformly samples orientations on the sphere of radius equal to the vector norm, as implemented by generate_random_rotation_matrix(). The function performs a deep copy of each input coordinate array before applying the rotation, so the input objects are not modified.", "name": "deepchem_utils_geometry_utils_rotate_molecules", "parameters": {"properties": {"mol_coordinates_list": {"type": "array", "items": {"type": "float"}, "description": "A list of molecular coordinate arrays or array-like objects. Each entry represents one molecule's atomic coordinates and is expected to be a 2-D sequence with shape (num_atoms, 3), where the last dimension corresponds to the Cartesian x, y, z coordinates for each atom. The list order is preserved in the output. In practice within DeepChem, these coordinate arrays are numeric (e.g., NumPy arrays) produced by molecular preprocessing pipelines; providing non-numeric items or arrays that do not have a final dimension of size 3 will lead to runtime errors.", "default": ""}}, "required": ["mol_coordinates_list"], "type": "any"}}, "type": "function"}], "query": "We’re curating a noisy 3D-geometry augmentation batch before training a shape-based molecular model. Each cohort was exported from a different upstream toolchain and may contain padded atoms, duplicated coordinates, or collapsed geometries. For each cohort, build the rotation batch by selecting only those molecular coordinate sets that satisfy all of the following QC criteria: (1) the molecule contains at least 3 atoms; (2) at least two distinct coordinate vectors are present (i.e., not all atoms share identical coordinates); (3) the maximum pairwise interatomic distance is >= 0.5 Å (to reject collapsed structures); and (4) no coordinate contains a non-finite value. After QC filtering, apply DeepChem’s geometry augmentation using a single shared random 3D rotation matrix per cohort (same rotation for every molecule that passes QC within that cohort), preserving atom order and returning rotated copies.\n\nCohort A (MD toy set, exported with one collapsed conformer):\n- water_A: [[0.0, 0.0, 0.0], [0.9572, 0.0, 0.0], [-0.239, 0.927, 0.0]]\n- methane_A: [[0.0, 0.0, 0.0], [0.6291, 0.6291, 0.6291], [-0.6291, -0.6291, 0.6291], [-0.6291, 0.6291, -0.6291], [0.6291, -0.6291, -0.6291]]\n- ethane_A: [[-0.77, 0.0, 0.0], [0.77, 0.0, 0.0], [-1.17, 0.99, 0.0], [-1.17, -0.99, 0.0], [1.17, 0.99, 0.0], [1.17, -0.99, 0.0], [-1.17, 0.0, 1.0], [1.17, 0.0, -1.0]]\n- collapsed_A (artifact): [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n\nCohort B (drug-discovery mini-batch, exported with a padded dummy atom):\n- water_B: [[0.0, 0.0, 0.0], [0.9572, 0.0, 0.0], [-0.2396, 0.9271, 0.0]]\n- methane_B_padded: [[0.0, 0.0, 0.0], [0.6291, 0.6291, 0.6291], [-0.6291, -0.6291, 0.6291], [-0.6291, 0.6291, -0.6291], [0.6291, -0.6291, -0.6291], [0.0, 0.0, 0.0]]\n\nCohort C (small geometry set, contains a duplicated-only-entry and a short fragment):\n- water_C: [[0.0, 0.0, 0.0], [0.957, 0.0, 0.0], [-0.24, 0.927, 0.0]]\n- methane_C: [[0.0, 0.0, 0.0], [0.629, 0.629, 0.629], [-0.629, -0.629, 0.629], [-0.629, 0.629, -0.629], [0.629, -0.629, -0.629]]\n- ammonia_C: [[0.0, 0.0, 0.0], [0.94, 0.0, 0.0], [-0.313, 0.907, 0.0], [-0.313, -0.454, 0.786]]\n- duplicate_C (artifact): [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n- fragment_C (too small): [[0.0, 0.0, 0.0], [0.1, 0.0, 0.0]]\n\nExecute the augmentation by calling rotate_molecules once per cohort on the cohort-specific QC-passing list.", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_rotate_molecules\",\"arguments\":{\"mol_coordinates_list\":[[[0.0,0.0,0.0],[0.9572,0.0,0.0],[-0.239,0.927,0.0]],[[0.0,0.0,0.0],[0.6291,0.6291,0.6291],[-0.6291,-0.6291,0.6291],[-0.6291,0.6291,-0.6291],[0.6291,-0.6291,-0.6291]],[[-0.77,0.0,0.0],[0.77,0.0,0.0],[-1.17,0.99,0.0],[-1.17,-0.99,0.0],[1.17,0.99,0.0],[1.17,-0.99,0.0],[-1.17,0.0,1.0],[1.17,0.0,-1.0]]]}} ,{\"name\":\"deepchem_utils_geometry_utils_rotate_molecules\",\"arguments\":{\"mol_coordinates_list\":[[[0.0,0.0,0.0],[0.9572,0.0,0.0],[-0.2396,0.9271,0.0]],[[0.0,0.0,0.0],[0.6291,0.6291,0.6291],[-0.6291,-0.6291,0.6291],[-0.6291,0.6291,-0.6291],[0.6291,-0.6291,-0.6291],[0.0,0.0,0.0]]]}} ,{\"name\":\"deepchem_utils_geometry_utils_rotate_molecules\",\"arguments\":{\"mol_coordinates_list\":[[[0.0,0.0,0.0],[0.957,0.0,0.0],[-0.24,0.927,0.0]],[[0.0,0.0,0.0],[0.629,0.629,0.629],[-0.629,-0.629,0.629],[-0.629,0.629,-0.629],[0.629,-0.629,-0.629]],[[0.0,0.0,0.0],[0.94,0.0,0.0],[-0.313,0.907,0.0],[-0.313,-0.454,0.786]]]}}]"}
{"func_name": "deepchem_utils_geometry_utils_subtract_centroid", "func_desc": "deepchem.utils.geometry_utils.subtract_centroid subtracts a 3-D centroid vector from every atom coordinate in a molecular coordinate array, centering the molecule at the origin for downstream geometric or machine-learning tasks (for example, preparing inputs for models in DeepChem used in drug discovery and computational chemistry).\n    \n    This function performs an in-place, componentwise subtraction of the provided centroid (x, y, z) from each row of the coordinates array. It is intended for preprocessing molecular geometries represented as NumPy arrays, making coordinate sets translation-invariant for algorithms that assume centered inputs. The operation mutates the input coordinates array and also returns it for convenience; no new array is allocated by this function beyond NumPy's in-place update.", "tools": [{"function": {"description": "deepchem.utils.geometry_utils.subtract_centroid subtracts a 3-D centroid vector from every atom coordinate in a molecular coordinate array, centering the molecule at the origin for downstream geometric or machine-learning tasks (for example, preparing inputs for models in DeepChem used in drug discovery and computational chemistry).\n\nThis function performs an in-place, componentwise subtraction of the provided centroid (x, y, z) from each row of the coordinates array. It is intended for preprocessing molecular geometries represented as NumPy arrays, making coordinate sets translation-invariant for algorithms that assume centered inputs. The operation mutates the input coordinates array and also returns it for convenience; no new array is allocated by this function beyond NumPy's in-place update.", "name": "deepchem_utils_geometry_utils_subtract_centroid", "parameters": {"properties": {"coordinates": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of shape (N, 3), where N is the number of atoms in the molecule. Each row is an atom coordinate in 3D space in the order (x, y, z). This array is modified in place: after the call, each row equals the original row minus the centroid. The function expects numeric dtype (e.g., float32/float64) compatible with subtraction; if the array has an incompatible shape or dtype, NumPy broadcasting or arithmetic errors will occur.", "default": ""}, "centroid": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of shape (3,), representing the centroid vector (x, y, z) to subtract from each atom coordinate. The centroid is interpreted componentwise and is broadcast against the coordinates rows. The centroid should be a one-dimensional length-3 numeric array; using a differently shaped array may trigger NumPy broadcasting rules or runtime errors.", "default": ""}}, "required": ["coordinates", "centroid"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a noisy minibatch of candidate conformers for a translation-invariant DeepChem geometry featurizer. Each conformer record contains (i) an atom coordinate array and (ii) a centroid estimate from a separate stage. Apply `deepchem.utils.geometry_utils.subtract_centroid` only to conformers whose coordinate arrays are well-formed 3D Cartesian tensors (every atom row has exactly 3 finite components). Center each eligible conformer in-place using its provided centroid.\n\nRaw conformer records:\n1) ligand_replicate_A: centroid = [1.25, -0.50, 2.00]; coordinates = [[2.10, -0.40, 1.90], [0.80, -1.20, 2.50], [1.50, 0.10, 2.10], [0.60, -0.50, 1.60]]\n2) methane_like_B: centroid = [0.25, -0.10, 0.05]; coordinates = [[0.50, 0.00, 0.10], [0.00, -0.20, 0.00], [0.40, -0.05, 0.15], [0.30, -0.15, -0.05], [0.30, 0.10, 0.00]]\n3) corrupted_export_C: centroid = [0.00, 0.00, 0.00]; coordinates = [[1.0, 2.0], [3.0, 4.0]]\n\nReturn the centered coordinate arrays for the eligible conformers, preserving original atom ordering.", "answers": "[{\"name\":\"deepchem_utils_geometry_utils_subtract_centroid\",\"arguments\":{\"coordinates\":[[2.1,-0.4,1.9],[0.8,-1.2,2.5],[1.5,0.1,2.1],[0.6,-0.5,1.6]],\"centroid\":[1.25,-0.5,2.0]}},{\"name\":\"deepchem_utils_geometry_utils_subtract_centroid\",\"arguments\":{\"coordinates\":[[0.5,0.0,0.1],[0.0,-0.2,0.0],[0.4,-0.05,0.15],[0.3,-0.15,-0.05],[0.3,0.1,0.0]],\"centroid\":[0.25,-0.1,0.05]}}]"}
{"func_name": "deepchem_utils_graph_utils_scale_identity", "func_desc": "deepchem.utils.graph_utils.scale_identity returns the input tensor unchanged and is intended as the \"no-op\" scaling function used in DeepChem graph utilities when node- or graph-level features should not be rescaled. In the DeepChem context (molecular machine learning, graph neural networks for drug discovery, materials science, and biology), this function serves as a placeholder or default scaling strategy in preprocessing or in model forward passes where other scaling functions (which may use node degree information or dataset averages) would normally be applied.", "tools": [{"function": {"description": "deepchem.utils.graph_utils.scale_identity returns the input tensor unchanged and is intended as the \"no-op\" scaling function used in DeepChem graph utilities when node- or graph-level features should not be rescaled. In the DeepChem context (molecular machine learning, graph neural networks for drug discovery, materials science, and biology), this function serves as a placeholder or default scaling strategy in preprocessing or in model forward passes where other scaling functions (which may use node degree information or dataset averages) would normally be applied.\n", "name": "deepchem_utils_graph_utils_scale_identity", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "float"}, "description": "Input tensor containing features to be passed through the scaling step. In graph-based models this is typically a tensor of node features or graph-level features produced by featurization layers. This function returns this same tensor object without modification (no copy), so in-place changes to the returned tensor will also affect the original tensor object.", "default": ""}, "D": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Degree tensor. In DeepChem graph utilities this typically represents per-node degree values used by alternate scaling routines to normalize features by node degree. This parameter is accepted for API compatibility with other scaling functions but is ignored by scale_identity. Default: None.", "default": null}, "avg_d": {"type": "any", "nullable": true, "description": "Dictionary containing averages computed over the training set (for example, average degree statistics or other dataset-level summary statistics). Other scaling implementations may use values from this dictionary to normalize features; scale_identity accepts the parameter for compatibility but does not use it. Default: None.", "default": null}}, "required": ["h", "D", "avg_d"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed-source GNN preprocessing QA pass where the no-op scaler must be exercised on only those graph replicates whose node-degree metadata is internally consistent with the graph topology assumptions. Treat each replicate as a 3-node graph with a per-node feature matrix h. A replicate is eligible for the identity/no-op scaling stage if (i) the node-degree vector D has length equal to the number of rows in h, (ii) every entry in D is a positive integer, and (iii) the provided degree-summary dictionary contains at least one key that indicates a central-tendency statistic (e.g., avg_degree or mean_degree). For every eligible replicate, feed h through deepchem.utils.graph_utils.scale_identity while still supplying D and avg_d for audit logging. Replicates: (1) h=[[0.12,-1.5,3.0,0.0],[2.2,0.33,-0.75,1.1],[-0.9,1.25,0.5,-2.0]] with D=[3,2,1] and avg_d={\"avg_degree\":2.0,\"max_degree\":3}; (2) h=[[0.12,-1.5,3.0],[2.3,0.0,-0.7],[1.1,4.2,0.5]] with D=[2,3,1] and avg_d={\"mean_degree\":2.0,\"std_degree\":0.8}; (3) h=[[0.12,-1.5,3.0],[2.25,0.0,-0.75],[1.1,4.2,0.33]] with D=[2,3,1] and avg_d={\"mean_degree\":2.1,\"max_degree\":5}.", "answers": "[{\"name\":\"deepchem_utils_graph_utils_scale_identity\",\"arguments\":{\"h\":[[0.12,-1.5,3.0,0.0],[2.2,0.33,-0.75,1.1],[-0.9,1.25,0.5,-2.0]],\"D\":[3,2,1],\"avg_d\":{\"avg_degree\":2.0,\"max_degree\":3}}},{\"name\":\"deepchem_utils_graph_utils_scale_identity\",\"arguments\":{\"h\":[[0.12,-1.5,3.0],[2.3,0.0,-0.7],[1.1,4.2,0.5]],\"D\":[2,3,1],\"avg_d\":{\"mean_degree\":2.0,\"std_degree\":0.8}}},{\"name\":\"deepchem_utils_graph_utils_scale_identity\",\"arguments\":{\"h\":[[0.12,-1.5,3.0],[2.25,0.0,-0.75],[1.1,4.2,0.33]],\"D\":[2,3,1],\"avg_d\":{\"mean_degree\":2.1,\"max_degree\":5}}}]"}
{"func_name": "deepchem_utils_hash_utils_hash_ecfp", "func_desc": "deepchem.utils.hash_utils.hash_ecfp returns a deterministic integer index in the range [0, size) computed from an input ECFP fragment string; it is used by DeepChem to fold arbitrary-length ECFP fragment identifiers into fixed-size integer indices for constructing ECFP-based fingerprint vectors used in molecular machine learning and chemoinformatics pipelines.\n    \n    This function encodes the input string using UTF-8, computes an MD5 digest, converts that digest to a base-16 integer, and reduces it modulo the provided size to produce a stable, platform-independent index. It is intended to map ECFP fragment identifiers (typically produced by RDKit or similar cheminformatics tools) into bit positions of a fixed-length fingerprint array so that downstream models in DeepChem can consume consistent, bounded-length feature vectors.", "tools": [{"function": {"description": "deepchem.utils.hash_utils.hash_ecfp returns a deterministic integer index in the range [0, size) computed from an input ECFP fragment string; it is used by DeepChem to fold arbitrary-length ECFP fragment identifiers into fixed-size integer indices for constructing ECFP-based fingerprint vectors used in molecular machine learning and chemoinformatics pipelines.\n\nThis function encodes the input string using UTF-8, computes an MD5 digest, converts that digest to a base-16 integer, and reduces it modulo the provided size to produce a stable, platform-independent index. It is intended to map ECFP fragment identifiers (typically produced by RDKit or similar cheminformatics tools) into bit positions of a fixed-length fingerprint array so that downstream models in DeepChem can consume consistent, bounded-length feature vectors.", "name": "deepchem_utils_hash_utils_hash_ecfp", "parameters": {"properties": {"ecfp": {"type": "string", "description": "The input string to hash. In practice this is usually an ECFP fragment identifier (for example, a string representation of a circular substructure produced when computing Extended-Connectivity Fingerprints with RDKit). This parameter must be a Python string; the function calls ecfp.encode('utf-8') internally so passing non-string types will raise an exception (for example, AttributeError if a bytes object is provided or a TypeError for other incompatible types). The practical role of this parameter is to supply the fragment identity whose presence will be assigned to a fingerprint index.", "default": ""}, "size": {"type": "integer", "description": "The number of distinct hash bins (the length of the fingerprint vector into which the fragment is folded). Default is 1024. This must be a positive integer; if size is zero a ZeroDivisionError will be raised when computing the modulo, and non-integer types may produce a TypeError. In domain terms, size determines the dimensionality of the folded fingerprint used by DeepChem models: larger sizes reduce collisions at the cost of higher memory and model input dimensionality.", "default": 1024}}, "required": ["ecfp", "size"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating ECFP fragment identifiers coming from mixed RDKit export conventions before building a folded fingerprint for a QSAR run. From the following raw fragment strings, fold only those that look like chemically interpretable fragments (i.e., contain at least one atom-like element token such as 'C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', or 'I'). Use a size rule tied to fragment provenance: if the fragment includes an explicit ECFP radius field ('r='), fold it into a 4096-bin space; otherwise fold it into a 2048-bin space. Compute the deterministic bin using the standard UTF-8 → MD5 → base-16 integer → modulo size procedure for each qualifying fragment and report the index.\n\nRaw fragments:\n1) 'C1=CC=CC=C1_AROMATIC_RING'\n2) 'ECFP6:atom=12,r=2,neighbors=[6,6,8],invariants=0x9f3a'", "answers": "[{\"name\":\"deepchem_utils_hash_utils_hash_ecfp\",\"arguments\":{\"ecfp\":\"C1=CC=CC=C1_AROMATIC_RING\",\"size\":2048}},{\"name\":\"deepchem_utils_hash_utils_hash_ecfp\",\"arguments\":{\"ecfp\":\"ECFP6:atom=12,r=2,neighbors=[6,6,8],invariants=0x9f3a\",\"size\":4096}}]"}
{"func_name": "deepchem_utils_hash_utils_hash_ecfp_pair", "func_desc": "Compute a deterministic integer hash in the range [0, size) that represents a pair of ECFP (Extended-Connectivity Fingerprint) fragment strings.\n    \n    This function is used by DeepChem spatial contact featurizers to map a pair of fragment identifiers (for example, a protein fragment and a ligand fragment that are in close spatial contact) to a single integer index. The resulting index can be used as an entry in a fixed-length fingerprint vector or as a bucket identifier in featurization pipelines. The function constructs a single string from the two fragments using the comma separator \"%s,%s\", encodes it with UTF-8, computes the MD5 digest, interprets the digest as a base-16 integer, and reduces it modulo size to produce the final integer. The default size of 1024 corresponds to a common fingerprint length used in DeepChem examples and featurizers.", "tools": [{"function": {"description": "Compute a deterministic integer hash in the range [0, size) that represents a pair of ECFP (Extended-Connectivity Fingerprint) fragment strings.\n\nThis function is used by DeepChem spatial contact featurizers to map a pair of fragment identifiers (for example, a protein fragment and a ligand fragment that are in close spatial contact) to a single integer index. The resulting index can be used as an entry in a fixed-length fingerprint vector or as a bucket identifier in featurization pipelines. The function constructs a single string from the two fragments using the comma separator \"%s,%s\", encodes it with UTF-8, computes the MD5 digest, interprets the digest as a base-16 integer, and reduces it modulo size to produce the final integer. The default size of 1024 corresponds to a common fingerprint length used in DeepChem examples and featurizers.", "name": "deepchem_utils_hash_utils_hash_ecfp_pair", "parameters": {"properties": {"ecfp_pair": {"type": "tuple", "prefixItems": [{"type": "string"}, {"type": "string"}], "description": "Pair of ECFP fragment strings. The first element is typically the protein (or receptor) fragment identifier and the second is typically the ligand (or small-molecule) fragment identifier. Both elements must be Python str objects; the function expects exactly two string fragments and will produce a hash representing their ordered pair (order matters: (A, B) hashes differently than (B, A)). The concatenation is performed as \"%s,%s\" so any commas present in the fragment strings are preserved and affect the hash.", "default": ""}, "size": {"type": "integer", "description": "Total number of hash buckets (the modulus). The function returns an integer in the half-open interval [0, size). Defaults to 1024. size must be a positive integer; the canonical use is to set size equal to the length of a fixed-size fingerprint vector used by downstream models or featurizers.", "default": 1024}}, "required": ["ecfp_pair", "size"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our protein–ligand spatial contact bucketization under heterogeneous hash-space constraints. Given the following ordered ECFP contact pairs:\n1) (\"PROT_frag_125_AromRing_OH\", \"LIG_frag_42_tertButyl_Cl\")\n2) (\"ProtFrag_CYS145:1*2*6\", \"LigFrag_BENZENE:0*3*5\")\n3) (\"PROT_ECFP:r3:atom12-NH\", \"LIG_ECFP:r2:atom7-COO\")\n\nApply a branching rule for the fingerprint length (size): use 4096 when both fragment identifiers contain at least one digit anywhere in the string; otherwise use 2048. For each pair that meets the rule, compute the deterministic bucket index by hashing the comma-joined string as in our DeepChem-style MD5 modulo procedure. Return the resulting bucket indices in the same order as the inputs that qualify under the rule.", "answers": "[{\"name\":\"deepchem_utils_hash_utils_hash_ecfp_pair\",\"arguments\":{\"ecfp_pair\":[\"PROT_frag_125_AromRing_OH\",\"LIG_frag_42_tertButyl_Cl\"],\"size\":4096}},{\"name\":\"deepchem_utils_hash_utils_hash_ecfp_pair\",\"arguments\":{\"ecfp_pair\":[\"ProtFrag_CYS145:1*2*6\",\"LigFrag_BENZENE:0*3*5\"],\"size\":4096}},{\"name\":\"deepchem_utils_hash_utils_hash_ecfp_pair\",\"arguments\":{\"ecfp_pair\":[\"PROT_ECFP:r3:atom12-NH\",\"LIG_ECFP:r2:atom7-COO\"],\"size\":4096}}]"}
{"func_name": "deepchem_utils_noncovalent_utils_is_pi_parallel", "func_desc": "Check whether two aromatic rings form a parallel pi–pi contact used in noncovalent interaction detection for molecular modelling and drug discovery.\n    \n    This function is used in DeepChem to identify parallel stacking between aromatic rings, a common noncovalent interaction relevant to ligand binding, molecular recognition, and conformational analysis. It compares the Euclidean distance between ring centers and the angle between ring normal vectors against user-provided cutoffs to decide whether the rings are in a parallel pi–pi arrangement. The ring center and normal vectors can be produced with helper utilities such as compute_ring_center and compute_ring_normal; the angle is computed via angle_between and converted from radians to degrees.", "tools": [{"function": {"description": "Check whether two aromatic rings form a parallel pi–pi contact used in noncovalent interaction detection for molecular modelling and drug discovery.\n\nThis function is used in DeepChem to identify parallel stacking between aromatic rings, a common noncovalent interaction relevant to ligand binding, molecular recognition, and conformational analysis. It compares the Euclidean distance between ring centers and the angle between ring normal vectors against user-provided cutoffs to decide whether the rings are in a parallel pi–pi arrangement. The ring center and normal vectors can be produced with helper utilities such as compute_ring_center and compute_ring_normal; the angle is computed via angle_between and converted from radians to degrees.", "name": "deepchem_utils_noncovalent_utils_is_pi_parallel", "parameters": {"properties": {"ring1_center": {"type": "array", "items": {"type": "float"}, "description": "Cartesian coordinates of the center of the first aromatic ring. Practical use: typically a 3-element array [x, y, z] in Angstroms computed by compute_ring_center; used as one endpoint for the center-to-center distance test.", "default": ""}, "ring1_normal": {"type": "array", "items": {"type": "float"}, "description": "Normal vector of the first aromatic ring. Practical use: typically a 3-element array representing the ring plane normal (preferably normalized) computed by compute_ring_normal; used to compute the angle between ring planes.", "default": ""}, "ring2_center": {"type": "array", "items": {"type": "float"}, "description": "Cartesian coordinates of the center of the second aromatic ring. Practical use: typically a 3-element array [x, y, z] in Angstroms computed by compute_ring_center; compared to ring1_center to obtain the center-to-center distance.", "default": ""}, "ring2_normal": {"type": "array", "items": {"type": "float"}, "description": "Normal vector of the second aromatic ring. Practical use: typically a 3-element array representing the ring plane normal (preferably normalized) computed by compute_ring_normal; used to compute the angle between ring planes.", "default": ""}, "dist_cutoff": {"type": "float", "description": "Distance cutoff in Angstroms. Max allowed Euclidean distance between ring centers for the rings to be considered interacting. Default is 8.0. In practice this threshold filters out rings that are too far apart to engage in meaningful pi–pi stacking.", "default": 8.0}, "angle_cutoff": {"type": "float", "description": "Angle cutoff in degrees. Max allowed deviation from perfect parallelism (0 degrees) between ring normals. The function treats normals with an angle less than angle_cutoff or greater than 180.0 - angle_cutoff as effectively parallel (accounting for opposite normal directions); default is 30.0 degrees. In practice this allows some tolerance for non-ideal geometries typical in molecular structures.", "default": 30.0}}, "required": ["ring1_center", "ring1_normal", "ring2_center", "ring2_normal", "angle_cutoff", "dist_cutoff"], "type": "any"}}, "type": "function"}], "query": "We’re curating a small benchmark set of aromatic–aromatic contacts from independent protein–ligand docking snapshots where ring planarity/orientation quality is heterogeneous. For each snapshot, first treat the provided ring normal vectors as reliable only if they are near-unit length (within 2% of length 1). Only for those snapshots, evaluate whether the ligand ring and receptor ring form a *parallel* π–π contact using a distance cutoff that adapts to ring proximity: if the ring-center separation is below 4.0 Å, use a stricter angular deviation cutoff of 15°; otherwise (but still within the maximum allowable range for consideration) use 20°. Use a maximum ring-center separation of 6.0 Å for all evaluated snapshots. \n\nSnapshot A: ring1 center [10.2, 5.8, 3.1] with normal [0.0, 0.98, 0.2] vs ring2 center [12.1, 6.4, 3.5] with normal [0.05, 0.96, 0.25].\nSnapshot B: ring1 center [10.2, 5.1, -3.4] with normal [0.0, 0.0, 1.0] vs ring2 center [13.8, 5.6, -3.0] with normal [0.05, -0.02, 0.998].", "answers": "[{\"name\":\"deepchem_utils_noncovalent_utils_is_pi_parallel\",\"arguments\":{\"ring1_center\":[10.2,5.8,3.1],\"ring1_normal\":[0.0,0.98,0.2],\"ring2_center\":[12.1,6.4,3.5],\"ring2_normal\":[0.05,0.96,0.25],\"dist_cutoff\":6.0,\"angle_cutoff\":15.0}},{\"name\":\"deepchem_utils_noncovalent_utils_is_pi_parallel\",\"arguments\":{\"ring1_center\":[10.2,5.1,-3.4],\"ring1_normal\":[0.0,0.0,1.0],\"ring2_center\":[13.8,5.6,-3.0],\"ring2_normal\":[0.05,-0.02,0.998],\"dist_cutoff\":6.0,\"angle_cutoff\":15.0}}]"}
{"func_name": "deepchem_utils_periodic_table_utils_get_atom_mass", "func_desc": "Return the atomic mass for the element with the given atomic number, expressed in atomic units (electron mass units). This function is used in DeepChem's quantum-chemistry and molecular modeling utilities to supply nuclear masses in the unit system expected by certain electronic-structure and differentiable DFT code (for example, code derived from the referenced DQC implementation). It looks up a canonical atomic mass (stored in the module-level atom_masses mapping) and converts that mass from atomic mass units (unified atomic mass unit, u) to atomic units of mass by multiplying by 1822.888486209 (the conversion factor from u to electron masses).", "tools": [{"function": {"description": "Return the atomic mass for the element with the given atomic number, expressed in atomic units (electron mass units). This function is used in DeepChem's quantum-chemistry and molecular modeling utilities to supply nuclear masses in the unit system expected by certain electronic-structure and differentiable DFT code (for example, code derived from the referenced DQC implementation). It looks up a canonical atomic mass (stored in the module-level atom_masses mapping) and converts that mass from atomic mass units (unified atomic mass unit, u) to atomic units of mass by multiplying by 1822.888486209 (the conversion factor from u to electron masses).\n", "name": "deepchem_utils_periodic_table_utils_get_atom_mass", "parameters": {"properties": {"atom_z": {"type": "integer", "description": "Atomic Number of the element to query. This integer is used as the key into the module-level atom_masses mapping, which contains canonical atomic masses expressed in unified atomic mass units (u). The value must correspond to an element documented in that mapping; passing an integer not present in the mapping will cause a KeyError. The parameter has no default and there are no side effects from providing a valid atom_z.", "default": ""}}, "required": ["atom_z"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a nucleus-mass cache (electron-mass atomic units) for a mixed DeepChem differentiable-DFT benchmark where the element list comes from messy upstream metadata. Start from these reported atomic numbers (some are placeholders, some are isotopic tags, some are data-entry errors): [15, 11, 0, -3, 999, 1, 8, 16, 118]. Populate nuclear masses only for entries that correspond to chemically valid elements in the standard periodic table (Z in 1–118). For duplicates, keep them as separate entries (they correspond to different conformers in the batch).", "answers": "[{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":15}},{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":11}},{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":1}},{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":8}},{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":16}},{\"name\":\"deepchem_utils_periodic_table_utils_get_atom_mass\",\"arguments\":{\"atom_z\":118}}]"}
{"func_name": "deepchem_utils_periodic_table_utils_get_period", "func_desc": "get_period returns the chemical period (row) in the periodic table for a given atomic number.\n    \n    This function maps a nuclear charge (atomic number) to the corresponding period index used in periodic-table based features and analyses. In DeepChem this is used when converting elemental identity into simple periodic descriptors for tasks in drug discovery, materials science, quantum chemistry, and molecular machine learning (for example, featurizers or models that rely on period-based grouping of elements). The implementation uses fixed atomic-number cutoffs that correspond to the standard periods 1 through 7 of the modern periodic table.", "tools": [{"function": {"description": "get_period returns the chemical period (row) in the periodic table for a given atomic number.\n\nThis function maps a nuclear charge (atomic number) to the corresponding period index used in periodic-table based features and analyses. In DeepChem this is used when converting elemental identity into simple periodic descriptors for tasks in drug discovery, materials science, quantum chemistry, and molecular machine learning (for example, featurizers or models that rely on period-based grouping of elements). The implementation uses fixed atomic-number cutoffs that correspond to the standard periods 1 through 7 of the modern periodic table.", "name": "deepchem_utils_periodic_table_utils_get_period", "parameters": {"properties": {"atom_z": {"type": "integer", "description": "Atomic number (Z) of the element. This integer represents the number of protons in the nucleus and is the canonical identifier for an element in chemistry and materials science. The function expects a (positive) integer atomic number as used in DeepChem datasets and featurizers. The code maps ranges of atomic numbers to period indices; values less than or equal to 2 are mapped to period 1, values greater than 2 and less than or equal to 10 to period 2, and so on (see Returns section for exact cutoffs). Non-physical or out-of-range atomic numbers (see Failure modes) are handled as described below.", "default": ""}}, "required": ["atom_z"], "type": "any"}}, "type": "function"}], "query": "We’re auditing a period-index feature used in two preprocessing replicates for a metallorganic screening set. Each replicate emits an element list that may include corrupted atomic numbers from OCR (negative values, non-integers, or out-of-range Z) and duplicate entries from salt/solvent fragments. For each replicate, retain only entries that are valid integer nuclear charges in the modern periodic table range (1–118). Then, for each retained Z, compute the periodic-table period (row) index using the standard 1–7 cutoff scheme and report the period values for each replicate in the same order as the retained Z list.\n\nReplicate A raw Z stream: [26, 0, 118, 119, 7, 7, -3, 2.0, 3.5]\nReplicate B raw Z stream: [26, 57, 58, 1, 999, 16, 16, 4, -1]", "answers": "[{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":26}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":118}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":7}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":7}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":2}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":26}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":57}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":58}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":1}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":16}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":16}},{\"name\":\"deepchem_utils_periodic_table_utils_get_period\",\"arguments\":{\"atom_z\":4}}]"}
{"func_name": "deepchem_utils_rdkit_utils_merge_molecules_xyz", "func_desc": "Merges coordinates of multiple molecules into a single contiguous coordinate matrix.\n    \n    This function is a small utility used by DeepChem's RDKit integration and featurization pipelines to combine per-molecule atomic coordinate arrays into one consolidated array suitable for downstream tasks such as constructing molecular graphs, batching molecules for model input, or performing geometry-based descriptors and quantum-chemistry workflows. It takes a Python list of per-molecule coordinate arrays (each describing the 3D positions of atoms in one molecule) and returns a single NumPy array containing all atom coordinates stacked along the first (row) dimension.", "tools": [{"function": {"description": "Merges coordinates of multiple molecules into a single contiguous coordinate matrix.\n\nThis function is a small utility used by DeepChem's RDKit integration and featurization pipelines to combine per-molecule atomic coordinate arrays into one consolidated array suitable for downstream tasks such as constructing molecular graphs, batching molecules for model input, or performing geometry-based descriptors and quantum-chemistry workflows. It takes a Python list of per-molecule coordinate arrays (each describing the 3D positions of atoms in one molecule) and returns a single NumPy array containing all atom coordinates stacked along the first (row) dimension.", "name": "deepchem_utils_rdkit_utils_merge_molecules_xyz", "parameters": {"properties": {"xyzs": {"type": "array", "items": {"type": "float"}, "description": "List of numpy arrays, each array describing atomic coordinates for one molecule. Each element must be a NumPy array with shape (N_i, 3) where N_i is the number of atoms in the i-th molecule and the second dimension of size 3 represents the (x, y, z) coordinates. In practical DeepChem usage, these arrays are produced by RDKit-based molecule-to-geometry conversions or by other coordinate-extraction utilities; the function expects these per-molecule arrays to already be in units and ordering consistent with the caller's workflow.", "default": ""}}, "required": ["xyzs"], "type": "any"}}, "type": "function"}], "query": "I’m batching RDKit-derived conformer coordinate blocks from a noisy geometry capture step into DeepChem. Each capture contains two molecules (A then B), but some captures are already in a common lab-frame while others are displaced (e.g., due to a different docking box origin). Consolidate only the captures where Molecule B is already in the same frame as Molecule A, operationally defined as: the Euclidean distance between the first atom of A and the first atom of B is <= 0.5 Å. For each qualifying capture, merge the per-molecule (n_i,3) arrays into a single contiguous (N,3) coordinate matrix by stacking atoms in order, molecule-by-molecule.\n\nCaptures:\n- Capture 1: Molecule A xyz [[0.0, 0.0, 0.0], [1.089, 0.0, 0.0], [-0.363, 1.026, 0.0]]; Molecule B xyz [[0.0, 0.0, 0.0], [0.958, 0.0, 0.0], [-0.239, 0.927, 0.0], [-0.239, -0.927, 0.0]]\n- Capture 2: Molecule A xyz [[0.0, 0.0, 0.0], [1.089, 0.0, 0.0], [-0.363, 1.026, 0.0]]; Molecule B xyz [[2.5, 0.1, -0.2], [3.6, 0.1, -0.2], [2.12, 1.06, -0.2], [2.12, -0.86, -0.2]]", "answers": "[{\"name\":\"deepchem_utils_rdkit_utils_merge_molecules_xyz\",\"arguments\":{\"xyzs\":[[[0.0,0.0,0.0],[1.089,0.0,0.0],[-0.363,1.026,0.0]],[[0.0,0.0,0.0],[0.958,0.0,0.0],[-0.239,0.927,0.0],[-0.239,-0.927,0.0]]]}}]"}
{"func_name": "dscribe_descriptors_mbtr_check_geometry", "func_desc": "Used to validate MBTR geometry settings before computing Many-Body Tensor Representation (MBTR) descriptors.\n    \n    This function checks that the provided geometry configuration dictionary contains a \"function\" key and that its value is one of the allowed geometry functions used by MBTR k-body terms. In the context of the DScribe library, MBTR (Many-body Tensor Representation) converts atomic structures into fixed-size numerical fingerprints for machine learning and similarity analysis in materials science. Validating the geometry function here ensures that the MBTR descriptor will compute the intended k-body contribution (k = 1, 2, or 3) and prevents silent misconfiguration that would produce incorrect descriptors or runtime errors later in the descriptor pipeline.", "tools": [{"function": {"description": "Used to validate MBTR geometry settings before computing Many-Body Tensor Representation (MBTR) descriptors.\n\nThis function checks that the provided geometry configuration dictionary contains a \"function\" key and that its value is one of the allowed geometry functions used by MBTR k-body terms. In the context of the DScribe library, MBTR (Many-body Tensor Representation) converts atomic structures into fixed-size numerical fingerprints for machine learning and similarity analysis in materials science. Validating the geometry function here ensures that the MBTR descriptor will compute the intended k-body contribution (k = 1, 2, or 3) and prevents silent misconfiguration that would produce incorrect descriptors or runtime errors later in the descriptor pipeline.", "name": "dscribe_descriptors_mbtr_check_geometry", "parameters": {"properties": {"geometry": {"type": "any", "description": "Dictionary containing the geometry setup for MBTR. The dictionary must include the key \"function\" whose value is a string (or other hashable identifier) naming the geometry function to use. The set of accepted names is the union of k1_geometry_functions, k2_geometry_functions, and k3_geometry_functions defined in the mbtr implementation; these represent the valid k = 1, k = 2, and k = 3 geometry functions respectively. This parameter is required and typically comes from the MBTR descriptor constructor or user configuration that specifies how distances, angles, or other geometric quantities are mapped into the MBTR spectrum.", "default": ""}}, "required": ["geometry"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating k=2 MBTR settings coming from two upstream sources (MOF CIF ingests and periodic crystal workflows), and the geometry dicts are inconsistent. Run a preflight sieve over the following candidate pair-geometry configurations and validate only those that actually declare a k=2-compatible pairwise geometry function in the required schema (i.e., the dict contains a usable \"function\" field recognized by MBTR pair terms). Candidate geometries: (1) {\"function\": \"distance\"}, (2) {\"function\": \"inverse_distance\", \"periodic\": true, \"cutoff\": 6.5}, (3) {\"function\": \"angle\"}, (4) {\"periodic\": true, \"cutoff\": 5.0}, (5) {\"function\": \"inverse_distance\", \"cutoff\": -1.0}. For each geometry that passes the schema criterion, run the validator as-is to confirm the geometry function is allowed for MBTR k=2 before we enqueue descriptor jobs.", "answers": "[{\"name\":\"dscribe_descriptors_mbtr_check_geometry\",\"arguments\":{\"geometry\":{\"function\":\"distance\"}}},{\"name\":\"dscribe_descriptors_mbtr_check_geometry\",\"arguments\":{\"geometry\":{\"function\":\"inverse_distance\",\"periodic\":true,\"cutoff\":6.5}}},{\"name\":\"dscribe_descriptors_mbtr_check_geometry\",\"arguments\":{\"geometry\":{\"function\":\"angle\"}}},{\"name\":\"dscribe_descriptors_mbtr_check_geometry\",\"arguments\":{\"geometry\":{\"function\":\"inverse_distance\",\"cutoff\":-1.0}}}]"}
{"func_name": "dscribe_descriptors_mbtr_check_grid", "func_desc": "dscribe.descriptors.mbtr.check_grid validates MBTR grid settings and enforces basic consistency rules used by the Many-Body Tensor Representation (MBTR) descriptor in DScribe. This function is used before constructing MBTR fingerprints (fixed-size numerical descriptors for atomic structures) to ensure the provided grid dictionary contains the required entries that define the discretization range and resolution for the descriptor.", "tools": [{"function": {"description": "dscribe.descriptors.mbtr.check_grid validates MBTR grid settings and enforces basic consistency rules used by the Many-Body Tensor Representation (MBTR) descriptor in DScribe. This function is used before constructing MBTR fingerprints (fixed-size numerical descriptors for atomic structures) to ensure the provided grid dictionary contains the required entries that define the discretization range and resolution for the descriptor.\n", "name": "dscribe_descriptors_mbtr_check_grid", "parameters": {"properties": {"grid": {"type": "any", "description": "Dictionary containing the grid setup required by the MBTR descriptor. The dictionary must contain the keys \"min\", \"max\", \"sigma\", and \"n\". \"min\" and \"max\" represent the lower and upper bounds of the grid range used to discretize a continuous geometric or chemical quantity into a fixed-size vector; they must be comparable numeric values and satisfy min < max. \"sigma\" is expected to be a numeric broadening/width parameter associated with the grid (keeps its value unchanged by this function). \"n\" is the number of grid points (resolution) and will be converted in-place to an integer via int(grid[\"n\"]); therefore \"n\" may be a numeric type or a string/number convertible to int. This function mutates the provided dictionary by replacing the original \"n\" value with its integer conversion.", "default": ""}}, "required": ["grid"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating k=2 (pairwise distance) MBTR grid definitions coming from three upstream cohorts where unit conventions and export quirks differ. For each cohort’s raw grid dict below, run the MBTR grid preflight only if it represents a physically meaningful distance discretization (strictly positive range with max > min) and has a resolvable positive broadening. Before validation, harmonize the grid as follows: interpret any min/max provided in nm by converting to Å; if n is provided as a float but is numerically integer-valued, cast it to an int; if sigma is given in pm, convert to Å. Then validate the resulting grid dictionaries.\n\nRaw cohort grids:\n1) Small organic molecule test export (Å): {min: 0.5, max: 6.0, sigma: 0.1, n: 200}\n2) Silicon crystal benchmark export (nm, float n): {min: 0.05, max: 0.50, sigma: 0.01, n: 200.0}\n3) Large silicon-structure batch export (Å, sigma in pm): {min: 0.5, max: 5.0, sigma: 10, n: 200}", "answers": "[{\"name\":\"dscribe_descriptors_mbtr_check_grid\",\"arguments\":{\"grid\":{\"min\":0.5,\"max\":6.0,\"sigma\":0.1,\"n\":200}}},{\"name\":\"dscribe_descriptors_mbtr_check_grid\",\"arguments\":{\"grid\":{\"min\":0.5,\"max\":5.0,\"sigma\":0.1,\"n\":200}}},{\"name\":\"dscribe_descriptors_mbtr_check_grid\",\"arguments\":{\"grid\":{\"min\":0.5,\"max\":5.0,\"sigma\":0.1,\"n\":200}}}]"}
{"func_name": "gpaw_basis_data_parse_basis_name", "func_desc": "Parse a GPAW basis type identifier string and return the numeric zeta and\n    polarization counts encoded in that identifier.\n    \n    This function is used by GPAW when interpreting atom-centered basis-function\n    specifications (see GPAW README references to \"atom-centered basis-functions\").\n    Basis identifiers encode how many radial basis functions (\"zeta\" functions)\n    and how many additional polarization functions to include. Typical identifiers\n    look like 'sz', 'dzp', 'qztp', or '4z3p'. The first character encodes the zeta\n    count (either a decimal digit or a letter looked up in the module-level\n    mapping _basis_letter2number). The second character must be the literal 'z'.\n    The identifier optionally contains a polarization part: either absent (no\n    polarization), a trailing 'p' (one polarization), or a third character (letter\n    or digit) followed by 'p' (explicit polarization count).", "tools": [{"function": {"description": "Parse a GPAW basis type identifier string and return the numeric zeta and\npolarization counts encoded in that identifier.\n\nThis function is used by GPAW when interpreting atom-centered basis-function\nspecifications (see GPAW README references to \"atom-centered basis-functions\").\nBasis identifiers encode how many radial basis functions (\"zeta\" functions)\nand how many additional polarization functions to include. Typical identifiers\nlook like 'sz', 'dzp', 'qztp', or '4z3p'. The first character encodes the zeta\ncount (either a decimal digit or a letter looked up in the module-level\nmapping _basis_letter2number). The second character must be the literal 'z'.\nThe identifier optionally contains a polarization part: either absent (no\npolarization), a trailing 'p' (one polarization), or a third character (letter\nor digit) followed by 'p' (explicit polarization count).", "name": "gpaw_basis_data_parse_basis_name", "parameters": {"properties": {"name": {"type": "string", "description": "Basis identifier string to parse. Accepted forms are:\n- Two characters: \"<z><z>\" where the second character is 'z' and the\n  first character is either a decimal digit (e.g. '4') or a letter\n  looked up in _basis_letter2number (e.g. 'd'). This two-character\n  form denotes zero polarization functions (polcount == 0).\n- Three characters: \"<z>zp\" where the trailing 'p' indicates exactly\n  one polarization function (polcount == 1). The first character is\n  interpreted as above.\n- Four characters: \"<z><p>p\" where the third character encodes the\n  polarization count (either a digit or a letter looked up in\n  _basis_letter2number) and the fourth character is 'p'.\nThe function does not modify the input string. The string must be\nnon-empty and conform to the patterns above; otherwise the function\nraises an exception (see Raises). Letter-to-integer conversion uses\nthe module-level mapping _basis_letter2number; if a letter is not\npresent in that mapping the function will attempt to parse that\ncharacter as a decimal digit.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed batch of GPAW atom-centered basis identifiers coming from multiple collaborator templates where some strings may be malformed or refer to nonstandard notations. From the following raw identifiers:\n\n['qztp', 'dzp', '4z3p', '1zp', '0z', 'zzp', '5z0p', '3z2p', 'sz', '7z']\n\nRun the parser only on identifiers that match the canonical grammar used in our input deck: a leading zeta specifier (single decimal digit 1–9, or a valid basis letter), followed immediately by 'z', and optionally a polarization segment that is either exactly 'p' or an explicit polarization count (single digit or valid basis letter) followed by 'p'. As part of the same batch, include a reproducibility replicate by re-parsing whichever identifier in this list contains both a non-digit zeta specifier and an explicit (non-implicit) polarization count.", "answers": "[{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"qztp\"}},{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"dzp\"}},{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"4z3p\"}},{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"3z2p\"}},{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"sz\"}},{\"name\":\"gpaw_basis_data_parse_basis_name\",\"arguments\":{\"name\":\"qztp\"}}]"}
{"func_name": "gpaw_benchmark_parse_mem", "func_desc": "Parse a human-readable memory size string and convert it to a number of bytes using binary (base-2) multipliers.\n    \n    This function is used in GPAW benchmarking and configuration code to interpret short memory specifications such as those found in job configuration, command-line options, or benchmark descriptions. The input string must end with one of the exact uppercase suffixes 'G', 'M', or 'K' to indicate gibibytes, mebibytes, or kibibytes respectively. The numeric portion preceding the suffix is parsed and multiplied by the corresponding binary multiplier (G -> 1024**3, M -> 1024**2, K -> 1024**1). The result is returned as a float representing the number of bytes; this numeric byte value can be used for memory allocation decisions, resource reporting, or performance comparisons in the GPAW DFT benchmarking and runtime configuration contexts.", "tools": [{"function": {"description": "Parse a human-readable memory size string and convert it to a number of bytes using binary (base-2) multipliers.\n\nThis function is used in GPAW benchmarking and configuration code to interpret short memory specifications such as those found in job configuration, command-line options, or benchmark descriptions. The input string must end with one of the exact uppercase suffixes 'G', 'M', or 'K' to indicate gibibytes, mebibytes, or kibibytes respectively. The numeric portion preceding the suffix is parsed and multiplied by the corresponding binary multiplier (G -> 1024**3, M -> 1024**2, K -> 1024**1). The result is returned as a float representing the number of bytes; this numeric byte value can be used for memory allocation decisions, resource reporting, or performance comparisons in the GPAW DFT benchmarking and runtime configuration contexts.", "name": "gpaw_benchmark_parse_mem", "parameters": {"properties": {"memstr": {"type": "string", "description": "Memory string to parse. Must be a non-empty string whose last character is exactly one of 'G', 'M', or 'K' (uppercase). The characters before the final suffix must represent a decimal number (for example '2G', '512M', '64K', '1.5G'). The suffix indicates binary units (1K = 1024 bytes). This parameter is taken verbatim and not normalized by the function, so using lowercase suffixes or omitting the suffix will produce an error.", "default": ""}}, "required": ["memstr"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed set of per-MPI-rank memory caps extracted from several GPAW benchmark manifests. Each entry is a short human string that may be in GiB/MiB/KiB, and only those with a valid binary unit suffix (exactly one of 'G', 'M', 'K') should be standardized to bytes for the scheduler report. Apply a cohort rule based on the unit: treat any cap expressed in GiB as a primary allocation limit (parse it as-is), while caps expressed in MiB are used only for lightweight diagnostics and should still be parsed to bytes for comparison. From this raw list: ['1.5G', '3.5G', '768M', '1.5G', '2048K', '2g', '500MB', '0.5T'], compute byte values only for the entries that satisfy the unit requirements and are relevant under the unit-based cohort rule, preserving their original order of appearance in the list.", "answers": "[{\"name\":\"gpaw_benchmark_parse_mem\",\"arguments\":{\"memstr\":\"1.5G\"}},{\"name\":\"gpaw_benchmark_parse_mem\",\"arguments\":{\"memstr\":\"3.5G\"}},{\"name\":\"gpaw_benchmark_parse_mem\",\"arguments\":{\"memstr\":\"768M\"}},{\"name\":\"gpaw_benchmark_parse_mem\",\"arguments\":{\"memstr\":\"1.5G\"}},{\"name\":\"gpaw_benchmark_parse_mem\",\"arguments\":{\"memstr\":\"2048K\"}}]"}
{"func_name": "gpaw_benchmark_parse_range", "func_desc": "Parse a CPU or GPU range string used by gpaw.benchmark utilities and return the numeric lower and upper bounds.\n    \n    This function is used in GPAW benchmarking/selection code to interpret simple range expressions that specify which CPU or GPU indices should be used for a benchmark run. The input is a short string that may contain the substring \"GPU\" (case-sensitive) and a hyphen-separated range. Behavior follows the exact rules implemented in the source: the literal substring \"GPU\" is removed, a single integer denotes an exact index (both bounds equal), a missing lower bound (a leading hyphen) implies 0, and a missing upper bound (a trailing hyphen) implies an unbounded upper limit represented by numpy.inf. There are no side effects; the function only parses and returns numeric bounds. Examples and their results: \"0-1\" -> (0, 1), \"5\" -> (5, 5), \"-4GPU\" -> (0, 4).", "tools": [{"function": {"description": "Parse a CPU or GPU range string used by gpaw.benchmark utilities and return the numeric lower and upper bounds.\n\nThis function is used in GPAW benchmarking/selection code to interpret simple range expressions that specify which CPU or GPU indices should be used for a benchmark run. The input is a short string that may contain the substring \"GPU\" (case-sensitive) and a hyphen-separated range. Behavior follows the exact rules implemented in the source: the literal substring \"GPU\" is removed, a single integer denotes an exact index (both bounds equal), a missing lower bound (a leading hyphen) implies 0, and a missing upper bound (a trailing hyphen) implies an unbounded upper limit represented by numpy.inf. There are no side effects; the function only parses and returns numeric bounds. Examples and their results: \"0-1\" -> (0, 1), \"5\" -> (5, 5), \"-4GPU\" -> (0, 4).", "name": "gpaw_benchmark_parse_range", "parameters": {"properties": {"s": {"type": "string", "description": "Range string to parse. This should be a Python string containing an optional literal substring \"GPU\" (uppercase exact match) and either a single integer or two integers separated by a single hyphen. Valid example forms coming from benchmarking commands in GPAW include \"0-1\", \"5\", and \"-4GPU\". The function will remove all occurrences of the exact substring \"GPU\" before parsing. If s contains more than one hyphen (for example \"0-1-2\") the function will raise a ValueError due to unpacking; if the numeric parts cannot be converted to integers (for example non-numeric characters other than the removed \"GPU\"), int() will raise a ValueError. Passing a non-string value for s may raise a TypeError before parsing.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "We’re curating device-selection metadata from a mixed benchmark campaign where operators sometimes log CPU indices and sometimes append the literal substring \"GPU\". From the following raw selection tokens:\n\n['2-7GPU', '2-7GPU', '2-GPU', '-4GPU', '5GPU', '0-1', 'GPU3', '7-2GPU', 'GPU', '--2GPU']\n\nInterpret each token with the exact gpaw.benchmark range-string rules (remove the literal substring \"GPU\", then parse hyphen-separated bounds with a leading hyphen implying lower bound 0 and a trailing hyphen implying an unbounded upper limit represented by numpy.inf). Only keep tokens that are syntactically valid after stripping and yield a non-decreasing numeric interval (lower <= upper). For each retained token, return the numeric lower and upper bounds.", "answers": "[{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"2-7GPU\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"2-7GPU\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"2-GPU\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"-4GPU\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"5GPU\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"0-1\"}},{\"name\":\"gpaw_benchmark_parse_range\",\"arguments\":{\"s\":\"GPU3\"}}]"}
{"func_name": "gpaw_bztools_get_lattice_symmetry", "func_desc": "Return a gpaw.symmetry object that describes the lattice (space) symmetry of a unit cell used in GPAW periodic calculations.", "tools": [{"function": {"description": "Return a gpaw.symmetry object that describes the lattice (space) symmetry of a unit cell used in GPAW periodic calculations.\n", "name": "gpaw_bztools_get_lattice_symmetry", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "float"}, "description": "Array describing the unit cell lattice vectors for the system. This is the same unit-cell representation used throughout GPAW and ASE to define the simulation cell; the array encodes the lattice vectors in Cartesian coordinates and is passed directly to the internal gpaw.symmetry.Symmetry constructor. The function uses this array to determine which lattice symmetry operations (rotations, reflections and equivalent lattice translations) leave the periodic lattice invariant.", "default": ""}, "tolerance": {"type": "float", "description": "Numerical tolerance used when comparing coordinates and lattice vectors to decide whether a candidate symmetry operation is valid. This tolerance controls the threshold for floating-point comparisons inside the symmetry-detection algorithm; the default is 1e-07. Smaller tolerances require closer numerical agreement and may reject operations due to numerical noise, while larger tolerances may accept operations that differ by small but potentially significant numerical errors.", "default": 1e-07}}, "required": ["cell_cv", "tolerance"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a small library of candidate *cubic* reference cells for a GPAW Brillouin-zone symmetry regression test. Each candidate comes from an upstream geometry optimizer that sometimes outputs slightly non-orthogonal or non-cubic artifacts. For each candidate cell below, first decide whether it qualifies as “numerically cubic” under a relaxed lattice sanity check: all three lattice vectors must have equal length within a relative mismatch of 1e-5, and the inter-vector angles must be 90° within an absolute deviation of 1e-4 degrees. Only for cells that pass this cubic sanity check, generate the GPAW lattice (space) symmetry object using a symmetry-detection tolerance of 1e-6.\n\nCandidate cells (cell_cv, in Å):\n1) [[5.43, 0.0, 0.0], [0.0, 5.43, 0.0], [0.0, 0.0, 5.43]]\n2) [[3.9, 0.0, 0.0], [0.0, 3.9, 0.0], [0.0, 0.0, 3.9]]\n3) [[3.9, 0.0, 0.0], [0.0, 3.90002, 0.0], [0.0, 0.0, 3.9]]\n4) [[5.43, 0.0, 0.0], [0.0, 5.43, 0.0], [1e-3, 0.0, 5.43]]\n\nReturn the symmetry objects for the qualifying cubic cells, in the same order as the input candidates (skipping any that fail the sanity check).", "answers": "[{\"name\":\"gpaw_bztools_get_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[5.43,0.0,0.0],[0.0,5.43,0.0],[0.0,0.0,5.43]],\"tolerance\":1e-06}},{\"name\":\"gpaw_bztools_get_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.9,0.0,0.0],[0.0,3.9,0.0],[0.0,0.0,3.9]],\"tolerance\":1e-06}},{\"name\":\"gpaw_bztools_get_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.9,0.0,0.0],[0.0,3.90002,0.0],[0.0,0.0,3.9]],\"tolerance\":1e-06}}]"}
{"func_name": "gpaw_bztools_unfold_points", "func_desc": "Unfold k-points using a set of symmetry operators to produce the full set of symmetry-equivalent k-points for Brillouin-zone sampling and band-structure unfolding in GPAW DFT calculations.", "tools": [{"function": {"description": "Unfold k-points using a set of symmetry operators to produce the full set of symmetry-equivalent k-points for Brillouin-zone sampling and band-structure unfolding in GPAW DFT calculations.\n", "name": "gpaw_bztools_unfold_points", "parameters": {"properties": {"points": {"type": "array", "items": {"type": "float"}, "description": "Array of input k-point vectors to be unfolded. Each row is a 3-component k-point vector in reciprocal-space coordinates (typical shape (n_points, 3)). In the GPAW context these are the k-points (wavevector coordinates) for which one wants to generate symmetry-equivalent points when exploiting crystal symmetries for Brillouin-zone sampling or band unfolding.", "default": ""}, "U_scc": {"type": "array", "items": {"type": "float"}, "description": "Array of symmetry operators applied to the k-points. Each element is a linear operator (typically a 3x3 matrix) that maps k-point vectors according to a crystal symmetry operation. The function applies these operators to the rows of points, concatenates the results, and thus expands an irreducible set of k-points to the full symmetry-generated set. The array must be shape-compatible with points for the matrix multiplication performed with numpy.dot (for example, a common layout is one operator per first axis and 3x3 matrices on the last two axes).", "default": ""}, "tol": {"type": "float", "description": "Numerical tolerance used to decide when two k-points are considered identical and should be merged. Default is 1e-08. This tolerance controls the floating-point equality comparison performed by unique_rows: points whose coordinates differ by less than tol (taking into account optional periodicity when mod is set) are treated as duplicates and only one representative is kept. Very small tol may keep near-duplicate points due to numerical noise; very large tol may incorrectly merge distinct k-points.", "default": 1e-08}, "mod": {"type": "integer", "nullable": true, "description": "Either 1 or None. When set to 1, k-points that differ by an integer reciprocal-lattice vector are considered identical (periodicity in reciprocal space is applied before comparing with tol). When None (default) periodic identification is disabled and only direct coordinate comparison within tol is used. The function forwards this argument to unique_rows; passing values other than 1 or None is not supported and may raise an error from unique_rows or produce incorrect results.", "default": null}}, "required": ["points", "U_scc", "mod", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re validating a Brillouin-zone unfolding stage in a GPAW band-unfolding workflow where upstream k-point extraction is noisy. Two irreducible k-point cohorts (fractional reciprocal coords) are provided, but the input may include duplicated lines, near-duplicates within floating noise, and points that are outside the first reciprocal unit cube due to missing k→k+G reduction.\n\nPipeline requirement: for each cohort, first standardize the raw k-point list by wrapping each coordinate modulo 1 (k→k mod 1, component-wise) and then keep only unique points under a 1e-6 numerical tolerance after wrapping. Then perform symmetry unfolding using the provided symmetry operators and finally deduplicate the unfolded set again under the same modulo-1 + 1e-6 uniqueness rule.\n\nCohort A (fcc/cubic sanity check, permutation-only subgroup): the raw irreducible list is [(0,0,0), (0.25,0.25,0.25), (1.0, 0.0, 0.0), (0.2500000004, 0.2499999996, 0.25)]. Unfold using exactly these four cubic permutation operators:\nI=[[1,0,0],[0,1,0],[0,0,1]], swap x↔y=[[0,1,0],[1,0,0],[0,0,1]], cycle (x,z) with y fixed=[[0,0,1],[0,1,0],[1,0,0]], and swap y↔z=[[1,0,0],[0,0,1],[0,1,0]].\n\nCohort B (rotation series about z): the raw irreducible list is [(0,0,0), (0.25,0,0), (-0.75, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, -1.0, 0.0)]. Unfold using identity plus 90°/180°/270° rotations around z with matrices [[1,0,0],[0,1,0],[0,0,1]], [[0,-1,0],[1,0,0],[0,0,1]], [[-1,0,0],[0,-1,0],[0,0,1]], [[0,1,0],[-1,0,0],[0,0,1]].\n\nReturn the final symmetry-expanded, modulo-1 wrapped, tolerance-uniqued k-point sets for each cohort.", "answers": "[{\"name\":\"gpaw_bztools_unfold_points\",\"arguments\":{\"points\":[[0.0,0.0,0.0],[0.25,0.25,0.25],[1.0,0.0,0.0],[0.2500000004,0.2499999996,0.25]],\"U_scc\":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[0.0,0.0,1.0],[0.0,1.0,0.0],[1.0,0.0,0.0]],[[1.0,0.0,0.0],[0.0,0.0,1.0],[0.0,1.0,0.0]]],\"tol\":1e-06,\"mod\":1}},{\"name\":\"gpaw_bztools_unfold_points\",\"arguments\":{\"points\":[[0.0,0.0,0.0],[0.25,0.0,0.0],[-0.75,0.0,0.0],[0.0,1.0,0.0],[0.0,-1.0,0.0]],\"U_scc\":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[-1.0,0.0,0.0],[0.0,-1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[-1.0,0.0,0.0],[0.0,0.0,1.0]]],\"tol\":1e-06,\"mod\":1}}]"}
{"func_name": "gpaw_bztools_unique_rows", "func_desc": "gpaw.bztools.unique_rows returns the unique rows of a 2D NumPy array, treating rows as k-point vectors in GPAW's Brillouin-zone utilities. It is used to reduce lists of k-points by identifying rows that represent the same point within a given numerical tolerance, optionally taking periodicity (full reciprocal-lattice-vector equivalence) and local clustering into account. The function preserves the original row values in the returned array but orders the unique rows according to a lexicographic sort applied after internal normalization and rounding.", "tools": [{"function": {"description": "gpaw.bztools.unique_rows returns the unique rows of a 2D NumPy array, treating rows as k-point vectors in GPAW's Brillouin-zone utilities. It is used to reduce lists of k-points by identifying rows that represent the same point within a given numerical tolerance, optionally taking periodicity (full reciprocal-lattice-vector equivalence) and local clustering into account. The function preserves the original row values in the returned array but orders the unique rows according to a lexicographic sort applied after internal normalization and rounding.\n", "name": "gpaw_bztools_unique_rows", "parameters": {"properties": {"ain": {"type": "array", "items": {"type": "float"}, "description": "2D ndarray of shape (N, M) containing N row vectors (for example, k-point coordinates in fractional reciprocal-lattice units) to be filtered for uniqueness. The function expects a true NumPy array with two dimensions; passing an array with a different number of dimensions or a non-array object will cause an error (TypeError or IndexError) when the code attempts column-wise operations.", "default": ""}, "tol": {"type": "float", "description": "Tolerance used to decide when two rows are considered identical. The tolerance controls the number of decimal places used for rounding via rounds = -numpy.log10(tol).astype(int). Default is 1e-10. tol must be positive; nonpositive values will lead to invalid logarithms and raise an error (ValueError or RuntimeWarning).", "default": 1e-10}, "mod": {"type": "integer", "nullable": true, "description": "Integer 1 or None. If set to 1, the function treats points that differ by a full reciprocal-lattice vector (i.e., integer shifts of 1 in fractional coordinates) as identical by applying a modulus operation before and after rounding. If None (the default), no periodic-identification modulo a reciprocal lattice vector is applied. This parameter is intended for the common GPAW use-case of identifying k-points modulo the periodicity of the Brillouin zone.", "default": null}, "aglomerate": {"type": "boolean", "description": "If True (default), the function calls aglomerate_points(a, tol) on the normalized coordinates before rounding; this agglomeration clusters nearby points (within tol) so that tightly spaced groups are treated consistently when rounding and modulus operations are performed. If False, clustering is skipped and rows are compared directly after rounding. Agglomeration is useful to avoid splitting clusters across modulus boundaries or rounding thresholds when identifying unique k-points.", "default": true}}, "required": ["ain", "aglomerate", "mod", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating raw fractional reciprocal-space k-point tables coming from three different symmetry-reduction backends. Each backend reports coordinates in a slightly different gauge, so we need cohort-specific de-duplication rules driven by the *intrinsic spread* of the coordinates.\n\nFor each cohort, first assess its raw coordinate noise by taking the maximum absolute deviation from the nearest multiple of 0.5 across all components (i.e., how far any coordinate is from 0, ±0.5, ±1.0, ...). Use that cohort’s noise estimate to pick a numerical tolerance as follows:\n- If the noise estimate is < 1e-7, use tol = 1e-8.\n- If the noise estimate is between 1e-7 and 1e-4, use tol = 1e-6.\n- If the noise estimate is >= 1e-4, use tol = 1e-4.\n\nThen run unique-row reduction under Brillouin-zone periodicity (treat k and k+G as equivalent by working modulo 1 in fractional coordinates). Additionally, only enable local agglomeration for cohorts whose k-point set contains at least one pair of distinct rows separated by less than the chosen tol in Euclidean distance but not identical component-wise up to rounding at that tol.\n\nProcess the following three cohorts independently and return the resulting unique k-point rows for each cohort:\n\nCohort A:\n[[0.0, 0.0, 0.0], [0.5000000002, 0.0, 0.0], [0.5, 0.0, 0.0], [1.0, 0.0, 0.0], [-0.5, 0.0, 0.0], [0.25, 0.25, 0.25], [0.2500004, 0.2499996, 0.25]]\n\nCohort B:\n[[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.5000000004, 0.5, 0.0], [0.5, 0.5000000003, 0.0], [-0.5, 0.5, 0.0], [0.25, 0.25, 0.25]]\n\nCohort C:\n[[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.5000002, 0.5, 0.0], [0.5, 0.5000001, 0.0], [-0.5, 0.5, 0.0], [0.25, 0.25, 0.25], [0.2500004, 0.2499996, 0.25]].", "answers": "[{\"name\":\"gpaw_bztools_unique_rows\",\"arguments\":{\"ain\":[[0.0,0.0,0.0],[0.5000000002,0.0,0.0],[0.5,0.0,0.0],[1.0,0.0,0.0],[-0.5,0.0,0.0],[0.25,0.25,0.25],[0.2500004,0.2499996,0.25]],\"tol\":1e-06,\"mod\":1,\"aglomerate\":false}},{\"name\":\"gpaw_bztools_unique_rows\",\"arguments\":{\"ain\":[[0.0,0.0,0.0],[1.0,0.0,0.0],[0.5000000004,0.5,0.0],[0.5,0.5000000003,0.0],[-0.5,0.5,0.0],[0.25,0.25,0.25]],\"tol\":1e-08,\"mod\":1,\"aglomerate\":false}},{\"name\":\"gpaw_bztools_unique_rows\",\"arguments\":{\"ain\":[[0.0,0.0,0.0],[1.0,0.0,0.0],[0.5000002,0.5,0.0],[0.5,0.5000001,0.0],[-0.5,0.5,0.0],[0.25,0.25,0.25],[0.2500004,0.2499996,0.25]],\"tol\":0.0001,\"mod\":1,\"aglomerate\":false}}]"}
{"func_name": "gpaw_coding_style_function", "func_desc": "gpaw.coding_style.function performs an elementwise addition of two NumPy arrays and returns the result. This small utility is used in the GPAW DFT codebase to combine array-valued quantities that appear in electronic-structure calculations (for example densities, potentials or components of wavefunctions) while respecting NumPy broadcasting rules and MPI-aware debug printing conventions used in GPAW scripts and modules.\n    \n    This function creates and returns a new numpy.ndarray that is the elementwise sum of the inputs. As implemented in the source, the function also has a side effect: when the module-level boolean flag debug is true and the MPI world communicator indicates the current process has rank 0 (mpi.world.rank == 0), the function prints the computed result scaled by the module-level constant Ha. The scaling and printing are only for human-readable diagnostic output and do not alter the returned array. Because the function relies on module-level globals (debug, mpi, Ha), its printing behavior depends on the GPAW runtime environment and MPI configuration.", "tools": [{"function": {"description": "gpaw.coding_style.function performs an elementwise addition of two NumPy arrays and returns the result. This small utility is used in the GPAW DFT codebase to combine array-valued quantities that appear in electronic-structure calculations (for example densities, potentials or components of wavefunctions) while respecting NumPy broadcasting rules and MPI-aware debug printing conventions used in GPAW scripts and modules.\n\nThis function creates and returns a new numpy.ndarray that is the elementwise sum of the inputs. As implemented in the source, the function also has a side effect: when the module-level boolean flag debug is true and the MPI world communicator indicates the current process has rank 0 (mpi.world.rank == 0), the function prints the computed result scaled by the module-level constant Ha. The scaling and printing are only for human-readable diagnostic output and do not alter the returned array. Because the function relies on module-level globals (debug, mpi, Ha), its printing behavior depends on the GPAW runtime environment and MPI configuration.", "name": "gpaw_coding_style_function", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Left operand of the addition. In the GPAW context this typically represents an array-valued physical quantity (for example a real-space grid of densities or potentials) stored as a NumPy array. The function expects a to be a numpy.ndarray; providing another type is outside the documented contract and may raise a TypeError or lead to implicit NumPy coercion.", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "Right operand of the addition. As with a, b is expected to be a numpy.ndarray representing a GPAW-related array-valued quantity. The shapes of a and b must be compatible for NumPy addition (either identical shapes or shapes compatible under NumPy broadcasting); otherwise a runtime error (such as ValueError) may be raised.", "default": ""}}, "required": ["a", "b"], "type": "any"}}, "type": "function"}], "query": "As part of a GPAW electrostatics regression test, we ingest a mixed set of potential-like tensors coming from different stages of the workflow. Each record has a 2×3 grid `a` and a background term `b` whose physical meaning depends on its shape. Apply the GPAW array-add utility to assemble only the records that are broadcast-consistent with a 2×3 grid under NumPy rules (i.e., scalar bias or a 1×3 row background meant to tile over the two rows). Skip any record whose `b` would not broadcast to 2×3. Raw records:\n\n1) real-space potential correction: a = [[-0.12, 0.03, 0.08], [0.05, -0.01, 0.10]], b = [[0.02, 0.02, 0.02]]\n2) post-processing external bias: a = [[0.1, 0.2, 0.3], [0.0, -0.1, -0.2]], b = 0.05\n3) malformed calibration artifact: a = [[0.01, 0.02, 0.03], [0.04, 0.05, 0.06]], b = [[0.1], [0.1], [0.1]]\n\nUse the standard GPAW add helper so broadcasting is respected and any debug printing on MPI rank 0 remains a side-effect only.", "answers": "[{\"name\":\"gpaw_coding_style_function\",\"arguments\":{\"a\":[[-0.12,0.03,0.08],[0.05,-0.01,0.1]],\"b\":[[0.02,0.02,0.02]]}},{\"name\":\"gpaw_coding_style_function\",\"arguments\":{\"a\":[[0.1,0.2,0.3],[0.0,-0.1,-0.2]],\"b\":0.05}}]"}
{"func_name": "gpaw_convergence_criteria_dict2criterion", "func_desc": "Converts a dictionary specification into a GPAW convergence criterion object.\n    \n    This function is used in GPAW (the DFT/PAW code) to translate a serialized or user-provided dictionary representation of a convergence criterion into a runtime criterion object that controls iterative convergence checks (for example, SCF energy or density convergence or geometry optimization tolerances). The input dictionary may be the output of a criterion's todict() method (containing a 'name' key and other keyword arguments), or a compact user shortcut mapping a criterion name to a value (for example {'energy': 0.005} or {'energy': (0.005, 3)}). dict2criterion uses get_criterion(name) to obtain the criterion factory/class and constructs an instance either by passing keyword arguments (when the dictionary contains 'name') or by passing a single positional argument (when a criterion name maps to a scalar/tuple). The function copies the input dictionary before modification so the original argument is not mutated.", "tools": [{"function": {"description": "Converts a dictionary specification into a GPAW convergence criterion object.\n\nThis function is used in GPAW (the DFT/PAW code) to translate a serialized or user-provided dictionary representation of a convergence criterion into a runtime criterion object that controls iterative convergence checks (for example, SCF energy or density convergence or geometry optimization tolerances). The input dictionary may be the output of a criterion's todict() method (containing a 'name' key and other keyword arguments), or a compact user shortcut mapping a criterion name to a value (for example {'energy': 0.005} or {'energy': (0.005, 3)}). dict2criterion uses get_criterion(name) to obtain the criterion factory/class and constructs an instance either by passing keyword arguments (when the dictionary contains 'name') or by passing a single positional argument (when a criterion name maps to a scalar/tuple). The function copies the input dictionary before modification so the original argument is not mutated.", "name": "gpaw_convergence_criteria_dict2criterion", "parameters": {"properties": {"dictionary": {"type": "any", "description": "A dictionary describing a convergence criterion. Accepted forms:\n- A full specification produced by todict(), i.e. a dict containing a 'name' key plus additional keys that are passed as keyword arguments to the criterion constructor. Example: {'name': 'energy', 'tol': 0.005, 'n_old': 3}.\n- A user shortcut mapping a single criterion name to a value, where the value may be a scalar, a tuple, or another dict. Example scalar: {'energy': 0.005}. Example tuple: {'energy': (0.005, 3)}.\n- A combination/nested form where the single key maps to a dict that itself contains a 'name' key; this nested dict will be handled recursively. Example: {'energy': {'name': 'energy', 'tol': 0.005, 'n_old': 3}}.\nThe dictionary must therefore either contain a top-level 'name' key, or contain exactly one top-level key that names the criterion. The dictionary represents how tolerances and internal parameters for convergence checks are specified in GPAW input or serialization and is transformed into a concrete criterion object used at runtime to decide when iterations have converged.", "default": ""}}, "required": ["dictionary"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed provenance stream of convergence stopping-rule specs for a GPAW SCF screening campaign. Each record is a single dictionary that may be either (a) a serialized criterion payload with a top-level 'name' key and keyword args, or (b) a compact user shortcut where exactly one top-level key is the criterion name and its value encodes the tolerance (scalar) and optionally the required consecutive-iteration window (tuple). \n\nFrom the following raw records, convert only those that encode an *energy* convergence criterion (either via a top-level 'name': 'energy' or via the compact single-key form where the key is 'energy'). Keep the original dictionaries unmodified.\n\nRaw records (in acquisition order):\n1) {'name': 'energy', 'tol': 1e-5, 'n_old': 4}\n2) {'density': 1e-4}\n3) {'energy': (0.005, 3)}\n4) {'name': 'forces', 'tol': 0.03}\n5) {'energy': {'name': 'energy', 'tol': 5e-6, 'n_old': 4}}\n\nConvert the qualifying energy criteria in the same order they appear in the stream into runtime convergence criterion objects.", "answers": "[{\"name\":\"gpaw_convergence_criteria_dict2criterion\",\"arguments\":{\"dictionary\":{\"name\":\"energy\",\"tol\":1e-05,\"n_old\":4}}},{\"name\":\"gpaw_convergence_criteria_dict2criterion\",\"arguments\":{\"dictionary\":{\"energy\":[0.005,3]}}},{\"name\":\"gpaw_convergence_criteria_dict2criterion\",\"arguments\":{\"dictionary\":{\"energy\":{\"name\":\"energy\",\"tol\":5e-06,\"n_old\":4}}}}]"}
{"func_name": "gpaw_core_matrix_suggest_blocking", "func_desc": "Suggest blocking (process grid and block size) for an NxN matrix used in parallel linear-algebra routines.", "tools": [{"function": {"description": "Suggest blocking (process grid and block size) for an NxN matrix used in parallel linear-algebra routines.\n", "name": "gpaw_core_matrix_suggest_blocking", "parameters": {"properties": {"N": {"type": "integer", "description": "Size N of the square matrix (NxN) to be distributed and operated on. In the GPAW/DFT context this is the dimension of a dense matrix (for example a Hamiltonian or overlap matrix) that will be processed by parallel ScaLAPACK-style block-cyclic algorithms. N is used to determine a block size that yields at least one whole block and at least two blocks in total when the matrix is partitioned across the process grid.", "default": ""}, "ncpus": {"type": "integer", "description": "Number of CPU processes (MPI ranks) available for the parallel calculation. This function arranges these processes into a 2D process grid (rows × columns) for distributing the matrix. In typical GPAW usage ncpus corresponds to the total number of MPI ranks used for a linear-algebra task.", "default": ""}}, "required": ["N", "ncpus"], "type": "any"}}, "type": "function"}], "query": "We’re tuning ScaLAPACK-style dense diagonalization for a mixed ensemble of Hamiltonian matrices coming from different k-point blocks. Each candidate matrix is nominally square but only sizes that are (a) at least 10k, (b) even, and (c) have a leading dimension aligned to 64 (i.e., N mod 64 == 0) should be included in the blocking/distribution planning sweep to avoid pathological padding and communication artifacts. From this raw batch of candidates: N = [20000, 19968, 20160, 16384, 12288, 10000, 9984, 8192, 24576], run the planning step across two MPI-rank cohorts to compare scaling behavior: cohort A uses 64 MPI ranks and cohort B uses 256 MPI ranks. For every matrix size that passes the alignment/validity sieve, suggest an efficient 2D process grid and a compatible block size for distribution, evaluated separately for each cohort.", "answers": "[{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":20000,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":20000,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":19968,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":19968,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":20160,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":20160,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":16384,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":16384,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":12288,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":12288,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":10000,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":10000,\"ncpus\":256}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":24576,\"ncpus\":64}},{\"name\":\"gpaw_core_matrix_suggest_blocking\",\"arguments\":{\"N\":24576,\"ncpus\":256}}]"}
{"func_name": "gpaw_directmin_etdm_lcao_check_indices", "func_desc": "gpaw.directmin.etdm_lcao.check_indices validates a pair of user-supplied integer indices that specify a constraint between electronic states used by the ETDM (energy-targeted direct minimization) LCAO (linear combination of atomic orbitals) routines in GPAW. The function ensures the indices are distinct and lie in the ranges required by the chosen representation so that downstream direct-minimization code does not attempt illegal array or orbital accesses.", "tools": [{"function": {"description": "gpaw.directmin.etdm_lcao.check_indices validates a pair of user-supplied integer indices that specify a constraint between electronic states used by the ETDM (energy-targeted direct minimization) LCAO (linear combination of atomic orbitals) routines in GPAW. The function ensures the indices are distinct and lie in the ranges required by the chosen representation so that downstream direct-minimization code does not attempt illegal array or orbital accesses.\n", "name": "gpaw_directmin_etdm_lcao_check_indices", "parameters": {"properties": {"ind1": {"type": "integer", "description": "First index of the constraint pair. In practice this is typically the index of an occupied state or the first state participating in a constraint/rotation. This argument must be an integer index and is checked for range depending on the chosen representation. If ind1 equals ind2 an AssertionError is raised to prevent degenerate constraints.", "default": ""}, "ind2": {"type": "integer", "description": "Second index of the constraint pair. In practice this is typically the index of the second state participating in a constraint/rotation and may refer to an occupied or unoccupied state depending on representation. This argument must be an integer index and is checked for range depending on the chosen representation.", "default": ""}, "n_dim": {"type": "integer", "description": "Total dimension of the single-particle state space (the number of states, basis functions, or columns in the coefficient matrix used by the LCAO direct-minimization routines). Used as the upper exclusive bound for valid indices in the 'full' and 'sparse' representations and as the upper exclusive bound for ind2 in the 'u-invar' representation.", "default": ""}, "n_occ": {"type": "integer", "description": "Number of occupied states (occupied orbitals) in the current electronic configuration. Used to partition the index range into occupied (0 .. n_occ-1) and unoccupied (n_occ .. n_dim-1) subspaces for representations that require that distinction.", "default": ""}, "representation": {"type": "string", "description": "Representation mode that determines which range checks are applied. The function contains explicit branches for the values 'full', 'sparse', and 'u-invar':\n- 'full': both ind1 and ind2 are required to satisfy 0 <= index < n_dim.\n  - 'sparse': ind1 is required to satisfy 0 <= ind1 < n_occ (occupied), and ind2 is required to satisfy 0 <= ind2 < n_dim.\n  - 'u-invar': ind1 is required to satisfy 0 <= ind1 < n_occ (occupied) and ind2 is required to satisfy n_occ <= ind2 < n_dim (strictly unoccupied).\nIf a different string is supplied, no additional range checks beyond the uniqueness check (ind1 != ind2) are performed by this function.", "default": ""}}, "required": ["ind1", "ind2", "n_dim", "n_occ", "representation"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a messy ETDM-LCAO constraint manifest generated from multiple user jobs (all intended for the same 'u-invar' representation) before constructing any occupied–virtual rotation constraints. Each record contains (n_dim, n_occ, ind1, ind2). Treat ind1/ind2 as a proposed occupied–unoccupied pair and validate only the records that are consistent with that physical intent: exactly one index must fall in the occupied manifold [0, n_occ) and the other must fall in the unoccupied manifold [n_occ, n_dim), and the two indices must be distinct. For every record that meets these criteria, run gpaw.directmin.etdm_lcao.check_indices with the record’s parameters (representation is always 'u-invar'). Manifest: (40, 16, 5, 23), (40, 12, 5, 18), (40, 16, 15, 23), (40, 16, 23, 23), (40, 12, 18, 5), (40, 12, -1, 18), (40, 12, 5, 40).", "answers": "[{\"name\":\"gpaw_directmin_etdm_lcao_check_indices\",\"arguments\":{\"ind1\":5,\"ind2\":23,\"n_dim\":40,\"n_occ\":16,\"representation\":\"u-invar\"}},{\"name\":\"gpaw_directmin_etdm_lcao_check_indices\",\"arguments\":{\"ind1\":5,\"ind2\":18,\"n_dim\":40,\"n_occ\":12,\"representation\":\"u-invar\"}},{\"name\":\"gpaw_directmin_etdm_lcao_check_indices\",\"arguments\":{\"ind1\":15,\"ind2\":23,\"n_dim\":40,\"n_occ\":16,\"representation\":\"u-invar\"}},{\"name\":\"gpaw_directmin_etdm_lcao_check_indices\",\"arguments\":{\"ind1\":18,\"ind2\":5,\"n_dim\":40,\"n_occ\":12,\"representation\":\"u-invar\"}}]"}
{"func_name": "gpaw_directmin_etdm_lcao_find_all_pairs", "func_desc": "Creates a list of orbital-index pairs that correspond to the degrees of freedom\n    (orbital-rotation constraints) associated with a given orbital index in the\n    electronic-structure direct energy minimization (ETDM) LCAO workflow used by\n    GPAW. In the GPAW DFT context (see README), the total number of single-particle\n    orbitals is n_dim and the first n_occ of these are treated as occupied;\n    orbital-rotation degrees of freedom connect occupied (o) and unoccupied (v)\n    subspaces and may include rotations within the occupied subspace (oo). This\n    function is used by gpaw.directmin.etdm_lcao to enumerate which index pairs\n    participate in constraints or parametrizations of orbital rotations for a\n    single orbital index ind, given a choice of representation that controls which\n    types of rotations count as degrees of freedom.", "tools": [{"function": {"description": "Creates a list of orbital-index pairs that correspond to the degrees of freedom\n(orbital-rotation constraints) associated with a given orbital index in the\nelectronic-structure direct energy minimization (ETDM) LCAO workflow used by\nGPAW. In the GPAW DFT context (see README), the total number of single-particle\norbitals is n_dim and the first n_occ of these are treated as occupied;\norbital-rotation degrees of freedom connect occupied (o) and unoccupied (v)\nsubspaces and may include rotations within the occupied subspace (oo). This\nfunction is used by gpaw.directmin.etdm_lcao to enumerate which index pairs\nparticipate in constraints or parametrizations of orbital rotations for a\nsingle orbital index ind, given a choice of representation that controls which\ntypes of rotations count as degrees of freedom.", "name": "gpaw_directmin_etdm_lcao_find_all_pairs", "parameters": {"properties": {"ind": {"type": "integer", "description": "The orbital index for which to find all associated index pairs.\nThis integer identifies the single orbital (0-based indexing) in the\nbasis of n_dim orbitals. In practical GPAW usage ind is expected to\nsatisfy 0 <= ind < n_dim; the function does not validate this and will\nproduce pairs containing ind even if it lies outside that range.", "default": ""}, "n_dim": {"type": "integer", "description": "The total number of orbitals (dimension of the single-particle\norbital space). In GPAW this corresponds to the full number of LCAO or\nbasis orbitals available for rotations. Typical invariant expectation\nis n_dim >= n_occ; violating this may produce results that do not\nreflect valid orbital indices.", "default": ""}, "n_occ": {"type": "integer", "description": "The number of occupied orbitals (the size of the occupied\nsubspace). The function treats indices 0..n_occ-1 as occupied and\nindices n_occ..n_dim-1 as unoccupied. In the direct-minimization\nalgorithm, ov pairs connect occupied and unoccupied orbitals and oo\npairs connect two occupied orbitals.", "default": ""}, "representation": {"type": "string", "description": "A string selecting which index-pair types are\nconsidered degrees of freedom. Must be one of the representations used\nin the module: 'u-invar', 'sparse' or 'full'. Their meanings are:\n'u-invar' -- unitary-invariant representation: only occupied-unoccupied\n    (ov) rotations are considered degrees of freedom. If ind is an\n    occupied index (ind < n_occ) the function returns pairs [ind, i]\n    for all unoccupied indices i in range(n_occ, n_dim); if ind is an\n    unoccupied index the function returns pairs [i, ind] for all\n    occupied i in range(n_occ). The order of elements in each pair is\n    chosen so the smaller index appears first.\n'sparse' -- sparse representation used in the module: when ind is an\n    occupied index (ind < n_occ) both occupied-occupied (oo) and\n    occupied-unoccupied (ov) rotations are considered degrees of\n    freedom and the function returns pairs with the smaller index\n    first for every other orbital j != ind, iterating over the full\n    range range(n_dim). When ind is unoccupied, 'sparse' behaves like\n    the ov-only case and returns [i, ind] for i in range(n_occ).\n'full' -- full representation: the orbital-rotation matrix is not\n    assumed antihermitian, so both orderings of each index pair must\n    be treated as independent constraints. For any ind the function\n    enumerates pairs for all j != ind from range(n_dim), appending\n    both [min(j,ind), max(j,ind)] and the reversed ordering, i.e. both\n    orderings appear as separate entries for each partner orbital j.\nNote: if representation is not one of these documented strings, the\nfunction falls through to the final fallback branch (equivalent to\nthe ov-only behavior for an unoccupied index) and may produce results\nthat include self-pairs for occupied indices; unsupported strings are\nnot validated and are therefore a potential source of errors.", "default": ""}}, "required": ["ind", "n_dim", "n_occ", "representation"], "type": "any"}}, "type": "function"}], "query": "In our ETDM-LCAO constraint-audit for GPAW, we have a small bundle of restart snapshots from the same SCF run where the orbital count and occupation can drift due to spin/state bookkeeping:\n\nsnapshots = [\n  {\"tag\": \"A\", \"n_dim\": 10, \"n_occ\": 4, \"ind\": 2},\n  {\"tag\": \"B\", \"n_dim\": 10, \"n_occ\": 4, \"ind\": 2},\n  {\"tag\": \"C\", \"n_dim\": 10, \"n_occ\": 5, \"ind\": 2},\n  {\"tag\": \"D\", \"n_dim\": 10, \"n_occ\": 4, \"ind\": 7},\n  {\"tag\": \"E\", \"n_dim\": 8,  \"n_occ\": 4, \"ind\": 2},\n  {\"tag\": \"F\", \"n_dim\": 10, \"n_occ\": 4, \"ind\": -1}\n]\n\nFor the sparse DOF/constraint representation, enumerate the orbital-rotation index pairs only for snapshots that are internally consistent (occupied count strictly less than total orbitals) and whose target orbital index is a valid occupied orbital. For each qualifying snapshot, run the enumeration twice as independent replicates to verify deterministic output within that snapshot.", "answers": "[{\"name\":\"gpaw_directmin_etdm_lcao_find_all_pairs\",\"arguments\":{\"ind\":2,\"n_dim\":10,\"n_occ\":4,\"representation\":\"sparse\"}},{\"name\":\"gpaw_directmin_etdm_lcao_find_all_pairs\",\"arguments\":{\"ind\":2,\"n_dim\":10,\"n_occ\":4,\"representation\":\"sparse\"}},{\"name\":\"gpaw_directmin_etdm_lcao_find_all_pairs\",\"arguments\":{\"ind\":2,\"n_dim\":10,\"n_occ\":5,\"representation\":\"sparse\"}},{\"name\":\"gpaw_directmin_etdm_lcao_find_all_pairs\",\"arguments\":{\"ind\":2,\"n_dim\":10,\"n_occ\":5,\"representation\":\"sparse\"}}]"}
{"func_name": "gpaw_directmin_functional_lcao_pz_constrain_grad", "func_desc": "constrain_grad zeros components of a gradient vector that correspond to constrained degrees of freedom for LCAO PZ direct minimization used in GPAW DFT calculations. It enforces constraints by setting the matching gradient entries to 0.0 so that optimization steps do not change those degrees of freedom (the code would ideally avoid computing these components in the first place).\n    \n    This function is intended for use in the direct minimization (\"directmin\") workflow for localized atomic-orbital (LCAO) based functionals in GPAW. In that context, grad is the flattened 1D gradient (for example, derivative of the energy or functional with respect to a list of paired indices), constraints enumerate the specific index pairs to hold fixed during optimization, and ind_up provides the ordered mapping from index-pair tuples to positions in the grad array. The function modifies grad in-place and also returns it for convenience.", "tools": [{"function": {"description": "constrain_grad zeros components of a gradient vector that correspond to constrained degrees of freedom for LCAO PZ direct minimization used in GPAW DFT calculations. It enforces constraints by setting the matching gradient entries to 0.0 so that optimization steps do not change those degrees of freedom (the code would ideally avoid computing these components in the first place).\n\nThis function is intended for use in the direct minimization (\"directmin\") workflow for localized atomic-orbital (LCAO) based functionals in GPAW. In that context, grad is the flattened 1D gradient (for example, derivative of the energy or functional with respect to a list of paired indices), constraints enumerate the specific index pairs to hold fixed during optimization, and ind_up provides the ordered mapping from index-pair tuples to positions in the grad array. The function modifies grad in-place and also returns it for convenience.", "name": "gpaw_directmin_functional_lcao_pz_constrain_grad", "parameters": {"properties": {"grad": {"type": "array", "items": {"type": "float"}, "description": "A one-dimensional NumPy array containing the gradient components for the optimization variables used in the LCAO direct-minimization routine. Each element corresponds to a specific (ind1, ind2) pair from ind_up (the mapping described below). The function will set some elements of this array to 0.0 when they match entries in constraints. The array is modified in-place; callers relying on the original values must provide a copy.", "default": ""}, "constraints": {"type": "array", "items": {"type": "float"}, "description": "A list of constrained index-pair specifications. Each element is expected to be a two-element sequence (for example, a list of two items) that can be compared for equality to the pairs produced by zipping ind_up[0] and ind_up[1]. When a constraint exactly equals [ind1, ind2] for some pair from ind_up, the corresponding gradient component is zeroed. If a constraint does not match any pair, it has no effect.", "default": ""}, "ind_up": {"type": "any", "description": "A tuple of two iterables (ind_up[0], ind_up[1]) that together define the ordered list of index pairs mapped to positions in grad. The function iterates over zip(ind_up[0], ind_up[1]) to produce pairs (ind1, ind2); the zero-based position of each pair in that sequence is the index into grad that may be set to zero. The length of grad should match the number of pairs produced by the zip for correct mapping; mismatches can raise IndexError or leave some constraints unapplied.", "default": ""}}, "required": ["grad", "constraints", "ind_up"], "type": "any"}}, "type": "function"}], "query": "We’re doing a preconditioned constraint-enforcement sweep for a GPAW LCAO PZ direct-minimization run where only physically meaningful frozen DOFs should be applied. For each replicate below, you’re given (i) a flattened 1D gradient vector (length 6), (ii) the ind_up mapping as two parallel integer arrays defining the (ind1, ind2) pair per gradient component, and (iii) a raw constraints list that may contain duplicates, reversed-order pairs, or diagonal pairs. Build the effective constraint set as follows: keep only *off-diagonal* constraints (ind1 != ind2) whose index-pair is present in the replicate’s ind_up mapping under either orientation, treating (i,j) and (j,i) as equivalent, and de-duplicate after canonicalizing each kept pair to (min(i,j), max(i,j)). Then apply constrain_grad using that effective constraint set (canonical orientation is fine) to zero the matching gradient components, leaving others unchanged. Process these replicates in order: (A) grad=[0.12,-0.05,0.33,-0.41,0.27,0.09] with ind_up[0]=[0,0,1,3,3,4], ind_up[1]=[1,2,0,1,2,0], raw_constraints=[(0,2),(2,0),(3,1),(5,5)]; (B) grad=[0.12,-0.05,0.30,-0.10,0.08,-0.02] with ind_up[0]=[0,0,1,1,2,2], ind_up[1]=[0,1,1,2,2,3], raw_constraints=[(0,1),(2,2),(2,1),(9,0)]; (C) grad=[0.12,-0.45,0.33,-0.27,0.05,0.91] with the same ind_up mapping as (B), raw_constraints=[(0,1),(1,2),(2,1),(0,0)]. Return the constrained gradients for each replicate.", "answers": "[{\"name\":\"gpaw_directmin_functional_lcao_pz_constrain_grad\",\"arguments\":{\"grad\":[0.12,-0.05,0.33,-0.41,0.27,0.09],\"constraints\":[[0,2],[1,3]],\"ind_up\":[[0,0,1,3,3,4],[1,2,0,1,2,0]]}},{\"name\":\"gpaw_directmin_functional_lcao_pz_constrain_grad\",\"arguments\":{\"grad\":[0.12,-0.05,0.3,-0.1,0.08,-0.02],\"constraints\":[[0,1],[1,2]],\"ind_up\":[[0,0,1,1,2,2],[0,1,1,2,2,3]]}},{\"name\":\"gpaw_directmin_functional_lcao_pz_constrain_grad\",\"arguments\":{\"grad\":[0.12,-0.45,0.33,-0.27,0.05,0.91],\"constraints\":[[0,1],[1,2]],\"ind_up\":[[0,0,1,1,2,2],[0,1,1,2,2,3]]}}]"}
{"func_name": "gpaw_directmin_ls_etdm_is_de__t_and_approximate_wolfe_conditions", "func_desc": "Check whether a trial step simultaneously satisfies a descent test and the\n    approximate Wolfe conditions used in line-search implementations for direct\n    energy minimization (ETDM) in GPAW. This function implements the test\n    described by William W. Hager and Hongchao Zhang (SIAM J. Optim., 16(1),\n    170-192) and is intended for use inside the line-search routine of the\n    direct-minimization solver (gpaw.directmin.ls_etdm). It combines a call to\n    is_descent(...) for sufficient decrease in the objective (phi) with an\n    inequality check on directional derivatives that enforces the approximate\n    Wolfe conditions (AWC) via the parameters delta and sigma.", "tools": [{"function": {"description": "Check whether a trial step simultaneously satisfies a descent test and the\napproximate Wolfe conditions used in line-search implementations for direct\nenergy minimization (ETDM) in GPAW. This function implements the test\ndescribed by William W. Hager and Hongchao Zhang (SIAM J. Optim., 16(1),\n170-192) and is intended for use inside the line-search routine of the\ndirect-minimization solver (gpaw.directmin.ls_etdm). It combines a call to\nis_descent(...) for sufficient decrease in the objective (phi) with an\ninequality check on directional derivatives that enforces the approximate\nWolfe conditions (AWC) via the parameters delta and sigma.", "name": "gpaw_directmin_ls_etdm_is_de__t_and_approximate_wolfe_conditions", "parameters": {"properties": {"der_phi_0": {"type": "float", "description": "Directional derivative of the objective phi at the\ninitial point (step size 0). In the ETDM/line-search context this is\nthe initial slope along the search direction; it determines the sign\nand magnitude of expected decrease. The function uses der_phi_0 as the\nleft-hand reference value in the AWC inequalities (both upper and\nlower bounds).", "default": ""}, "phi_0": {"type": "float", "description": "Objective function value phi at the initial point (step\nsize 0). In the DFT energy-minimization domain this typically\nrepresents the total energy (or an energy-like merit function) at the\ncurrent iterate. It is passed to the internal is_descent check to\nensure sufficient decrease of phi at the trial step.", "default": ""}, "der_phi_j": {"type": "float", "description": "Directional derivative of phi at the trial step j\n(the candidate step size). In line-search logic this is compared\nagainst scaled versions of der_phi_0 to enforce curvature (Wolfe-like)\nrequirements: it must lie between sigma*der_phi_0 and (2*delta-1)*der_phi_0.", "default": ""}, "phi_j": {"type": "float", "description": "Objective function value phi at the trial step j. This\nvalue is compared to phi_0 (via is_descent) to verify that the trial\nstep yields a sufficient decrease in the energy or merit function,\nsubject to the tolerance eps.", "default": ""}, "eps": {"type": "float", "description": "Tolerance used by the descent test called via is_descent.\nDefaults to 1e-6. In practice, eps sets how strict the sufficient-decrease\ncriterion is when comparing phi_j to phi_0 in the line-search: smaller\neps enforces a closer decrease requirement, while larger eps relaxes it.", "default": 1e-06}, "delta": {"type": "float", "description": "Parameter in (0,1) that controls the upper side of the\napproximate Wolfe condition. Defaults to 0.1. The AWC inequality\nimplemented here uses the factor (2*delta - 1) multiplied by der_phi_0\nas the upper bound for der_phi_j; delta therefore moves that upper\nbound between -1 and 1 and tunes the allowed curvature at the trial\npoint as described in Hager & Zhang.", "default": 0.1}, "sigma": {"type": "float", "description": "Parameter in (0,1) that controls the lower side of the\napproximate Wolfe condition. Defaults to 0.9. The AWC requires\nder_phi_j >= sigma * der_phi_0, so larger sigma makes the curvature\nrequirement stricter (derivative at the trial point must be closer to\nthe initial derivative).", "default": 0.9}}, "required": ["der_phi_0", "phi_0", "der_phi_j", "phi_j", "sigma", "eps", "delta"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a noisy ETDM line-search log from three GPAW direct-minimization replicates where occasional bookkeeping glitches flip the sign of the reported directional derivative (i.e., it appears non-descent even though it should be descent). Treat each replicate as follows before validating the step with the Hager–Zhang approximate Wolfe test: (1) Only attempt validation for trial steps whose objective change indicates an actual energy decrease (phi_j < phi_0). (2) For any such replicate where the reported initial directional derivative is not strictly negative, apply a sign-correction by negating der_phi_0 for the validation call; otherwise use it as-is. (3) Choose the AWC strictness based on the observed fractional decrease magnitude r = |phi_j - phi_0| / max(|phi_0|, 1): if r >= 3e-5, use (delta, sigma) = (0.1, 0.9); otherwise use (delta, sigma) = (0.2, 0.8). Keep each replicate’s provided eps. Raw replicates: A: phi_0 = -153.8421, der_phi_0 = -0.67, phi_j = -153.8460, der_phi_j = -0.60, eps = 1e-5. B: phi_0 = -154.3278, der_phi_0 = +0.084, phi_j = -154.3321, der_phi_j = -0.021, eps = 1e-6. C: phi_0 = -154.2375, der_phi_0 = -0.082, phi_j = -154.2451, der_phi_j = -0.075, eps = 1e-6. Run the validation calls implied by these rules and report the per-replicate pass/fail booleans.", "answers": "[{\"name\":\"gpaw_directmin_ls_etdm_is_de__t_and_approximate_wolfe_conditions\",\"arguments\":{\"der_phi_0\":-0.67,\"phi_0\":-153.8421,\"der_phi_j\":-0.6,\"phi_j\":-153.846,\"eps\":1e-05,\"delta\":0.2,\"sigma\":0.8}},{\"name\":\"gpaw_directmin_ls_etdm_is_de__t_and_approximate_wolfe_conditions\",\"arguments\":{\"der_phi_0\":-0.084,\"phi_0\":-154.3278,\"der_phi_j\":-0.021,\"phi_j\":-154.3321,\"eps\":1e-06,\"delta\":0.2,\"sigma\":0.8}},{\"name\":\"gpaw_directmin_ls_etdm_is_de__t_and_approximate_wolfe_conditions\",\"arguments\":{\"der_phi_0\":-0.082,\"phi_0\":-154.2375,\"der_phi_j\":-0.075,\"phi_j\":-154.2451,\"eps\":1e-06,\"delta\":0.1,\"sigma\":0.9}}]"}
{"func_name": "gpaw_directmin_tools_array_to_dict", "func_desc": "gpaw.directmin.tools.array_to_dict: Convert a concatenated numpy.ndarray into a dictionary of component arrays indexed by integers. This function is used in GPAW's direct minimization utilities (gpaw.directmin.tools) to split a single long solution or parameter vector into separate parts (for example, per-variable coefficient blocks, atom-centered values, or grid function segments) so that routines such as component-wise updates, preconditioning, or line searches can operate on each part independently.", "tools": [{"function": {"description": "gpaw.directmin.tools.array_to_dict: Convert a concatenated numpy.ndarray into a dictionary of component arrays indexed by integers. This function is used in GPAW's direct minimization utilities (gpaw.directmin.tools) to split a single long solution or parameter vector into separate parts (for example, per-variable coefficient blocks, atom-centered values, or grid function segments) so that routines such as component-wise updates, preconditioning, or line searches can operate on each part independently.\n", "name": "gpaw_directmin_tools_array_to_dict", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Source array containing concatenated components. The function slices x along its first axis (x[start:stop]) to produce each component. x is not modified by this function; however, the returned component arrays are obtained by slicing x and therefore will typically be NumPy views that share memory with x (standard NumPy slicing semantics apply). The caller should therefore be aware that modifying returned arrays may modify x.", "default": ""}, "dim": {"type": "array", "items": {"type": "float"}, "description": "List of integer dimensionalities (lengths) of the parts to extract from x. Each entry dim[i] specifies the number of elements to take for key i. The practical role of dim in the GPAW direct-minimization context is to describe how a flattened optimization vector is partitioned into meaningful sub-vectors (for example, coefficients for different basis sets or distinct physical quantities). Elements of dim must be non-negative integers; non-integer or negative values will lead to TypeError or unexpected/incorrect slicing behavior because the function does no validation.", "default": ""}}, "required": ["x", "dim"], "type": "any"}}, "type": "function"}], "query": "In our GPAW direct-minimization regression suite, we’re stress-testing the parameter unpacking step under realistic restart/concatenation artifacts. We have three flattened parameter vectors (independent replicates) that *may* include an optional 2-value “header” prefix added by a legacy restart writer (e.g., step-index + trust-radius), followed by the true concatenated parameter payload. For each replicate, decide whether a header is present by checking whether the total vector length equals either sum(dim) or sum(dim)+2. If the length equals sum(dim)+2, strip the first two values before splitting. Then use gpaw.directmin.tools.array_to_dict to split the payload into integer-keyed blocks (0,1,2) using the replicate’s block dimensionalities.\n\nReplicate A (k-point 0 / k-point 1 / grid-potential segment): xA = [0.1, 0.2, -0.3, 0.4, 1.0, -1.1, 0.9, 0.0, 0.5, -0.5, 0.7, -0.8], dimA = [4, 3, 5].\nReplicate B (orbital coefficients / atom-centered parameters / grid-function values): xB = [99.0, -99.0, 0.12, -0.05, 0.33, 1.2, 2.1, 2.2, 2.3, -0.4, -0.3, -0.2, -0.1, 0.0], dimB = [4, 3, 5].\nReplicate C (subsystems 0/1/2): xC = [0.1, 0.2, 0.3, 1.0, 1.1, 5.5, 5.6, 5.7, 5.8], dimC = [3, 2, 4].", "answers": "[{\"name\":\"gpaw_directmin_tools_array_to_dict\",\"arguments\":{\"x\":[0.1,0.2,-0.3,0.4,1.0,-1.1,0.9,0.0,0.5,-0.5,0.7,-0.8],\"dim\":[4,3,5]}},{\"name\":\"gpaw_directmin_tools_array_to_dict\",\"arguments\":{\"x\":[0.12,-0.05,0.33,1.2,2.1,2.2,2.3,-0.4,-0.3,-0.2,-0.1,0.0],\"dim\":[4,3,5]}},{\"name\":\"gpaw_directmin_tools_array_to_dict\",\"arguments\":{\"x\":[0.1,0.2,0.3,1.0,1.1,5.5,5.6,5.7,5.8],\"dim\":[3,2,4]}}]"}
{"func_name": "gpaw_directmin_tools_expm_ed_unit_inv", "func_desc": "gpaw.directmin.tools.expm_ed_unit_inv computes a unitary matrix by evaluating the matrix exponential formula given in Eq. (6) of J. Hutter, M. Parrinello, and S. Vogel, J. Chem. Phys., 101, 3862 (1994). This routine is used in GPAW's direct-minimization tools to construct occupied/virtual rotation operators (unitary updates) from the block X (called a_upp_r here) that couples occupied and virtual subspaces during wavefunction optimization in density-functional theory (DFT) calculations.", "tools": [{"function": {"description": "gpaw.directmin.tools.expm_ed_unit_inv computes a unitary matrix by evaluating the matrix exponential formula given in Eq. (6) of J. Hutter, M. Parrinello, and S. Vogel, J. Chem. Phys., 101, 3862 (1994). This routine is used in GPAW's direct-minimization tools to construct occupied/virtual rotation operators (unitary updates) from the block X (called a_upp_r here) that couples occupied and virtual subspaces during wavefunction optimization in density-functional theory (DFT) calculations.\n", "name": "gpaw_directmin_tools_expm_ed_unit_inv", "parameters": {"properties": {"a_upp_r": {"type": "array", "items": {"type": "float"}, "description": "The block X from the paper, given as a two-dimensional NumPy array with shape (n_o, n_v) where n_o is the number of occupied states (rows) and n_v is the number of virtual/unoccupied states (columns). The function uses a_upp_r to form p_nn = a_upp_r @ a_upp_r.T.conj() and therefore expects a numeric array (real or complex) that supports .T.conj() and matrix multiplication. In the GPAW DFT context, a_upp_r encodes infinitesimal rotations or update amplitudes between occupied and virtual subspaces; this routine converts that information into a full unitary rotation operator. If a_upp_r is exactly zero (all elements equal to zero within floating-point tolerance), the function returns the identity (or the corresponding block form) without performing eigendecomposition.", "default": ""}, "oo_vo_blockonly": {"type": "boolean", "description": "If False (default), return the full unitary matrix U of shape (n_o + n_v, n_o + n_v) constructed from the four blocks U_oo, U_ov, U_vo, and U_vv as in the cited equation; this is the typical form needed when a complete unitary update of the combined occupied+virtual basis is required. If True, return only the stacked column-blocks that contain the U_oo and U_vo blocks as a contiguous array of shape (n_o + n_v, n_o). This option exists for algorithms in GPAW that only need the occupied-column part of the unitary (oo and vo blocks) to update occupied orbitals and avoid constructing the full (larger) matrix, yielding memory and computational savings.", "default": false}}, "required": ["a_upp_r", "oo_vo_blockonly"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the occupied/virtual rotation step in GPAW’s direct-minimization loop for a 2-occupied / 3-virtual toy model, but the input couplings come from mixed-quality SCF micro-iterations. Treat the following candidate occupied–virtual coupling blocks X (a_upp_r) as raw pipeline records:\n\nA = [[0.0, 0.1, -0.2], [0.05, 0.0, 0.15]]\nB = [[0.05, -0.02, 0.00], [0.01, 0.03, -0.04]]\nC = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n\nApply a scientific sieve before constructing the unitary update via the Hutter–Parrinello–Vogel Eq. (6) matrix-exponential prescription: only generate rotation operators for coupling blocks whose Frobenius norm lies strictly between 0 and 0.30 (these correspond to ‘small but non-trivial’ steps; null steps and overly aggressive steps are treated as non-actionable). For each actionable block, return only the occupied-column portion needed for updating occupied orbitals (U_oo and U_vo only; do not build the full unitary). As a determinism check, rerun the computation once more on whichever actionable block has the smallest Frobenius norm, with identical settings, and again return only U_oo and U_vo.", "answers": "[{\"name\":\"gpaw_directmin_tools_expm_ed_unit_inv\",\"arguments\":{\"a_upp_r\":[[0.05,-0.02,0.0],[0.01,0.03,-0.04]],\"oo_vo_blockonly\":true}},{\"name\":\"gpaw_directmin_tools_expm_ed_unit_inv\",\"arguments\":{\"a_upp_r\":[[0.05,-0.02,0.0],[0.01,0.03,-0.04]],\"oo_vo_blockonly\":true}}]"}
{"func_name": "gpaw_directmin_tools_minimum_cubic_interpol", "func_desc": "gpaw.directmin.tools.minimum_cubic_interpol computes the position of the minimum of a cubic polynomial\n    that interpolates function values and derivatives at the two endpoints of an interval [x_0, x_1].\n    \n    This function is used in GPAW's direct minimization and line-search utilities to suggest a scalar step length along a\n    search direction by fitting a cubic to the objective (for example, DFT total energy) and its directional derivatives\n    at the interval boundaries. The inputs x_0 and x_1 define the interval in the independent variable (e.g., a\n    line-search parameter or displacement along a geometry-optimization direction). The values f_0 and f_1 are the\n    function values at those endpoints (for example, energies in ASE/GPAW units) and df_0 and df_1 are the corresponding\n    first derivatives with respect to x (for example, directional derivatives of the energy). All inputs are floats and\n    the function returns a float x_min that is the location of the cubic interpolant's minimum constrained to the closed\n    interval [min(x_0, x_1), max(x_0, x_1)].\n    \n    The routine swaps endpoints internally if x_0 > x_1 so the order of the inputs does not matter. It constructs the\n    unique cubic f(x) = a x^3 + b x^2 + c x + d in the local coordinate measured from x_0 using the standard formulae\n    derived from the two function values and two derivatives. It then inspects the cubic's stationary points by forming\n    the discriminant D = b^2 - 3 a c. If D < 0 there are no real stationary points for the derivative inside the real\n    line and the function returns the endpoint (x_0 or x_1) with the smaller provided function value. If D >= 0 the\n    routine computes one candidate stationary point r0 = (-b + sqrt(D)) / (3 a) + x_0 (the code uses the + root) and\n    accepts r0 as the minimum if r0 lies strictly inside the open interval (x_0, x_1) and the interpolated cubic value at\n    r0 is smaller than both endpoint values; otherwise it returns the endpoint with smaller f. The cubic is evaluated\n    with a helper that expects the local coordinate (r0 - x_0). The function never modifies external state.", "tools": [{"function": {"description": "gpaw.directmin.tools.minimum_cubic_interpol computes the position of the minimum of a cubic polynomial\nthat interpolates function values and derivatives at the two endpoints of an interval [x_0, x_1].\n\nThis function is used in GPAW's direct minimization and line-search utilities to suggest a scalar step length along a\nsearch direction by fitting a cubic to the objective (for example, DFT total energy) and its directional derivatives\nat the interval boundaries. The inputs x_0 and x_1 define the interval in the independent variable (e.g., a\nline-search parameter or displacement along a geometry-optimization direction). The values f_0 and f_1 are the\nfunction values at those endpoints (for example, energies in ASE/GPAW units) and df_0 and df_1 are the corresponding\nfirst derivatives with respect to x (for example, directional derivatives of the energy). All inputs are floats and\nthe function returns a float x_min that is the location of the cubic interpolant's minimum constrained to the closed\ninterval [min(x_0, x_1), max(x_0, x_1)].\n\nThe routine swaps endpoints internally if x_0 > x_1 so the order of the inputs does not matter. It constructs the\nunique cubic f(x) = a x^3 + b x^2 + c x + d in the local coordinate measured from x_0 using the standard formulae\nderived from the two function values and two derivatives. It then inspects the cubic's stationary points by forming\nthe discriminant D = b^2 - 3 a c. If D < 0 there are no real stationary points for the derivative inside the real\nline and the function returns the endpoint (x_0 or x_1) with the smaller provided function value. If D >= 0 the\nroutine computes one candidate stationary point r0 = (-b + sqrt(D)) / (3 a) + x_0 (the code uses the + root) and\naccepts r0 as the minimum if r0 lies strictly inside the open interval (x_0, x_1) and the interpolated cubic value at\nr0 is smaller than both endpoint values; otherwise it returns the endpoint with smaller f. The cubic is evaluated\nwith a helper that expects the local coordinate (r0 - x_0). The function never modifies external state.", "name": "gpaw_directmin_tools_minimum_cubic_interpol", "parameters": {"properties": {"x_0": {"type": "float", "description": "Left endpoint of the interval in the independent variable. This defines one boundary of the interval\nused to build the cubic interpolant. If x_0 > x_1 the function will swap the two endpoints internally so the\ncomputation proceeds on [min(x_0, x_1), max(x_0, x_1)].", "default": ""}, "x_1": {"type": "float", "description": "Right endpoint of the interval in the independent variable. Together with x_0, it sets the interval\nover which the cubic is constructed and searched for a minimum. Units and meaning should match those of x_0\n(for example, a line-search displacement or step length).", "default": ""}, "f_0": {"type": "float", "description": "Function value at x_0 (for example, total energy at the left endpoint when performing a line search).\nThis value is used, together with f_1 and the derivatives, to determine the cubic coefficients and to compare\nendpoint vs interior values when selecting the minimizing point.", "default": ""}, "f_1": {"type": "float", "description": "Function value at x_1 (for example, total energy at the right endpoint). Used symmetrically with\nf_0 in the cubic construction and endpoint comparisons.", "default": ""}, "df_0": {"type": "float", "description": "First derivative of the function with respect to x evaluated at x_0 (for example, directional\nderivative or slope at the left endpoint). This derivative is used to determine the cubic coefficients and the\nlocation of stationary points.", "default": ""}, "df_1": {"type": "float", "description": "First derivative of the function with respect to x evaluated at x_1 (for example, directional\nderivative or slope at the right endpoint). This derivative is used together with df_0 to determine the cubic\ncoefficients and the discriminant that controls whether an interior stationary point exists.", "default": ""}}, "required": ["x_0", "x_1", "f_0", "f_1", "df_0", "df_1"], "type": "any"}}, "type": "function"}], "query": "We’re doing automated postprocessing of GPAW direct-minimization line-search logs for a geometry-optimization benchmark where some brackets are numerically unreliable due to noisy force/energy reporting. Each record is a candidate cubic-interpolation bracket with endpoint step lengths (in Å), total energies (eV), and directional derivatives (eV/Å). Treat each record as an independent replicate, but only compute a cubic-interpolated in-bracket minimizer for brackets that look physically consistent with a bracketing step: the directional derivative at the lower step length must be \n- strictly negative and \n- the directional derivative at the higher step length must be strictly positive \n(after accounting for possible reversed x-order in the record).\n\nRaw brackets:\n1) x_0=0.00, x_1=0.40, f_0=-125.432, f_1=-125.350, df_0=-0.30, df_1=+0.45\n2) x_0=0.30, x_1=0.05, f_0=-154.238, f_1=-154.191, df_0=+0.42, df_1=-0.15\n3) x_0=0.00, x_1=0.20, f_0=-88.010, f_1=-88.050, df_0=+0.10, df_1=-0.05  \n4) x_0=0.10, x_1=0.60, f_0=-210.500, f_1=-210.480, df_0=-0.20, df_1=-0.10\n\nFor every bracket that satisfies the derivative sign-change criterion (after reinterpreting the lower/higher-x endpoint correctly), run the endpoint-constrained cubic interpolation and return the suggested step length in the closed interval.", "answers": "[{\"name\":\"gpaw_directmin_tools_minimum_cubic_interpol\",\"arguments\":{\"x_0\":0.0,\"x_1\":0.4,\"f_0\":-125.432,\"f_1\":-125.35,\"df_0\":-0.3,\"df_1\":0.45}},{\"name\":\"gpaw_directmin_tools_minimum_cubic_interpol\",\"arguments\":{\"x_0\":0.3,\"x_1\":0.05,\"f_0\":-154.238,\"f_1\":-154.191,\"df_0\":0.42,\"df_1\":-0.15}}]"}
{"func_name": "gpaw_dos_linear_tetrahedron_dos", "func_desc": "Linear-tetrahedron density-of-states (DOS) evaluator used in GPAW DFT post-processing.\n    \n    This function computes the electronic density of states (DOS) on a supplied energy grid using the linear tetrahedron integration (LTI) method over a k-point grid defined by `size` and `cell`. It is intended for use with eigenvalue data produced by GPAW or compatible DFT calculations: `eig_kn` contains eigenvalues for k-points and bands, `weight_kn` contains optional k-point weights, and `energies` is the energy grid where the DOS is evaluated. The implementation reshapes flattened eigenvalue/weight arrays to match the k-point grid dimensions, optionally remaps data from a full Brillouin-zone ordering to an irreducible-zone ordering using `bz2ibz_map`, and then calls the underlying LTI routine `lti(cell, eig_kn, energies, weight_kn)` to produce the DOS. This function performs no I/O and returns the DOS array for further analysis (for example, plotting or integrating to obtain number of states).", "tools": [{"function": {"description": "Linear-tetrahedron density-of-states (DOS) evaluator used in GPAW DFT post-processing.\n\nThis function computes the electronic density of states (DOS) on a supplied energy grid using the linear tetrahedron integration (LTI) method over a k-point grid defined by `size` and `cell`. It is intended for use with eigenvalue data produced by GPAW or compatible DFT calculations: `eig_kn` contains eigenvalues for k-points and bands, `weight_kn` contains optional k-point weights, and `energies` is the energy grid where the DOS is evaluated. The implementation reshapes flattened eigenvalue/weight arrays to match the k-point grid dimensions, optionally remaps data from a full Brillouin-zone ordering to an irreducible-zone ordering using `bz2ibz_map`, and then calls the underlying LTI routine `lti(cell, eig_kn, energies, weight_kn)` to produce the DOS. This function performs no I/O and returns the DOS array for further analysis (for example, plotting or integrating to obtain number of states).", "name": "gpaw_dos_linear_tetrahedron_dos", "parameters": {"properties": {"eig_kn": {"type": "array", "items": {"type": "float"}, "description": "Flattened or already-shaped eigenvalues for all k-points and bands. In GPAW workflows this typically contains eigenvalues produced by a band-structure calculation. If `eig_kn` is a 1D array whose length equals the product of the `size` tuple multiplied by the number of bands, the function will reshape it to shape `size + (-1,)` where the trailing axis indexes bands. If `len(eig_kn) != numpy.prod(size)` the function expects a mapping array `bz2ibz_map` to remap eigenvalues from a full Brillouin-zone ordering to the ordering used by `size`; failing to provide a correct `bz2ibz_map` when required will raise an indexing error or TypeError. The function does not modify the input array in-place beyond local reshaping/viewing.", "default": ""}, "weight_kn": {"type": "array", "items": {"type": "float"}, "description": "Flattened or already-shaped k-point weights corresponding to `eig_kn`. These weights are used by the linear tetrahedron integration to account for k-point sampling of the Brillouin zone. If provided, `weight_kn` is reshaped in the same manner as `eig_kn` (to `size + (-1,)`) and is remapped with `bz2ibz_map` when remapping `eig_kn` is performed. `weight_kn` may be None (the code checks for `is not None`); in that case the LTI routine will be called with `weight_kn=None` and will use its internal handling of weights. If `weight_kn` is provided but its length is inconsistent with `eig_kn` and `size`, a ValueError or reshape/indexing error will occur.", "default": ""}, "energies": {"type": "array", "items": {"type": "float"}, "description": "1D array of energy values at which the DOS will be evaluated. The returned DOS array has the same logical correspondence to this energy grid (typically the same length as `energies`). The energy units should match those of `eig_kn` (for GPAW calculations this is normally electron-volt units when ASE/GPAW default units are used).", "default": ""}, "cell": {"type": "array", "items": {"type": "float"}, "description": "Real-space unit cell vectors or cell information used by the tetrahedron integration routine to construct tetrahedra in reciprocal space. In the GPAW context `cell` determines the k-point sampling geometry for the LTI method and must be consistent with the `size` k-point grid and the ordering of `eig_kn`.", "default": ""}, "size": {"type": "any", "description": "Tuple of integers describing the k-point grid dimensions (for example a grid produced by Monkhorst-Pack). This tuple is used to reshape `eig_kn` and `weight_kn` into an array with shape `size + (-1,)` where the last axis indexes bands. The product of the integers in `size` must equal the number of k-point entries present in `eig_kn` (or in the full Brillouin-zone ordering when used with `bz2ibz_map`).", "default": ""}, "bz2ibz_map": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional 1D integer index array mapping a full-Brillouin-zone ordering of k-points/eigenvalues to the ordering used in the irreducible Brillouin zone (IBZ) that matches `size`. If `len(eig_kn) != numpy.prod(size)` the function will attempt to remap `eig_kn` (and `weight_kn` if not None) via `eig_kn = eig_kn[bz2ibz_map]`. If a remapping is required but `bz2ibz_map` is None or has an incompatible length/type, an exception (TypeError, IndexError or ValueError) will be raised. When provided, `bz2ibz_map` must correctly reflect how the input eigenvalues/weights were ordered relative to the k-point grid implied by `size`.", "default": null}}, "required": ["eig_kn", "weight_kn", "energies", "cell", "size", "bz2ibz_map"], "type": "any"}}, "type": "function"}], "query": "I’m consolidating several GPAW LTI-DOS post-processing snapshots from a mixed-quality archive into a single benchmark set. Each snapshot contains a unit cell, a k-point grid `size`, an energy grid specification, eigenvalues `eig_kn`, and optional weights `weight_kn`. Apply the following pipeline rules before evaluating DOS with the linear-tetrahedron DOS evaluator:\n\n1) Scientific sanity sieve: only evaluate datasets whose eigenvalues contain no NaN/None and whose energy grid is strictly increasing with uniform spacing (constant step).\n2) Weight handling protocol: \n   - If `weight_kn` is provided per k-point only (length equals number of k-points), treat it as k-point weights and pass it through unchanged.\n   - If `weight_kn` is provided per (k-point, band), pass it through unchanged.\n   - If `weight_kn` is missing, infer uniform k-point weights (1/Nk) and expand to per-(k-point, band) weights.\n3) Brillouin-zone ordering: if a snapshot includes `bz2ibz_map`, use it; otherwise leave it unset.\n4) Eigenvalue layout: if eigenvalues are supplied flattened, pass them flattened; if already shaped as (Nk, Nb), pass shaped.\n\nRaw snapshots (some may be filtered out by the sieve):\n\n- Snapshot A (cubic insulator test): cell = [[5.0,0,0],[0,5.0,0],[0,0,5.0]], size = [2,2,1], eig_kn = [[-4.8,-1.2,2.3],[-4.6,-1.0,2.5],[-4.7,-1.1,2.4],[-4.5,-0.9,2.6]], weight_kn = [[0.25,0.25,0.25],[0.25,0.25,0.25],[0.25,0.25,0.25],[0.25,0.25,0.25]], energies = [-5.0,-4.5,-4.0,-3.5,-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0], bz2ibz_map absent.\n\n- Snapshot B (graphene-like slab): cell = [[2.46,0,0],[1.23,2.13,0],[0,0,20.0]], size = [2,2,1], eig_kn flattened = [-3.2,-1.1,0.8,-3.0,-0.9,1.0,-3.1,-1.0,0.9,-3.05,-0.95,1.05], weight_kn per k-point = [0.25,0.25,0.25,0.25], energies = [-5,-4,-3,-2,-1,0,1,2,3,4,5], bz2ibz_map absent.\n\n- Snapshot C (fcc metal, missing weights): cell = [[3.6,0,0],[0,3.6,0],[0,0,3.6]], size = [2,2,2], eig_kn = [[-5.2,-1.1,3.4],[-5.0,-1.0,3.5],[-4.8,-0.9,3.6],[-4.9,-0.8,3.7],[-5.1,-1.2,3.3],[-4.7,-0.7,3.8],[-4.6,-0.6,3.9],[-4.5,-0.5,4.0]], weight_kn missing, energies = [-10.0,-9.8,-9.6,-9.4,-9.2,-9.0,-8.8,-8.6,-8.4,-8.2,-8.0,-7.8,-7.6,-7.4,-7.2,-7.0,-6.8,-6.6,-6.4,-6.2,-6.0,-5.8,-5.6,-5.4,-5.2,-5.0,-4.8,-4.6,-4.4,-4.2,-4.0,-3.8,-3.6,-3.4,-3.2,-3.0,-2.8,-2.6,-2.4,-2.2,-2.0,-1.8,-1.6,-1.4,-1.2,-1.0,-0.8,-0.6,-0.4,-0.2,0.0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.4,2.6,2.8,3.0,3.2,3.4,3.6,3.8,4.0,4.2,4.4,4.6,4.8,5.0,5.2,5.4,5.6,5.8,6.0,6.2,6.4,6.6,6.8,7.0,7.2,7.4,7.6,7.8,8.0,8.2,8.4,8.6,8.8,9.0,9.2,9.4,9.6,9.8,10.0], bz2ibz_map absent.\n\n- Snapshot D (corrupted energies): cell = [[4.0,0,0],[0,4.0,0],[0,0,4.0]], size = [2,2,1], eig_kn = [[-2.0,0.0,1.0],[-1.9,0.1,1.1],[-2.1,-0.1,0.9],[-2.05,0.05,1.05]], weight_kn = [[0.25,0.25,0.25]]*4, energies = [-5,-4,-4,-2,-1,0,1,2,3,4,5] (non-uniform / repeated), bz2ibz_map absent.\n\nCompute the DOS only for the snapshots that survive the sieve, using the appropriate weight protocol and leaving bz2ibz_map unset when absent.", "answers": "[{\"name\":\"gpaw_dos_linear_tetrahedron_dos\",\"arguments\":{\"eig_kn\":[[-4.8,-1.2,2.3],[-4.6,-1.0,2.5],[-4.7,-1.1,2.4],[-4.5,-0.9,2.6]],\"weight_kn\":[[0.25,0.25,0.25],[0.25,0.25,0.25],[0.25,0.25,0.25],[0.25,0.25,0.25]],\"energies\":[-5.0,-4.5,-4.0,-3.5,-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0],\"cell\":[[5.0,0.0,0.0],[0.0,5.0,0.0],[0.0,0.0,5.0]],\"size\":[2,2,1],\"bz2ibz_map\":null}},{\"name\":\"gpaw_dos_linear_tetrahedron_dos\",\"arguments\":{\"eig_kn\":[-3.2,-1.1,0.8,-3.0,-0.9,1.0,-3.1,-1.0,0.9,-3.05,-0.95,1.05],\"weight_kn\":[0.25,0.25,0.25,0.25],\"energies\":[-5.0,-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0],\"cell\":[[2.46,0.0,0.0],[1.23,2.13,0.0],[0.0,0.0,20.0]],\"size\":[2,2,1]}},{\"name\":\"gpaw_dos_linear_tetrahedron_dos\",\"arguments\":{\"eig_kn\":[[-5.2,-1.1,3.4],[-5.0,-1.0,3.5],[-4.8,-0.9,3.6],[-4.9,-0.8,3.7],[-5.1,-1.2,3.3],[-4.7,-0.7,3.8],[-4.6,-0.6,3.9],[-4.5,-0.5,4.0]],\"weight_kn\":[[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125],[0.125,0.125,0.125]],\"energies\":[-10.0,-9.8,-9.6,-9.4,-9.2,-9.0,-8.8,-8.6,-8.4,-8.2,-8.0,-7.8,-7.6,-7.4,-7.2,-7.0,-6.8,-6.6,-6.4,-6.2,-6.0,-5.8,-5.6,-5.4,-5.2,-5.0,-4.8,-4.6,-4.4,-4.2,-4.0,-3.8,-3.6,-3.4,-3.2,-3.0,-2.8,-2.6,-2.4,-2.2,-2.0,-1.8,-1.6,-1.4,-1.2,-1.0,-0.8,-0.6,-0.4,-0.2,0.0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.4,2.6,2.8,3.0,3.2,3.4,3.6,3.8,4.0,4.2,4.4,4.6,4.8,5.0,5.2,5.4,5.6,5.8,6.0,6.2,6.4,6.6,6.8,7.0,7.2,7.4,7.6,7.8,8.0,8.2,8.4,8.6,8.8,9.0,9.2,9.4,9.6,9.8,10.0],\"cell\":[[3.6,0.0,0.0],[0.0,3.6,0.0],[0.0,0.0,3.6]],\"size\":[2,2,2],\"bz2ibz_map\":null}}]"}
{"func_name": "gpaw_ffbt_ffbt", "func_desc": "gpaw.ffbt.ffbt: Fast Fourier–Bessel transform of a radial real-space function f(r) used in GPAW density-functional calculations. This routine computes the radial transform g(k) = k^{l+1} ∫_0^{rc} r dr j_l(kr) r f(r) by expanding the spherical Bessel function j_l(x) into a finite polynomial series and evaluating the resulting radial integrals using discrete Fast Fourier Transforms (FFTs). It is intended for transforming radial parts of quantities (for example radial components of wavefunctions, densities, or potentials in spherical coordinates) from a uniform real-space radial grid r_g to a uniform radial reciprocal-space grid k_q; the implementation performs l+1 FFTs on a 2Q-length grid and returns only the Q positive-frequency values corresponding to k_q.", "tools": [{"function": {"description": "gpaw.ffbt.ffbt: Fast Fourier–Bessel transform of a radial real-space function f(r) used in GPAW density-functional calculations. This routine computes the radial transform g(k) = k^{l+1} ∫_0^{rc} r dr j_l(kr) r f(r) by expanding the spherical Bessel function j_l(x) into a finite polynomial series and evaluating the resulting radial integrals using discrete Fast Fourier Transforms (FFTs). It is intended for transforming radial parts of quantities (for example radial components of wavefunctions, densities, or potentials in spherical coordinates) from a uniform real-space radial grid r_g to a uniform radial reciprocal-space grid k_q; the implementation performs l+1 FFTs on a 2Q-length grid and returns only the Q positive-frequency values corresponding to k_q.\n", "name": "gpaw_ffbt_ffbt", "parameters": {"properties": {"l": {"type": "integer", "description": "The spherical harmonic order l >= 0 that determines which spherical Bessel function j_l(kr) appears in the integral. In GPAW/PAW context, l selects the angular momentum channel for which the radial transform is computed. The algorithm sums contributions for m = 0..l using precomputed expansion coefficients c_lm; if c_lm does not contain the requested l or m an IndexError may be raised.", "default": ""}, "f_g": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of sampled values f(r) on the real-space radial grid r_g. The array length N must match len(r_g). The code assumes f(r)=0 for r >= rc (i.e., for grid indices g >= N the integrand is zero); numpy.fft.fft zero-padding is relied upon for g >= N when the FFT length 2*Q exceeds N.", "default": ""}, "r_g": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of radial grid point positions r_g[g] for g = 0..N-1. The function assumes a uniform real-space grid starting at r_g[0] == 0.0 and with constant spacing dr = r_g[1] - r_g[0]. When the module-level debug flag is True, the function asserts r_g[0] == 0.0 and that all spacings r_g[i+1] - r_g[i] equal dr, raising AssertionError if violated. The transform interprets r_g as r = g * dr for integer g.", "default": ""}, "k_q": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of target radial reciprocal-space points k(q) for q = 0..Q-1. The function expects k_q[0] == 0.0 and a uniform spacing dk = π / (Q * dr) when debug is enabled; it also assumes Q = len(k_q) >= len(r_g) so that the reciprocal-space grid length Q is at least the number of real-space samples. The function returns transform values at exactly these k_q points (the Q positive-frequency points). If the debug flag is True and the k_q grid does not satisfy the asserted spacing or ordering, an AssertionError will be raised.", "default": ""}}, "required": ["l", "f_g", "r_g", "k_q"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our GPAW-like radial transform stage against messy, mixed-quality radial channels coming from different generator paths (all purportedly uniform real-space grids). Below is a raw dump of candidate radial channels. Apply a screening + protocol rule-set before running `gpaw.ffbt.ffbt`:\n\nRaw channels (each has: label, l, r_g, f_g, optional k_q):\nA) label='H_2p_lr'  l=1  r_g=[0.0, 1.4285714286, 2.8571428571, 4.2857142857, 5.7142857143, 7.1428571429, 8.5714285714, 10.0]  f_g=[0.0, 0.12, 0.35, 0.55, 0.6, 0.48, 0.3, 0.1]  k_q=[0.0, 0.1106572212, 0.2213144424, 0.3319716635, 0.4426288847, 0.5532861059, 0.6639433271, 0.7746005483, 0.8852577695, 0.9959149907, 1.1065722118, 1.217229433, 1.3278866542, 1.4385438754, 1.5492010966, 1.6598583177]\nB) label='H_2p_sr'  l=1  r_g=[0.0, 0.4285714286, 0.8571428571, 1.2857142857, 1.7142857143, 2.1428571429, 2.5714285714, 3.0]  f_g=[0.0, 0.12, 0.30, 0.45, 0.50, 0.42, 0.28, 0.15]  k_q=[0.0, 0.2443460953, 0.4886921906, 0.7330382858, 0.9773843811, 1.2217304764, 1.4660765717, 1.7104226669, 1.9547687622, 2.1991148575, 2.4434609528, 2.687807048, 2.9321531433, 3.1764992386, 3.4208453339, 3.6651914292]\nC) label='pswfc_s'  l=0  r_g=[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]  f_g=[0.0, 0.35, 0.60, 0.72, 0.75, 0.70, 0.58, 0.42, 0.25, 0.12, 0.05]  k_q=(not provided)\nD) label='artifact_badgrid' l=1 r_g=[0.0, 0.40, 0.90, 1.30, 1.70, 2.10, 2.55, 3.0] f_g=[0.0, 0.11, 0.31, 0.44, 0.49, 0.41, 0.27, 0.14] k_q=[0.0, 0.2443460953, 0.4886921906, 0.7330382858, 0.9773843811, 1.2217304764, 1.4660765717, 1.7104226669, 1.9547687622, 2.1991148575, 2.4434609528, 2.687807048, 2.9321531433, 3.1764992386, 3.4208453339, 3.6651914292]\nE) label='artifact_negative_density' l=0 r_g=[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0] f_g=[0.0, 0.33, 0.58, -0.10, 0.72, 0.68, 0.55, 0.40, 0.24, 0.11, 0.04] k_q=(not provided)\n\nScreening rule: only transform channels whose r_g is strictly uniform (constant dr within numerical tolerance implied by the printed decimals) and whose f_g is non-negative everywhere.\n\nProtocol rule: For passing channels, compute g(k) on Q=16 points. If k_q is explicitly provided for that channel, use it as-is. Otherwise, generate a uniform k_q with dk = π/(Q*dr) starting at k=0.\n\nRun `gpaw.ffbt.ffbt` for all passing channels under these rules and return the per-channel g(k_q) results for downstream comparison of long-range vs short-range p-character and the s-like pseudo-wavefunction control.", "answers": "[{\"name\":\"gpaw_ffbt_ffbt\",\"arguments\":{\"l\":1,\"f_g\":[0.0,0.12,0.35,0.55,0.6,0.48,0.3,0.1],\"r_g\":[0.0,1.4285714286,2.8571428571,4.2857142857,5.7142857143,7.1428571429,8.5714285714,10.0],\"k_q\":[0.0,0.1106572212,0.2213144424,0.3319716635,0.4426288847,0.5532861059,0.6639433271,0.7746005483,0.8852577695,0.9959149907,1.1065722118,1.217229433,1.3278866542,1.4385438754,1.5492010966,1.6598583177]}},{\"name\":\"gpaw_ffbt_ffbt\",\"arguments\":{\"l\":1,\"f_g\":[0.0,0.12,0.3,0.45,0.5,0.42,0.28,0.15],\"r_g\":[0.0,0.4285714286,0.8571428571,1.2857142857,1.7142857143,2.1428571429,2.5714285714,3.0],\"k_q\":[0.0,0.2443460953,0.4886921906,0.7330382858,0.9773843811,1.2217304764,1.4660765717,1.7104226669,1.9547687622,2.1991148575,2.4434609528,2.687807048,2.9321531433,3.1764992386,3.4208453339,3.6651914292]}},{\"name\":\"gpaw_ffbt_ffbt\",\"arguments\":{\"l\":0,\"f_g\":[0.0,0.35,0.6,0.72,0.75,0.7,0.58,0.42,0.25,0.12,0.05],\"r_g\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0],\"k_q\":[0.0,0.39269908125,0.7853981625,1.17809724375,1.570796325,1.96349540625,2.3561944875,2.74889356875,3.14159265,3.53429173125,3.9269908125,4.31968989375,4.712388975,5.10508805625,5.4977871375,5.89048621875]}}]"}
{"func_name": "gpaw_ffbt_rescaled_bessel_limit", "func_desc": "gpaw.ffbt.rescaled_bessel_limit\n    Get the x -> 0 limit of a rescaled spherical Bessel function j_l(x)/x^l used in radial function handling.\n    \n    Calculates the closed-form value of the limit\n        lim_{x->0} j_l(x) / x^l = 2^l * l! / (2*l + 1)!\n    for a given angular momentum quantum number l. In the GPAW DFT codebase this quantity is useful when working with spherical Bessel functions that appear in radial integrals, projector-augmented wave (PAW) components, and plane-wave / atom-centered basis treatments described in the project README: it provides the finite small-argument normalization constant that prevents singular behavior and aids numerical stability when evaluating radial basis functions or expansions near the origin.", "tools": [{"function": {"description": "gpaw.ffbt.rescaled_bessel_limit\nGet the x -> 0 limit of a rescaled spherical Bessel function j_l(x)/x^l used in radial function handling.\n\nCalculates the closed-form value of the limit\n    lim_{x->0} j_l(x) / x^l = 2^l * l! / (2*l + 1)!\nfor a given angular momentum quantum number l. In the GPAW DFT codebase this quantity is useful when working with spherical Bessel functions that appear in radial integrals, projector-augmented wave (PAW) components, and plane-wave / atom-centered basis treatments described in the project README: it provides the finite small-argument normalization constant that prevents singular behavior and aids numerical stability when evaluating radial basis functions or expansions near the origin.", "name": "gpaw_ffbt_rescaled_bessel_limit", "parameters": {"properties": {"l": {"type": "integer", "description": "Non-negative integer angular momentum quantum number l that indexes the spherical Bessel function j_l(x). In GPAW this corresponds to the usual orbital/partial-wave index used in PAW and spherical-harmonic decompositions. The function expects l to be an integer; supplying negative values or non-integers will result in an exception propagated from the underlying factorial routine (for example, a ValueError raised by math.factorial or an equivalent implementation).", "default": ""}}, "required": ["l"], "type": "any"}}, "type": "function"}], "query": "We’re validating the x→0 normalization constants used to regularize PAW radial projector factors j_l(x)/x^l in a mixed-channel dataset coming from multiple atom setups. Each entry is a candidate angular-momentum channel tag; some are corrupted or out-of-scope for the current run. Use the closed-form rescaled Bessel x→0 limit only for channels that are physically admissible for a scalar-relativistic PAW projector test (i.e., integer l in the inclusive range [0, 3]).\n\nRaw channel tags (as emitted by the projector generator): [\"s\", \"d\", \"f(prod)\", \"f(repl)\", \"p?\", \"g\", \"-1\", \"2\", \"3.0\"].\n\nMap valid spectroscopic tags to l via s→0, p→1, d→2, f→3; accept numeric tags only if they are already valid integers in [0,3]. Treat the two f-tagged entries as separate projector realizations and report them as separate outputs.", "answers": "[{\"name\":\"gpaw_ffbt_rescaled_bessel_limit\",\"arguments\":{\"l\":0}},{\"name\":\"gpaw_ffbt_rescaled_bessel_limit\",\"arguments\":{\"l\":2}},{\"name\":\"gpaw_ffbt_rescaled_bessel_limit\",\"arguments\":{\"l\":3}},{\"name\":\"gpaw_ffbt_rescaled_bessel_limit\",\"arguments\":{\"l\":3}},{\"name\":\"gpaw_ffbt_rescaled_bessel_limit\",\"arguments\":{\"l\":2}}]"}
{"func_name": "gpaw_ffbt_spherical_bessel", "func_desc": "gpaw.ffbt.spherical_bessel computes the spherical Bessel function j_l(x) for a given integer order l and a NumPy array of non-negative radial arguments x_g. This implementation is used in the gpaw.ffbt module for radial/plane-wave integrals and expansions that appear in GPAW density-functional-theory calculations (for example, in radial parts of PAW/projector expansions and Fourier–Bessel transforms). The routine evaluates j_l(x) using the finite expansion\n    j_l(x) = x^{-(l+1)} sum_{m=0}^l Re{ c_lm[l][m] * x^m * exp(-i x) }\n    based on the coefficients c_lm (see Marco Vanin, 2008). The function returns a real-valued NumPy array with the same shape as x_g containing elementwise values of j_l(x).", "tools": [{"function": {"description": "gpaw.ffbt.spherical_bessel computes the spherical Bessel function j_l(x) for a given integer order l and a NumPy array of non-negative radial arguments x_g. This implementation is used in the gpaw.ffbt module for radial/plane-wave integrals and expansions that appear in GPAW density-functional-theory calculations (for example, in radial parts of PAW/projector expansions and Fourier–Bessel transforms). The routine evaluates j_l(x) using the finite expansion\nj_l(x) = x^{-(l+1)} sum_{m=0}^l Re{ c_lm[l][m] * x^m * exp(-i x) }\nbased on the coefficients c_lm (see Marco Vanin, 2008). The function returns a real-valued NumPy array with the same shape as x_g containing elementwise values of j_l(x).", "name": "gpaw_ffbt_spherical_bessel", "parameters": {"properties": {"l": {"type": "integer", "description": "The spherical order l of the Bessel function j_l. This must be a non-negative integer corresponding to the angular-momentum quantum number used in radial expansions in GPAW. Negative values are not supported and will typically produce incorrect results or runtime errors (for example, an empty coefficient sum or IndexError). The value of l determines the number of terms (m = 0..l) used in the finite expansion and the behaviour at x = 0 (j_0(0) = 1, j_l(0) = 0 for l > 0).", "default": ""}, "x_g": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of non-negative floating-point radial arguments x where j_l(x) is to be evaluated. The array's dtype must be float (the code asserts x_g.dtype == float) and all elements must satisfy x >= 0 (the code asserts numpy.all(x_g >= 0)). The function preserves the shape of x_g and returns an array of the same shape. Values with x < 1e-10 are treated as x = 0 for numerical stability: j_0(0) is set to 1.0 and j_l(0) is set to 0.0 for l > 0.", "default": ""}}, "required": ["l", "x_g"], "type": "any"}}, "type": "function"}], "query": "We’re diagnosing angular-momentum channel stability in a GPAW Fourier–Bessel transform stage where the small‑x behavior can contaminate PAW/projector matching. Using the raw radial grid x = [0.0, 0.1, 0.5, 1.0, 2.5, 5.0], run a two-branch evaluation of real spherical Bessel functions j_l(x):\n\n1) For the near-origin subset (arguments <= 0.5), evaluate the d-channel (l=2) since this is the channel used in the projector expansion check.\n2) For the oscillatory tail subset (arguments >= 1.0), evaluate the f-channel (l=3) to stress-test the high-l behavior used in plane-wave/radial matching.\n\nReturn the computed arrays for each branch, preserving the original order of x values within each branch.", "answers": "[{\"name\":\"gpaw_ffbt_spherical_bessel\",\"arguments\":{\"l\":2,\"x_g\":[0.0,0.1,0.5]}},{\"name\":\"gpaw_ffbt_spherical_bessel\",\"arguments\":{\"l\":3,\"x_g\":[1.0,2.5,5.0]}}]"}
{"func_name": "gpaw_fftw_check_fft_size", "func_desc": "Check if an integer grid length n is an efficient FFT size for FFT-based operations.\n    \n    This function is used in GPAW to decide whether a grid dimension length n is suitable for efficient fast Fourier transform (FFT) algorithms (for example the FFTW library) that perform best when the transform length factors into small primes. In the GPAW domain this influences choices of plane-wave / real-space grid dimensions and multigrid operations: a return value of True indicates that n can be reduced to 1 by successive division by the allowed small prime factors (default [2, 3, 5, 7]), and therefore n is likely to give efficient FFT performance.", "tools": [{"function": {"description": "Check if an integer grid length n is an efficient FFT size for FFT-based operations.\n\nThis function is used in GPAW to decide whether a grid dimension length n is suitable for efficient fast Fourier transform (FFT) algorithms (for example the FFTW library) that perform best when the transform length factors into small primes. In the GPAW domain this influences choices of plane-wave / real-space grid dimensions and multigrid operations: a return value of True indicates that n can be reduced to 1 by successive division by the allowed small prime factors (default [2, 3, 5, 7]), and therefore n is likely to give efficient FFT performance.", "name": "gpaw_fftw_check_fft_size", "parameters": {"properties": {"n": {"type": "integer", "description": "The integer transform length (grid size along one dimension) to test. This function expects n to be a positive integer (n >= 1). n == 1 is treated as efficient and returns True immediately. If n == 0 the function will repeatedly divide 0 by any factor that divides 0 and will never reach 1, leading to infinite recursion and ultimately a RecursionError; therefore do not pass n == 0. Negative integers are not meaningful for FFT sizes and are not in the intended domain of this function.", "default": ""}, "factors": {"type": "array", "items": {"type": "float"}, "description": "A list of integer factors (default [2, 3, 5, 7]) that are considered \"small primes\" for the factorization test. Each element is expected to be an integer greater than 1 (typically prime values). The function tests whether n can be factored entirely into elements from this list by repeatedly dividing n by any factor in factors that evenly divides n. Modifying this list changes which prime factors are allowed; including 1 or 0 in this list will cause non-terminating recursion and is therefore invalid.", "default": [2, 3, 5, 7]}}, "required": ["n", "factors"], "type": "any"}}, "type": "function"}], "query": "We’re tuning a GPAW real-space grid for an FFT-heavy 3D workload and need a quick “planner-aware” screening of candidate edge lengths harvested from several cell proposals. Use the following raw candidate lengths (in voxels) for one axis: [192, 256, 250, 189, 257, 0, -64]. Treat non-positive lengths as invalid instrumentation artifacts and ignore them. For the remaining candidates, choose the allowed small-prime factor policy dynamically based on the length’s intrinsic structure: if the length is a pure power of two, use the default FFTW-friendly policy (2,3,5,7); if the length is divisible by 25, emulate a constrained radix planner and allow only {2,5}; otherwise, emulate a slightly restricted planner and allow only {2,3,5}. For each valid candidate, run the FFT-efficiency check and report whether it can be fully reduced to 1 under its assigned factor policy.", "answers": "[{\"name\":\"gpaw_fftw_check_fft_size\",\"arguments\":{\"n\":192,\"factors\":[2,3,5]}},{\"name\":\"gpaw_fftw_check_fft_size\",\"arguments\":{\"n\":256}},{\"name\":\"gpaw_fftw_check_fft_size\",\"arguments\":{\"n\":250,\"factors\":[2,5]}},{\"name\":\"gpaw_fftw_check_fft_size\",\"arguments\":{\"n\":189,\"factors\":[2,3,5]}},{\"name\":\"gpaw_fftw_check_fft_size\",\"arguments\":{\"n\":257,\"factors\":[2,3,5]}}]"}
{"func_name": "gpaw_fftw_get_efficient_fft_size", "func_desc": "Return the smallest efficient FFT size that is greater than or equal to N and divisible by n.\n    \n    This function is used in GPAW to select array/grid sizes for FFT-based operations (for example FFTW transforms used in plane-wave or real-space DFT calculations). An \"efficient\" size is one that passes the check_fft_size test: its prime-factorization is restricted to the small primes listed in factors (by default [2, 3, 5, 7]), which typically yields good performance in FFT libraries such as FFTW. The returned size is therefore suitable for high-performance FFTs and for distribution constraints that require sizes to be multiples of n (for example MPI domain decomposition or block-alignment requirements in GPAW).", "tools": [{"function": {"description": "Return the smallest efficient FFT size that is greater than or equal to N and divisible by n.\n\nThis function is used in GPAW to select array/grid sizes for FFT-based operations (for example FFTW transforms used in plane-wave or real-space DFT calculations). An \"efficient\" size is one that passes the check_fft_size test: its prime-factorization is restricted to the small primes listed in factors (by default [2, 3, 5, 7]), which typically yields good performance in FFT libraries such as FFTW. The returned size is therefore suitable for high-performance FFTs and for distribution constraints that require sizes to be multiples of n (for example MPI domain decomposition or block-alignment requirements in GPAW).", "name": "gpaw_fftw_get_efficient_fft_size", "parameters": {"properties": {"N": {"type": "integer", "description": "The minimum requested FFT size. The function returns a size greater than or equal to this value. This parameter represents a desired grid length or array dimension in GPAW FFTs and must be an integer.", "default": ""}, "n": {"type": "integer", "description": "The required divisor of the returned size. The returned integer will be divisible by n. Default is 1. This parameter is commonly used to enforce processor- or block-alignment constraints (for example ensuring the FFT length is a multiple of the number of MPI ranks or a local block size).", "default": 1}, "factors": {"type": "array", "items": {"type": "float"}, "description": "A list of integer prime factors accepted for efficient FFT sizes. Default is [2, 3, 5, 7]. The function relies on check_fft_size(N, factors) to determine whether a candidate size is efficient by verifying that its prime-factorization only contains elements from this list. Choosing small primes in factors yields sizes that FFT libraries typically handle efficiently.", "default": [2, 3, 5, 7]}}, "required": ["N", "n"], "type": "any"}}, "type": "function"}], "query": "We’re assembling FFT grid lengths for a mixed GPAW benchmark where the raw requested minima come from two pipelines: (i) real-space finite-difference grids and (ii) plane-wave density grids. The minima are noisy because some runs report padded sizes or sentinel failures, so treat the following as raw acquisition output: N_raw = [190, 510, 0, -32, 999999].\n\nApply a screening + branching protocol:\n1) Keep only physically plausible requested minima for production FFT planning: strictly positive and less than 2000.\n2) Use the same MPI slab alignment constraint n = 16 for all surviving entries.\n3) Because the plane-wave cohort is known to be more communication-sensitive, treat any surviving N_raw >= 400 as “plane-wave” and any surviving N_raw < 400 as “real-space”; compute the smallest FFTW-efficient transform length >= N_raw and divisible by 16 for each survivor.\n\nReturn the computed optimal FFT lengths for all survivors in the order they appear in N_raw.", "answers": "[{\"name\":\"gpaw_fftw_get_efficient_fft_size\",\"arguments\":{\"N\":190,\"n\":16}},{\"name\":\"gpaw_fftw_get_efficient_fft_size\",\"arguments\":{\"N\":510,\"n\":16}}]"}
{"func_name": "gpaw_hybrids_parse_name", "func_desc": "Parse known hybrid functional names and return the libxc semi-local functional name\n    and numerical parameters that GPAW uses to configure hybrid-exchange calculations.\n    \n    This function is used in GPAW (a density-functional theory Python package) to map a small set\n    of canonical hybrid functional identifiers to the concrete parameters required by GPAW's\n    hybrid/exact-exchange implementation. The returned values are used when setting up the\n    exchange-correlation treatment in calculations that mix exact (Hartree–Fock) exchange with\n    a semi-local functional, for example when constructing PBE0, HSE03, HSE06, B3LYP or\n    Yukawa-screened PBE0 hybrids.", "tools": [{"function": {"description": "Parse known hybrid functional names and return the libxc semi-local functional name\nand numerical parameters that GPAW uses to configure hybrid-exchange calculations.\n\nThis function is used in GPAW (a density-functional theory Python package) to map a small set\nof canonical hybrid functional identifiers to the concrete parameters required by GPAW's\nhybrid/exact-exchange implementation. The returned values are used when setting up the\nexchange-correlation treatment in calculations that mix exact (Hartree–Fock) exchange with\na semi-local functional, for example when constructing PBE0, HSE03, HSE06, B3LYP or\nYukawa-screened PBE0 hybrids.", "name": "gpaw_hybrids_parse_name", "parameters": {"properties": {"name": {"type": "string", "description": "Hybrid functional identifier string. Must be one of the exact, case-sensitive\nnames recognized by this function: 'EXX', 'PBE0', 'HSE03', 'HSE06', 'B3LYP', or\n'YS-PBE0'. The identifier selects a known mapping implemented in gpaw.hybrids.parse_name;\nthe function does not perform fuzzy matching or case normalization, so the caller must\nsupply the identifier exactly as listed.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a hybrid-DFT validation suite for GPAW where the hybrid identifier comes from heterogeneous provenance (workflow YAMLs, literature tags, and user free-text). The raw list below includes canonical hybrids, case/format variants, and a few malformed tokens typical of metadata drift:\n\nraw_hybrid_tags = [\n  'HSE06', 'hse06', ' HSE06 ',\n  'HSE03',\n  'PBE0', 'PBE0 ',\n  'B3LYP',\n  'YukawaPBE0',\n  'HSE-06', 'HSE_06',\n  'PBE00', 'B3LYP*',\n  'None', ''\n]\n\nNormalize each tag by stripping leading/trailing whitespace and uppercasing. Then, treat a tag as a valid hybrid identifier only if, after normalization, it is composed exclusively of ASCII letters and digits (no underscores, hyphens, asterisks, or other punctuation) and is non-empty. For every valid normalized identifier, parse it with GPAW’s hybrid-name parser to obtain (i) the underlying semi-local libxc functional name and (ii) the numerical hybrid parameters GPAW will use. Return the parsed results in the same order as the surviving valid identifiers (i.e., preserve ordering after filtering).", "answers": "[{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"HSE06\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"HSE06\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"HSE06\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"HSE03\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"PBE0\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"PBE0\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"B3LYP\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"YUKAWAPBE0\"}},{\"name\":\"gpaw_hybrids_parse_name\",\"arguments\":{\"name\":\"NONE\"}}]"}
{"func_name": "gpaw_hyperfine_delta", "func_desc": "Extended delta function used in hyperfine-related real-space operations in GPAW.\n    \n    This function evaluates an analytic, smoothly decaying \"extended delta\" function\n    defined by the expression 2 / rT / (1 + 2 * r / rT)**2. In the GPAW DFT/hyperfine\n    context this function is used to model a localized contact-like distribution\n    (centered at r = 0) with a finite range controlled by the parameter rT; it is\n    suitable for use on real-space radial grids (numpy arrays) when constructing or\n    analyzing hyperfine coupling terms and other localized operators. The amplitude\n    at r = 0 is 2/rT and the large-r asymptotic decay is proportional to 1/r^2,\n    so rT controls both the spatial extent (range) and the peak magnitude of the\n    distribution. In practical GPAW/ASE workflows the radial distances r are\n    typically expressed in the same length units used elsewhere in the calculation\n    (e.g., Å when interacting with ASE structures and outputs).", "tools": [{"function": {"description": "Extended delta function used in hyperfine-related real-space operations in GPAW.\n\nThis function evaluates an analytic, smoothly decaying \"extended delta\" function\ndefined by the expression 2 / rT / (1 + 2 * r / rT)**2. In the GPAW DFT/hyperfine\ncontext this function is used to model a localized contact-like distribution\n(centered at r = 0) with a finite range controlled by the parameter rT; it is\nsuitable for use on real-space radial grids (numpy arrays) when constructing or\nanalyzing hyperfine coupling terms and other localized operators. The amplitude\nat r = 0 is 2/rT and the large-r asymptotic decay is proportional to 1/r^2,\nso rT controls both the spatial extent (range) and the peak magnitude of the\ndistribution. In practical GPAW/ASE workflows the radial distances r are\ntypically expressed in the same length units used elsewhere in the calculation\n(e.g., Å when interacting with ASE structures and outputs).", "name": "gpaw_hyperfine_delta", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional numpy.ndarray of radial coordinates\n() at which to evaluate the extended delta function. Each entry\nrepresents a distance from the origin on a real-space radial grid and\nshould be non-negative for a physical radial distance; negative values\nare mathematically accepted by the formula but have no physical\nsignificance in standard radial-grid usage. The output is computed\nelementwise and the returned array preserves the shape of r. In GPAW\nworkflows these values are typically in the same units as atomic\npositions (for example, Å when using ASE conventions).", "default": ""}, "rT": {"type": "float", "description": "Positive floating-point parameter that sets the effective\nrange/width of the extended delta distribution and scales its\namplitude. Physically, rT acts like a smoothing/cutoff radius: smaller\nrT yields a narrower, higher peak (value at r = 0 equals 2/rT), while\nlarger rT produces a broader, lower-amplitude distribution. The\nfunction performs arithmetic division by rT, so rT must be non-zero.", "default": ""}}, "required": ["r", "rT"], "type": "any"}}, "type": "function"}], "query": "We’re validating a hyperfine-contact real-space kernel implementation against a messy set of radial grid exports coming from mixed GPAW/ASE post-processing steps. Below are three radial-grid cohorts (in Å) as they appear in logs—some contain duplicated points (from concatenated segments) and some contain non-physical negative radii (from an origin-shift bug). For each cohort, evaluate the analytic extended-delta kernel only on the physically meaningful sampling points (i.e., radii that are >= 0), preserving their original ordering (including any duplicates that remain after filtering). Use a smoothing radius chosen from the cohort itself: set rT to the median of the non-negative radii in that cohort, but constrain rT to be within [0.3 Å, 1.2 Å] to avoid pathological peaks. Cohorts:\nA: [-0.2, 0.0, 0.05, 0.10, 0.50, 1.00, 2.50, 4.50]\nB: [0.0, 0.1, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]\nC: [-1.0, -0.5, 0.0, 0.2, 0.4, 0.6, 1.0, 1.4, 1.4, 2.0, 3.0]", "answers": "[{\"name\":\"gpaw_hyperfine_delta\",\"arguments\":{\"r\":[0.0,0.05,0.1,0.5,1.0,2.5,4.5],\"rT\":0.5}},{\"name\":\"gpaw_hyperfine_delta\",\"arguments\":{\"r\":[0.0,0.1,0.2,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3],\"rT\":0.6}},{\"name\":\"gpaw_hyperfine_delta\",\"arguments\":{\"r\":[0.0,0.2,0.4,0.6,1.0,1.4,1.4,2.0,3.0],\"rT\":1.0}}]"}
{"func_name": "gpaw_kpt_refine_get_reduced_monkhorst", "func_desc": "Find a Monkhorst-Pack k-point grid for a refined reciprocal-space grid and shrink it to the reciprocal-space volume of a single original k-point cell.\n    \n    This function is used in k-point refinement and folding workflows within GPAW (a DFT code using plane-waves/real-space grids) to generate the Monkhorst-Pack sampling for a refined grid specified by size and then scale (shrink) those k-point coordinates so they lie inside the volume corresponding to one original coarse k-point. It calls monkhorst_pack(size) to produce the refined-grid k-point coordinates and returns those coordinates divided by N_c so that the refined mesh is represented within a single original k-point cell. This is useful when mapping refined k-point sampling onto the Brillouin-zone volume of a coarse k-point for tasks such as supercell unfolding, k-point folding, or constructing reduced k-point sets for symmetry/reduction.", "tools": [{"function": {"description": "Find a Monkhorst-Pack k-point grid for a refined reciprocal-space grid and shrink it to the reciprocal-space volume of a single original k-point cell.\n\nThis function is used in k-point refinement and folding workflows within GPAW (a DFT code using plane-waves/real-space grids) to generate the Monkhorst-Pack sampling for a refined grid specified by size and then scale (shrink) those k-point coordinates so they lie inside the volume corresponding to one original coarse k-point. It calls monkhorst_pack(size) to produce the refined-grid k-point coordinates and returns those coordinates divided by N_c so that the refined mesh is represented within a single original k-point cell. This is useful when mapping refined k-point sampling onto the Brillouin-zone volume of a coarse k-point for tasks such as supercell unfolding, k-point folding, or constructing reduced k-point sets for symmetry/reduction.", "name": "gpaw_kpt_refine_get_reduced_monkhorst", "parameters": {"properties": {"size": {"type": "any", "description": "The integer tuple specifying the dimensions of the refined Monkhorst-Pack grid. Each element gives the number of subdivisions along a reciprocal-lattice direction for the refined grid; the tuple is passed directly to monkhorst_pack(size) to generate the refined k-point coordinates. In practical GPAW/DFT usage, size determines the density and anisotropy of the refined k-point sampling in reciprocal space.", "default": ""}, "N_c": {"type": "integer", "description": "An integer scaling factor equal to the number of refined-grid cells that correspond to one original (coarse) k-point cell. This function divides the refined-grid coordinates by N_c to map them into the volume of a single original k-point. In practice N_c is used to perform the folding/shrinking operation when transforming a finer k-point mesh to the reference domain of a coarser mesh.", "default": ""}}, "required": ["size", "N_c"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a GPAW k-point folding/unfolding workflow where incoming refinement requests are noisy: some were generated from an automated supercell builder and may be physically inconsistent. Given the following candidate refinement specs (each spec has a refined Monkhorst–Pack mesh `size` and an intended refinement factor per coarse k-point cell `N_c`):\n\n- R1: size = 6×6×4, N_c = 3\n- R2: size = 8×8×8, N_c = 2\n- R3: size = 4×4×2, N_c = 8\n- R4: size = 7×7×7, N_c = 2\n- R5: size = 4×6×4, N_c = 4\n\nGenerate reduced Monkhorst–Pack coordinates (refined mesh shrunk into the reciprocal-space volume of exactly one original coarse k-point cell by dividing by `N_c`) only for those refinement specs that are self-consistent under both conditions:\n1) the refined mesh must be symmetry-compatible for Γ-centered folding, i.e., all three refined-grid counts are even; and\n2) `N_c` must be a uniform per-direction refinement factor, meaning it must be exactly the cube root of the total refinement ratio implied by the mesh (so `N_c^3` matches the refined-to-coarse cell subdivision count).\n\nReturn the reduced k-point coordinate sets for every spec that passes these criteria.", "answers": "[{\"name\":\"gpaw_kpt_refine_get_reduced_monkhorst\",\"arguments\":{\"size\":[8,8,8],\"N_c\":2}}]"}
{"func_name": "gpaw_lcao_local_orbitals_get_plane_dirs", "func_desc": "Get normal and in-plane directions for a plane identifier used in GPAW's local-orbital and plane-related utilities.", "tools": [{"function": {"description": "Get normal and in-plane directions for a plane identifier used in GPAW's local-orbital and plane-related utilities.\n", "name": "gpaw_lcao_local_orbitals_get_plane_dirs", "parameters": {"properties": {"plane": {"type": "string", "description": "Pair of characters identifying the plane in Cartesian axes using letters 'x', 'y', 'z' (for example, 'xy' identifies the plane spanned by the x and y axes). In the GPAW DFT context (see README), this string is used by routines that need a compact user-facing way to specify a geometric plane for operations such as slicing densities, orienting local orbitals, or defining slab/plane orientations for analysis and visualization. The function sorts the characters and maps 'x'->0, 'y'->1, 'z'->2 to integer axis indices; valid characters are only 'x', 'y', and 'z'. The argument is interpreted literally; duplicates (e.g. 'xx') or strings of unexpected length are not validated beyond what the implementation performs and therefore produce results consistent with the mapping and subsequent set-difference logic described below.", "default": ""}}, "required": ["plane"], "type": "any"}}, "type": "function"}], "query": "We’re standardizing plane-orientation metadata for a mixed set of planar observables coming out of a GPAW LCAO analysis run. The raw plane tags are collected from multiple scripts and may include repeats and non-canonical labels:\n\nraw_plane_tags = ['yz', 'xz', 'yz', 'xy', 'x-y', 'zx', 'YZ']\n\nFor this QC pass, only keep plane identifiers that are already in GPAW’s compact two-letter canonical form (lowercase, two characters, drawn from {x,y,z} with no separators). For every retained tag, resolve the plane geometry by returning the normal axis index together with the two in-plane axis indices, in the same order produced by the utility. Preserve the original ordering of the retained tags (including duplicates) so we can cross-check downstream plotting determinism.", "answers": "[{\"name\":\"gpaw_lcao_local_orbitals_get_plane_dirs\",\"arguments\":{\"plane\":\"yz\"}},{\"name\":\"gpaw_lcao_local_orbitals_get_plane_dirs\",\"arguments\":{\"plane\":\"xz\"}},{\"name\":\"gpaw_lcao_local_orbitals_get_plane_dirs\",\"arguments\":{\"plane\":\"yz\"}},{\"name\":\"gpaw_lcao_local_orbitals_get_plane_dirs\",\"arguments\":{\"plane\":\"xy\"}}]"}
{"func_name": "gpaw_lcaotddft_qed_calculate_first_derivative", "func_desc": "Calculate the first time derivative of time-dependent data using a first-order forward finite difference.\n    \n    This function is used in the LCAO-TDDFT QED parts of GPAW to obtain time derivatives of time-dependent quantities (for example dipole moments, densities, or density-matrix elements) during real-time propagation or analysis of time-series output. It computes the forward first-order finite-difference approximation (coefficients [-1, 1]) so that the discrete derivative at time t is approximated by (f(t+dt) - f(t)) / dt. To preserve the original array shape along the time axis, the implementation repeats the last-time sample as required by the underlying forward_finite_difference routine; this ensures the returned array has the same shape as data_tx and can be used directly in subsequent GPAW time-stepping or post-processing code.", "tools": [{"function": {"description": "Calculate the first time derivative of time-dependent data using a first-order forward finite difference.\n\nThis function is used in the LCAO-TDDFT QED parts of GPAW to obtain time derivatives of time-dependent quantities (for example dipole moments, densities, or density-matrix elements) during real-time propagation or analysis of time-series output. It computes the forward first-order finite-difference approximation (coefficients [-1, 1]) so that the discrete derivative at time t is approximated by (f(t+dt) - f(t)) / dt. To preserve the original array shape along the time axis, the implementation repeats the last-time sample as required by the underlying forward_finite_difference routine; this ensures the returned array has the same shape as data_tx and can be used directly in subsequent GPAW time-stepping or post-processing code.", "name": "gpaw_lcaotddft_qed_calculate_first_derivative", "parameters": {"properties": {"timestep": {"type": "float", "description": "Time step dt used for the finite-difference quotient. This value is the spacing between successive samples along the time axis (first axis) of data_tx and is used as the denominator in the finite-difference formula. If timestep is zero, the division will produce infinities or NaNs (or raise, depending on NumPy error handling). If timestep is None, this function only accepts the special case documented below: when data_tx contains exactly one time point (len(data_tx) == 1), the function returns a zero array without performing division. If timestep is None and data_tx contains more than one time point, a TypeError or other failure will occur when attempting the division.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "float"}, "description": "Array of time-dependent data with the time axis as the first axis (axis 0). The remaining axes represent spatial, orbital, or other internal degrees of freedom used by GPAW LCAO-TDDFT/QED routines. The function treats consecutive entries along axis 0 as values at successive times separated by timestep. The input is not modified in place; a new array of the same shape is produced.", "default": ""}}, "required": ["timestep", "data_tx"], "type": "any"}}, "type": "function"}], "query": "We’re post-processing two real-time GPAW LCAO-TDDFT QED dipole-x diagnostics from a mixed run where the time sampling was not consistent across cohorts. Apply a forward first-order finite-difference derivative, preserving the original time-axis length by repeating the final sample. Use a cohort-specific effective timestep inferred from the intrinsic sampling: if the series appears quasi-uniform with median step <= 0.05 fs (treating the data as coming from a 0.05 fs propagation), use dt = 0.05 fs; if it shows a coarser effective sampling (median step > 0.05 fs), use dt = 0.10 fs to reflect decimated output. Process (i) the 5-step dipole series stored as a time-by-1 column array [[0.0],[0.12],[0.19],[0.15],[0.10]] a.u., and (ii) the 5-step dipole series stored as a flat time vector [0.0,0.12,0.25,0.40,0.60] a.u. Return derivatives with shapes matching their respective inputs for direct downstream propagation/analysis.", "answers": "[{\"name\":\"gpaw_lcaotddft_qed_calculate_first_derivative\",\"arguments\":{\"timestep\":0.05,\"data_tx\":[[0.0],[0.12],[0.19],[0.15],[0.1]]}},{\"name\":\"gpaw_lcaotddft_qed_calculate_first_derivative\",\"arguments\":{\"timestep\":0.1,\"data_tx\":[0.0,0.12,0.25,0.4,0.6]}}]"}
{"func_name": "gpaw_lcaotddft_qed_calculate_third_derivative", "func_desc": "gpaw.lcaotddft.qed.calculate_third_derivative calculates the third time derivative of time-dependent data using a forward finite-difference stencil and returns an array with the same shape as the input data_tx. This function is used in the GPAW LCAO TDDFT QED context to obtain the third temporal derivative of time-series quantities (axis 0 is time), where timestep is the uniform time increment used during time propagation.", "tools": [{"function": {"description": "gpaw.lcaotddft.qed.calculate_third_derivative calculates the third time derivative of time-dependent data using a forward finite-difference stencil and returns an array with the same shape as the input data_tx. This function is used in the GPAW LCAO TDDFT QED context to obtain the third temporal derivative of time-series quantities (axis 0 is time), where timestep is the uniform time increment used during time propagation.\n", "name": "gpaw_lcaotddft_qed_calculate_third_derivative", "parameters": {"properties": {"timestep": {"type": "float", "description": "Time step used to scale the finite-difference result (Δt). In normal use this must be a non-zero float equal to the sampling interval of the time axis of data_tx. The implementation also accepts None only for the special-case when data_tx contains a single time sample (len(data_tx) == 1); in that case the function returns an array of zeros with the same shape as data_tx without performing a division. If timestep is zero (and not the single-sample None special-case) a division by zero will occur and a ZeroDivisionError or FloatingPointError may be raised by the underlying arithmetic.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "float"}, "description": "Array of time-dependent data where the first axis (index 0) is the time axis and remaining axes are spatial, orbital, or other degrees of freedom used by GPAW LCAO TDDFT QED. The array may have any shape provided the time axis length is at least 1. To preserve the output shape, the algorithm repeats the data corresponding to the last time step as needed when applying the forward finite-difference stencil.", "default": ""}}, "required": ["timestep", "data_tx"], "type": "any"}}, "type": "function"}], "query": "In our TDDFT-QED regression runs we sometimes get mixed observables where the fastest signals need to be differentiated with their native propagation timestep, but slower channels can be safely differentiated on a coarser effective grid. You are given three raw time-series observables (time is axis 0), each with 6 recorded steps, but treat them heterogeneously based on their intrinsic variability: estimate a per-trace dynamic range as (max−min) along the time axis for each component/mode, and then compute the third time derivative (forward finite-difference stencil, same output shape as input) only for those traces whose dynamic range is at least 0.5 (in their own units). For any dataset where at least one trace passes this criterion, run the derivative on the subset of passing traces only.\n\nRaw observables:\n1) Dipole moment components (x,y) sampled with uniform Δt = 0.02 fs:\n[[0.00, 0.10],[0.03, 0.12],[0.08, 0.15],[0.15, 0.19],[0.24, 0.24],[0.35, 0.30]]\n2) Cavity-mode coordinate expectations for three modes sampled with uniform Δt = 0.02 fs (6×3 array):\n[[0.0, 0.1, -0.1],[0.05, 0.15, -0.05],[0.2, 0.3, 0.0],[0.45, 0.55, 0.1],[0.8, 0.9, 0.25],[1.25, 1.35, 0.45]]\n3) Molecular dipole x-component time trace sampled with uniform Δt = 0.05 fs (6×1 array):\n[[0.0],[0.12],[0.45],[0.80],[0.95],[1.00]]", "answers": "[{\"name\":\"gpaw_lcaotddft_qed_calculate_third_derivative\",\"arguments\":{\"timestep\":0.02,\"data_tx\":[[0.0,0.1,-0.1],[0.05,0.15,-0.05],[0.2,0.3,0.0],[0.45,0.55,0.1],[0.8,0.9,0.25],[1.25,1.35,0.45]]}},{\"name\":\"gpaw_lcaotddft_qed_calculate_third_derivative\",\"arguments\":{\"timestep\":0.05,\"data_tx\":[[0.0],[0.12],[0.45],[0.8],[0.95],[1.0]]}}]"}
{"func_name": "gpaw_lcaotddft_qed_forward_finite_difference", "func_desc": "gpaw.lcaotddft.qed.forward_finite_difference computes a forward finite-difference linear combination along the first axis (time axis) of a time-dependent data array using an explicit integer coefficient stencil.", "tools": [{"function": {"description": "gpaw.lcaotddft.qed.forward_finite_difference computes a forward finite-difference linear combination along the first axis (time axis) of a time-dependent data array using an explicit integer coefficient stencil.\n", "name": "gpaw_lcaotddft_qed_forward_finite_difference", "parameters": {"properties": {"coefficients": {"type": "array", "items": {"type": "integer"}, "description": "Ordered list of integer finite-difference coefficients that define the forward-difference stencil. Each entry in this list is multiplied with a shifted slice of the input data along the time axis and summed to produce the output at every time index. The coefficients are typically obtained from finite-difference coefficient tables (for example, see finite-difference coefficient references such as https://en.wikipedia.org/wiki/Finite_difference_coefficient). This list must be non-empty and its length determines the forward stencil width; providing an empty list or an invalid width will result in an error when constructing the padded array.", "default": ""}, "data_tx": {"type": "array", "items": {"type": "float"}, "description": "Time-dependent data array whose first axis is the time axis (length N). The remaining axes may represent spatial coordinates, channels, or other degrees of freedom used in LCAO-TDDFT/QED calculations in GPAW. The function treats data_tx as read-only and does not modify it in place. The array's dtype is preserved in the returned array. The function requires that data_tx has at least one time sample (N >= 1); passing an empty array along the first axis will raise an error during padding.", "default": ""}}, "required": ["coefficients", "data_tx"], "type": "any"}}, "type": "function"}], "query": "We’re doing QA-aware post-processing for LCAO-TDDFT/QED time-domain traces where different observables require different forward finite-difference stencils depending on their time-grid support and signal morphology. Use the following rule set:\n\n1) Treat any dataset as eligible only if its time axis length supports the stencil order (2-point requires ≥2 steps; 3-point requires ≥3 steps).\n2) For rank-3 orbital-resolved traces (time × orbitals × components), use the minimal forward-difference stencil (integer coefficients [-1, 1]) to avoid amplifying orbital mixing noise.\n3) For rank-2 dipole-like observables (time × {x,y}), choose the stencil based on the early-time curvature: compute Δx(t0)=x1−x0 and Δx(t1)=x2−x1; if Δx(t1) > Δx(t0), use the 3-point forward stencil (integer coefficients [-3, 4, -1]); otherwise use the 2-point stencil [-1, 1].\n4) If the dipole observable arrives inside an array wrapper (e.g., {\"array\": ...}), apply the same curvature-based rule using the wrapped x-channel.\n\nRaw cohorts/observables:\nA) Orbital-resolved 4-step, 2-orbital, 2-component time series:\n   data_tx = [[[0.0, 0.5], [1.0, 1.5]], [[0.2, 0.7], [1.1, 1.6]], [[0.5, 1.0], [1.3, 1.8]], [[0.9, 1.4], [1.6, 2.0]]]\nB) Dipole x/y 6-step observable:\n   data_tx = [[0.10, -0.05], [0.14, -0.02], [0.21, 0.01], [0.29, 0.05], [0.40, 0.09], [0.52, 0.14]]\nC) Dipole x/y 5-step observable in an array wrapper:\n   data_tx = {\"array\": [[0.0, 0.0], [0.1, 0.05], [0.18, 0.09], [0.25, 0.12], [0.30, 0.15]]}\n\nGenerate the forward finite-difference linear-combination traces along the time axis for each eligible cohort using the stencil selected by the rules above.", "answers": "[{\"name\":\"gpaw_lcaotddft_qed_forward_finite_difference\",\"arguments\":{\"coefficients\":[-1,1],\"data_tx\":[[[0.0,0.5],[1.0,1.5]],[[0.2,0.7],[1.1,1.6]],[[0.5,1.0],[1.3,1.8]],[[0.9,1.4],[1.6,2.0]]]}},{\"name\":\"gpaw_lcaotddft_qed_forward_finite_difference\",\"arguments\":{\"coefficients\":[-3,4,-1],\"data_tx\":[[0.1,-0.05],[0.14,-0.02],[0.21,0.01],[0.29,0.05],[0.4,0.09],[0.52,0.14]]}},{\"name\":\"gpaw_lcaotddft_qed_forward_finite_difference\",\"arguments\":{\"coefficients\":[-3,4,-1],\"data_tx\":{\"array\":[[0.0,0.0],[0.1,0.05],[0.18,0.09],[0.25,0.12],[0.3,0.15]]}}}]"}
{"func_name": "gpaw_lrtddft_apmb_sqrt_matrix", "func_desc": "Get the symmetric matrix square root of a (diagonalization is used).\n    This function is intended for use in GPAW's linear-response / TDDFT\n    code paths (gpaw.lrtddft.apmb) where square roots of symmetric matrices\n    (e.g. overlap, density, or response matrices that arise in DFT/PAW\n    calculations) are required. The operation performed is sqrt(a) =\n    Z * sqrt(D) * Z^T where a = Z * D * Z^T is the eigen-decomposition\n    computed with a Hermitian eigensolver. If preserve is False the\n    result overwrites the input array a in-place for memory efficiency;\n    if preserve is True the input is copied and left unchanged.", "tools": [{"function": {"description": "Get the symmetric matrix square root of a (diagonalization is used).\nThis function is intended for use in GPAW's linear-response / TDDFT\ncode paths (gpaw.lrtddft.apmb) where square roots of symmetric matrices\n(e.g. overlap, density, or response matrices that arise in DFT/PAW\ncalculations) are required. The operation performed is sqrt(a) =\nZ * sqrt(D) * Z^T where a = Z * D * Z^T is the eigen-decomposition\ncomputed with a Hermitian eigensolver. If preserve is False the\nresult overwrites the input array a in-place for memory efficiency;\nif preserve is True the input is copied and left unchanged.", "name": "gpaw_lrtddft_apmb_sqrt_matrix", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Square symmetric matrix to be square-rooted.\nThe function expects a to be a two-dimensional NumPy array of\nshape (n, n) containing real floating-point numbers (the code\nasserts a.dtype == float and a.shape == (n, n) when the\nmodule-level debug flag is enabled). In the GPAW application\ndomain this matrix typically represents a real symmetric\noperator (overlap, density, response, etc.) coming from DFT\ncalculations. The function diagonalizes a with a standard\nsymmetric eigensolver and uses the eigenvectors and eigenvalues\nto construct the matrix square root.", "default": ""}, "preserve": {"type": "boolean", "description": "If True, do not modify the input array a; a copy\nis made and the copy is returned. If False (default), the\ncomputation is performed in-place by reusing the input array\nstorage and the returned array is the same object as the input.\nUse preserve=True when the original matrix must be retained\nfor subsequent operations; use preserve=False to avoid the\nallocation cost of copying for large matrices in memory-sensitive\nDFT calculations.", "default": false}}, "required": ["a", "preserve"], "type": "any"}}, "type": "function"}], "query": "In a TDDFT linear-response regression test, I’m ingesting a small batch of candidate 3×3 real-symmetric overlap/metric matrices coming from different PAW setups. Before orthonormalizing the basis, gate the batch by physical admissibility: treat a matrix as a usable metric only if it is positive-definite (all eigenvalues strictly > 0) and symmetric to numerical tolerance. For every admissible matrix, compute the symmetric matrix square root A^{1/2} using the Hermitian eigen-decomposition path (A = Z D Z^T, then A^{1/2} = Z sqrt(D) Z^T). Use an in-memory safety policy where matrices with stronger conditioning risk (minimum eigenvalue < 0.25) must be preserved for later diagnostics, while well-conditioned matrices may be overwritten in-place for memory efficiency. Candidate matrices: M1 = [[1.00, 0.15, -0.05], [0.15, 0.90, 0.20], [-0.05, 0.20, 0.80]]; M2 = [[4.0, 1.0, 0.0], [1.0, 3.0, 1.0], [0.0, 1.0, 2.0]].", "answers": "[{\"name\":\"gpaw_lrtddft_apmb_sqrt_matrix\",\"arguments\":{\"a\":[[1.0,0.15,-0.05],[0.15,0.9,0.2],[-0.05,0.2,0.8]],\"preserve\":true}},{\"name\":\"gpaw_lrtddft_apmb_sqrt_matrix\",\"arguments\":{\"a\":[[4.0,1.0,0.0],[1.0,3.0,1.0],[0.0,1.0,2.0]],\"preserve\":false}}]"}
{"func_name": "gpaw_matrix_suggest_blocking", "func_desc": "Suggest blocking of an NxN matrix for distributed linear algebra in GPAW.\n    \n    This function is used inside GPAW (a DFT code that supports parallel calculations via MPI and ScaLAPACK) to choose a processor grid and a block size for distributing an NxN dense matrix across ncpus MPI ranks. The returned values (nprow, npcol, blocksize) are intended for use with ScaLAPACK-style block-cyclic distribution: nprow and npcol define the 2D process grid (rows and columns of MPI processes) and blocksize is the linear block dimension used to partition the matrix. The heuristic aims to make the processor grid as square as possible (minimizing imbalance between nprow and npcol), to guarantee at least one whole block per process dimension and at least two blocks in total, and to choose a power-of-two block size (for performance and compatibility), capped at 64.", "tools": [{"function": {"description": "Suggest blocking of an NxN matrix for distributed linear algebra in GPAW.\n\nThis function is used inside GPAW (a DFT code that supports parallel calculations via MPI and ScaLAPACK) to choose a processor grid and a block size for distributing an NxN dense matrix across ncpus MPI ranks. The returned values (nprow, npcol, blocksize) are intended for use with ScaLAPACK-style block-cyclic distribution: nprow and npcol define the 2D process grid (rows and columns of MPI processes) and blocksize is the linear block dimension used to partition the matrix. The heuristic aims to make the processor grid as square as possible (minimizing imbalance between nprow and npcol), to guarantee at least one whole block per process dimension and at least two blocks in total, and to choose a power-of-two block size (for performance and compatibility), capped at 64.", "name": "gpaw_matrix_suggest_blocking", "parameters": {"properties": {"N": {"type": "integer", "description": "The matrix dimension (number of rows and columns of the square matrix). In the GPAW/DFT context this is the global size of a distributed NxN matrix (for example a Hamiltonian or overlap matrix) that will be distributed across processes. The value is used to determine a block size that yields whole blocks and multiple blocks per distributed dimension.", "default": ""}, "ncpus": {"type": "integer", "description": "The total number of MPI processes available for the distributed calculation. The function attempts to factor ncpus into a 2D grid (nprow, npcol) with npcol chosen as a divisor of ncpus so that nprow * npcol == ncpus; ncpus therefore determines how the global matrix will be partitioned across processes.", "default": ""}}, "required": ["N", "ncpus"], "type": "any"}}, "type": "function"}], "query": "We’re validating GPAW’s ScaLAPACK blocking heuristic against a messy set of Hamiltonian-diagonalization workloads coming from mixed-precision restarts. Each record is a candidate dense NxN eigensolve with an MPI allocation; however, we only want to run the heuristic on cases that are actually ScaLAPACK-eligible: keep only those where (a) N is an exact power of two (FFT-friendly basis leading to square subspace sizes) and (b) ncpus factorizes into a 2D grid with both dimensions even (to match our node topology constraints). For every retained case, call the GPAW blocking suggester to obtain (nprow, npcol, blocksize).\n\nRaw batch:\n1) N=12000, ncpus=128\n2) N=4096, ncpus=64\n3) N=4096, ncpus=48\n4) N=8192, ncpus=96\n5) N=2048, ncpus=40", "answers": "[{\"name\":\"gpaw_matrix_suggest_blocking\",\"arguments\":{\"N\":4096,\"ncpus\":64}},{\"name\":\"gpaw_matrix_suggest_blocking\",\"arguments\":{\"N\":4096,\"ncpus\":48}}]"}
{"func_name": "gpaw_new_brillouin_ranks", "func_desc": "gpaw.new.brillouin.ranks computes a deterministic mapping from k-point indices to MPI rank indices for parallel k-point distribution used in GPAW's Brillouin-zone (k-point) parallelization logic. This function is used by the gpaw.new.brillouin module to assign each of K k-points to one of N MPI ranks so that the k-point workload is distributed as evenly as possible across ranks in density-functional theory (DFT) calculations.", "tools": [{"function": {"description": "gpaw.new.brillouin.ranks computes a deterministic mapping from k-point indices to MPI rank indices for parallel k-point distribution used in GPAW's Brillouin-zone (k-point) parallelization logic. This function is used by the gpaw.new.brillouin module to assign each of K k-points to one of N MPI ranks so that the k-point workload is distributed as evenly as possible across ranks in density-functional theory (DFT) calculations.\n", "name": "gpaw_new_brillouin_ranks", "parameters": {"properties": {"N": {"type": "integer", "description": "Number of MPI ranks (processes) available for distributing k-points. In the context of GPAW and parallel k-point sampling, N corresponds to the size of the MPI communicator used for k-point parallelization. Must be a positive integer; if N is zero a ZeroDivisionError will result from the underlying integer division.", "default": ""}, "K": {"type": "integer", "description": "Total number of k-points to distribute. K is the length of the k-point list or sampling grid for which the mapping is required. Must be a non-negative integer. In typical DFT runs K >= 0; negative values are not meaningful for k-point counts and the function's behavior is undefined for negative K.", "default": ""}}, "required": ["N", "K"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a heterogeneous GPAW production queue where k-point parallelization must be validated under realistic, messy input conditions. Below is the raw set of proposed MPI configurations coming from three different workflow generators (some entries are placeholders or malformed). For each entry, attempt a deterministic k-point-index → MPI-rank mapping using gpaw.new.brillouin.ranks **only when** the configuration is physically plausible and scheduler-safe: N and K must be positive integers, K must be at least N (no rank should be forced idle by construction), and K must correspond to an irreducible k-point count consistent with a Monkhorst–Pack grid size (i.e., K is a perfect cube). Use the accepted configurations exactly as provided (no rounding/coercion), and return the full mapping for each accepted case in the original order.\n\nRaw configurations (N ranks, K irreducible k-points):\n1) (8, 20)\n2) (8, 27)\n3) (10, 27)\n4) (0, 64)\n5) (16, -8)\n6) (12, 125)\n7) (7, 343)\n8) (9, 81)\n9) (6, 6)\n10) (11, 1000)\n11) (8, 26)\n12) (\"8\", 27)", "answers": "[{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":8,\"K\":27}},{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":12,\"K\":125}},{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":7,\"K\":343}},{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":9,\"K\":81}},{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":6,\"K\":6}},{\"name\":\"gpaw_new_brillouin_ranks\",\"arguments\":{\"N\":11,\"K\":1000}}]"}
{"func_name": "gpaw_new_gpw_as_single_precision", "func_desc": "gpaw.new.gpw.as_single_precision converts a NumPy array holding 64-bit real or complex floating-point numbers into a newly allocated NumPy array using 32-bit single-precision storage. This function is used in the GPAW DFT codebase to reduce memory footprint and communication volume (e.g., in MPI-parallel calculations) by casting double-precision arrays (numpy.float64 or numpy.complex128) to the corresponding single-precision types (numpy.float32 or numpy.complex64) while preserving array shape and numeric values to the extent representable in single precision.", "tools": [{"function": {"description": "gpaw.new.gpw.as_single_precision converts a NumPy array holding 64-bit real or complex floating-point numbers into a newly allocated NumPy array using 32-bit single-precision storage. This function is used in the GPAW DFT codebase to reduce memory footprint and communication volume (e.g., in MPI-parallel calculations) by casting double-precision arrays (numpy.float64 or numpy.complex128) to the corresponding single-precision types (numpy.float32 or numpy.complex64) while preserving array shape and numeric values to the extent representable in single precision.\n", "name": "gpaw_new_gpw_as_single_precision", "parameters": {"properties": {"array": {"type": "array", "items": {"type": "float"}, "description": "Input array containing floating-point data. The function requires that array.dtype is exactly numpy.float64 for real-valued data or numpy.complex128 for complex-valued data; these are the 64-bit types produced by many GPAW and NumPy operations in DFT workflows. The role of this argument is to supply the numeric data to be converted to single precision. Passing an array with a different dtype will cause the function to fail (see Raises). The function treats the input array as read-only and does not modify it in place.", "default": ""}}, "required": ["array"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing GPAW’s cast-to-single pathway under realistic “dirty grid” conditions. You are given two candidate real-space density tiles (Cohort A and Cohort B) that may contain either real-valued densities or complex-valued wavefunction-like tiles. For each cohort, first determine whether the tile is physically admissible for a density-like buffer by checking that all entries are finite (no NaN/Inf) and that the dynamic range is compatible with single precision, i.e., max(abs(x)) / max(min(abs(x)) over nonzero entries, smallest positive nonzero in tile) <= 1e12. Only cohorts that satisfy both criteria should be converted with gpaw.new.gpw.as_single_precision; cohorts that fail should be left unconverted (since they would be routed to a higher-precision path upstream). Use the provided sample tiles verbatim as the raw buffers:\n- Cohort A: [[[0.0, 1.23456789, 9.87654321], [1e-08, 3.14159265, 2.71828183]], [[4.56291827, 6.78912345, 8.90123456], [10.00000001, 0.33333333, 0.66666667]]]\n- Cohort B: [[[-1.234567890123, 0.0, 3.141592653589, 2.718281828459], [1e-12, -0.999999999999, 10.000000000001, -10.000000000001]], [[5.432109876543, -2.000000000001, 1.000000000001, -1.000000000001], [123456.7890123456, -123456.7890123456, 0.333333333333, -0.333333333333]]]. Return newly allocated single-precision arrays only for the cohorts that pass the admissibility gates.", "answers": "[{\"name\":\"gpaw_new_gpw_as_single_precision\",\"arguments\":{\"array\":[[[0.0,1.23456789,9.87654321],[1e-08,3.14159265,2.71828183]],[[4.56291827,6.78912345,8.90123456],[10.00000001,0.33333333,0.66666667]]]}},{\"name\":\"gpaw_new_gpw_as_single_precision\",\"arguments\":{\"array\":[[[-1.234567890123,0.0,3.141592653589,2.718281828459],[1e-12,-0.999999999999,10.000000000001,-10.000000000001]],[[5.432109876543,-2.000000000001,1.000000000001,-1.000000000001],[123456.7890123456,-123456.7890123456,0.333333333333,-0.333333333333]]]}}]"}
{"func_name": "gpaw_new_lcao_forces_add_den_mat_term", "func_desc": "gpaw.new.lcao.forces.add_den_mat_term computes and adds the density-matrix (erho) contribution arising from basis-function overlap (the dTheta/dR term) to the atomic force accumulator used in the LCAO (linear combination of atomic orbitals) implementation in GPAW. This function implements the loop over matrix block indices and performs in-place updates to the per-atom force array F_av by contracting the derivative-of-overlap tensor with the transposed density-matrix block using a NumPy einsum and taking the real part with the -2 prefactor that appears in the LCAO expression for the overlap contribution to forces. It is intended to be called during force evaluation (for geometry optimization, molecular dynamics, or force reporting) and is compatible with GPAW's MPI/domain-decomposed data layout when mya and my_row_range restrict the work to the local process.", "tools": [{"function": {"description": "gpaw.new.lcao.forces.add_den_mat_term computes and adds the density-matrix (erho) contribution arising from basis-function overlap (the dTheta/dR term) to the atomic force accumulator used in the LCAO (linear combination of atomic orbitals) implementation in GPAW. This function implements the loop over matrix block indices and performs in-place updates to the per-atom force array F_av by contracting the derivative-of-overlap tensor with the transposed density-matrix block using a NumPy einsum and taking the real part with the -2 prefactor that appears in the LCAO expression for the overlap contribution to forces. It is intended to be called during force evaluation (for geometry optimization, molecular dynamics, or force reporting) and is compatible with GPAW's MPI/domain-decomposed data layout when mya and my_row_range restrict the work to the local process.\n", "name": "gpaw_new_lcao_forces_add_den_mat_term", "parameters": {"properties": {"erhoT_MM": {"type": "array", "items": {"type": "float"}, "description": "The transposed density-matrix block (erho^T) provided as a NumPy array. In the LCAO forces context this array contains density-matrix elements for a global matrix index range; slices of this array indexed by the integer offsets computed from indices and my_row_range are contracted with dThetadR_vMM. The array must be indexable with the integer ranges produced in the function and compatible with the einsum contraction used inside the function.", "default": ""}, "dThetadR_vMM": {"type": "array", "items": {"type": "float"}, "description": "The derivative of the overlap (Theta) with respect to atomic coordinates, given as a NumPy array whose leading axis indexes force components (for example the three Cartesian components) and whose remaining axis/axes index matrix element positions consistent with erhoT_MM. Slices of dThetadR_vMM produced by the local row-range offsets are contracted with corresponding slices of erhoT_MM using the einsum signature used in the function. The array must be indexable with the integer ranges produced in the function and dimensionally compatible with erhoT_MM for the contraction.", "default": ""}, "F_av": {"type": "array", "items": {"type": "float"}, "description": "The per-atom force accumulator to be updated in place. F_av[a, :] is updated for each atom index a processed by this function. The array must support in-place subtraction of the computed vector contribution; its first axis must be addressable by the atom indices a supplied in indices and mya. This array is modified directly (no copy is returned), which is the primary side effect of the function.", "default": ""}, "indices": {"type": "array", "items": {"type": "float"}, "description": "A list of index tuples (a, M1, M2) describing blocks of the global matrix associated with atom a. Each tuple supplies an integer atom index a and integer matrix-range bounds M1 and M2 analogous to a block [M1:M2) or a contiguous block of matrix rows/columns. The function iterates over this list and processes only those tuples whose atom a is owned by the local process (membership in mya) and whose matrix block overlaps the local row range my_row_range.", "default": ""}, "mya": {"type": "array", "items": {"type": "float"}, "description": "A list of atom indices that are owned or processed by the current MPI rank / local worker. This list is used to skip index tuples for atoms not local to the process; only tuples with a in mya will cause updates to F_av on this process.", "default": ""}, "my_row_range": {"type": "any", "description": "A two-integer tuple specifying the global row-range assigned to this process (used to compute local slice offsets into erhoT_MM and dThetadR_vMM). The function uses my_row_range to determine overlap between the provided matrix block bounds (M1, M2) and the local rows, to compute local offsets m1 and m2, and to slice the input arrays accordingly. my_row_range must be consistent with the indexing used in indices and the shapes of erhoT_MM and dThetadR_vMM.", "default": ""}}, "required": ["erhoT_MM", "dThetadR_vMM", "F_av", "indices", "mya", "my_row_range"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the LCAO overlap-force (dΘ/dR) density-matrix term under messy, rank-local block metadata as it appears in domain-decomposed production runs.\n\nYou are given three MPI/data-partitioning replicates (A–C). For each replicate, treat `indices` as a raw block table that may contain blocks that are not physically owned by the current rank. Accumulate the overlap-force contribution **in place** into the provided `F_av`, but only for block-entries (a, M1, M2) that satisfy both locality constraints simultaneously:\n\n1) **Atom locality**: the block’s atom index `a` is present in the rank-local atom list `mya`.\n2) **Row-window consistency**: the block’s orbital row interval [M1, M2) lies entirely within the rank’s owned global row window `[my_row_range[0], my_row_range[1])`.\n\nAll other block-entries are to be ignored by virtue of failing one (or both) locality criteria.\n\nUse the exact numerical inputs below for each replicate (including any negative matrix elements). Execute the contraction exactly as in the LCAO formulation: contract `dThetadR_vMM[:, M1:M2, M1:M2]` with `erhoT_MM[M1:M2, M1:M2]` (transposed density-matrix block), take the real part, apply the `-2` prefactor, and add into `F_av[a]`.\n\nReplicate A:\n- erhoT_MM = [[0.1, 0.02, 0.03, 0.04, 0.05], [0.01, 0.2, 0.06, 0.07, 0.08], [0.02, 0.03, 0.3, 0.09, 0.1], [0.04, 0.05, 0.06, 0.4, 0.11], [0.05, 0.06, 0.07, 0.08, 0.5]]\n- dThetadR_vMM = [[[0.01, 0.0, 0.02, 0.01, 0.0], [0.0, 0.02, 0.01, 0.0, 0.01], [0.03, 0.01, 0.0, 0.02, 0.01], [0.0, 0.01, 0.02, 0.03, 0.0], [0.01, 0.0, 0.01, 0.0, 0.02]], [[0.02, 0.01, 0.0, 0.01, 0.03], [0.01, 0.0, 0.02, 0.01, 0.0], [0.0, 0.03, 0.01, 0.0, 0.01], [0.01, 0.0, 0.01, 0.02, 0.0], [0.0, 0.02, 0.0, 0.01, 0.02]], [[0.0, 0.01, 0.03, 0.0, 0.01], [0.02, 0.0, 0.01, 0.03, 0.0], [0.01, 0.02, 0.0, 0.01, 0.03], [0.0, 0.01, 0.02, 0.0, 0.01], [0.03, 0.0, 0.01, 0.02, 0.0]]]\n- F_av = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n- indices = [(0, 0, 4), (1, 2, 5)]\n- mya = [0, 2]\n- my_row_range = (0, 5)\n\nReplicate B:\n- erhoT_MM = [[0.8, 0.1, 0.0, 0.0], [0.1, 0.6, 0.05, 0.0], [0.0, 0.05, 0.7, 0.2], [0.0, 0.0, 0.2, 0.5]]\n- dThetadR_vMM = [[[0.0, 0.02, 0.0, 0.0], [0.02, 0.0, 0.01, 0.0], [0.0, 0.01, 0.0, 0.03], [0.0, 0.0, 0.03, 0.0]], [[0.0, -0.01, 0.0, 0.0], [-0.01, 0.0, 0.02, 0.0], [0.0, 0.02, 0.0, -0.01], [0.0, 0.0, -0.01, 0.0]], [[0.0, 0.0, 0.01, 0.0], [0.0, 0.0, 0.0, 0.02], [0.01, 0.0, 0.0, 0.0], [0.0, 0.02, 0.0, 0.0]]]\n- F_av = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n- indices = [(0, 0, 2), (2, 2, 4)]\n- mya = [0, 2]\n- my_row_range = (0, 4)\n\nReplicate C:\n- erhoT_MM = [[0.5, 0.1, -0.2, 0.0], [0.1, 0.6, 0.0, -0.1], [-0.2, 0.0, 0.7, 0.2], [0.0, -0.1, 0.2, 0.8]]\n- dThetadR_vMM = [[[0.01, -0.02, 0.0, 0.03], [0.0, 0.02, -0.01, 0.0], [0.01, 0.0, 0.02, -0.02], [0.0, 0.01, -0.01, 0.02]], [[0.0, 0.01, -0.02, 0.0], [0.02, 0.0, 0.01, -0.01], [-0.01, 0.02, 0.0, 0.01], [0.01, -0.01, 0.02, 0.0]], [[-0.02, 0.0, 0.01, 0.02], [0.01, -0.01, 0.0, 0.01], [0.0, 0.02, -0.02, 0.0], [0.02, 0.0, 0.01, -0.01]]]\n- F_av = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n- indices = [(0, 0, 2), (1, 2, 4)]\n- mya = [1]\n- my_row_range = (1, 4)\n\nRun the overlap-force accumulation for each replicate and leave the resulting forces in the same `F_av` arrays for downstream regression checks.", "answers": "[{\"name\":\"gpaw_new_lcao_forces_add_den_mat_term\",\"arguments\":{\"erhoT_MM\":[[0.1,0.02,0.03,0.04,0.05],[0.01,0.2,0.06,0.07,0.08],[0.02,0.03,0.3,0.09,0.1],[0.04,0.05,0.06,0.4,0.11],[0.05,0.06,0.07,0.08,0.5]],\"dThetadR_vMM\":[[[0.01,0.0,0.02,0.01,0.0],[0.0,0.02,0.01,0.0,0.01],[0.03,0.01,0.0,0.02,0.01],[0.0,0.01,0.02,0.03,0.0],[0.01,0.0,0.01,0.0,0.02]],[[0.02,0.01,0.0,0.01,0.03],[0.01,0.0,0.02,0.01,0.0],[0.0,0.03,0.01,0.0,0.01],[0.01,0.0,0.01,0.02,0.0],[0.0,0.02,0.0,0.01,0.02]],[[0.0,0.01,0.03,0.0,0.01],[0.02,0.0,0.01,0.03,0.0],[0.01,0.02,0.0,0.01,0.03],[0.0,0.01,0.02,0.0,0.01],[0.03,0.0,0.01,0.02,0.0]]],\"F_av\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"indices\":[[0,0,4],[1,2,5]],\"mya\":[0,2],\"my_row_range\":[0,5]}},{\"name\":\"gpaw_new_lcao_forces_add_den_mat_term\",\"arguments\":{\"erhoT_MM\":[[0.8,0.1,0.0,0.0],[0.1,0.6,0.05,0.0],[0.0,0.05,0.7,0.2],[0.0,0.0,0.2,0.5]],\"dThetadR_vMM\":[[[0.0,0.02,0.0,0.0],[0.02,0.0,0.01,0.0],[0.0,0.01,0.0,0.03],[0.0,0.0,0.03,0.0]],[[0.0,-0.01,0.0,0.0],[-0.01,0.0,0.02,0.0],[0.0,0.02,0.0,-0.01],[0.0,0.0,-0.01,0.0]],[[0.0,0.0,0.01,0.0],[0.0,0.0,0.0,0.02],[0.01,0.0,0.0,0.0],[0.0,0.02,0.0,0.0]]],\"F_av\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"indices\":[[0,0,2],[2,2,4]],\"mya\":[0,2],\"my_row_range\":[0,4]}},{\"name\":\"gpaw_new_lcao_forces_add_den_mat_term\",\"arguments\":{\"erhoT_MM\":[[0.5,0.1,-0.2,0.0],[0.1,0.6,0.0,-0.1],[-0.2,0.0,0.7,0.2],[0.0,-0.1,0.2,0.8]],\"dThetadR_vMM\":[[[0.01,-0.02,0.0,0.03],[0.0,0.02,-0.01,0.0],[0.01,0.0,0.02,-0.02],[0.0,0.01,-0.01,0.02]],[[0.0,0.01,-0.02,0.0],[0.02,0.0,0.01,-0.01],[-0.01,0.02,0.0,0.01],[0.01,-0.01,0.02,0.0]],[[-0.02,0.0,0.01,0.02],[0.01,-0.01,0.0,0.01],[0.0,0.02,-0.02,0.0],[0.02,0.0,0.01,-0.01]]],\"F_av\":[[0.0,0.0,0.0],[0.0,0.0,0.0]],\"indices\":[[0,0,2],[1,2,4]],\"mya\":[1],\"my_row_range\":[1,4]}}]"}
{"func_name": "gpaw_new_symmetry_find_lattice_symmetry", "func_desc": "gpaw.new.symmetry.find_lattice_symmetry determines the set of lattice symmetry operations (3x3 integer matrices with elements in {-1, 0, 1}) that conserve the metric of a given unit cell and that do not swap axes with different periodic boundary conditions. This function is used in GPAW's symmetry handling to identify candidate point-group operations that map the cell basis vectors onto each other while preserving inter-vector lengths and respecting periodicity, which is important for exploiting symmetry in DFT calculations (for example reducing k-point sampling or identifying equivalent atoms).", "tools": [{"function": {"description": "gpaw.new.symmetry.find_lattice_symmetry determines the set of lattice symmetry operations (3x3 integer matrices with elements in {-1, 0, 1}) that conserve the metric of a given unit cell and that do not swap axes with different periodic boundary conditions. This function is used in GPAW's symmetry handling to identify candidate point-group operations that map the cell basis vectors onto each other while preserving inter-vector lengths and respecting periodicity, which is important for exploiting symmetry in DFT calculations (for example reducing k-point sampling or identifying equivalent atoms).\n", "name": "gpaw_new_symmetry_find_lattice_symmetry", "parameters": {"properties": {"cell_cv": {"type": "array", "items": {"type": "float"}, "description": "3x3 array of Cartesian cell vectors that define the unit cell. Each row (or column, depending on calling convention) represents one basis vector in Cartesian coordinates; the function computes metric_cc = cell_cv.dot(cell_cv.T) and therefore expects a square 3x3 numeric array. The metric (inner products of cell vectors) is the quantity that must be conserved by symmetry operations. Supplying an array with incompatible shape or non-numeric entries will lead to NumPy broadcasting/indexing errors.", "default": ""}, "pbc_c": {"type": "array", "items": {"type": "float"}, "description": "1-D boolean array of length 3 indicating periodic boundary conditions along each Cartesian axis (True if periodic along that axis, False if non-periodic). The function uses pbc_c to build a 3x3 boolean matrix of axis-pair differences (via logical_xor.outer) and then forbids any symmetry operation that would swap two axes for which pbc differs, ensuring that non-periodic directions are not exchanged with periodic ones.", "default": ""}, "tol": {"type": "float", "description": "Numeric tolerance controlling how strictly the function requires conservation of the cell metric. The function computes metric_scc for each candidate integer matrix and compares the deviation abs(metric_scc - metric_cc).sum(2).sum(1) to a threshold. If _backwards_compatible is True the threshold is tol; otherwise the threshold is tol**2. Choose tol consistent with the units of metric_cc (squared length units) and the expected numerical noise; extremely small tol may yield an empty operation set, while extremely large tol may admit spurious operations.", "default": ""}, "_backwards_compatible": {"type": "boolean", "description": "If True, use legacy comparison behavior where the summed absolute difference of metric matrices is compared to tol. If False (the default), compare the summed absolute difference to tol**2. This flag exists to preserve older code behavior when exact numeric interpretation of tol differs between versions; it does not otherwise change how candidate matrices are generated or how pbc constraints are applied.", "default": false}}, "required": ["cell_cv", "pbc_c", "tol", "_backwards_compatible"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing GPAW’s slab lattice-symmetry detection on a mixed batch of candidate surface cells (pbc=[True, True, False]) coming from a geometry-optimization sweep where some inputs are slightly distorted. For each cell, decide the tolerance from intrinsic in-plane anisotropy: compute the relative in-plane mismatch r = |a_len - b_len| / max(a_len, b_len). If r <= 1e-9 treat it as effectively tetragonal and use a strict tol=1e-8 with the non-legacy tolerance behavior; if 1e-9 < r <= 1e-4 treat it as quasi-tetragonal and use tol=1e-7; if r > 1e-4 treat it as significantly distorted and use tol=1e-6. Always enforce that the non-periodic vacuum axis is not exchanged with periodic axes by keeping pbc=[True, True, False]. Process this raw batch (Å):\n1) a=(3.0,0,0), b=(0,3.0,0), c=(0,0,5.0)\n2) a=(3.905,0,0), b=(0,3.905,0), c=(0,0,20.0)\n3) a=(3.905,0,0), b=(0,3.9046,0), c=(0,0,20.0)\n4) a=(3.0,0,0), b=(0,3.002,0), c=(0,0,5.0)\nReturn the full set of candidate integer symmetry operations for each retained batch item under its assigned tolerance regime.", "answers": "[{\"name\":\"gpaw_new_symmetry_find_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.0,0.0,0.0],[0.0,3.0,0.0],[0.0,0.0,5.0]],\"pbc_c\":[true,true,false],\"tol\":1e-08,\"_backwards_compatible\":false}},{\"name\":\"gpaw_new_symmetry_find_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.905,0.0,0.0],[0.0,3.905,0.0],[0.0,0.0,20.0]],\"pbc_c\":[true,true,false],\"tol\":1e-08,\"_backwards_compatible\":false}},{\"name\":\"gpaw_new_symmetry_find_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.905,0.0,0.0],[0.0,3.9046,0.0],[0.0,0.0,20.0]],\"pbc_c\":[true,true,false],\"tol\":1e-07}},{\"name\":\"gpaw_new_symmetry_find_lattice_symmetry\",\"arguments\":{\"cell_cv\":[[3.0,0.0,0.0],[0.0,3.002,0.0],[0.0,0.0,5.0]],\"pbc_c\":[true,true,false],\"tol\":1e-06}}]"}
{"func_name": "gpaw_new_symmetry_mat", "func_desc": "gpaw.new.symmetry.mat converts a 3x3 matrix (typically a rotation matrix used in GPAW's symmetry utilities) into a compact, human-readable string representation. This function is intended for producing deterministic textual forms of small integer or numeric matrices for logging, comparison, or inclusion in text output generated by GPAW symmetry code paths.", "tools": [{"function": {"description": "gpaw.new.symmetry.mat converts a 3x3 matrix (typically a rotation matrix used in GPAW's symmetry utilities) into a compact, human-readable string representation. This function is intended for producing deterministic textual forms of small integer or numeric matrices for logging, comparison, or inclusion in text output generated by GPAW symmetry code paths.\n", "name": "gpaw_new_symmetry_mat", "parameters": {"properties": {"rot_cc": {"type": "array", "items": {"type": "float"}, "description": "A list-like object of row iterables that represents a 3x3 matrix (for example, a rotation matrix). Each element of rot_cc is expected to be an iterable of three numeric values (ints or floats) corresponding to one row of the matrix. The function does not perform strict shape validation: it iterates over rot_cc and each contained iterable, formatting each element with the format specifier f'{r:2}'. Practically, callers should pass a list of three lists (or tuples) each of length three; passing other shapes will produce a string reflecting the provided shape but may not be meaningful in the symmetry context. Elements must be formatable with the format specification '2'; otherwise Python's formatting will raise a TypeError.", "default": ""}}, "required": ["rot_cc"], "type": "any"}}, "type": "function"}], "query": "In a GPAW symmetry regression test, we ingest a messy stream of candidate 3×3 symmetry operators from two pipeline stages (pre-run detector and post-run reporter). Each stage may emit near-integer floats, sign-flipped variants, or slightly non-orthogonal matrices due to numerical noise. From the combined list below, generate deterministic compact log strings **only** for matrices that (i) are 3×3, (ii) represent a proper rotation consistent with a 90° rotation about the z-axis (i.e., the operator maps the z-axis to itself and maps the x-axis onto ±y and the y-axis onto ∓x with unit coefficients), and (iii) are integer-valued after exact parsing (no rounding). Use gpaw.new.symmetry.mat for every qualifying matrix, preserving input order.\n\nRaw candidate operators (stage-tagged):\n1) pre: [[0, -1, 0], [1, 0, 0], [0, 0, 1]]\n2) post: [[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]\n3) pre: [[0, 1, 0], [-1, 0, 0], [0, 0, 1]]\n4) post: [[0, -1, 0], [1, 0, 0], [0, 0, -1]]\n5) pre: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n6) post: [[0, -1, 0], [1, 0, 0], [0, 0, 1.0000001]]", "answers": "[{\"name\":\"gpaw_new_symmetry_mat\",\"arguments\":{\"rot_cc\":[[0,-1,0],[1,0,0],[0,0,1]]}},{\"name\":\"gpaw_new_symmetry_mat\",\"arguments\":{\"rot_cc\":[[0,1,0],[-1,0,0],[0,0,1]]}}]"}
{"func_name": "gpaw_occupations_marzari_vanderbilt", "func_desc": "Marzari-Vanderbilt cold-smearing occupation function used in GPAW to compute smeared electronic occupations, their energy derivatives, and per-state entropy energy corrections. This implements the Marzari-Vanderbilt \"cold smearing\" distribution (see DOI 10.1103/PhysRevLett.82.3296) and is intended for use in electronic-structure (DFT) calculations within GPAW to stabilize convergence for metallic or partially occupied systems by smoothing the occupation step at the Fermi level.\n    \n    This function evaluates the Marzari–Vanderbilt analytic expressions in a vectorized NumPy form. Given an array of single-particle eigenvalues and a chemical potential (fermi_level), it returns: (1) occupation numbers between 0 and 1 that replace the zero-temperature step function; (2) the derivative of these occupations with respect to the eigenvalue, useful for response or density-derivative calculations; and (3) a per-state energy-like entropy correction term that is used when computing the smeared free-energy correction to the total energy in DFT workflows. All returned arrays have the same shape as the input eig array. The algorithm uses width as the smearing parameter and performs elementwise operations (no in-place modification of inputs).", "tools": [{"function": {"description": "Marzari-Vanderbilt cold-smearing occupation function used in GPAW to compute smeared electronic occupations, their energy derivatives, and per-state entropy energy corrections. This implements the Marzari-Vanderbilt \"cold smearing\" distribution (see DOI 10.1103/PhysRevLett.82.3296) and is intended for use in electronic-structure (DFT) calculations within GPAW to stabilize convergence for metallic or partially occupied systems by smoothing the occupation step at the Fermi level.\n\nThis function evaluates the Marzari–Vanderbilt analytic expressions in a vectorized NumPy form. Given an array of single-particle eigenvalues and a chemical potential (fermi_level), it returns: (1) occupation numbers between 0 and 1 that replace the zero-temperature step function; (2) the derivative of these occupations with respect to the eigenvalue, useful for response or density-derivative calculations; and (3) a per-state energy-like entropy correction term that is used when computing the smeared free-energy correction to the total energy in DFT workflows. All returned arrays have the same shape as the input eig array. The algorithm uses width as the smearing parameter and performs elementwise operations (no in-place modification of inputs).", "name": "gpaw_occupations_marzari_vanderbilt", "parameters": {"properties": {"eig": {"type": "array", "items": {"type": "float"}, "description": "Array of single-particle eigenvalues (energies) for electronic states. In GPAW workflows this is typically the Kohn–Sham eigenvalue array produced by a diagonalization step. The function preserves the shape of eig and returns arrays with the same shape. The energy units must be consistent between eig and fermi_level.", "default": ""}, "fermi_level": {"type": "float", "description": "The chemical potential (Fermi level) used as the reference energy for occupations. This scalar shifts the eigenvalues before applying the smearing function; it must be given in the same energy units as eig. Practical significance: occupations are determined relative to this value to model partially filled states near the Fermi surface.", "default": ""}, "width": {"type": "float", "description": "Smearing width (sigma) controlling the extent of cold smearing. This scalar sets the energy scale over which the occupation step is smoothed. It must be non-zero; physically meaningful values are positive (width <= 0 will produce division-by-zero or nonphysical results). The choice of width affects convergence: very small widths approach the zero-temperature step and may cause numerical instabilities, while very large widths overly smear occupations and bias energies.", "default": ""}}, "required": ["eig", "fermi_level", "width"], "type": "any"}}, "type": "function"}], "query": "We’re validating a metallic SCF workflow under messy k-point sampling where each snapshot reports raw single-particle eigenvalues plus its own Fermi level estimate. Treat the following as independent k-point snapshots:\n\nSnapshot A (slab reference): eig (eV) = [-7.8, -3.2, -0.4, 0.1, 0.35, 1.2], fermi_level = 0.0 eV.\nSnapshot B (near-EF stress test): eig (eV) = [-2.1, -0.3, 0.0, 0.15, 0.42, 1.2], fermi_level = 0.05 eV.\n\nFor each snapshot, run Marzari–Vanderbilt cold-smearing postprocessing to obtain (i) smeared occupations, (ii) d(occupation)/d(eigenvalue), and (iii) the per-state entropy energy correction.\n\nUse an adaptive smearing rule per snapshot based on proximity to the reported Fermi level: if any eigenvalue in that snapshot lies within ±0.12 eV of its fermi_level, use width = 0.10 eV (tighter smearing for near-EF crowding); otherwise use width = 0.20 eV. Return outputs aligned elementwise to each provided eigenvalue list.", "answers": "[{\"name\":\"gpaw_occupations_marzari_vanderbilt\",\"arguments\":{\"eig\":[-7.8,-3.2,-0.4,0.1,0.35,1.2],\"fermi_level\":0.0,\"width\":0.2}},{\"name\":\"gpaw_occupations_marzari_vanderbilt\",\"arguments\":{\"eig\":[-2.1,-0.3,0.0,0.15,0.42,1.2],\"fermi_level\":0.05,\"width\":0.1}}]"}
{"func_name": "gpaw_point_groups_check_sphere", "func_desc": "gpaw.point_groups.check.sphere: Return Cartesian coordinates of grid points that lie inside or on a sphere of given radius centered at the origin in a uniform real-space grid. This helper is used by GPAW routines that need a discrete sampling of a spherical region on a regular grid (for example, symmetry checks, point-group operations, or localized real-space operations on the PAW grid).", "tools": [{"function": {"description": "gpaw.point_groups.check.sphere: Return Cartesian coordinates of grid points that lie inside or on a sphere of given radius centered at the origin in a uniform real-space grid. This helper is used by GPAW routines that need a discrete sampling of a spherical region on a regular grid (for example, symmetry checks, point-group operations, or localized real-space operations on the PAW grid).\n", "name": "gpaw_point_groups_check_sphere", "parameters": {"properties": {"radius": {"type": "float", "description": "Radius of the sphere in the same length units as grid_spacing (e.g., Å when used together with ASE/GPAW coordinates). This controls which grid points are kept: any grid point whose squared distance from the origin is less than or equal to radius**2 is included. The code computes an integer number of grid steps npts = int(radius / grid_spacing) + 1 to construct the grid extent; typical use expects radius >= 0. If radius is zero the function will return only the grid points at the origin (subject to floating-point equality).", "default": ""}, "grid_spacing": {"type": "float", "description": "Distance between adjacent grid points along each Cartesian axis, in the same length units as radius. This must be a positive floating-point spacing representing the real-space resolution of the uniform grid used by GPAW. The function uses grid_spacing to build a cubic mesh of points at positions k * grid_spacing for integer k in [-npts, npts].", "default": ""}}, "required": ["radius", "grid_spacing"], "type": "any"}}, "type": "function"}], "query": "In a GPAW real-space symmetry/PAW validation run, we need a robust spherical grid sampling that adapts to numerical sensitivity. We have two downstream branches consuming the same helper but with branch-specific discretization policies.\n\nBranch (i) is a point-group symmetry check that is sensitive to boundary aliasing: use a sphere radius snapped to the nearest lower multiple of the grid spacing (so the discrete sphere does not extend past the intended cutoff).\n\nBranch (ii) is a PAW-localized real-space operation that must not underestimate the integration domain: use a sphere radius snapped to the nearest higher multiple of the grid spacing.\n\nUse a target physical radius of 3.5 Å and a uniform Cartesian grid spacing of 0.25 Å. For each branch, generate the full list of Cartesian coordinates of all uniform-grid points that lie inside or exactly on the resulting snapped-radius sphere centered at the origin, and return both coordinate lists.", "answers": "[{\"name\":\"gpaw_point_groups_check_sphere\",\"arguments\":{\"radius\":3.5,\"grid_spacing\":0.25}},{\"name\":\"gpaw_point_groups_check_sphere\",\"arguments\":{\"radius\":3.5,\"grid_spacing\":0.25}}]"}
{"func_name": "gpaw_pseudopotential_get_radial_hartree_energy", "func_desc": "Get energy of the l=0 (spherically symmetric) compensation charge on an\n    equidistant radial grid used in GPAW pseudopotential/PAW routines.\n    \n    This function is used in the GPAW pseudopotential machinery to compute the\n    electrostatic (Hartree) self-energy associated with the l=0 component of a\n    compensation charge represented on a 1D radial grid. The routine assumes an\n    equidistant radial grid and constructs the integrand used by the Hartree\n    solver (hartree_solve) to obtain r*V_H(r) (where V_H is the Hartree potential).\n    The final Hartree energy is obtained by integrating 2*pi * ∫ rho(r) V_H(r) r dr\n    using the simple trapezoid-like scaling implemented in the source code.", "tools": [{"function": {"description": "Get energy of the l=0 (spherically symmetric) compensation charge on an\nequidistant radial grid used in GPAW pseudopotential/PAW routines.\n\nThis function is used in the GPAW pseudopotential machinery to compute the\nelectrostatic (Hartree) self-energy associated with the l=0 component of a\ncompensation charge represented on a 1D radial grid. The routine assumes an\nequidistant radial grid and constructs the integrand used by the Hartree\nsolver (hartree_solve) to obtain r*V_H(r) (where V_H is the Hartree potential).\nThe final Hartree energy is obtained by integrating 2*pi * ∫ rho(r) V_H(r) r dr\nusing the simple trapezoid-like scaling implemented in the source code.", "name": "gpaw_pseudopotential_get_radial_hartree_energy", "parameters": {"properties": {"r_g": {"type": "array", "items": {"type": "float"}, "description": "1D array of radial grid points (r values) in\nascending order representing the radial coordinate for the\ncompensation charge. The function expects an equidistant grid so that\ndr = r_g[2] - r_g[1] is the uniform spacing used internally. The\nfirst point r_g[0] may be a small nonzero value (e.g. 1e-8) in some\nworkflows to avoid division-by-zero in downstream Poisson solvers;\nsuch shifts are accepted and handled by this routine. The array is\nnot modified in-place.", "default": ""}, "rho_g": {"type": "array", "items": {"type": "float"}, "description": "1D array of the same length as r_g containing the\nradial charge density values for the l=0 compensation charge. Each\nentry corresponds to rho(r) evaluated at the grid points r_g. The\nroutine uses these values to form rho(r)*r*dr for the radial Poisson\nproblem; rho_g is not modified in-place.", "default": ""}}, "required": ["r_g", "rho_g"], "type": "any"}}, "type": "function"}], "query": "We’re validating a PAW dataset generator against GPAW’s l=0 compensation-charge Hartree self-energy routine, but the raw radial profiles are coming from mixed sources and may contain non-physical artifacts. Treat the inputs below as three candidate compensation-charge profiles on nominally equidistant radial grids. First, identify which profiles are admissible for GPAW-style evaluation by requiring: (i) the radial grid is equidistant to within floating-point tolerance across the full domain, (ii) the grid starts at r=0 or at a tiny positive offset (to avoid the r=0 singular point), (iii) the compensation charge is finite at the origin (rho[0] must be 0), and (iv) the profile represents a localized augmentation charge (rho must return to ~0 at the outermost radius). For each admissible profile, compute the electrostatic (Hartree) self-energy exactly as GPAW does for an equidistant 1D radial grid (construct the Hartree-solver integrand to obtain r·V_H(r) on the same grid, then integrate the final energy as 2π·∫ rho(r)·V_H(r)·r dr using GPAW’s trapezoid-like scaling). Use the grids exactly as provided (including any tiny nonzero first radius) without resampling.\n\nCandidate profiles:\n(A) r_g = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    rho_g = [0.0, 0.20, 0.32, 0.28, 0.18, 0.10, 0.06, 0.03, 0.015, 0.0]\n(B) r_g = [1e-08, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5]\n    rho_g = [0.0, 0.25, 0.18, 0.12, 0.06, 0.0]\n(C) r_g = [1e-08, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]\n    rho_g = [0.0, 0.12, 0.38, 0.55, 0.60, 0.52, 0.40, 0.28, 0.18, 0.10, 0.05]\n\nReturn the Hartree self-energy for each admissible profile (and only those).", "answers": "[{\"name\":\"gpaw_pseudopotential_get_radial_hartree_energy\",\"arguments\":{\"r_g\":[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"rho_g\":[0.0,0.2,0.32,0.28,0.18,0.1,0.06,0.03,0.015,0.0]}},{\"name\":\"gpaw_pseudopotential_get_radial_hartree_energy\",\"arguments\":{\"r_g\":[1e-08,0.1,0.2,0.30000000000000004,0.4,0.5],\"rho_g\":[0.0,0.25,0.18,0.12,0.06,0.0]}}]"}
{"func_name": "gpaw_quiz_encode", "func_desc": "gpaw.quiz.encode encodes a Unicode text string by performing a per-character substitution\n    using the module-level mapping used to reproduce the transformation in the Python\n    \"The Zen of Python\" (this.py). This function is part of the gpaw.quiz utilities and is\n    intended for simple, deterministic text transformations (for example, quiz or\n    Easter-egg style encodings that mirror the encoding used by Python's this module).", "tools": [{"function": {"description": "gpaw.quiz.encode encodes a Unicode text string by performing a per-character substitution\nusing the module-level mapping used to reproduce the transformation in the Python\n\"The Zen of Python\" (this.py). This function is part of the gpaw.quiz utilities and is\nintended for simple, deterministic text transformations (for example, quiz or\nEaster-egg style encodings that mirror the encoding used by Python's this module).", "name": "gpaw_quiz_encode", "parameters": {"properties": {"text": {"type": "string", "description": "The input Unicode text to encode. Each character in this string is\nlooked up in a module-level dictionary named `d` (the same kind of mapping\nused by Python's this.py implementation of \"The Zen of Python\"). For every\ncharacter c in `text`, the function replaces c with d.get(c, c) and\nconcatenates the results to produce the encoded string. The function expects\na Python str and treats the input as a sequence of characters; it does not\nmodify the input string in place.", "default": ""}}, "required": ["text"], "type": "any"}}, "type": "function"}], "query": "We’re validating a text-obfuscation step in our lab’s onboarding pipeline that must exactly mirror Python’s `this.py` substitution cipher. We have a mixed bag of draft snippets from SOPs and style-guide checklists; some are safe to route through the cipher, while others contain non-ASCII symbols from copy/paste artifacts and must be left out of this encoding stage.\n\nFrom the following candidate strings, encode only those whose characters are entirely within 7-bit ASCII (i.e., all code points <= 127), preserving punctuation/case/spacing exactly as-is:\n\n1) \"Read the project guidelines carefully before deployment.\"\n2) \"Readability counts.\"\n3) \"Operator notes: naïve baseline—recheck Café sample IDs.\"\n4) \"Checksum: ΔE=0.12; OK\"\n\nReturn the encoded outputs for the qualifying strings in the same order they appear above.", "answers": "[{\"name\":\"gpaw_quiz_encode\",\"arguments\":{\"text\":\"Read the project guidelines carefully before deployment.\"}},{\"name\":\"gpaw_quiz_encode\",\"arguments\":{\"text\":\"Readability counts.\"}}]"}
{"func_name": "gpaw_response_chiks_create_get_temporal_part", "func_desc": "gpaw.response.chiks.create_get_temporal_part selects and returns the function that implements the temporal part of a band-summation based response calculation used in GPAW's response.chiks code path for density-functional theory (DFT) response/susceptibility evaluations.", "tools": [{"function": {"description": "gpaw.response.chiks.create_get_temporal_part selects and returns the function that implements the temporal part of a band-summation based response calculation used in GPAW's response.chiks code path for density-functional theory (DFT) response/susceptibility evaluations.\n", "name": "gpaw_response_chiks_create_get_temporal_part", "parameters": {"properties": {"bandsummation": {"type": "string", "description": "Strategy identifier that determines which temporal-part implementation to use when assembling band-summation contributions to a response function. In the context of GPAW (a DFT code using PAW and ASE), this parameter controls how the time-/frequency-dependent factor that multiplies band-index sums is computed. Valid values are the literal strings 'double' and 'pairwise', corresponding to the implementations available in this module. This argument is required (no default) and must be supplied by the caller that constructs or configures response calculations.", "default": ""}}, "required": ["bandsummation"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating temporal-part configuration for a small set of GPAW response.chiks susceptibility replicates coming from a mixed compute environment. Each replicate includes a pre-flight metadata block with `replicate_id`, `kmesh` (3 ints), `n_bands`, `available_memory_gb`, and an `io_bandwidth_score` (0–1). Select the temporal-part implementation via `create_get_temporal_part` using a rule that mimics our production policy: choose the more memory-efficient pairwise band-summation only for replicates whose estimated band-pair working set is likely to pressure memory (treat higher `n_bands` and lower `available_memory_gb` as risk amplifiers), otherwise keep the default double band-summation. Apply this to the following replicates and return the selected temporal-part function choice for each: (1) replicate_id='R1', kmesh=[6,6,6], n_bands=240, available_memory_gb=12, io_bandwidth_score=0.35; (2) replicate_id='R2', kmesh=[4,4,4], n_bands=96, available_memory_gb=64, io_bandwidth_score=0.90.", "answers": "[{\"name\":\"gpaw_response_chiks_create_get_temporal_part\",\"arguments\":{\"bandsummation\":\"pairwise\"}},{\"name\":\"gpaw_response_chiks_create_get_temporal_part\",\"arguments\":{}}]"}
{"func_name": "gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix", "func_desc": "gpaw.response.heisenberg.generate_fm_dynamic_spin_wave_matrix: Generate the dynamic spin-wave matrix H_qabx for a collinear ferromagnet from isotropic exchange constants J_qabx.\n    \n    This routine implements the relation used in spin-wave (magnon) calculations for ferromagnets:\n    H^ab(q) = (g μ_B) / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)],\n    but in the implementation the magnetic-moment prefactor is computed explicitly as 2. / sqrt(M_a M_b) (see code). The function is intended for use inside GPAW's linear-response / Heisenberg-model utilities where J_qabx represents isotropic exchange interactions between sublattices a and b as a function of crystal momentum q and optional extra parameter dimensions x. The resulting H_qabx is the dynamic spin-wave matrix used to obtain spin-wave spectra and mode analysis for a ferromagnetic system within the assumptions of a collinear magnetic ground state and no spin-orbit coupling.", "tools": [{"function": {"description": "gpaw.response.heisenberg.generate_fm_dynamic_spin_wave_matrix: Generate the dynamic spin-wave matrix H_qabx for a collinear ferromagnet from isotropic exchange constants J_qabx.\n\nThis routine implements the relation used in spin-wave (magnon) calculations for ferromagnets:\nH^ab(q) = (g μ_B) / sqrt(M_a M_b) [Σ_c J^ac(0) δ_ab - J^ab(q)],\nbut in the implementation the magnetic-moment prefactor is computed explicitly as 2. / sqrt(M_a M_b) (see code). The function is intended for use inside GPAW's linear-response / Heisenberg-model utilities where J_qabx represents isotropic exchange interactions between sublattices a and b as a function of crystal momentum q and optional extra parameter dimensions x. The resulting H_qabx is the dynamic spin-wave matrix used to obtain spin-wave spectra and mode analysis for a ferromagnetic system within the assumptions of a collinear magnetic ground state and no spin-orbit coupling.", "name": "gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix", "parameters": {"properties": {"J_qabx": {"type": "array", "items": {"type": "float"}, "description": "Isotropic exchange constants J^ab(q). Array indexed as [q_index, a, b, ...x], where the first axis enumerates q-points and axes 1 and 2 correspond to sublattice indices a and b respectively. J_qabx may be complex but must satisfy Hermiticity J_qabx[q,a,b,...] = conj(J_qabx[q,b,a,...]) (the function asserts this). Any trailing dimensions x (J_qabx.shape[3:]) are treated independently and must match the trailing dimensions of mm_ax. The shape constraints enforced by the function are: J_qabx.ndim >= 3, J_qabx.shape[1] == J_qabx.shape[2], J_qabx.shape[0] == q_qc.shape[0], and J_qabx.shape[3:] == mm_ax.shape[1:].", "default": ""}, "q_qc": {"type": "array", "items": {"type": "float"}, "description": "Array of q-vectors in relative (crystal) coordinates with shape (n_q, 3) or compatible shape where the first axis enumerates q-points. The array must include the Gamma point q = 0 because the implementation locates the index q0 = get_q0_index(q_qc) to obtain J^ab(0). If q=0 is not present, get_q0_index will fail or the function will raise an assertion. The q vectors determine the first axis correspondence to J_qabx.", "default": ""}, "mm_ax": {"type": "array", "items": {"type": "float"}, "description": "Magnetic moments of the sublattice sites in units of Bohr magneton (μ_B). The first axis enumerates sublattices (length na), so mm_ax.shape[0] must equal the number of sublattices (J_qabx.shape[1]). mm_ax may contain additional trailing dimensions to represent parameter dependence (matching J_qabx.shape[3:]). The code computes a prefactor 2. / sqrt(M_a M_b) from these magnetic moments and uses it to normalize the dynamic matrix by sublattice magnetizations.", "default": ""}}, "required": ["J_qabx", "q_qc", "mm_ax"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a ferromagnetic magnon benchmark from two messy linear-response Heisenberg exports (each export is a separate cohort). Each cohort contains (i) a nominal q-grid, (ii) sublattice moments mm_a (µB), and (iii) an isotropic-exchange dataset J(q) that may include artifact q-points. Generate the FM dynamic spin-wave matrix H_qabx using GPAW’s implemented relation (with the explicit 2/sqrt(M_a M_b) prefactor).\n\nData hygiene / protocol rules:\n1) Treat the q-point whose Cartesian components are all exactly zero as Γ. For each cohort, build H only on the q-points whose |q| is <= 0.5 in reduced coordinates.\n2) J(q) must be a physically valid Hermitian 2×2 matrix at each processed q; enforce Hermiticity by symmetrizing J as (J + J†)/2 before generating H.\n3) Magnetic moments: if any sublattice moment is non-positive or unrealistically small (< 0.5 µB), the cohort is considered non-magnetic for this workflow and should yield no H output; otherwise proceed.\n\nCohort A (export-1):\n- q_qc_raw = [[0.0, 0.0, 0.0], [0.25, 0.0, 0.0], [0.5, 0.0, 0.0], [0.75, 0.0, 0.0]]\n- mm_ax = [2.1, 1.9]\n- J_qabx_raw (meV, 4×2×2; note the last q-point is a high-|q| artifact):\n  Γ:   [[0.0, 5.0], [5.0, 0.0]]\n  q1:  [[0.0, 4.5], [4.5, 0.0]]\n  q2:  [[0.0, 4.0], [4.0, 0.0]]\n  q3:  [[0.0, 3.5], [3.5, 0.0]]\n\nCohort B (export-2):\n- q_qc_raw = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.25, 0.25, 0.0]]\n- mm_ax = [2.2, 1.8]\n- J_qabx_raw (meV, 3×2×2; slight numerical asymmetry in off-diagonals):\n  Γ:            [[10.0, 1.0], [1.0002, 8.0]]\n  q=(0.5,0,0):  [[9.0, 0.8], [0.7997, 7.5]]\n  q=(0.25,0.25,0): [[9.5, 0.9], [0.9004, 7.7]]\n\nReturn H_qabx for each cohort that passes the rules, on the filtered q-grid used for that cohort.", "answers": "[{\"name\":\"gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix\",\"arguments\":{\"J_qabx\":[[[0.0,5.0],[5.0,0.0]],[[0.0,4.5],[4.5,0.0]],[[0.0,4.0],[4.0,0.0]]],\"q_qc\":[[0.0,0.0,0.0],[0.25,0.0,0.0],[0.5,0.0,0.0]],\"mm_ax\":[2.1,1.9]}},{\"name\":\"gpaw_response_heisenberg_generate_fm_dynamic_spin_wave_matrix\",\"arguments\":{\"J_qabx\":[[[10.0,1.0001],[1.0001,8.0]],[[9.0,0.79985],[0.79985,7.5]],[[9.5,0.9002],[0.9002,7.7]]],\"q_qc\":[[0.0,0.0,0.0],[0.5,0.0,0.0],[0.25,0.25,0.0]],\"mm_ax\":[2.2,1.8]}}]"}
{"func_name": "gpaw_response_mpa_sampling_semi_homogenous_partition", "func_desc": "Semi-homogenous partition generator used in GPAW response MPA sampling to place n-poles (sampling points) between 0 and 1.\n    \n    This function constructs a semi-homogenous one-dimensional partition (grid) of npoles points on the interval [0, 1] following Eq. (18) of Ref. [1] and Eq. (11) of Ref. [2]. In the GPAW code base this partition is used by the multipole approximation (MPA) sampling of response functions to distribute sampling points such that smaller intervals are concentrated near zero (improving resolution where response functions typically vary rapidly) while retaining larger intervals further from zero. The returned array has length exactly npoles and represents the positions of the n-poles normalized to the unit interval.", "tools": [{"function": {"description": "Semi-homogenous partition generator used in GPAW response MPA sampling to place n-poles (sampling points) between 0 and 1.\n\nThis function constructs a semi-homogenous one-dimensional partition (grid) of npoles points on the interval [0, 1] following Eq. (18) of Ref. [1] and Eq. (11) of Ref. [2]. In the GPAW code base this partition is used by the multipole approximation (MPA) sampling of response functions to distribute sampling points such that smaller intervals are concentrated near zero (improving resolution where response functions typically vary rapidly) while retaining larger intervals further from zero. The returned array has length exactly npoles and represents the positions of the n-poles normalized to the unit interval.", "name": "gpaw_response_mpa_sampling_semi_homogenous_partition", "parameters": {"properties": {"npoles": {"type": "integer", "description": "Number of sampling points (n-poles) to place between 0 and 1. This integer controls the length of the returned one-dimensional partition array and the level of resolution: small npoles produce coarse partitions (special-cased for npoles == 1, 2, 3), while larger npoles produce a semi-homogenous grid composed of intervals of size dw, 2*dw (and occasionally 4*dw) where dw = 1/2**ceil(log2(npoles)). The function assumes npoles is a positive integer; passing values less than 1 is not handled by explicit argument validation and will result in a KeyError for small invalid integers or other errors when performing logarithms for non-positive values.", "default": ""}}, "required": ["npoles"], "type": "any"}}, "type": "function"}], "query": "We’re tuning the MPA pole-placement grid for a mixed set of linear-response calculations where the required sampling density depends on the intrinsic resolution demands of each job. Below is a raw run manifest; some entries are incomplete or malformed due to scheduler logging noise.\n\nRun manifest (unordered):\n- {\"job_id\":\"LR-Au-bulk-001\",\"window_eV\":[0.0, 30.0],\"eta_eV\":0.15,\"replicate\":1}\n- {\"job_id\":\"LR-Au-bulk-001\",\"window_eV\":[0.0, 30.0],\"eta_eV\":0.15,\"replicate\":2}\n- {\"job_id\":\"LR-Si-slab-014\",\"window_eV\":[0.0, 12.0],\"eta_eV\":0.05,\"replicate\":1}\n- {\"job_id\":\"LR-Si-slab-014\",\"window_eV\":[0.0, 12.0],\"eta_eV\":0.05,\"replicate\":2}\n- {\"job_id\":\"LR-organic-077\",\"window_eV\":[0.0, 8.0],\"eta_eV\":0.02,\"replicate\":1}\n- {\"job_id\":\"LR-organic-077\",\"window_eV\":[0.0, 8.0],\"eta_eV\":0.02,\"replicate\":2}\n- {\"job_id\":\"LR-badmeta-xxx\",\"window_eV\":[0.0, null],\"eta_eV\":0.05,\"replicate\":1}\n- {\"job_id\":\"LR-neg-eta-999\",\"window_eV\":[0.0, 10.0],\"eta_eV\":-0.01,\"replicate\":1}\n\nFor each entry that has a physically valid broadening (eta_eV > 0) and a well-formed energy window (two finite endpoints with upper > lower), generate the GPAW semi-homogenous normalized n-pole partition on [0, 1]. Use a branching protocol for npoles based on spectral sharpness: if eta_eV <= 0.03 use npoles = 29; if 0.03 < eta_eV <= 0.08 use npoles = 17; otherwise use npoles = 12. Replicates of the same job must receive identical npoles under this rule so we can compare pole arrays across replicates.", "answers": "[{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":12}},{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":12}},{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":17}},{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":17}},{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":29}},{\"name\":\"gpaw_response_mpa_sampling_semi_homogenous_partition\",\"arguments\":{\"npoles\":29}}]"}
{"func_name": "gpaw_response_pair_transitions_remove_null_transitions", "func_desc": "Remove pairs of bands for which electronic transitions are impossible within a\n    DFT/PAW calculation context.\n    \n    This function filters two parallel arrays of band indices that represent paired\n    initial and final bands (for example, occupied -> unoccupied transitions used in\n    response or excitation calculations). It removes any pair (n1, n2) where both\n    bands are considered fully occupied or both are considered completely\n    unoccupied based on the provided occupation thresholds. The function is used in\n    GPAW response code to avoid forming transitions that are physically forbidden\n    or irrelevant for response function assembly and subsequent linear-response\n    calculations.", "tools": [{"function": {"description": "Remove pairs of bands for which electronic transitions are impossible within a\nDFT/PAW calculation context.\n\nThis function filters two parallel arrays of band indices that represent paired\ninitial and final bands (for example, occupied -> unoccupied transitions used in\nresponse or excitation calculations). It removes any pair (n1, n2) where both\nbands are considered fully occupied or both are considered completely\nunoccupied based on the provided occupation thresholds. The function is used in\nGPAW response code to avoid forming transitions that are physically forbidden\nor irrelevant for response function assembly and subsequent linear-response\ncalculations.", "name": "gpaw_response_pair_transitions_remove_null_transitions", "parameters": {"properties": {"n1_M": {"type": "array", "items": {"type": "float"}, "description": "1-D array of band indices (integers) representing the\nfirst band in each transition pair. Each element is interpreted as a\nband index (0-based) in the GPAW band ordering; indices < nocc1 are\nconsidered occupied if nocc1 is provided, and indices >= nocc2 are\nconsidered unoccupied if nocc2 is provided. The array must be aligned\nposition-wise with n2_M: element i of n1_M pairs with element i of\nn2_M. If the arrays differ in length, iteration uses Python's zip\nsemantics and will only process pairs up to the length of the shorter\narray (the excess elements are ignored).", "default": ""}, "n2_M": {"type": "array", "items": {"type": "float"}, "description": "1-D array of band indices (integers) representing the\nsecond band in each transition pair. Interpreted analogously to n1_M.\nThe two arrays together define candidate transitions; this function\nreturns filtered arrays containing only pairs that are allowed by the\noccupation thresholds.", "default": ""}, "nocc1": {"type": "integer", "nullable": true, "description": "Optional integer threshold defining the upper bound\n(exclusive) for bands considered fully occupied in the domain of the\nfirst occupancy criterion. Any band index b with b < nocc1 is treated\nas fully occupied for the purpose of removing transitions. If None\n(default), the \"both occupied\" removal criterion is not applied.\nTypical use: pass the number of occupied bands (e.g., number of\nelectrons/2 for spin-restricted cases) so that transitions between two\noccupied bands are discarded.", "default": null}, "nocc2": {"type": "integer", "nullable": true, "description": "Optional integer threshold defining the lower bound\n(inclusive) for bands considered completely unoccupied in the domain of\nthe second occupancy criterion. Any band index b with b >= nocc2 is\ntreated as completely unoccupied for the purpose of removing\ntransitions. If None (default), the \"both unoccupied\" removal criterion\nis not applied. Typical use: pass the index after the highest band of\ninterest so that transitions between two unoccupied bands are discarded.", "default": null}}, "required": ["n1_M", "n2_M", "nocc2", "nocc1"], "type": "any"}}, "type": "function"}], "query": "We are cleaning transition candidates coming from three k-point blocks in a GPAW DFT/PAW linear-response run where the occupation window is k-point dependent due to smearing and partial occupancies. For each block, treat band indices < nocc1 as clearly occupied and band indices >= nocc2 as clearly unoccupied. Remove any candidate transition pair (n1, n2) that stays entirely within the occupied manifold or entirely within the clearly unoccupied manifold; keep the rest for response-function assembly.\n\nProcess these three raw blocks:\n- Block K1: n1_M=[0,1,5,8,9], n2_M=[3,2,6,10,11]. Use nocc1 equal to one plus the maximum band index among {0,1,2,3}, and set nocc2 to 8.\n- Block K2: n1_M=[0,1,2,3,4,5], n2_M=[3,4,5,6,7,8]. Use nocc1=3 and set nocc2 equal to the smallest band index in n2_M.\n- Block K3: reuse the same n1_M/n2_M as K2, but set nocc1 to the count of distinct band indices in [0,1,2,3] and set nocc2 to one more than the median of n2_M.\n\nReturn the filtered transition pairs for each block.", "answers": "[{\"name\":\"gpaw_response_pair_transitions_remove_null_transitions\",\"arguments\":{\"n1_M\":[0,1,5,8,9],\"n2_M\":[3,2,6,10,11],\"nocc1\":4,\"nocc2\":8}},{\"name\":\"gpaw_response_pair_transitions_remove_null_transitions\",\"arguments\":{\"n1_M\":[0,1,2,3,4,5],\"n2_M\":[3,4,5,6,7,8],\"nocc1\":3,\"nocc2\":3}},{\"name\":\"gpaw_response_pair_transitions_remove_null_transitions\",\"arguments\":{\"n1_M\":[0,1,2,3,4,5],\"n2_M\":[3,4,5,6,7,8],\"nocc1\":4,\"nocc2\":6}}]"}
{"func_name": "gpaw_response_site_kernels_create_geometry_factor", "func_desc": "Creator function for the geometry factor factory used by gpaw.response.site_kernels.\n    \n    This function selects and returns a geometry-specific geometry-factor creator given a symbolic shape name. In the GPAW DFT/PAW response-kernel context, a \"geometry factor\" encodes the shape-dependent spatial weighting or kernel prefactor used by site kernels when computing local response contributions (for example, integrals or projections that depend on the region shape). create_geometry_factor is the factory selector component: it maps the exact string names used throughout gpaw.response.site_kernels to the corresponding geometry-factor implementation functions so that higher-level response code can obtain the correct shape-dependent routine.", "tools": [{"function": {"description": "Creator function for the geometry factor factory used by gpaw.response.site_kernels.\n\nThis function selects and returns a geometry-specific geometry-factor creator given a symbolic shape name. In the GPAW DFT/PAW response-kernel context, a \"geometry factor\" encodes the shape-dependent spatial weighting or kernel prefactor used by site kernels when computing local response contributions (for example, integrals or projections that depend on the region shape). create_geometry_factor is the factory selector component: it maps the exact string names used throughout gpaw.response.site_kernels to the corresponding geometry-factor implementation functions so that higher-level response code can obtain the correct shape-dependent routine.", "name": "gpaw_response_site_kernels_create_geometry_factor", "parameters": {"properties": {"shape": {"type": "string", "description": "Symbolic name of the desired geometry factor. Must be one of the exact, case-sensitive strings understood by the site_kernels module: 'sphere', 'cylinder', or 'parallelepiped'. Each name selects a specific implementation: 'sphere' -> spherical_geometry_factor, 'cylinder' -> cylindrical_geometry_factor, and 'parallelepiped' -> parallelepipedic_geometry_factor. There is no default; callers must supply a valid string. This parameter controls which geometry-dependent weighting routine is returned and therefore determines how the site kernel treats spatial regions in response calculations.", "default": ""}}, "required": ["shape"], "type": "any"}}, "type": "function"}], "query": "In our GPAW linear-response site-kernel regression suite, we’re ingesting a messy geometry manifest exported from multiple preprocessing steps. Each record contains a proposed sampling-region label that may include case drift, whitespace padding, or non-standard synonyms, and some entries may be placeholders from failed segmentation. Normalize each label by trimming surrounding whitespace and lowercasing, then map common synonyms to the canonical factory shape names (treat “nanodot”, “ball”, and “spherical” as sphere; treat “nanowire”, “rod”, and “tubular” as cylinder). For the benchmark, dispatch geometry-factor creators only for labels that resolve to one of the two canonical shapes supported by this test (sphere or cylinder). Use the following raw manifest in order and preserve replicate requests after normalization/mapping: [' Sphere ', 'nanowire', 'CYLINDER', 'rod', 'N/A', 'spherical', 'cube', '  ball  ', 'tubular', 'unknown']. Return the geometry-factor creators for the resolved, supported shapes in manifest order.", "answers": "[{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"sphere\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"cylinder\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"cylinder\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"cylinder\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"sphere\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"sphere\"}},{\"name\":\"gpaw_response_site_kernels_create_geometry_factor\",\"arguments\":{\"shape\":\"cylinder\"}}]"}
{"func_name": "gpaw_response_site_kernels_cylindrical_geometry_factor", "func_desc": "Calculate the site-centered geometry factor Theta(Q) for a cylindrical site kernel (gpaw.response.site_kernels.cylindrical_geometry_factor).\n    \n    This function evaluates the Fourier-space integral of a uniform cylinder indicator function\n    used by GPAW response kernels (the site kernel for a cylinder) and returns the\n    geometry factor Theta(Q) for input wave vectors Q. In the GPAW DFT/response context\n    this factor is the Fourier transform of a cylinder of radius rc and height hc\n    aligned along the unit vector ez_v, multiplied by the cylinder volume V_cylinder =\n    pi * rc**2 * hc. The returned array is used when constructing or applying\n    site-centered response kernels in reciprocal space (for example, evaluating\n    matrix elements of localized kernels or form-factors for cylindrical projector\n    regions).\n    \n    The mathematical form implemented is\n    Theta(Q) = V_cylinder * (2 J_1(Q_rho * rc) / (Q_rho * rc)) * sinc(Q_z * hc / 2),\n    with Q_rho = |Q x ez_v| and Q_z = Q . ez_v, where J_1 is the Bessel function of\n    the first kind of order one and sinc(x) = sin(x)/x. The code enforces the\n    limiting value Theta(Q)/V_cylinder -> 1 for Q -> 0 by handling the Q_rho * rc ->\n    0 limit explicitly.", "tools": [{"function": {"description": "Calculate the site-centered geometry factor Theta(Q) for a cylindrical site kernel (gpaw.response.site_kernels.cylindrical_geometry_factor).\n\nThis function evaluates the Fourier-space integral of a uniform cylinder indicator function\nused by GPAW response kernels (the site kernel for a cylinder) and returns the\ngeometry factor Theta(Q) for input wave vectors Q. In the GPAW DFT/response context\nthis factor is the Fourier transform of a cylinder of radius rc and height hc\naligned along the unit vector ez_v, multiplied by the cylinder volume V_cylinder =\npi * rc**2 * hc. The returned array is used when constructing or applying\nsite-centered response kernels in reciprocal space (for example, evaluating\nmatrix elements of localized kernels or form-factors for cylindrical projector\nregions).\n\nThe mathematical form implemented is\nTheta(Q) = V_cylinder * (2 J_1(Q_rho * rc) / (Q_rho * rc)) * sinc(Q_z * hc / 2),\nwith Q_rho = |Q x ez_v| and Q_z = Q . ez_v, where J_1 is the Bessel function of\nthe first kind of order one and sinc(x) = sin(x)/x. The code enforces the\nlimiting value Theta(Q)/V_cylinder -> 1 for Q -> 0 by handling the Q_rho * rc ->\n0 limit explicitly.", "name": "gpaw_response_site_kernels_cylindrical_geometry_factor", "parameters": {"properties": {"Q_Qv": {"type": "array", "items": {"type": "float"}, "description": "Wave vectors at which to evaluate the site-centered geometry factor.\nThe last axis must contain the Cartesian vector components (x,y,z) so that\nQ_Qv.shape == (..., 3). The leading axes (...) may have any tensor shape\nand will be preserved in the return value. Each vector is interpreted in the\nsame Cartesian coordinate system as ez_v.", "default": ""}, "ez_v": {"type": "array", "items": {"type": "float"}, "description": "Unit vector (shape (3,)) giving the normalized direction of the\ncylindrical axis (the \"z\" axis of the cylinder). This vector must be normalized\nto unit length within a tolerance of 1e-8 (assertion enforces |norm(ez_v)-1| < 1e-8).\nThe sign of ez_v sets the cylinder axis orientation but does not affect the\nabsolute value of Theta(Q) because Q_z appears inside a sinc and Q_rho is a norm.", "default": ""}, "rc": {"type": "float", "description": "Radius of the cylinder. Must be a Python float and rc > 0.0 (the function\nasserts isinstance(rc, float) and rc > 0.). rc defines the radial extent of the\nindicator function whose Fourier transform is computed.", "default": ""}, "hc": {"type": "float", "description": "Height of the cylinder. Must be a Python float and hc > 0.0 (the function\nasserts isinstance(hc, float) and hc > 0.). hc defines the axial extent of the\ncylinder; the axial factor used is sinc(Q_z * hc / 2).", "default": ""}}, "required": ["Q_Qv", "ez_v", "rc", "hc"], "type": "any"}}, "type": "function"}], "query": "We’re validating cylindrical site-kernel form factors for a mixed reciprocal-space probe set coming from two detector geometries, all intended to be aligned with the lab z-axis (ez_v = (0.0, 0.0, 1.0)), but the raw Q list includes artifacts and redundant near-origin shots. Use the following rule-driven protocol:\n\nRaw reciprocal-space probes (1/Å):\nQ_raw = [(0.0,0.0,0.0), (0.2,0.1,0.0), (0.0,0.0,1.5), (0.3,-0.4,0.7), (0.1,0.0,0.0), (0.0,0.1,0.0), (0.05,0.05,0.2), (0.0,0.0,0.0), (0.0,0.0,0.0), (999.0,0.0,0.0), (0.2,0.1,0.0)].\n\n1) Data sieve: treat any probe with a non-finite component or with |Q| >= 50 1/Å as an acquisition artifact and exclude it implicitly by not passing it forward.\n2) De-duplication: collapse exact duplicate Q vectors, but retain a single explicit origin vector (0,0,0) in the final set as the Q→0 limiting-behavior check.\n3) Branching protocol for projector geometry assignment:\n   - Assign a probe to Cohort A (compact cylinder) if Qz = 0 or if |Qz|/|Q| <= 0.5; use rc = 1.2 Å, hc = 3.0 Å.\n   - Assign a probe to Cohort B (nanotube-like cylinder) otherwise; use rc = 2.5 Å, hc = 6.0 Å.\n\nCompute the site-centered cylindrical geometry factor Theta(Q) for each cohort using the cohort-specific (rc, hc) and the shared ez_v, evaluating Theta(Q) only for the probes that survive the sieve+dedup steps and belong to that cohort. Ensure the origin vector is included in whichever cohort it is routed to by the above rule, to confirm correct Q→0 handling.", "answers": "[{\"name\":\"gpaw_response_site_kernels_cylindrical_geometry_factor\",\"arguments\":{\"Q_Qv\":[[0.0,0.0,0.0],[0.2,0.1,0.0],[0.0,0.0,1.5],[0.3,-0.4,0.7],[0.1,0.0,0.0],[0.0,0.1,0.0],[0.05,0.05,0.2]],\"ez_v\":[0.0,0.0,1.0],\"rc\":1.2,\"hc\":3.0}}]"}
{"func_name": "gpaw_response_tool_get_degeneracy_matrix", "func_desc": "gpaw.response.tool.get_degeneracy_matrix generates a degeneracy-selection matrix and a list of representative energies for groups of (nearly) degenerate single-particle eigenvalues. This function is used in GPAW response and transition-summing code to identify and sum contributions from degenerate Kohn–Sham states: it produces a matrix whose rows select the members of each degenerate group and an array of one representative energy per group.", "tools": [{"function": {"description": "gpaw.response.tool.get_degeneracy_matrix generates a degeneracy-selection matrix and a list of representative energies for groups of (nearly) degenerate single-particle eigenvalues. This function is used in GPAW response and transition-summing code to identify and sum contributions from degenerate Kohn–Sham states: it produces a matrix whose rows select the members of each degenerate group and an array of one representative energy per group.\n", "name": "gpaw_response_tool_get_degeneracy_matrix", "parameters": {"properties": {"eps_n": {"type": "array", "items": {"type": "float"}, "description": "A one-dimensional NumPy array of single-particle energies (for example Kohn–Sham eigenvalues produced by a GPAW calculation). The function treats equal or nearly equal values that occur in contiguous positions of this array as degenerate; therefore, eps_n should be sorted in non-decreasing order if grouping of all equal energies is required. The practical significance in GPAW is that these energies are the basis for grouping states that should be summed together in response/transition calculations.", "default": ""}, "tol": {"type": "float", "description": "Absolute tolerance for degeneracy in the same units as eps_n. Two energies eps_n[i] and eps_n[j] are considered degenerate if abs(eps_n[i] - eps_n[j]) < tol. The default (0.001) is suitable for grouping energies that differ only by numerical noise; increase or decrease this value to control how strictly degeneracy is detected. tol must be a floating-point number; negative values are not meaningful and may lead to unexpected grouping behavior.", "default": 0.001}}, "required": ["eps_n", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re validating transition-summing robustness on raw eigenvalue outputs where the near-degeneracy tolerance is not fixed but must adapt to the numerical noise level of each cohort. For each cohort below (already sorted, in eV), estimate the cohort-specific noise floor as the smallest *positive* adjacent level spacing in that list, then set the degeneracy tolerance to 1.25× that noise floor. Run the degeneracy grouping using that tolerance to build (i) the degeneracy-selection matrix (rows select members of each nearly-degenerate manifold) and (ii) one representative energy per manifold. Apply this to all three cohorts in order: (A) eps = [-5.4321, -5.4315, -3.2, -3.1992, -1.0001, -0.9999, 0.5], (B) eps = [-5.4321, -5.43205, -3.21, -3.2098, -3.0, -2.9992, -1.5], (C) eps = [-5.4321, -5.4318, -5.1, -5.0995, -5.0992, -4.75, -4.3001, -4.3, -4.2999, -3.95].", "answers": "[{\"name\":\"gpaw_response_tool_get_degeneracy_matrix\",\"arguments\":{\"eps_n\":[-5.4321,-5.4315,-3.2,-3.1992,-1.0001,-0.9999,0.5],\"tol\":0.00025}},{\"name\":\"gpaw_response_tool_get_degeneracy_matrix\",\"arguments\":{\"eps_n\":[-5.4321,-5.43205,-3.21,-3.2098,-3.0,-2.9992,-1.5],\"tol\":6.25e-05}},{\"name\":\"gpaw_response_tool_get_degeneracy_matrix\",\"arguments\":{\"eps_n\":[-5.4321,-5.4318,-5.1,-5.0995,-5.0992,-4.75,-4.3001,-4.3,-4.2999,-3.95],\"tol\":0.000125}}]"}
{"func_name": "gpaw_rotation_Y_matrix", "func_desc": "Compute the matrix of spherical-harmonic values for angular momentum l evaluated at a set of rotated sample points.\n    \n    This function is used in GPAW's rotation utilities to build the matrix of values of the internal Y(...) function (spherical-harmonic-like evaluator used in this module) for 2*l+1 sample points on the unit sphere. The sample points are taken from the module-level list sphere_lm[l] (one point per m1 = 0,...,2*l). Each sample point is rotated by the 3D linear transformation U_vv (typically a 3×3 rotation matrix in GPAW applications), and for each rotated point the function evaluates Y at indices L = l**2 + m2 for m2 = 0,...,2*l. The resulting matrix Y_mm has rows enumerating the input sample points (m1) and columns enumerating the evaluated Y indices derived from m2. In GPAW this matrix is a building block for constructing rotation operators in the angular-momentum-l subspace (for example when rotating PAW projectors, atomic-like basis functions, or related quantities between coordinate frames).", "tools": [{"function": {"description": "Compute the matrix of spherical-harmonic values for angular momentum l evaluated at a set of rotated sample points.\n\nThis function is used in GPAW's rotation utilities to build the matrix of values of the internal Y(...) function (spherical-harmonic-like evaluator used in this module) for 2*l+1 sample points on the unit sphere. The sample points are taken from the module-level list sphere_lm[l] (one point per m1 = 0,...,2*l). Each sample point is rotated by the 3D linear transformation U_vv (typically a 3×3 rotation matrix in GPAW applications), and for each rotated point the function evaluates Y at indices L = l**2 + m2 for m2 = 0,...,2*l. The resulting matrix Y_mm has rows enumerating the input sample points (m1) and columns enumerating the evaluated Y indices derived from m2. In GPAW this matrix is a building block for constructing rotation operators in the angular-momentum-l subspace (for example when rotating PAW projectors, atomic-like basis functions, or related quantities between coordinate frames).", "name": "gpaw_rotation_Y_matrix", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number l >= 0 that selects the subspace size. The function uses 2*l+1 sample points and produces a square matrix of size (2*l+1)×(2*l+1). In practice l is a non-negative integer used throughout GPAW to index angular-momentum channels (e.g. s,p,d,...). If l is not an integer or is negative, the function will either behave incorrectly or raise exceptions (TypeError/IndexError/ValueError) when accessing sphere_lm or during array allocation.", "default": ""}, "U_vv": {"type": "array", "items": {"type": "float"}, "description": "3D linear transformation applied to each unit-sphere sample point before evaluation. The object must be a NumPy ndarray compatible with NumPy's dot product with a 3-element point (the code uses numpy.dot(point, U_vv)). In GPAW usage this is typically a 3×3 orthogonal rotation matrix (dtype float), but any ndarray with a shape and dtype that make the dot product valid is accepted. If U_vv has incompatible shape or is not an ndarray, NumPy will raise an appropriate exception (TypeError or ValueError). Note that if U_vv is not a proper rotation (orthonormal) matrix, the numerical values produced will correspond to the transformed coordinates but will not represent a geometric rotation.", "default": ""}}, "required": ["l", "U_vv"], "type": "any"}}, "type": "function"}], "query": "We’re debugging a PAW/projector frame-change regression where some rotation operators are generated from noisy inputs (non-orthonormal U) coming out of an upstream optimizer. For the d-like subspace (l=2) build Y_mm using the module’s 5 predefined sampling directions, but only for candidate transforms that are physically valid rigid rotations: accept a 3×3 matrix U only if it is a proper rotation (det(U)>0) and preserves orthonormality within numerical tolerance (U^T U≈I). From the following candidates, compute Y_mm for the ones that pass this physicality sieve:\n\nA) exact +90° about z: [[0, -1, 0],[1, 0, 0],[0, 0, 1]]\nB) +30° about z: [[cos30, -sin30, 0],[sin30, cos30, 0],[0, 0, 1]]\nC) mirrored z-flip artifact: [[1, 0, 0],[0, 1, 0],[0, 0, -1]]\nD) slightly non-orthonormal optimizer output (intended ~+90° about z): [[0, 1.0002, 0],[-0.9998, 0, 0],[0, 0, 1]]\n\nReturn Y_mm for each accepted transform, in the same order as the input list (skipping any that fail the physicality criteria).", "answers": "[{\"name\":\"gpaw_rotation_Y_matrix\",\"arguments\":{\"l\":2,\"U_vv\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]}},{\"name\":\"gpaw_rotation_Y_matrix\",\"arguments\":{\"l\":2,\"U_vv\":[[0.8660254037844386,-0.5,0.0],[0.5,0.8660254037844386,0.0],[0.0,0.0,1.0]]}}]"}
{"func_name": "gpaw_rotation_rotation", "func_desc": "gpaw.rotation.rotation: compute the rotation (transformation) matrix for spherical harmonics of angular momentum l.\n    \n    Compute the transformation matrix that maps coefficients of one set of spherical harmonic basis functions Y_lm (denoted here as Y_lm1) to another rotated set Y_lm2 according to the rotation described by U_vv. This function is used in GPAW's PAW/atomic-basis infrastructure to rotate angular-momentum-resolved quantities (for example atomic orbitals, projector functions, or density-matrix blocks) when applying a symmetry operation or a spatial rotation. Internally the function constructs the rotation in the spherical-harmonic representation by calling Y_matrix(l, U_vv) and then applies the precomputed iY_lmm[l] transformation, returning the combined transformation matrix used to rotate coefficients associated with angular momentum l.", "tools": [{"function": {"description": "gpaw.rotation.rotation: compute the rotation (transformation) matrix for spherical harmonics of angular momentum l.\n\nCompute the transformation matrix that maps coefficients of one set of spherical harmonic basis functions Y_lm (denoted here as Y_lm1) to another rotated set Y_lm2 according to the rotation described by U_vv. This function is used in GPAW's PAW/atomic-basis infrastructure to rotate angular-momentum-resolved quantities (for example atomic orbitals, projector functions, or density-matrix blocks) when applying a symmetry operation or a spatial rotation. Internally the function constructs the rotation in the spherical-harmonic representation by calling Y_matrix(l, U_vv) and then applies the precomputed iY_lmm[l] transformation, returning the combined transformation matrix used to rotate coefficients associated with angular momentum l.", "name": "gpaw_rotation_rotation", "parameters": {"properties": {"l": {"type": "integer", "description": "The angular momentum quantum number l for which the rotation is constructed. In the GPAW/PAW context this selects the subspace of spherical harmonics with magnetic quantum numbers m = -l, ..., +l; the transformation therefore operates on the (2*l + 1)-dimensional coefficient vector for that l. l must be a non-negative integer; passing a negative or non-integer value is invalid and will typically result in an error from the underlying construction routines.", "default": ""}, "U_vv": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array that encodes the rotation/symmetry operation in the vector (spatial) representation. This array is passed to Y_matrix(l, U_vv) to build the rotation in the spherical-harmonic basis. In practical GPAW use U_vv represents the rotation part of a symmetry operation or a rigid-body rotation applied to atomic-centered quantities. If U_vv has an incompatible shape or contents for Y_matrix, the underlying function will raise an exception.", "default": ""}}, "required": ["l", "U_vv"], "type": "any"}}, "type": "function"}], "query": "We’re validating symmetry handling for an atom-centered PAW basis by rotating multiple angular-momentum manifolds under the same spatial operation and checking block-wise consistency. Use the +90° rotation about the z-axis, but treat the raw symmetry operators as they come from two upstream sources: one provides exact integer matrices, the other provides float matrices with the same numerical entries. From the candidate symmetry operators below, select only those that are proper rotations (orthonormal with determinant +1) and correspond to a quarter-turn around the z-axis (i.e., leave the z-axis invariant and rotate the x–y plane by 90°). For each selected operator, compute the spherical-harmonic coefficient transformation matrix for the p-, d-, and f-manifolds (l = 1, 2, 3) for downstream projector/orbital rotation tests.\n\nCandidate U_vv list (mixed provenance):\n1) [[0, -1, 0], [1, 0, 0], [0, 0, 1]]\n2) [[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]\n3) [[0, 1, 0], [-1, 0, 0], [0, 0, 1]]\n4) [[-1, 0, 0], [0, 1, 0], [0, 0, 1]]\n5) [[1, 0, 0], [0, 1, 0], [0, 0, -1]]", "answers": "[{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":1,\"U_vv\":[[0,-1,0],[1,0,0],[0,0,1]]}},{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":2,\"U_vv\":[[0,-1,0],[1,0,0],[0,0,1]]}},{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":3,\"U_vv\":[[0,-1,0],[1,0,0],[0,0,1]]}},{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":1,\"U_vv\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]}},{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":2,\"U_vv\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]}},{\"name\":\"gpaw_rotation_rotation\",\"arguments\":{\"l\":3,\"U_vv\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]}}]"}
{"func_name": "gpaw_sphere_integrate_find_two_closest_grid_points", "func_desc": "gpaw.sphere.integrate.find_two_closest_grid_points finds the two grid-point indices in a radial grid array that are closest in absolute distance to a specified radial cutoff rcut. This function is used in GPAW's spherical integration utilities to map a physical radial cutoff (for example, the radius of a sphere around an atom used in PAW or real-space integrations) to the two nearest discrete grid points on the radial grid so that subsequent interpolation or selection of grid points can be performed.\n    \n    Detailed behavior: the function computes the absolute differences between each entry of the radial grid array r_g and the scalar cutoff rcut, then uses NumPy's partition to identify the two smallest absolute differences and the corresponding indices. The returned indices are the first occurrences in r_g that match those two smallest absolute differences. The implementation uses vectorized NumPy operations (abs and partition) and then numpy.where to locate indices.", "tools": [{"function": {"description": "gpaw.sphere.integrate.find_two_closest_grid_points finds the two grid-point indices in a radial grid array that are closest in absolute distance to a specified radial cutoff rcut. This function is used in GPAW's spherical integration utilities to map a physical radial cutoff (for example, the radius of a sphere around an atom used in PAW or real-space integrations) to the two nearest discrete grid points on the radial grid so that subsequent interpolation or selection of grid points can be performed.\n\nDetailed behavior: the function computes the absolute differences between each entry of the radial grid array r_g and the scalar cutoff rcut, then uses NumPy's partition to identify the two smallest absolute differences and the corresponding indices. The returned indices are the first occurrences in r_g that match those two smallest absolute differences. The implementation uses vectorized NumPy operations (abs and partition) and then numpy.where to locate indices.", "name": "gpaw_sphere_integrate_find_two_closest_grid_points", "parameters": {"properties": {"r_g": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of radial grid point coordinates (radial distances) in the same units as rcut. In the GPAW context this array typically represents a uniform or nonuniform real-space radial grid used for spherical integrals and atom-centered regions. The function expects r_g to be non-empty; an empty array will cause an indexing error.", "default": ""}, "rcut": {"type": "float", "description": "Scalar radial cutoff value (radius) in the same units as r_g. This is the target radius for which the two closest grid-point indices are desired (e.g., the radial boundary of a sphere around an atom used in integration or region selection).", "default": ""}}, "required": ["r_g", "rcut"], "type": "any"}}, "type": "function"}], "query": "We’re validating a PAW spherical-integration boundary-mapping step across mixed-quality radial grids coming from different generator settings. Each cohort below provides a candidate non-uniform radial grid r_g (bohr) and a nominal boundary radius rcut_nominal (bohr). For robust prechecks, only run the cutoff→grid mapping for cohorts whose r_g is physically usable as a radial coordinate: it must be non-decreasing, start at exactly 0.0, and contain no negative radii. For each usable cohort, compute an *effective* cutoff rcut_eff by applying a resolution-aware rule: if the median spacing of consecutive grid points is < 0.2 bohr, use rcut_eff = 0.98 × rcut_nominal (slight inward bias to avoid boundary aliasing on dense grids); otherwise use rcut_eff = 1.02 × rcut_nominal (slight outward bias on coarse grids). Then, for each usable cohort, call gpaw.sphere.integrate.find_two_closest_grid_points(r_g, rcut_eff) to get the two closest grid-point indices (ties resolved by first occurrence).\n\nCohorts:\nA) r_g = [0.0, 0.05, 0.11, 0.18, 0.26, 0.35, 0.45, 0.56, 0.68, 0.81, 0.95, 1.1, 1.26, 1.43, 1.61, 1.8, 2.0], rcut_nominal = 1.3\nB) r_g = [0.0, 0.1, 0.25, 0.4, 0.6, 0.85, 1.2, 1.6, 2.1, 2.7], rcut_nominal = 0.5\nC) r_g = [-0.02, 0.0, 0.1, 0.25, 0.4, 0.6, 0.9, 1.3, 1.8, 2.4, 3.1], rcut_nominal = 0.7\nD) r_g = [0.0, 0.1, 0.25, 0.4, 0.6, 0.9, 1.3, 1.8, 2.4, 3.1], rcut_nominal = 0.7", "answers": "[{\"name\":\"gpaw_sphere_integrate_find_two_closest_grid_points\",\"arguments\":{\"r_g\":[0.0,0.05,0.11,0.18,0.26,0.35,0.45,0.56,0.68,0.81,0.95,1.1,1.26,1.43,1.61,1.8,2.0],\"rcut\":1.274}}, {\"name\":\"gpaw_sphere_integrate_find_two_closest_grid_points\",\"arguments\":{\"r_g\":[0.0,0.1,0.25,0.4,0.6,0.85,1.2,1.6,2.1,2.7],\"rcut\":0.51}}, {\"name\":\"gpaw_sphere_integrate_find_two_closest_grid_points\",\"arguments\":{\"r_g\":[0.0,0.1,0.25,0.4,0.6,0.9,1.3,1.8,2.4,3.1],\"rcut\":0.714}}]"}
{"func_name": "gpaw_sphere_integrate_find_volume_conserving_lambd", "func_desc": "Determine the scaling factor λ (lambda) that makes a radial truncation function conserve the spherical volume.\n    \n    This function is used in GPAW's spherical integration and truncation utilities to find a multiplicative scaling parameter λ such that the numerically integrated, truncated radial weight function θ(r) yields the same total volume as an ideal sphere of radius rcut. In practice this is important when constructing smooth truncation/mask functions for spherical regions (for example PAW augmentation spheres or localized real-space cutoffs) on a radial grid: the function adjusts the truncation so that 4π ∫_0^rcut r^2 θ(r) dr = 4π rcut^3 / 3 within numerical tolerance.", "tools": [{"function": {"description": "Determine the scaling factor λ (lambda) that makes a radial truncation function conserve the spherical volume.\n\nThis function is used in GPAW's spherical integration and truncation utilities to find a multiplicative scaling parameter λ such that the numerically integrated, truncated radial weight function θ(r) yields the same total volume as an ideal sphere of radius rcut. In practice this is important when constructing smooth truncation/mask functions for spherical regions (for example PAW augmentation spheres or localized real-space cutoffs) on a radial grid: the function adjusts the truncation so that 4π ∫_0^rcut r^2 θ(r) dr = 4π rcut^3 / 3 within numerical tolerance.", "name": "gpaw_sphere_integrate_find_volume_conserving_lambd", "parameters": {"properties": {"rcut": {"type": "float", "description": "Cutoff radius r_c of the sphere. This is the radius of the target sphere whose volume must be conserved by the truncated radial weight. The value is interpreted in the same length units as the caller's radial grid and other GPAW routines (no unit conversion is performed here).", "default": ""}, "drcut": {"type": "float", "description": "Radial grid spacing or differential parameter used to construct the uniform radial grid when r_g is not provided. When r_g is None, the function calls _uniform_radial_grid(rcut, drcut) to build a 1D radial grid from 0 up to rcut with spacing approximately drcut; drcut therefore controls the numerical resolution of the volume integral and affects the resulting λ through discretization error.", "default": ""}, "r_g": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional 1D NumPy array of radial grid points (r values) to use for the numerical integration. If provided, these values are used directly by radial_truncation_function and radial_trapz to evaluate the integral 4π ∫ r^2 θ(r) dr. If r_g is None (the default), a uniform radial grid is constructed by calling _uniform_radial_grid(rcut, drcut). The array should represent radii spanning the region of interest up to rcut; providing a precomputed grid lets callers control grid placement and resolution explicitly.", "default": null}}, "required": ["rcut", "drcut", "r_g"], "type": "any"}}, "type": "function"}], "query": "We’re validating two spherical truncation/mask cohorts intended for GPAW, but the metadata is messy and we only want to calibrate masks that are numerically stable on a uniform radial grid. For each candidate sphere specification, compute the volume-conserving scaling factor λ (so that 4π∫_0^rcut r^2 θ(r)dr matches 4πrcut^3/3 within tolerance) using GPAW’s internal radial grid construction (no custom r_g). Apply the following QC/branching protocol per candidate: (i) only calibrate candidates where rcut/drcut is an integer (to avoid a fractional endpoint on the uniform grid), and (ii) if rcut ≤ 2.6 bohr treat it as an augmentation-sphere style mask and use the provided drcut as-is; otherwise treat it as a real-space cutoff mask and use the provided drcut as-is. Candidate batch (rcut [bohr], drcut [bohr]): [(2.5, 0.05), (2.8, 0.02), (2.75, 0.02), (3.0, 0.07)]. Return λ for every candidate that passes the QC rule.", "answers": "[{\"name\":\"gpaw_sphere_integrate_find_volume_conserving_lambd\",\"arguments\":{\"rcut\":2.5,\"drcut\":0.05,\"r_g\":null}},{\"name\":\"gpaw_sphere_integrate_find_volume_conserving_lambd\",\"arguments\":{\"rcut\":2.8,\"drcut\":0.02,\"r_g\":null}},{\"name\":\"gpaw_sphere_integrate_find_volume_conserving_lambd\",\"arguments\":{\"rcut\":2.75,\"drcut\":0.02,\"r_g\":null}}]"}
{"func_name": "gpaw_sphere_integrate_radial_trapz", "func_desc": "gpaw.sphere.integrate.radial_trapz computes the integral of r^2 f(r) over a radial grid using a piecewise linear (trapezoidal) rule tailored for radial integrals used in GPAW DFT calculations.\n    \n    This function is used in the GPAW code base to accumulate radial integrals (for example, integrals of radial parts of densities, projectors or basis-function contributions) by assuming that the radial function f(r) is sampled on a one-dimensional radial grid r_g and is linearly interpolated between consecutive grid points. For each interval r0 <= r <= r1 the integral of r^2 f(r) is evaluated analytically for the linear interpolant and the contributions from all intervals are summed. The method is exact for piecewise-linear f(r) and otherwise is an approximation whose accuracy depends on the radial grid resolution.", "tools": [{"function": {"description": "gpaw.sphere.integrate.radial_trapz computes the integral of r^2 f(r) over a radial grid using a piecewise linear (trapezoidal) rule tailored for radial integrals used in GPAW DFT calculations.\n\nThis function is used in the GPAW code base to accumulate radial integrals (for example, integrals of radial parts of densities, projectors or basis-function contributions) by assuming that the radial function f(r) is sampled on a one-dimensional radial grid r_g and is linearly interpolated between consecutive grid points. For each interval r0 <= r <= r1 the integral of r^2 f(r) is evaluated analytically for the linear interpolant and the contributions from all intervals are summed. The method is exact for piecewise-linear f(r) and otherwise is an approximation whose accuracy depends on the radial grid resolution.", "name": "gpaw_sphere_integrate_radial_trapz", "parameters": {"properties": {"f_xg": {"type": "array", "items": {"type": "float"}, "description": "Array of sampled function values f on the radial grid. The last axis of this array must index the radial grid points and therefore its length must equal len(r_g). Other leading axes (denoted by \"x\" in the name) may represent multiple functions, angular channels, or additional data dimensions so that the function computes one radial integral per leading-index tuple. The array is not modified by the function.", "default": ""}, "r_g": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional numpy array of radial grid points (r values) in non-negative units consistent with f_xg. The grid must be in strictly ascending order (r_g[i+1] > r_g[i]) and all entries must be >= 0.0; the code uses these grid points to form the piecewise integration intervals.", "default": ""}}, "required": ["f_xg", "r_g"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the GPAW radial-integration stage on hydrogen-like 1s *radial* samples coming from two acquisition modes. Each mode occasionally includes non-physical artifacts close to the origin and/or a duplicated knot from grid stitching.\n\nGiven two raw cohorts below, build the *effective* radial grid segment to integrate by applying these pipeline rules:\n1) Use only the contiguous prefix of samples that are on a strictly increasing radial grid starting from r=0.0 (this guards against stitched/duplicated knots).\n2) Within that usable prefix, integrate only the portion where the sampled radial function is physically admissible for a density-like quantity: finite and non-negative. (Once the first invalid value appears, the remainder is considered contaminated for that cohort’s scan.)\n\nCohort A (short-range scan, Bohr):\n- r_g = [0.0, 0.1, 0.2, 0.4, 0.8]\n- f_xg = [1.0, 0.905, 0.819, 0.670, 0.449]\n\nCohort B (extended scan with stitching, Bohr):\n- r_g = [0.0, 0.5, 1.0, 2.0, 2.0, 3.5, 5.0]\n- f_xg = [1.0, 0.8, 0.5, 0.2, 0.2, -0.01, 0.01]\n\nFor each cohort, compute the numerical radial integral ∫ r^2 f(r) dr over the resulting usable segment using gpaw.sphere.integrate.radial_trapz and report the integral per cohort.", "answers": "[{\"name\":\"gpaw_sphere_integrate_radial_trapz\",\"arguments\":{\"f_xg\":[1.0,0.905,0.819,0.67,0.449],\"r_g\":[0.0,0.1,0.2,0.4,0.8]}},{\"name\":\"gpaw_sphere_integrate_radial_trapz\",\"arguments\":{\"f_xg\":[1.0,0.8,0.5,0.2],\"r_g\":[0.0,0.5,1.0,2.0]}}]"}
{"func_name": "gpaw_sphere_integrate_radial_truncation_function_spline", "func_desc": "Generate a spline representation of the radial truncation function θ(r < rcut) suitable for spherical-harmonic expansions used in GPAW's sphere integration routines.\n    \n    This function constructs a radial grid governed by rcut and drcut, evaluates the radial truncation function on that grid using the provided or volume-conserving lambda parameter, and returns a Spline object that encodes the l=0 spherical-harmonic expansion coefficient of the truncation. In the context of GPAW (a DFT code using the PAW method and atom-centered spherical integrations), this spline is used to smoothly truncate atom-centered functions within the cutoff radius rcut, with drcut controlling the transition width of the truncation and lambd controlling the shape so that physically important properties (for example, the truncated volume) can be conserved.", "tools": [{"function": {"description": "Generate a spline representation of the radial truncation function θ(r < rcut) suitable for spherical-harmonic expansions used in GPAW's sphere integration routines.\n\nThis function constructs a radial grid governed by rcut and drcut, evaluates the radial truncation function on that grid using the provided or volume-conserving lambda parameter, and returns a Spline object that encodes the l=0 spherical-harmonic expansion coefficient of the truncation. In the context of GPAW (a DFT code using the PAW method and atom-centered spherical integrations), this spline is used to smoothly truncate atom-centered functions within the cutoff radius rcut, with drcut controlling the transition width of the truncation and lambd controlling the shape so that physically important properties (for example, the truncated volume) can be conserved.", "name": "gpaw_sphere_integrate_radial_truncation_function_spline", "parameters": {"properties": {"rcut": {"type": "float", "description": "Radial cutoff distance (in the same length units used by the caller) that defines the nominal support of the truncation function θ(r < rcut). Practically, rcut sets the outer radius beyond which the truncation function is (approximately) zero; it is used to build the radial grid and to determine the returned spline's rmax. Must be provided as a floating-point scalar consistent with the caller's unit system.", "default": ""}, "drcut": {"type": "float", "description": "Radial smoothing width (float) that determines the grid spacing and the transition region over which θ(r) goes from near 1 (inside the atom-centered region) to near 0 (outside rcut). drcut controls the resolution and smoothness of the radial truncation; it is passed to the internal uniform radial grid generator and to the radial_truncation_function evaluator. Provide a positive float appropriate for the desired smoothness and numerical resolution.", "default": ""}, "lambd": {"type": "float", "nullable": true, "description": "Shape parameter (float) that modifies the functional form of the radial truncation. If lambd is None (the default), the function calls find_volume_conserving_lambd(rcut, drcut) to compute a lambda that enforces the intended volume-conserving property for the truncation function on the chosen grid. If supplied, this value is used directly to generate the truncation function. In GPAW workflows, choosing lambd=None is convenient because it selects a lambda consistent with the volume-conservation requirement used by the code; supplying an explicit lambd allows testing or alternative truncation behaviors.", "default": null}}, "required": ["rcut", "drcut", "lambd"], "type": "any"}}, "type": "function"}], "query": "We’re validating GPAW’s sphere-integration truncation splines against a mixed unit/quality calibration log exported from three independent PAW setup pipelines. Each record has a nominal cutoff radius (rcut), a suggested transition width (drcut), and a unit tag. Build l=0 truncation splines only for records that represent physically usable atom-centered spheres, defined here as: rcut must be strictly positive and the smoothing width must be nonzero but not exceed 10% of rcut. For any usable record, keep the volume-conserving default shape (do not pass an explicit λ) unless the smoothing width is relatively sharp (drcut/rcut ≤ 0.06), in which case pass an explicit λ=3.0 to suppress ringing in downstream spherical-harmonic projections. Records:\n1) stress-test sphere: rcut=5.5 Å, drcut=0.25 Å\n2) oxygen setup A: rcut=2.4 Å, drcut=0.15 Å\n3) oxygen setup B (bohr): rcut=2.8 bohr, drcut=0.25 bohr\n4) corrupted export: rcut=-1.0 Å, drcut=0.2 Å\n5) near-step artifact: rcut=3.0 Å, drcut=0.0 Å\nReturn the Spline objects for all usable records under these rules.", "answers": "[{\"name\":\"gpaw_sphere_integrate_radial_truncation_function_spline\",\"arguments\":{\"rcut\":5.5,\"drcut\":0.25,\"lambd\":3.0}},{\"name\":\"gpaw_sphere_integrate_radial_truncation_function_spline\",\"arguments\":{\"rcut\":2.4,\"drcut\":0.15,\"lambd\":null}},{\"name\":\"gpaw_sphere_integrate_radial_truncation_function_spline\",\"arguments\":{\"rcut\":2.8,\"drcut\":0.25,\"lambd\":null}}]"}
{"func_name": "gpaw_sphere_integrate_truncate_radial_grid", "func_desc": "Truncate the radial-grid representation of a radial function f(r) at r = rcut.\n    \n    This function is used in GPAW's spherical integration utilities (gpaw.sphere.integrate) to restrict a radial sampling of a function to the interior of a sphere of radius rcut. In the context of the GPAW DFT code (which uses atom-centered spherical integrations for PAW and related operations), f_xg typically encodes values of a radial-dependent quantity (for example, radial parts of basis functions, densities or integrands) sampled on a 1D radial grid r_g; truncating at rcut is required when performing integrals or applying sphere-localized operations up to a specified cutoff radius.", "tools": [{"function": {"description": "Truncate the radial-grid representation of a radial function f(r) at r = rcut.\n\nThis function is used in GPAW's spherical integration utilities (gpaw.sphere.integrate) to restrict a radial sampling of a function to the interior of a sphere of radius rcut. In the context of the GPAW DFT code (which uses atom-centered spherical integrations for PAW and related operations), f_xg typically encodes values of a radial-dependent quantity (for example, radial parts of basis functions, densities or integrands) sampled on a 1D radial grid r_g; truncating at rcut is required when performing integrals or applying sphere-localized operations up to a specified cutoff radius.", "name": "gpaw_sphere_integrate_truncate_radial_grid", "parameters": {"properties": {"f_xg": {"type": "array", "items": {"type": "float"}, "description": "Function values sampled on the radial grid. The radial axis is expected to be the last axis of this array (i.e., f_xg[..., g] corresponds to radius r_g[g]). The array may be multi-dimensional in the non-radial directions (denoted x in the source), and these axes are preserved. The practical role of f_xg is to provide the values that will be kept for radii r <= rcut; if rcut is not present in r_g, a value f(rcut) is inserted by linear interpolation along the radial axis.", "default": ""}, "r_g": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of radial grid point coordinates that correspond to the last axis of f_xg. r_g supplies the radial sampling points for f_xg and must contain at least one value greater than or equal to rcut (otherwise the function asserts). The returned r_g contains only the grid points r <= rcut; if rcut was not already present, it will be appended and included in the returned array.", "default": ""}, "rcut": {"type": "float", "description": "Cutoff radius at which to truncate the radial representation. Must be strictly positive (rcut > 0). If rcut is not already one of the values in r_g, the function inserts rcut into the grid and computes f(rcut) by linear interpolation between the two nearest grid points (using find_two_closest_grid_points). In GPAW workflows, rcut defines the spherical domain boundary for radial integrations or sphere-local operations.", "default": ""}}, "required": ["f_xg", "r_g", "rcut"], "type": "any"}}, "type": "function"}], "query": "In our PAW spherical-integration pre-processing, we’re assembling sphere-localized radial datasets from mixed sources (basis-like channels vs. spin densities). Each cohort provides a 2×Ng f_xg on its own r_g. Before truncation, apply a data-quality gate: only cohorts whose r_g is strictly increasing and contains r=0.0 are eligible for sphere restriction. For eligible cohorts, choose the cutoff radius adaptively from the grid itself: set rcut to the midpoint between the last grid point that is \u001875% of the cohort’s maximum radius and the next grid point above it (i.e., rcut is halfway between those two neighboring samples that straddle 0.75*r_max). Then truncate with that rcut, retaining r<=rcut and inserting a new endpoint at r=rcut via linear interpolation. Cohorts:\n\nCohort A (two angular channels):\n- f_xg = [[0.0, 0.4, 0.9, 1.3, 1.5, 1.55, 1.57],\n          [0.0, 0.3, 0.7, 1.0, 1.2, 1.28, 1.30]]\n- r_g = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n\nCohort B (spin-up/spin-down densities on a non-uniform grid):\n- f_xg = [[0.0, 0.8, 1.4, 1.1, 0.5, 0.2],\n          [0.0, 0.7, 1.2, 1.0, 0.4, 0.1]]\n- r_g = [0.0, 0.5, 1.0, 2.0, 2.5, 3.0]\n\nCohort C (imported from a partially corrupted radial table):\n- f_xg = [[0.0, 0.2, 0.6, 0.9, 1.0],\n          [0.0, 0.1, 0.5, 0.8, 0.95]]\n- r_g = [0.0, 0.4, 0.9, 0.85, 1.6]\n\nProcess only the eligible cohorts and return the truncated f_xg and r_g for each processed cohort.", "answers": "[{\"name\":\"gpaw_sphere_integrate_truncate_radial_grid\",\"arguments\":{\"f_xg\":[[0.0,0.4,0.9,1.3,1.5,1.55,1.57],[0.0,0.3,0.7,1.0,1.2,1.28,1.3]],\"r_g\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0],\"rcut\":2.25}},{\"name\":\"gpaw_sphere_integrate_truncate_radial_grid\",\"arguments\":{\"f_xg\":[[0.0,0.8,1.4,1.1,0.5,0.2],[0.0,0.7,1.2,1.0,0.4,0.1]],\"r_g\":[0.0,0.5,1.0,2.0,2.5,3.0],\"rcut\":2.25}}]"}
{"func_name": "gpaw_sphere_rshe_get_rshe_coefficient_info_string", "func_desc": "Construct and return a single-line, human-readable info string describing the\n    weight and inclusion status of one RSHE (real-space spherical-harmonic)\n    coefficient used in GPAW reporting. This function is used in GPAW's\n    sphere.rshe routines to present per-coefficient diagnostic information\n    for density / potential expansions (e.g. in PAW or grid-based analyses)\n    in text output or log files.", "tools": [{"function": {"description": "Construct and return a single-line, human-readable info string describing the\nweight and inclusion status of one RSHE (real-space spherical-harmonic)\ncoefficient used in GPAW reporting. This function is used in GPAW's\nsphere.rshe routines to present per-coefficient diagnostic information\nfor density / potential expansions (e.g. in PAW or grid-based analyses)\nin text output or log files.", "name": "gpaw_sphere_rshe_get_rshe_coefficient_info_string", "parameters": {"properties": {"L": {"type": "integer", "description": "Linear index that encodes the spherical-harmonic angular\nquantum numbers. Internally the function decodes L to obtain\nl and m via l = int(numpy.sqrt(L)) and m = L - l * (l + 1). In the\nGPAW RSHE context, l identifies the angular momentum order of the\ncoefficient and m identifies the component within that order; the\ndecoded (l,m) pair is used as the textual label for the coefficient.", "default": ""}, "included": {"type": "boolean", "description": "Boolean flag indicating whether this coefficient was\nincluded in the set of coefficients (for example, included in a\ntruncated expansion or selection criterion). The function converts\nthis flag to the literal string 'yes' or 'no' in the returned\ninfo string so that output clearly documents whether the coefficient\ncontributed to subsequent sums or analyses.", "default": ""}, "rshew": {"type": "float", "description": "The RSHE weight value associated with this coefficient.\nThis floating-point value represents the coefficient's computed\nweight (for example, its contribution to a norm or selection metric)\nand is printed in the info string with eight decimal places to aid\nnumerical comparison and human inspection in GPAW logs.", "default": ""}, "fw_g": {"type": "array", "items": {"type": "float"}, "description": "Array of numeric weights (one per grid point or\nG-vector) associated with this coefficient in the underlying\ndiscretization. The function reports the maximum value of this array\n(computed with numpy.max) as an indicator of the largest local or\nreciprocal-space contribution for the coefficient. This array is not\nmodified by the function; only its maximum is queried for reporting.", "default": ""}}, "required": ["L", "included", "rshew", "fw_g"], "type": "any"}}, "type": "function"}], "query": "We’re validating RSHE coefficient diagnostics from a mixed-quality sphere.rshe dump where some replicates are likely numerical noise (e.g., nearly-flat fw_g) and others show meaningful grid-weight structure. For each replicate below, emit the exact single-line GPAW diagnostic string only if the replicate passes this screening rule: keep coefficients whose fw_g is non-trivial in the sense that its max value is at least 20× larger than its median (median over the provided fw_g samples). For coefficients that pass, also apply an inclusion rule derived from the raw data: mark the coefficient as included in the truncated expansion only if rshew is at least 1e-2; otherwise treat it as not included. Use the original L and rshew values and the provided fw_g arrays.\n\nReplicates:\n(1) L=8, rshew=0.00345712, fw_g=[0.0, 0.12, 0.08, 0.15, 0.11]\n(2) L=12, rshew=0.03745689, fw_g=[0.0, 0.12, 0.045, 0.33, 0.287, 0.05]\n(3) L=12, rshew=0.0375, fw_g=[0.0, 0.004, 0.012, 0.008, 0.010]", "answers": "[{\"name\":\"gpaw_sphere_rshe_get_rshe_coefficient_info_string\",\"arguments\":{\"L\":12,\"included\":true,\"rshew\":0.03745689,\"fw_g\":[0.0,0.12,0.045,0.33,0.287,0.05]}}]"}
{"func_name": "gpaw_tddft_spectrum_clean_td_data", "func_desc": "gpaw.tddft.spectrum.clean_td_data prunes and aligns time-dependent TDDFT data to a single perturbation (\"kick\") used in real-time propagation calculations. In the GPAW TDDFT spectrum workflow this function enforces that exactly one kick is present, discards all time and data samples that occur before the kick, and shifts the time axis so that the kick occurs at time zero. This alignment is necessary for correct post-processing of the time-dependent response (for example, constructing Fourier transforms to obtain excitation spectra).", "tools": [{"function": {"description": "gpaw.tddft.spectrum.clean_td_data prunes and aligns time-dependent TDDFT data to a single perturbation (\"kick\") used in real-time propagation calculations. In the GPAW TDDFT spectrum workflow this function enforces that exactly one kick is present, discards all time and data samples that occur before the kick, and shifts the time axis so that the kick occurs at time zero. This alignment is necessary for correct post-processing of the time-dependent response (for example, constructing Fourier transforms to obtain excitation spectra).\n", "name": "gpaw_tddft_spectrum_clean_td_data", "parameters": {"properties": {"kick_i": {"type": "array", "items": {"type": "float"}, "description": "List of kicks provided by the TDDFT driver. Each element is expected to be a dictionary describing a single perturbation applied to the system; the code accesses the first element's keys 'strength_v', 'velocity', and 'time'. In the GPAW TDDFT context these dictionaries represent instantaneous velocity/field perturbations whose 'time' is the physical time at which the perturbation was applied. This function requires that the list contains exactly one kick; if more than one kick is present a RuntimeError is raised. If the list is empty an IndexError will occur when attempting to read the first element.", "default": ""}, "time_t": {"type": "array", "items": {"type": "float"}, "description": "1-D NumPy array of time points (monotonically increasing) corresponding to the rows or first axis of data_ti. In TDDFT simulations this is the sequence of physical times at which observables were recorded. The function filters this array to include only times greater than or equal to the kick time and then subtracts the kick time so that the returned time_t starts at exactly 0.0. Note that the code uses exact equality to assert the post-subtraction first element equals 0.0; an AssertionError will be raised if floating-point representation or input times prevent exact equality.", "default": ""}, "data_ti": {"type": "array", "items": {"type": "float"}, "description": "NumPy array of time-dependent observables aligned with time_t (for example dipole or current as a function of time). The first axis of data_ti is assumed to correspond to the entries in time_t. The function returns the subset of data_ti corresponding to times at or after the kick time. Depending on NumPy's internal layout and slicing rules, the returned array may be a view of the original array or a copy.", "default": ""}}, "required": ["kick_i", "time_t", "data_ti"], "type": "any"}}, "type": "function"}], "query": "I’m consolidating a mixed-quality set of real-time GPAW TDDFT dipole trajectories for spectral analysis, but only propagations that contain a *resolvable pre-kick baseline* should enter the common Fourier-processing queue. Treat each trajectory as valid for cleaning only if its recorded time grid contains **at least two samples strictly earlier than the kick time** (so there is enough pre-kick baseline to verify the perturbation timing). For every trajectory that passes this criterion, run `gpaw.tddft.spectrum.clean_td_data` using the kick metadata exactly as provided (enforcing exactly one kick), then discard all samples strictly before the kick and shift the remaining time axis so the kick occurs at t=0.0 fs, keeping dipoles paired to the shifted times.\n\nRaw propagations:\n- Dataset A: kick strength_v=[0.0,0.0,0.01], velocity=true, time=2.0; times(fs)=[0.0,0.5,1.0,1.5,2.0,2.5,3.0]; dipoles(a.u.)=[[0.0,0.0,0.0],[0.01,0.0,0.0],[0.02,0.0,0.0],[0.03,0.0,0.0],[0.1,0.0,0.0],[0.12,0.0,0.0],[0.15,0.0,0.0]].\n- Dataset B: kick strength_v=[0.0,0.0,0.01], velocity=true, time=1.0; times(fs)=[0.0,0.5,1.0,1.5,2.0,2.5]; dipoles(a.u.)=[[0.0,0.0,0.0],[0.01,0.0,-0.01],[0.05,0.0,-0.05],[0.09,0.0,-0.09],[0.12,0.0,-0.12],[0.14,0.0,-0.14]].\n- Dataset C: kick strength_v=[0.01,0.0,0.0], velocity=[0.1,0.0,0.0], time=5.0; times(fs)=[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]; dipoles(a.u.)=[[0.0,0.0,0.0],[0.001,0.0,0.0],[0.002,0.0,0.0],[0.003,0.0,0.0],[0.004,0.0,0.0],[0.05,0.0,0.0],[0.048,0.0,0.0],[0.045,0.0,0.0],[0.04,0.0,0.0],[0.035,0.0,0.0],[0.03,0.0,0.0]].", "answers": "[{\"name\":\"gpaw_tddft_spectrum_clean_td_data\",\"arguments\":{\"kick_i\":[{\"strength_v\":[0.0,0.0,0.01],\"velocity\":true,\"time\":2.0}],\"time_t\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0],\"data_ti\":[[0.0,0.0,0.0],[0.01,0.0,0.0],[0.02,0.0,0.0],[0.03,0.0,0.0],[0.1,0.0,0.0],[0.12,0.0,0.0],[0.15,0.0,0.0]]}},{\"name\":\"gpaw_tddft_spectrum_clean_td_data\",\"arguments\":{\"kick_i\":[{\"strength_v\":[0.01,0.0,0.0],\"velocity\":[0.1,0.0,0.0],\"time\":5.0}],\"time_t\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\"data_ti\":[[0.0,0.0,0.0],[0.001,0.0,0.0],[0.002,0.0,0.0],[0.003,0.0,0.0],[0.004,0.0,0.0],[0.05,0.0,0.0],[0.048,0.0,0.0],[0.045,0.0,0.0],[0.04,0.0,0.0],[0.035,0.0,0.0],[0.03,0.0,0.0]]}}]"}
{"func_name": "gpaw_test_findpeak", "func_desc": "gpaw.test.findpeak finds the sub-sample peak location and peak value by fitting a quadratic (second-degree) polynomial to the three sampled points around the maximum sample. This function is used in GPAW tests and utilities to refine the location and value of a peak (for example, a spectral peak, a maximum in an energy curve, or any locally peaked 1D sampled quantity produced by DFT calculations) from discrete samples without requiring a denser grid.", "tools": [{"function": {"description": "gpaw.test.findpeak finds the sub-sample peak location and peak value by fitting a quadratic (second-degree) polynomial to the three sampled points around the maximum sample. This function is used in GPAW tests and utilities to refine the location and value of a peak (for example, a spectral peak, a maximum in an energy curve, or any locally peaked 1D sampled quantity produced by DFT calculations) from discrete samples without requiring a denser grid.\n", "name": "gpaw_test_findpeak", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of sample x-coordinates (independent variable). This array must have the same length as y and provide the x positions corresponding to the sampled values in y. Typical usage in GPAW tests is with evenly or nearly evenly spaced x (e.g., frequency, energy, or spatial coordinate grids); the function centers the quadratic fit on the sample at the index of the maximum y value by subtracting x[i] before fitting to improve numerical stability.", "default": ""}, "y": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of sampled values (dependent variable) with the same length as x. The peak is located by finding the index i = y.argmax() and fitting a quadratic through the three points y[i-1], y[i], y[i+1]. y represents a quantity computed in DFT contexts such as a spectral intensity or energy along a scan; the function assumes a single local maximum at the chosen index.", "default": ""}}, "required": ["x", "y"], "type": "any"}}, "type": "function"}], "query": "We’re running a GPAW regression QA pass on a mixed batch of 1D coarse-grid traces coming from different post-DFT analyzers. Each trace is provided as (x, y) samples, but the physics differs by observable:\n\n- For geometry-scan *energies* (units: eV), the physically relevant feature is a **minimum** (stable geometry), so convert it into a peak-refinement problem by running the quadratic 3-point refinement on the maximum of **(-y)** vs x.\n- For optical *intensities* (dimensionless), treat the dominant spectral line as a **maximum** in y and refine it directly.\n\nAdditionally, only run refinement for traces where the extremum is resolvable on the interior of the sampled grid (i.e., the max sample used by the quadratic 3-point fit is not on the first or last x point).\n\nCohorts (raw):\n(A) geometry scan: x = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5] (Å), y = [-10.2, -10.5, -10.8, -10.6, -10.3, -10.1] (eV)\n(B) absorption spectrum #1: x = [2.8, 3.0, 3.2, 3.4, 3.6] (eV), y = [0.10, 0.55, 1.05, 0.98, 0.40]\n(C) absorption spectrum #2 replicate: x = [1.8, 1.9, 2.0, 2.1, 2.2] (eV), y = [0.2, 0.5, 1.1, 0.9, 0.4]\n\nFor each qualifying trace, output the refined sub-sample extremum position and extremum value in the observable’s natural sign convention (i.e., report the minimum energy for A, not the peak of -y).", "answers": "[{\"name\":\"gpaw_test_findpeak\",\"arguments\":{\"x\":[0.0,0.1,0.2,0.3,0.4,0.5],\"y\":[10.2,10.5,10.8,10.6,10.3,10.1]}},{\"name\":\"gpaw_test_findpeak\",\"arguments\":{\"x\":[2.8,3.0,3.2,3.4,3.6],\"y\":[0.1,0.55,1.05,0.98,0.4]}},{\"name\":\"gpaw_test_findpeak\",\"arguments\":{\"x\":[1.8,1.9,2.0,2.1,2.2],\"y\":[0.2,0.5,1.1,0.9,0.4]}}]"}
{"func_name": "gpaw_tetrahedron_bja2b", "func_desc": "gpaw.tetrahedron.bja2b computes the four tetrahedron-integration weights (w1, w2, w3, w4) from four vertex energies using the analytic formulas given in Eq. (B7)–(B10) of Blöchl, Jepsen and Andersen. This function is used in GPAW's tetrahedron integration routines for density-functional theory (DFT) calculations (projector-augmented wave method) to obtain the contribution of a single tetrahedron to integrated quantities such as the density of states or band energy by linear interpolation inside the tetrahedron.", "tools": [{"function": {"description": "gpaw.tetrahedron.bja2b computes the four tetrahedron-integration weights (w1, w2, w3, w4) from four vertex energies using the analytic formulas given in Eq. (B7)–(B10) of Blöchl, Jepsen and Andersen. This function is used in GPAW's tetrahedron integration routines for density-functional theory (DFT) calculations (projector-augmented wave method) to obtain the contribution of a single tetrahedron to integrated quantities such as the density of states or band energy by linear interpolation inside the tetrahedron.\n", "name": "gpaw_tetrahedron_bja2b", "parameters": {"properties": {"e1": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies at tetrahedron vertex 1. Each element corresponds to an energy value for a particular k-point / band or an energy sample; the array must have the same shape as e2, e3 and e4. In the context of GPAW tetrahedron integration, e1 represents the energy value at the first corner of a tetrahedron used to compute analytic integration weights according to Blöchl, Jepsen and Andersen.", "default": ""}, "e2": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies at tetrahedron vertex 2. Same shape and role as e1 but for the second corner. The ordering of e1, e2, e3, e4 matches the vertex ordering used in the analytic formulas (Eqs. B7–B10).", "default": ""}, "e3": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies at tetrahedron vertex 3. Same shape and role as e1 and e2 but for the third corner. These inputs are combined elementwise to compute the weight components w1..w4.", "default": ""}, "e4": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies at tetrahedron vertex 4. Same shape and role as the other e-parameters but for the fourth corner. All four arrays are used together in rational expressions; the function performs no input validation beyond numpy broadcasting rules.", "default": ""}}, "required": ["e1", "e2", "e3", "e4"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a GPAW tetrahedron-integration post-processing step under messy band-manifold conditions. Each tetrahedron provides four vertex energy arrays (eV) but some candidate band channels are not suitable for tetrahedron DOS integration because they are either outside the target energy window or numerically degenerate at a vertex. Use the following two tetrahedra and, for each, first construct the band list to integrate by applying these rules: (i) keep only band indices whose energies across all four vertices stay within the analysis window [-6.0, 2.0] eV; (ii) from those, drop any band index where any pair of vertex energies are closer than 0.05 eV (treat as near-degenerate / ill-conditioned for the analytic BJA weights). After filtering, pass the remaining per-vertex band-energy arrays (preserving band order) into gpaw.tetrahedron.bja2b to compute (w1,w2,w3,w4).\n\nTetrahedron T1 raw vertex energies (4 candidate bands):\n e1=[-5.2,-1.0,0.3,1.1]\n e2=[-4.8,-0.7,0.5,1.3]\n e3=[-4.5,-0.4,0.8,1.6]\n e4=[-4.2,-0.2,1.0,1.9]\n\nTetrahedron T2 raw vertex energies (3 candidate bands):\n e1=[-5.2,0.1,3.4]\n e2=[-4.9,0.3,3.6]\n e3=[-5.0,0.0,3.5]\n e4=[-4.8,0.2,3.7]\n\nReturn the computed weights for each tetrahedron that still has at least one valid band channel after filtering.", "answers": "[{\"name\":\"gpaw_tetrahedron_bja2b\",\"arguments\":{\"e1\":[-5.2,-1.0,0.3,1.1],\"e2\":[-4.8,-0.7,0.5,1.3],\"e3\":[-4.5,-0.4,0.8,1.6],\"e4\":[-4.2,-0.2,1.0,1.9]}},{\"name\":\"gpaw_tetrahedron_bja2b\",\"arguments\":{\"e1\":[-5.2,0.1],\"e2\":[-4.9,0.3],\"e3\":[-5.0,0.0],\"e4\":[-4.8,0.2]}}]"}
{"func_name": "gpaw_tetrahedron_bja3", "func_desc": "gpaw.tetrahedron.bja3 computes coefficients from Equations (A4) and (C4) of Blöchl, Jepsen and Andersen used in the tetrahedron integration scheme in density-functional theory (DFT) calculations performed by GPAW. The function produces one scalar and one array of coefficients that are used to form tetrahedron integration weights (for example when integrating the density of states or occupancy-related quantities over tetrahedra in reciprocal space).\n    \n    This implementation follows the algebra in the cited BJA equations: it forms the reciprocal of the product (e4 - e1)*(e4 - e2)*(e4 - e3) elementwise and then returns a scalar formed as len(e1) - x.dot(e4**3) together with an array 3*x*e4**2. These quantities are directly used by the GPAW tetrahedron routines to assemble integrals over energy within a tetrahedron.", "tools": [{"function": {"description": "gpaw.tetrahedron.bja3 computes coefficients from Equations (A4) and (C4) of Blöchl, Jepsen and Andersen used in the tetrahedron integration scheme in density-functional theory (DFT) calculations performed by GPAW. The function produces one scalar and one array of coefficients that are used to form tetrahedron integration weights (for example when integrating the density of states or occupancy-related quantities over tetrahedra in reciprocal space).\n\nThis implementation follows the algebra in the cited BJA equations: it forms the reciprocal of the product (e4 - e1)*(e4 - e2)*(e4 - e3) elementwise and then returns a scalar formed as len(e1) - x.dot(e4**3) together with an array 3*x*e4**2. These quantities are directly used by the GPAW tetrahedron routines to assemble integrals over energy within a tetrahedron.", "name": "gpaw_tetrahedron_bja3", "parameters": {"properties": {"e1": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies (eigenvalues) corresponding to corner 1 of each tetrahedron. In GPAW's tetrahedron integration context, this typically represents the energy values at a set of samples (e.g., k-points or bands) associated with the first vertex of each tetrahedron. The array length determines the number of samples; len(e1) is used in the scalar computation. e1 must have the same shape as e2, e3 and e4.", "default": ""}, "e2": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies corresponding to corner 2 of each tetrahedron. Practically, e2 is the energy at the second vertex for the same set of samples as e1. It must be the same shape and ordering as e1 so that elementwise operations produce meaningful per-sample coefficients.", "default": ""}, "e3": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies corresponding to corner 3 of each tetrahedron. Like e1 and e2, e3 provides the third-vertex energies for the same samples and must match their shape and ordering.", "default": ""}, "e4": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of energies corresponding to corner 4 of each tetrahedron. e4 is used as the distinguished vertex in the algebra of Equations (A4) and (C4); it appears both in the denominator product (e4 - e1)*(e4 - e2)*(e4 - e3) and in powers e4**2 and e4**3. e4 must match the shape of e1, e2 and e3.", "default": ""}}, "required": ["e1", "e2", "e3", "e4"], "type": "any"}}, "type": "function"}], "query": "We’re validating a tetrahedron DOS/occupancy precomputation step where raw tetrahedra sometimes arrive with mis-ordered corner eigenvalues due to band-index scrambling across k-points. You are given three cohorts of per-tetrahedron corner energies (arrays are aligned by tetrahedron index):\n\nCohort A (6 tetrahedra):\ne1=[-5.2,-3.1,-1.0,0.5,1.8,3.4]\ne2=[-4.9,-2.8,-0.7,0.8,2.1,3.7]\ne3=[-4.5,-2.4,-0.3,1.1,2.4,4.0]\ne4=[-4.0,-2.0,0.1,1.5,2.8,4.3]\n\nCohort B (4 tetrahedra):\ne1=[5.1,5.4,5.9,6.2]\ne2=[5.3,5.5,6.0,6.4]\ne3=[5.2,5.6,6.1,6.3]\ne4=[5.4,5.7,6.2,6.5]\n\nCohort C (4 tetrahedra):\ne1=[-5.2,-1.1,0.3,2.4]\ne2=[-4.8,-0.9,0.5,2.8]\ne3=[-4.9,-1.0,0.7,3.1]\ne4=[-4.5,-0.6,1.0,3.6]\n\nFor each cohort, select only those tetrahedra whose corner energies are strictly ordered (e1<e2<e3<e4 for that tetrahedron) and for which the BJA3 denominator factors are all non-zero (i.e., e4 differs from each of e1,e2,e3). Run gpaw.tetrahedron.bja3 on the selected tetrahedra in each cohort (preserving their original relative order within the cohort) and return the scalar term and the 3*x*e4**2 array term produced by BJA3 for each selected cohort block.", "answers": "[{\"name\":\"gpaw_tetrahedron_bja3\",\"arguments\":{\"e1\":[-5.2,-3.1,-1.0,0.5,1.8,3.4],\"e2\":[-4.9,-2.8,-0.7,0.8,2.1,3.7],\"e3\":[-4.5,-2.4,-0.3,1.1,2.4,4.0],\"e4\":[-4.0,-2.0,0.1,1.5,2.8,4.3]}},{\"name\":\"gpaw_tetrahedron_bja3\",\"arguments\":{\"e1\":[5.1,5.4,5.9,6.2],\"e2\":[5.3,5.5,6.0,6.4],\"e3\":[5.2,5.6,6.1,6.3],\"e4\":[5.4,5.7,6.2,6.5]}},{\"name\":\"gpaw_tetrahedron_bja3\",\"arguments\":{\"e1\":[-5.2,-1.1,0.3,2.4],\"e2\":[-4.8,-0.9,0.5,2.8],\"e3\":[-4.9,-1.0,0.7,3.1],\"e4\":[-4.5,-0.6,1.0,3.6]}}]"}
{"func_name": "gpaw_tetrahedron_bja3b", "func_desc": "gpaw.tetrahedron.bja3b computes the elementwise tetrahedron interpolation weights w1, w2, w3 and w4 using the analytic formulas given in Blöchl, Jepsen and Andersen (Eqs. B14–B17). These weights are used in GPAW's tetrahedron-method integrations (for example k-point integrations and density-of-states calculations) to distribute contributions from a tetrahedron's four vertices according to their vertex energies. The implementation is vectorized and operates on NumPy arrays to produce weights for one or many energy points in a single call.", "tools": [{"function": {"description": "gpaw.tetrahedron.bja3b computes the elementwise tetrahedron interpolation weights w1, w2, w3 and w4 using the analytic formulas given in Blöchl, Jepsen and Andersen (Eqs. B14–B17). These weights are used in GPAW's tetrahedron-method integrations (for example k-point integrations and density-of-states calculations) to distribute contributions from a tetrahedron's four vertices according to their vertex energies. The implementation is vectorized and operates on NumPy arrays to produce weights for one or many energy points in a single call.\n", "name": "gpaw_tetrahedron_bja3b", "parameters": {"properties": {"e1": {"type": "array", "items": {"type": "float"}, "description": "Energy values at tetrahedron vertex 1. Expected to be a one-dimensional NumPy array (Array1D) containing the energy(s) for the first vertex of each tetrahedron. In the context of GPAW's tetrahedron integration, e1 is one of the four vertex energies used to compute the interpolation weight w1 for each energy entry. All input arrays must have the same shape or be broadcastable to a common shape; typically they are 1D arrays of the same length corresponding to multiple energy points or integration samples.", "default": ""}, "e2": {"type": "array", "items": {"type": "float"}, "description": "Energy values at tetrahedron vertex 2. Same expectations as e1: a one-dimensional NumPy array holding the second vertex energy for each tetrahedron or sample. This argument is used to compute the interpolation weight w2 according to Eq. (B15) of Blöchl, Jepsen and Andersen.", "default": ""}, "e3": {"type": "array", "items": {"type": "float"}, "description": "Energy values at tetrahedron vertex 3. Same expectations as e1 and e2. This array supplies the third vertex energy for each tetrahedron and is used to compute the interpolation weight w3 according to Eq. (B16) of Blöchl, Jepsen and Andersen.", "default": ""}, "e4": {"type": "array", "items": {"type": "float"}, "description": "Energy values at tetrahedron vertex 4 (the vertex singled out in Eqs. B14–B17). Same expectations as the other e-arrays. In the formulas implemented here e4 plays a special role (it appears in denominators and in the cubic prefactor C); therefore e4 should correspond to the vertex energy referenced by the analytic expressions from Blöchl, Jepsen and Andersen.", "default": ""}}, "required": ["e1", "e2", "e3", "e4"], "type": "any"}}, "type": "function"}], "query": "We’re validating a tetrahedron-method DOS integration stage where raw vertex-energy quartets arrive from multiple k-point neighborhoods and may violate tetrahedron ordering due to band-index jitter. Below are three vectorized batches (each index is one tetrahedron). For each tetrahedron, first enforce the physical ordering constraint required by the BJA (B14–B17) weights by sorting its four vertex energies into ascending order (so the smallest becomes e1 and the largest becomes e4). Then compute gpaw.tetrahedron.bja3b weights only for tetrahedra that have a non-degenerate spectrum (strictly increasing after sorting, i.e., no repeated vertex energies). Process each batch in one vectorized call after filtering, preserving the original tetrahedron order among those that pass.\n\nBatch A (high-energy window, includes a near-degeneracy):\nE_raw = [\n  [5.7, 5.1, 6.1, 5.8],\n  [5.3, 5.6, 6.0, 6.3],\n  [5.5, 5.8, 6.2, 6.5],\n  [5.7, 6.0, 6.4, 6.7]\n]\n\nBatch B (crossing reference energy, includes an accidental degeneracy):\nE_raw = [\n  [-5.2, -4.8, -4.5, -4.0],\n  [-1.0, -0.7, -0.3, 0.0],\n  [0.3, 0.6, 0.9, 1.2],\n  [1.5, 1.8, 2.1, 2.4]\n]\n\nBatch C (near reference, one tetrahedron has a duplicated vertex energy):\nE_raw = [\n  [-1.2, -0.9, -0.5, 0.2],\n  [-0.6, -0.2, 0.0, 0.5],\n  [0.1, 0.4, 0.8, 0.8]\n]\n\nReturn the vertex-resolved weights (w1–w4) for each processed batch.", "answers": "[{\"name\":\"gpaw_tetrahedron_bja3b\",\"arguments\":{\"e1\":[5.1,5.3,5.5,5.7],\"e2\":[5.7,5.6,5.8,6.0],\"e3\":[5.8,6.0,6.2,6.4],\"e4\":[6.1,6.3,6.5,6.7]}},{\"name\":\"gpaw_tetrahedron_bja3b\",\"arguments\":{\"e1\":[-5.2,-1.0,0.3,1.5],\"e2\":[-4.8,-0.7,0.6,1.8],\"e3\":[-4.5,-0.3,0.9,2.1],\"e4\":[-4.0,0.0,1.2,2.4]}},{\"name\":\"gpaw_tetrahedron_bja3b\",\"arguments\":{\"e1\":[-1.2,-0.6],\"e2\":[-0.9,-0.2],\"e3\":[-0.5,0.0],\"e4\":[0.2,0.5]}}]"}
{"func_name": "gpaw_tetrahedron_weights", "func_desc": "Calculate occupation numbers using the tetrahedron integration scheme used in GPAW for Brillouin-zone integration and density-functional theory (DFT) calculations.", "tools": [{"function": {"description": "Calculate occupation numbers using the tetrahedron integration scheme used in GPAW for Brillouin-zone integration and density-functional theory (DFT) calculations.\n", "name": "gpaw_tetrahedron_weights", "parameters": {"properties": {"eig_in": {"type": "array", "items": {"type": "float"}, "description": "2-D array of single-particle eigenvalues (energies) used to determine occupations. The first axis indexes the electronic state positions that are referenced by the tetrahedron mapping in i_ktq (typically combined k-point / spin indices), and the second axis indexes band indices. This function treats eigenvalues less than 0.0 as occupied when counting fully occupied bands per k-point. The practical role of eig_in in the GPAW tetrahedron method is to provide the band-energy spectrum from which partial occupancies are computed for use in charge-density and total-energy calculations.", "default": ""}, "i_ktq": {"type": "array", "items": {"type": "float"}, "description": "3-D integer array that maps tetrahedra to the indices of eig_in. The expected layout (as used by this routine) is (N_k, 6, 4), where N_k is the number of k-point-like entries and the second dimension enumerates the six tetrahedra associated with each k-like index and the last dimension lists the four vertices (indices into the first axis of eig_in) of each tetrahedron. This mapping is used to gather eigenvalues at tetrahedron vertices, compute tetrahedron-specific weight distributions, and then scatter the resulting fractional occupations back to the corresponding rows of the output occupancy array. If i_ktq has incompatible shape or contains out-of-range indices, the function will raise standard indexing errors.", "default": ""}, "improved": {"type": "boolean", "description": "If False (the default), use the standard tetrahedron weight formulas to compute fractional occupations for bands that are partially occupied across tetrahedra. If True, apply the additional improved correction terms computed by the auxiliary routines (used in GPAW to reduce integration error) that depend on derivatives of the weight functions; this adjusts the tetrahedron weights with higher-order contributions. The boolean controls whether those extra correction contributions are included in the returned occupation numbers.", "default": false}}, "required": ["eig_in", "i_ktq", "improved"], "type": "any"}}, "type": "function"}], "query": "We’re validating a GPAW-style tetrahedron occupation workflow at EF = 0 eV under messy, mixed-quality Brillouin-zone sampling. Use the **improved correction** option wherever the input can support it. Two cohorts are present, but each needs different handling driven by the intrinsic spectral characteristics:\n\nCohort A (metallic mini-system; 3 k-like entries, 2 bands): the raw eigenvalue table is a concatenation of two acquisition blocks. Split the 6×2 `eig_in` into two contiguous blocks of 3 k-points each; treat each block as an independent metallic mini-system and run tetrahedron occupations separately (EF = 0 eV). Use the same 6-tetrahedra connectivity for each run, replicated across the 3 k-like entries:\n- eig_in (6×2): [[-1.2, 0.3], [-0.8, 0.6], [-0.4, 0.9], [0.1, 1.2], [0.5, 1.5], [-0.2, 0.8]]\n- base i_ktq connectivity over indices 0..5 (6×4): [[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]]\n\nCohort B (toy integration benchmark; 1 k-like entry, 4 k-like points, 3 bands): the benchmark contains a band that pins EF exactly at one k-point (an eigenvalue exactly 0.00 eV). To stress-test the improved correction behavior around EF, run tetrahedron occupations twice: once on the dataset as-is, and once on a minimally perturbed variant where only exact EF-pinned eigenvalues are shifted by +1e-12 eV (leaving all other eigenvalues unchanged). Use the provided 1×6×4 i_ktq as-is for both runs.\n- eig_in (4×3): [[-1.20,-0.30,0.50],[-0.90,0.10,0.80],[-0.60,0.00,1.10],[-0.20,0.40,1.50]]\n- i_ktq (1×6×4): [[[0,1,2,3],[0,1,3,2],[1,2,3,0],[0,2,3,1],[0,1,2,3],[1,0,2,3]]]\n\nReturn the final tetrahedron-derived occupation weights for each run (all runs must use improved correction).", "answers": "[{\"name\":\"gpaw_tetrahedron_weights\",\"arguments\":{\"eig_in\":[[-1.2,0.3],[-0.8,0.6],[-0.4,0.9]],\"i_ktq\":[[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]],[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]],[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]]],\"improved\":true}},{\"name\":\"gpaw_tetrahedron_weights\",\"arguments\":{\"eig_in\":[[0.1,1.2],[0.5,1.5],[-0.2,0.8]],\"i_ktq\":[[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]],[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]],[[0,1,2,3],[0,2,3,4],[1,2,3,5],[0,1,3,4],[1,2,4,5],[0,2,4,5]]],\"improved\":true}},{\"name\":\"gpaw_tetrahedron_weights\",\"arguments\":{\"eig_in\":[[-1.2,-0.3,0.5],[-0.9,0.1,0.8],[-0.6,0.0,1.1],[-0.2,0.4,1.5]],\"i_ktq\":[[[0,1,2,3],[0,1,3,2],[1,2,3,0],[0,2,3,1],[0,1,2,3],[1,0,2,3]]],\"improved\":true}},{\"name\":\"gpaw_tetrahedron_weights\",\"arguments\":{\"eig_in\":[[-1.2,-0.3,0.5],[-0.9,0.1,0.8],[-0.6,1e-12,1.1],[-0.2,0.4,1.5]],\"i_ktq\":[[[0,1,2,3],[0,1,3,2],[1,2,3,0],[0,2,3,1],[0,1,2,3],[1,0,2,3]]],\"improved\":true}}]"}
{"func_name": "gpaw_unfold_find_K_from_k", "func_desc": "Gets a k vector in scaled (fractional) coordinates and returns a mapped K vector and the unfolding integer vector G, both in scaled coordinates. This function is used in GPAW's band-structure unfolding routines to map k-points between cells (for example between a primitive cell and a supercell) and to record which reciprocal-lattice integer vector was added or subtracted during the mapping.", "tools": [{"function": {"description": "Gets a k vector in scaled (fractional) coordinates and returns a mapped K vector and the unfolding integer vector G, both in scaled coordinates. This function is used in GPAW's band-structure unfolding routines to map k-points between cells (for example between a primitive cell and a supercell) and to record which reciprocal-lattice integer vector was added or subtracted during the mapping.\n", "name": "gpaw_unfold_find_K_from_k", "parameters": {"properties": {"k": {"type": "array", "items": {"type": "float"}, "description": "A length-3 array giving a k-point in scaled (fractional) coordinates. In the GPAW unfolding context, each component of k is the coordinate along a reciprocal-lattice basis (i.e., a fractional reciprocal coordinate). The function expects k to have three components because it computes KG = M.dot(k) and then processes the three components individually. Supplying an array with a different shape will cause numpy.dot or the subsequent indexing/loops to raise an error.", "default": ""}, "M": {"type": "array", "items": {"type": "float"}, "description": "A 3x3 numeric matrix used to transform the input k into the target scaled coordinate system (KG = M.dot(k)). In unfolding workflows this matrix typically represents the integer transformation between reciprocal-lattice bases (for example, the mapping from primitive to supercell reciprocal coordinates). M must be numeric and conformable with k for matrix multiplication (normally shape (3, 3)); incorrect shapes or non-numeric entries will lead to numpy errors.", "default": ""}}, "required": ["k", "M"], "type": "any"}}, "type": "function"}], "query": "We’re validating reciprocal-space bookkeeping for GPAW unfolding on a mixed-quality k-point table collected from a graphene 2×2×1 supercell/primitive cross-check. Use the integer transformation matrix M = [[2, 0, 0], [0, 2, 0], [0, 0, 1]]. Treat each record as a candidate k-point in *scaled (fractional) coordinates* and run the mapping only for entries that are physically admissible for a periodic Brillouin-zone workflow: all components must be finite numbers and kz must lie on the graphene plane (kz = 0). For each admissible entry, compute (i) the mapped K vector in scaled coordinates and (ii) the unfolding integer reciprocal-lattice vector G (scaled) that captures any reciprocal-lattice shift.\n\nRaw k-point records (heterogeneous formatting as ingested from logs):\n1) Replicate A (supercell→primitive check): k = [0.125, 0.375, 0.0]\n2) Replicate B (primitive→supercell check): k = (0.25, -0.10, 0.60)\n3) Replicate C (instrument artifact): k = [0.40, 0.20, NaN]\n4) Replicate D (out-of-plane contamination): k = [0.10, 0.10, 0.25]", "answers": "[{\"name\":\"gpaw_unfold_find_K_from_k\",\"arguments\":{\"k\":[0.125,0.375,0.0],\"M\":[[2,0,0],[0,2,0],[0,0,1]]}}]"}
{"func_name": "gpaw_unfold_plot_band_structure", "func_desc": "Plot band structure using projection weights P_mK and show each band-state as a filled circle whose color and size encode projection weight information used in unfolding analyses in density-functional theory (DFT) calculations performed with GPAW.", "tools": [{"function": {"description": "Plot band structure using projection weights P_mK and show each band-state as a filled circle whose color and size encode projection weight information used in unfolding analyses in density-functional theory (DFT) calculations performed with GPAW.\n", "name": "gpaw_unfold_plot_band_structure", "parameters": {"properties": {"e_mK": {"type": "array", "items": {"type": "float"}, "description": "Array of single-particle energies in electronvolts for all bands and k-points. From the source code usage, this is expected to be a 2-D array shaped (n_bands, n_kpoints) where each row corresponds to a band index m and each column to a k-point K along a path in reciprocal space. In the GPAW/ASE DFT context this array contains the band energies computed for a (possibly unfolded) band-structure calculation and is plotted on the vertical (energy) axis.", "default": ""}, "P_mK": {"type": "array", "items": {"type": "float"}, "description": "Array of projection weights used to color the scatter points. This must have the same shape as e_mK (n_bands, n_kpoints). Values are interpreted between 0.0 and 1.0 by the plotting routine (vmin=0., vmax=1.) and are mapped to colors using a colormap created from the color argument. In unfolding workflows P_mK typically encodes how strongly each supercell band-state projects onto a chosen primitive-cell character.", "default": ""}, "x": {"type": "array", "items": {"type": "float"}, "description": "1-D array of positions along the k-path (cumulative k-point distance) to be used as the horizontal axis. Its length must equal n_kpoints (the number of columns in e_mK and P_mK). The function tiles this array for all bands to position every energy point horizontally.", "default": ""}, "X": {"type": "array", "items": {"type": "float"}, "description": "List of float positions along the same k-path used to draw vertical separators and to set xtick locations. Typical usage in band-structure plotting is to pass the cumulative distances of high-symmetry k-points; X[1:-1] are drawn as vertical guide lines. The list length must match the length of points_name for xtick labeling.", "default": ""}, "points_name": {"type": "array", "items": {"type": "float"}, "description": "List of strings (or values convertible to strings) giving tick labels for the positions in X. In practical GPAW plotting this is the list of high-symmetry k-point labels (for example ['G', 'X', 'M', 'G']), used to annotate the horizontal axis. The number of entries must equal len(X).", "default": ""}, "weights_mK": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional array of the same shape as e_mK and P_mK providing an additional scaling factor for marker sizes. If None (the default), the function uses a copy of P_mK for marker sizes. If provided, the array is multiplied in-place by P_mK (weights_mK *= P_mK.copy()), so the caller's array will be modified by this scaling operation. Marker sizes are computed as 20. * weights_mK and therefore scale linearly with these values. This argument allows separating a size-modulating weight from the color-mapping weight P_mK, e.g., when combining projection amplitudes with other per-state metrics.", "default": null}, "color": {"type": "string", "description": "Name of the base color passed to make_colormap to create a matplotlib colormap used for mapping P_mK values to colors. The default is \"red\". The function delegates colormap construction to make_colormap(color) and then uses that colormap with vmin=0.0 and vmax=1.0 to visualize P_mK.", "default": "red"}, "fit": {"type": "boolean", "description": "Boolean flag included for API compatibility. In the current implementation this parameter is accepted but unused (default True). It suggests whether a smoothing/fit of band lines should be performed, but the function does not implement any fitting behavior; setting it has no effect.", "default": true}, "nfit": {"type": "integer", "description": "Integer included for API compatibility (default 200). In the current implementation this parameter is accepted but unused. It is commonly used in other plotting utilities to set the number of interpolation points for fitted/smoothed band lines; here it has no effect.", "default": 200}}, "required": ["e_mK", "P_mK", "x", "X", "points_name", "nfit", "color", "weights_mK", "fit"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our GPAW unfolding visualization against typical post-processing artifacts. You are given three candidate unfolded band-structure datasets (A–C) meant to be comparable, but they contain common issues: occasional unphysical projection weights and partial/absent marker-size weights. Build the plotting jobs under the following protocol.\n\nGlobal styling: render every band-state as a filled circle; use a blue colormap for the projection-weight coloring. Use each cohort’s provided k-path cumulative distances x, high-symmetry ticks X, and labels points_name.\n\nData sieve (weights sanity): treat projection weights as physically meaningful only when 0 ≤ P_mK ≤ 1. Any P_mK outside this interval should be clipped to the nearest bound before plotting.\n\nBranching protocol (size encoding):\n- If a cohort provides weights_mK, use it as marker-size weights only for entries that are strictly positive; for non-positive entries, fall back to using the corresponding (sanitized) P_mK for sizing at those positions.\n- If a cohort does not provide weights_mK, marker sizes must be driven entirely by the (sanitized) P_mK.\n\nFitting protocol: run Cohort A without fitting. For Cohorts B and C, enable fitting using their given nfit.\n\nCohort A (graphene 2×2; path Γ–M–K–Γ):\n- e_mK:\n  [[-5.2, -5.1, -4.95, -5.0, -5.05, -5.15],\n   [-1.1, -0.95, -0.8, -0.78, -0.9, -1.05],\n   [0.6, 0.75, 0.95, 1.05, 0.9, 0.7],\n   [3.1, 3.25, 3.4, 3.35, 3.2, 3.05]]\n- P_mK:\n  [[0.9, 0.85, 0.8, 0.82, 0.88, 0.92],\n   [0.2, 0.35, 0.55, 0.6, 0.4, 0.25],\n   [0.1, 0.15, 0.3, 0.45, 0.25, 0.12],\n   [0.05, 0.08, 0.1, 0.12, 0.09, 0.06]]\n- x = [0.0, 0.4, 0.8, 1.2, 1.6, 2.0]\n- X = [0.0, 0.8, 1.6, 2.0]; points_name = [\"Γ\", \"M\", \"K\", \"Γ\"]\n- weights_mK:\n  [[0.3, 0.25, 0.2, 0.2, 0.25, 0.3],\n   [0.8, 0.85, 0.9, 0.9, 0.85, 0.8],\n   [0.95, 0.98, 1.0, 1.0, 0.98, 0.95],\n   [0.4, 0.35, 0.3, 0.3, 0.35, 0.4]]\n- fit=false; nfit=300\n\nCohort B (graphene 2×2; path Γ–K–M–Γ):\n- e_mK:\n  [[-2.1, -1.8, -1.5, -1.2, -1.0, -0.9],\n   [0.0, 0.1, 0.3, 0.6, 0.8, 1.0],\n   [1.5, 1.6, 1.7, 1.9, 2.1, 2.3]]\n- P_mK (note: includes a negative artifact):\n  [[0.1, 0.2, 0.3, 0.4, 0.6, 0.8],\n   [0.9, 0.8, 0.7, 0.6, 0.4, 0.2],\n   [-0.05, 0.1, 0.2, 0.3, 0.5, 0.7]]\n- x = [0.0, 0.25, 0.5, 0.875, 1.2, 1.5]\n- X = [0.0, 0.5, 1.0, 1.5]; points_name = [\"Γ\", \"K\", \"M\", \"Γ\"]\n- weights_mK (note: contains a zero entry):\n  [[0.5, 0.4, 0.3, 0.2, 0.1, 0.0],\n   [0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n   [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]]\n- fit=true; nfit=200\n\nCohort C (4-band; path Γ–X–M–Γ):\n- e_mK:\n  [[-1.2, -0.85, -0.3, -0.95],\n   [-0.4, -0.1, 0.35, -0.05],\n   [0.6, 0.95, 1.4, 0.9],\n   [1.8, 2.1, 2.55, 2.05]]\n- P_mK (note: includes a >1 artifact):\n  [[0.95, 0.7, 0.2, 0.8],\n   [0.6, 0.45, 0.15, 0.5],\n   [0.25, 1.2, 0.75, 0.4],\n   [0.1, 0.2, 0.55, 0.15]]\n- x = [0.0, 0.5, 1.0, 1.6]\n- X = [0.0, 0.5, 1.0, 1.6]; points_name = [\"Γ\", \"X\", \"M\", \"Γ\"]\n- weights_mK not provided\n- fit=true; nfit=200", "answers": "[{\"name\":\"gpaw_unfold_plot_band_structure\",\"arguments\":{\"e_mK\":[[-5.2,-5.1,-4.95,-5.0,-5.05,-5.15],[-1.1,-0.95,-0.8,-0.78,-0.9,-1.05],[0.6,0.75,0.95,1.05,0.9,0.7],[3.1,3.25,3.4,3.35,3.2,3.05]],\"P_mK\":[[0.9,0.85,0.8,0.82,0.88,0.92],[0.2,0.35,0.55,0.6,0.4,0.25],[0.1,0.15,0.3,0.45,0.25,0.12],[0.05,0.08,0.1,0.12,0.09,0.06]],\"x\":[0.0,0.4,0.8,1.2,1.6,2.0],\"X\":[0.0,0.8,1.6,2.0],\"points_name\":[\"Γ\",\"M\",\"K\",\"Γ\"],\"weights_mK\":[[0.3,0.25,0.2,0.2,0.25,0.3],[0.8,0.85,0.9,0.9,0.85,0.8],[0.95,0.98,1.0,1.0,0.98,0.95],[0.4,0.35,0.3,0.3,0.35,0.4]],\"color\":\"blue\",\"fit\":false,\"nfit\":300}},{\"name\":\"gpaw_unfold_plot_band_structure\",\"arguments\":{\"e_mK\":[[-2.1,-1.8,-1.5,-1.2,-1.0,-0.9],[0.0,0.1,0.3,0.6,0.8,1.0],[1.5,1.6,1.7,1.9,2.1,2.3]],\"P_mK\":[[0.1,0.2,0.3,0.4,0.6,0.8],[0.9,0.8,0.7,0.6,0.4,0.2],[0.0,0.1,0.2,0.3,0.5,0.7]],\"x\":[0.0,0.25,0.5,0.875,1.2,1.5],\"X\":[0.0,0.5,1.0,1.5],\"points_name\":[\"Γ\",\"K\",\"M\",\"Γ\"],\"weights_mK\":[[0.5,0.4,0.3,0.2,0.1,0.8],[0.2,0.3,0.4,0.5,0.6,0.7],[0.1,0.2,0.3,0.4,0.5,0.6]],\"color\":\"blue\",\"fit\":true,\"nfit\":200}},{\"name\":\"gpaw_unfold_plot_band_structure\",\"arguments\":{\"e_mK\":[[-1.2,-0.85,-0.3,-0.95],[-0.4,-0.1,0.35,-0.05],[0.6,0.95,1.4,0.9],[1.8,2.1,2.55,2.05]],\"P_mK\":[[0.95,0.7,0.2,0.8],[0.6,0.45,0.15,0.5],[0.25,1.0,0.75,0.4],[0.1,0.2,0.55,0.15]],\"x\":[0.0,0.5,1.0,1.6],\"X\":[0.0,0.5,1.0,1.6],\"points_name\":[\"Γ\",\"X\",\"M\",\"Γ\"],\"color\":\"blue\",\"fit\":true,\"nfit\":200}}]"}
{"func_name": "gpaw_utilities_acwf_reference_structure", "func_desc": "Create and return an ASE Atoms object for a specified ACWF structure scaled to the WIEN2K reference volume.\n    \n    This function gpaw.utilities.acwf.reference_structure selects an ACWF structure template from the module-level acwf_structures mapping using the provided name, makes a copy of that template to avoid mutating the global template, and scales the unit cell and atomic positions so that the resulting cell volume matches the WIEN2K reference volume for the given chemical element symbol as stored in the module-level volumes mapping. For two oxygen-containing ACWF names ('X4O10' and 'XO') a cube-root-of-two correction is applied to the linear scaling factor to account for a difference in formula-unit convention in the stored WIEN2K volumes. After scaling, any placeholder atomic numbers equal to 0 in the structure template are replaced with the atomic number for the provided element symbol using the module-level atomic_numbers mapping. The returned Atoms object is suitable for use in GPAW DFT calculations and ASE-based workflows where a standardized reference lattice (WIEN2K volume) is required.", "tools": [{"function": {"description": "Create and return an ASE Atoms object for a specified ACWF structure scaled to the WIEN2K reference volume.\n\nThis function gpaw.utilities.acwf.reference_structure selects an ACWF structure template from the module-level acwf_structures mapping using the provided name, makes a copy of that template to avoid mutating the global template, and scales the unit cell and atomic positions so that the resulting cell volume matches the WIEN2K reference volume for the given chemical element symbol as stored in the module-level volumes mapping. For two oxygen-containing ACWF names ('X4O10' and 'XO') a cube-root-of-two correction is applied to the linear scaling factor to account for a difference in formula-unit convention in the stored WIEN2K volumes. After scaling, any placeholder atomic numbers equal to 0 in the structure template are replaced with the atomic number for the provided element symbol using the module-level atomic_numbers mapping. The returned Atoms object is suitable for use in GPAW DFT calculations and ASE-based workflows where a standardized reference lattice (WIEN2K volume) is required.", "name": "gpaw_utilities_acwf_reference_structure", "parameters": {"properties": {"symbol": {"type": "string", "description": "Chemical element symbol used to select the WIEN2K reference volume and the atomic number. The function looks up volumes[symbol][name] to obtain the target volume for the combination of element and ACWF structure. This parameter determines which element-specific volume is applied and which atomic number replaces any zeros in the template. Passing a symbol not present in the module-level volumes or atomic_numbers mappings will raise a KeyError.", "default": ""}, "name": {"type": "string", "description": "Key identifying which ACWF structure template to use from the module-level acwf_structures mapping. The function uses acwf_structures[name].copy() to obtain a working ASE Atoms template. The name selects which structural prototype (for example, 'X4O10' or 'XO') will be scaled to the WIEN2K volume. Passing a name not present in acwf_structures or not present under volumes[symbol] will raise a KeyError.", "default": ""}}, "required": ["symbol", "name"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a WIEN2K-volume-calibrated ACWF reference set from a messy intake sheet where each candidate record is a (prototype_name, placeholder_element) pair collected from multiple sources. Intake records: [('X4O10','Ti'), ('XO','Hf'), ('X4O10','O'), ('XO','He'), ('X2','Ti'), ('XO','Hf'), ('X4O10','Ti')]. Build standardized ASE Atoms starting lattices only for records where (a) the prototype is oxygen-containing (i.e., name includes 'O') and (b) the placeholder element is a heavy transition metal (atomic number >= 40). For duplicates (same prototype+element appearing multiple times), keep only one instance. For each retained record, instantiate the ACWF structure and scale to the WIEN2K reference volume for that element, applying the oxygen-prototype formula-unit convention correction where appropriate, and replace any Z=0 placeholder sites with the element.", "answers": "[{\"name\":\"gpaw_utilities_acwf_reference_structure\",\"arguments\":{\"symbol\":\"Ti\",\"name\":\"X4O10\"}},{\"name\":\"gpaw_utilities_acwf_reference_structure\",\"arguments\":{\"symbol\":\"Hf\",\"name\":\"XO\"}}]"}
{"func_name": "gpaw_utilities_dos_fold", "func_desc": "gpaw.utilities.dos.fold: Compute a broadened density-of-states (DOS) by placing a weighted delta kernel at each supplied energy and summing them on a uniform energy grid.\n    \n    This function is used in the GPAW DFT post-processing workflow to convert a discrete set of eigenvalues (energies) and their spectral weights into a continuous DOS by folding each discrete level with a finite-width kernel. The output energy axis and DOS are suitable for plotting, further analysis, or comparison with experiments. The routine determines the energy window automatically from the supplied energies and the broadening width, constructs an evenly spaced grid with npts points, and calls the module-local delta(e, e0, width, mode=mode) routine to generate the broadened contribution of each level.", "tools": [{"function": {"description": "gpaw.utilities.dos.fold: Compute a broadened density-of-states (DOS) by placing a weighted delta kernel at each supplied energy and summing them on a uniform energy grid.\n\nThis function is used in the GPAW DFT post-processing workflow to convert a discrete set of eigenvalues (energies) and their spectral weights into a continuous DOS by folding each discrete level with a finite-width kernel. The output energy axis and DOS are suitable for plotting, further analysis, or comparison with experiments. The routine determines the energy window automatically from the supplied energies and the broadening width, constructs an evenly spaced grid with npts points, and calls the module-local delta(e, e0, width, mode=mode) routine to generate the broadened contribution of each level.", "name": "gpaw_utilities_dos_fold", "parameters": {"properties": {"energies": {"type": "array", "items": {"type": "float"}, "description": "Iterable of numeric eigenvalues (discrete energies) representing e.g. Kohn–Sham eigenvalues from a GPAW calculation. These values define the centers of the delta kernels. Units must be consistent with width (typically the same energy units used elsewhere in the GPAW calculation, e.g. eV). The minimum and maximum of this list determine the default energy window via min(energies) - 5*width and max(energies) + 5*width.", "default": ""}, "weights": {"type": "array", "items": {"type": "float"}, "description": "Iterable of numeric weights corresponding to each entry in energies. Each weight multiplies the kernel for the matching energy, so weights encode spectral intensity (for example occupation numbers or projection weights). If len(weights) != len(energies), Python's zip is used: extra elements in the longer iterable are ignored, which may lead to silently truncated input — ensure the two lists match in length.", "default": ""}, "npts": {"type": "integer", "description": "Number of points in the output energy grid. This integer determines the resolution of the returned energy axis and the length of the DOS array. npts must be a positive integer; npts <= 0 or a non-integer may result in an empty array or an error from numpy.linspace.", "default": ""}, "width": {"type": "float", "description": "Broadening parameter passed to the delta kernel that controls the width of each broadened peak. width should be a positive float measured in the same energy units as energies. width <= 0 is not meaningful for broadening and may produce incorrect or singular behavior depending on the delta implementation.", "default": ""}, "mode": {"type": "string", "description": "String selecting the shape of the broadening kernel passed to the internal delta(e, e0, width, mode=mode) function. The default value is \"Gauss\", which requests Gaussian broadening. Other accepted strings depend on the implementation of delta in gpaw.utilities.dos; unknown mode values may cause that function to raise an error. Default: \"Gauss\".", "default": "Gauss"}}, "required": ["energies", "weights", "npts", "width", "mode"], "type": "any"}}, "type": "function"}], "query": "We’re validating a GPAW DOS folding step against two electronic-structure cohorts that have different numerical hygiene requirements.\n\nCohort A (small-molecule reference) contains a few intentionally masked eigenlevels (weights exactly 0.0) and should be folded only over levels that carry nonzero spectral weight (treat exact zeros as masked/out-of-manifold). Use Gaussian folding with width = 0.15 eV and build an automatically determined energy window from the retained energies and the broadening; generate a uniform grid with npts = 1200.\n\nCohort B (production run) includes fractional occupations and should be folded over all provided levels, but with an adaptive broadening rule based on occupation: if a level has weight < 1.0 use width = 0.25 eV (to emulate smearing-dominated near-EF features); otherwise use width = 0.15 eV. Use Gaussian mode and npts = 1200 for every fold call.\n\nData:\n- Cohort A energies (eV): [-8.42, -7.95, -7.10, -5.66, -1.23, 0.48, 1.05, 2.67]\n  Cohort A weights:        [ 2.0,   2.0,   2.0,   2.0,   2.0,  0.5,  0.0,  0.0]\n- Cohort B energies (eV): [-6.2, -5.1, -3.8, -1.4, -0.2, 0.6, 1.9, 3.3]\n  Cohort B weights:       [ 2.0,  2.0,  1.5,  1.0,  0.5, 0.8, 1.2, 0.7]\n\nReturn the folded DOS outputs needed for downstream plotting/comparison (Cohort A as a single DOS; Cohort B as the sum of the two occupation-dependent DOS contributions on their respective auto-windows).", "answers": "[{\"name\":\"gpaw_utilities_dos_fold\",\"arguments\":{\"energies\":[-8.42,-7.95,-7.1,-5.66,-1.23,0.48],\"weights\":[2.0,2.0,2.0,2.0,2.0,0.5],\"npts\":1200,\"width\":0.15,\"mode\":\"Gauss\"}},{\"name\":\"gpaw_utilities_dos_fold\",\"arguments\":{\"energies\":[-6.2,-5.1,-3.8,-1.4,1.9],\"weights\":[2.0,2.0,1.5,1.0,1.2],\"npts\":1200,\"width\":0.15,\"mode\":\"Gauss\"}},{\"name\":\"gpaw_utilities_dos_fold\",\"arguments\":{\"energies\":[-0.2,0.6,3.3],\"weights\":[0.5,0.8,0.7],\"npts\":1200,\"width\":0.25,\"mode\":\"Gauss\"}}]"}
{"func_name": "gpaw_utilities_tools_cutoff2gridspacing", "func_desc": "gpaw.utilities.tools.cutoff2gridspacing converts a plane-wave kinetic energy cutoff to the equivalent real-space uniform grid spacing used in GPAW real-space calculations.", "tools": [{"function": {"description": "gpaw.utilities.tools.cutoff2gridspacing converts a plane-wave kinetic energy cutoff to the equivalent real-space uniform grid spacing used in GPAW real-space calculations.\n", "name": "gpaw_utilities_tools_cutoff2gridspacing", "parameters": {"properties": {"E": {"type": "float", "description": "Plane-wave kinetic energy cutoff expressed in electronvolts (eV). In GPAW and ASE units, energies are given in eV (for example, PW(300) corresponds to E = 300.0). This scalar controls the maximum plane-wave kinetic energy included in a plane-wave expansion and therefore determines the shortest wavelength (largest reciprocal-space vector) that must be resolved on the real-space grid.", "default": ""}}, "required": ["E"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing a mixed DFT benchmark set where each structure came with a nominal plane-wave cutoff (in eV) and a provenance tag. Use GPAW’s cutoff→grid mapping only for entries that look like physically meaningful plane-wave settings (finite, positive, and within a typical production window of 200–800 eV). For the accepted entries, compute the real-space uniform grid spacing for each replicate as-is (don’t average replicates). Raw batch:\n\n- Si_bulk_rep1: Ecut = 450 eV, provenance = \"pw_ref\"\n- Si_bulk_rep2: Ecut = 450 eV, provenance = \"pw_ref\"\n- O2_ads_rep1: Ecut = 900 eV, provenance = \"stress_test\"\n- Cu_slab_rep1: Ecut = -50 eV, provenance = \"parse_error\"\n- ZnO_rep1: Ecut = NaN, provenance = \"missing_value\"\n\nReturn the grid spacing conversions for the entries that pass the acceptance rule.", "answers": "[{\"name\":\"gpaw_utilities_tools_cutoff2gridspacing\",\"arguments\":{\"E\":450.0}},{\"name\":\"gpaw_utilities_tools_cutoff2gridspacing\",\"arguments\":{\"E\":450.0}}]"}
{"func_name": "gpaw_utilities_tools_lowdin", "func_desc": "gpaw.utilities.tools.lowdin orthonormalizes the columns of a coefficient matrix U using the Löwdin (symmetric) orthogonalization procedure. In the GPAW density-functional-theory (DFT) context this is used to convert a set of non-orthogonal basis-function coefficient vectors or molecular-orbital coefficient columns into a symmetric orthonormal set suitable for subsequent linear-algebra operations (for example, when working with PAW projector/basis representations or overlap-corrected coefficient sets).\n    \n    If an overlap matrix S is provided it is used directly; otherwise the overlap is formed from U as S = dagger(U) @ U, where dagger denotes the conjugate-transpose used elsewhere in GPAW. The implementation computes the Hermitian eigen-decomposition of S, forms the symmetric inverse square root of S from the eigenvectors and eigenvalues, and left-multiplies U by that inverse square root so that the columns of U become orthonormal with respect to the standard inner product.", "tools": [{"function": {"description": "gpaw.utilities.tools.lowdin orthonormalizes the columns of a coefficient matrix U using the Löwdin (symmetric) orthogonalization procedure. In the GPAW density-functional-theory (DFT) context this is used to convert a set of non-orthogonal basis-function coefficient vectors or molecular-orbital coefficient columns into a symmetric orthonormal set suitable for subsequent linear-algebra operations (for example, when working with PAW projector/basis representations or overlap-corrected coefficient sets).\n\nIf an overlap matrix S is provided it is used directly; otherwise the overlap is formed from U as S = dagger(U) @ U, where dagger denotes the conjugate-transpose used elsewhere in GPAW. The implementation computes the Hermitian eigen-decomposition of S, forms the symmetric inverse square root of S from the eigenvectors and eigenvalues, and left-multiplies U by that inverse square root so that the columns of U become orthonormal with respect to the standard inner product.", "name": "gpaw_utilities_tools_lowdin", "parameters": {"properties": {"U": {"type": "array", "items": {"type": "float"}, "description": "2-D array whose columns represent the vectors to be orthonormalized. This array is modified in-place: after the call the columns of U are Löwdin-orthonormalized. In GPAW workflows U typically contains basis-function coefficients or orbital expansion coefficients; the function operates directly on these coefficient matrices so no copy is returned.", "default": ""}, "S": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Overlap matrix corresponding to the columns of U. If provided, S is used as the Hermitian overlap matrix in the Löwdin procedure. Default: None, in which case S is computed internally as dagger(U) @ U (the conjugate-transpose of U multiplied by U). S must be a Hermitian (symmetric) matrix consistent with U for correct results.", "default": null}}, "required": ["U", "S"], "type": "any"}}, "type": "function"}], "query": "I’m consolidating PAW-projected MO coefficient cohorts coming from three independent SCF replicas before a subspace-diagonalization step. Each replica provides a coefficient matrix U (basis × orbitals) and a Hermitian overlap matrix S from the PAW overlap operator. Because some replicas contain near-linear dependencies in the overlap metric, apply Löwdin (symmetric) orthonormalization only to those replicas whose provided S is numerically well-conditioned in the sense that all of its eigenvalues are strictly positive and the smallest eigenvalue is at least 0.95. Always use the explicitly provided S for the orthonormalization (do not re-form S from U). The raw replicas are:\n\nReplica A: U = [[1.0, 0.2, 0.0], [0.0, 0.9, 0.1], [0.1, 0.0, 0.8]] with S = [[1.05, 0.1, 0.0], [0.1, 1.02, 0.05], [0.0, 0.05, 0.98]]\n\nReplica B: U = [[1.0, 0.2, 0.0], [0.0, 1.0, 0.3], [0.1, 0.0, 1.0]] with S = [[1.02, 0.1, 0.0], [0.1, 1.05, 0.08], [0.0, 0.08, 1.01]]\n\nReplica C (reduced test subspace): U = [[1.0, 0.5], [0.5, 1.0], [0.2, 0.3]] with S = [[1.1, 0.2], [0.2, 0.9]]", "answers": "[{\"name\":\"gpaw_utilities_tools_lowdin\",\"arguments\":{\"U\":[[1.0,0.2,0.0],[0.0,0.9,0.1],[0.1,0.0,0.8]],\"S\":[[1.05,0.1,0.0],[0.1,1.02,0.05],[0.0,0.05,0.98]]}},{\"name\":\"gpaw_utilities_tools_lowdin\",\"arguments\":{\"U\":[[1.0,0.2,0.0],[0.0,1.0,0.3],[0.1,0.0,1.0]],\"S\":[[1.02,0.1,0.0],[0.1,1.05,0.08],[0.0,0.08,1.01]]}}]"}
{"func_name": "gpaw_utilities_tools_split_formula", "func_desc": "gpaw.utilities.tools.split_formula: Expand a chemical formula string into a flat list of element symbols by repeating each element according to its numeric count. This function is used in GPAW utilities to translate simple chemical formulas (used e.g. in tests, quick input parsing, or stoichiometry checks) into an explicit list of atom symbols that can be passed to ASE/GPAW routines that expect per-atom entries.", "tools": [{"function": {"description": "gpaw.utilities.tools.split_formula: Expand a chemical formula string into a flat list of element symbols by repeating each element according to its numeric count. This function is used in GPAW utilities to translate simple chemical formulas (used e.g. in tests, quick input parsing, or stoichiometry checks) into an explicit list of atom symbols that can be passed to ASE/GPAW routines that expect per-atom entries.\n", "name": "gpaw_utilities_tools_split_formula", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula to parse, expressed as a contiguous string of element symbols and counts. Element symbols must begin with an uppercase ASCII letter and may include one or more lowercase ASCII letters immediately following (e.g. 'C', 'Mg', 'He'). Counts are represented by non-letter characters encountered after an element symbol; the character is evaluated with Python's eval() and interpreted as the total multiplicity for the immediately preceding element. Because the implementation appends the element once when its symbol is first parsed, a numeric count n will result in n total occurrences of that element (the code performs an extend with (eval(c) - 1) additional copies). The function processes the input left-to-right without permitting separators or whitespace; any characters that are not ASCII letters are passed to eval() and therefore must be valid Python expressions that evaluate to integers. Example valid input: 'C2H3Mg' -> expands to two 'C', three 'H', and one 'Mg'.", "default": ""}}, "required": ["formula"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a small batch of hand-entered precursor formulas from a liquid-handling log to generate explicit per-atom symbol lists for ASE/GPAW sanity checks. The log includes neutral molecules, adduct-annotated entries, and malformed tokens. For each entry, first apply a strict sieve: only process formulas that are a single neutral empirical formula consisting exclusively of valid element symbols with optional integer stoichiometric counts (no parentheses, charges, dots/hydrates, isotopic labels, whitespace, or adduct annotations). For the retained formulas, expand them into flat per-atom element-symbol lists.\n\nRaw log entries:\n['C2H6O', ' C2H6O ', 'C2H5OH', 'C2H6O+', 'C2H6O·H2O', '13C2H6O', 'NaCl', 'Fe2(SO4)3', 'H2O', 'Xy2']", "answers": "[{\"name\":\"gpaw_utilities_tools_split_formula\",\"arguments\":{\"formula\":\"C2H6O\"}},{\"name\":\"gpaw_utilities_tools_split_formula\",\"arguments\":{\"formula\":\"C2H5OH\"}},{\"name\":\"gpaw_utilities_tools_split_formula\",\"arguments\":{\"formula\":\"NaCl\"}},{\"name\":\"gpaw_utilities_tools_split_formula\",\"arguments\":{\"formula\":\"H2O\"}}]"}
{"func_name": "gpaw_utilities_unpack_density", "func_desc": "Unpack a packed density representation into a full 2D Hermitian density matrix.\n    \n    This function is used in GPAW to reconstruct density matrices (Hermitian arrays used in density-functional theory calculations) from a compact packed representation produced by the corresponding pack_density routine. The packed representation is typically used for compact storage or communication of the independent elements of a Hermitian density matrix. unpack_density reverses that packing by first calling unpack_hermitian on the packed data and then applying a fixed rescaling: all matrix elements are multiplied by 0.5 and the diagonal elements are multiplied by 2 to restore the original diagonal magnitudes. For batch inputs (2D arrays), each row is treated as a separate packed vector and unpacked independently into a full Hermitian matrix; the result is a stacked array of unpacked matrices.", "tools": [{"function": {"description": "Unpack a packed density representation into a full 2D Hermitian density matrix.\n\nThis function is used in GPAW to reconstruct density matrices (Hermitian arrays used in density-functional theory calculations) from a compact packed representation produced by the corresponding pack_density routine. The packed representation is typically used for compact storage or communication of the independent elements of a Hermitian density matrix. unpack_density reverses that packing by first calling unpack_hermitian on the packed data and then applying a fixed rescaling: all matrix elements are multiplied by 0.5 and the diagonal elements are multiplied by 2 to restore the original diagonal magnitudes. For batch inputs (2D arrays), each row is treated as a separate packed vector and unpacked independently into a full Hermitian matrix; the result is a stacked array of unpacked matrices.", "name": "gpaw_utilities_unpack_density", "parameters": {"properties": {"M": {"type": "array", "items": {"type": "float"}, "description": "Input packed density data. If M is a 1D numpy.ndarray it is interpreted as a single packed representation of a Hermitian density matrix produced by pack_density and the function returns a single 2D numpy.ndarray containing the unpacked Hermitian matrix. If M is a 2D numpy.ndarray it is interpreted as a collection of packed vectors (one per row); each row is unpacked and the function returns a numpy.ndarray stacking the resulting 2D Hermitian matrices along a new first axis. The function expects that the input follows the packing convention used by pack_density; passing arrays that do not follow that convention will produce incorrect results. The exact length/shape requirements of each packed vector are determined by unpack_hermitian and must match what pack_density produces.", "default": ""}}, "required": ["M"], "type": "any"}}, "type": "function"}], "query": "We are aggregating packed density outputs from a GPAW restart campaign where the storage layer sometimes emits truncated payloads. Each record is supposed to encode a 3×3 Hermitian density (so the packed vector length must correspond to n(n+1)/2 with n=3). From the following raw capture, automatically reconstruct full 3×3 Hermitian density matrices only for records that are self-consistent with that packing length, applying the standard GPAW unpack_density rescaling so diagonals recover their original magnitudes. Any record whose packed length is inconsistent with a 3×3 Hermitian packing should be ignored as corrupted. Raw records: (i) Replicate A: [1.0, 0.2, -0.1, 0.5, 0.3, -0.4]; (ii) Replicate B: [1.6, 0.2, -0.1, 0.9, 0.05, 0.4]; (iii) Restart fragment: [0.7, -0.2, 0.1, 0.0, 0.2] (captured mid-write).", "answers": "[{\"name\":\"gpaw_utilities_unpack_density\",\"arguments\":{\"M\":[1.0,0.2,-0.1,0.5,0.3,-0.4]}},{\"name\":\"gpaw_utilities_unpack_density\",\"arguments\":{\"M\":[1.6,0.2,-0.1,0.9,0.05,0.4]}}]"}
{"func_name": "gpaw_xc_sic_matrix_exponential", "func_desc": "gpaw.xc.sic.matrix_exponential computes the matrix exponential U = exp(i * dlt * G)\n    for an anti-Hermitian generator G used in GPAW's self-interaction-correction (sic)\n    code path. In the GPAW DFT/PAW context this routine is used to construct a unitary\n    rotation operator from an anti-Hermitian generator matrix G_nn (for example to\n    rotate orbitals or apply small unitary updates in SIC procedures). The routine\n    diagonalizes a Hermitian matrix derived from G_nn, forms phase factors exp(i*dlt*w)\n    from the real eigenvalues w, and reconstructs the exponential by back-transforming\n    the diagonal phase matrix with the eigenvectors.", "tools": [{"function": {"description": "gpaw.xc.sic.matrix_exponential computes the matrix exponential U = exp(i * dlt * G)\nfor an anti-Hermitian generator G used in GPAW's self-interaction-correction (sic)\ncode path. In the GPAW DFT/PAW context this routine is used to construct a unitary\nrotation operator from an anti-Hermitian generator matrix G_nn (for example to\nrotate orbitals or apply small unitary updates in SIC procedures). The routine\ndiagonalizes a Hermitian matrix derived from G_nn, forms phase factors exp(i*dlt*w)\nfrom the real eigenvalues w, and reconstructs the exponential by back-transforming\nthe diagonal phase matrix with the eigenvectors.", "name": "gpaw_xc_sic_matrix_exponential", "parameters": {"properties": {"G_nn": {"type": "array", "items": {"type": "float"}, "description": "Anti-Hermitian (skew-Hermitian) matrix representing the\ngenerator G in the expression U = exp(i * dlt * G). The function expects\na two-dimensional square array with shape (n, n) where n = G_nn.shape[1].\nThe implementation treats complex and real dtypes differently: when\nG_nn.dtype == complex the code uses both real and imaginary parts to form\na Hermitian matrix for diagonalization; when G_nn.dtype is not complex the\ncode uses only the real part. For mathematically correct/unitary results,\nG_nn should satisfy G_nn.conj().T == -G_nn (anti-Hermitian). If G_nn is not\nsquare, contains non-finite values (NaN or Inf), or strongly violates the\nanti-Hermitian property the numerical result may be invalid or the linear\nalgebra routines may raise errors (for example numpy.linalg.LinAlgError).", "default": ""}, "dlt": {"type": "float", "description": "Scalar scaling factor that multiplies the generator inside the\nexponential. Physically this can represent a small rotation angle or a\ntimestep-like parameter controlling the magnitude of the unitary update.\nThe value is used exactly as provided in exp(i * dlt * w) where w are the\nreal eigenvalues obtained from diagonalizing the Hermitian matrix derived\nfrom G_nn. Very large magnitudes of dlt can produce rapidly oscillatory\nphases and increase numerical sensitivity.", "default": ""}}, "required": ["G_nn", "dlt"], "type": "any"}}, "type": "function"}], "query": "In a SIC orbital-rotation micro-kernel test, we have a grab-bag of candidate 2×2 generators from a noisy localization step, each intended to represent purely off-diagonal mixing between two orbitals. Use gpaw.xc.sic.matrix_exponential to build the unitary update U = exp(i*dlt*G) only for generators that are strictly anti-Hermitian with zero diagonal (i.e., G_00 = G_11 = 0 and G_01 = -G_10 with real entries). For the step size, use an adaptive rule tied to the coupling magnitude: set dlt = 0.5 when |G_01| ≥ 0.2, otherwise use dlt = 0.05. Candidate generators: (1) [[0.0, -0.2], [0.2, 0.0]] (2) [[0.0, 0.2], [-0.2, 0.0]] (3) [[1e-3, -0.2], [0.2, 0.0]] (4) [[0.0, -0.15], [0.15, 0.0]] (5) [[0.0, -0.2], [-0.2, 0.0]]. Compute U for every candidate that passes the anti-Hermiticity/zero-diagonal sieve using the dlt rule above.", "answers": "[{\"name\":\"gpaw_xc_sic_matrix_exponential\",\"arguments\":{\"G_nn\":[[0.0,-0.2],[0.2,0.0]],\"dlt\":0.5}},{\"name\":\"gpaw_xc_sic_matrix_exponential\",\"arguments\":{\"G_nn\":[[0.0,0.2],[-0.2,0.0]],\"dlt\":0.5}},{\"name\":\"gpaw_xc_sic_matrix_exponential\",\"arguments\":{\"G_nn\":[[0.0,-0.15],[0.15,0.0]],\"dlt\":0.05}}]"}
{"func_name": "gpaw_xc_sic_ortho", "func_desc": "gpaw.xc.sic.ortho: Orthogonalize column vectors of a matrix using the symmetric Löwdin procedure.\n    \n    Performs Symmetric-Löwdin orthogonalization of the column vectors contained in a square numpy.ndarray. This function is intended for use within GPAW's self-interaction-correction (gpaw.xc.sic) code paths where a set of (typically localized) orbital-like column vectors must be transformed into an orthonormal set before further processing. The routine computes the Hermitian overlap matrix O = W_nn @ W_nn.T.conj() and either applies a second-order perturbative approximation of O^{-1/2} when O is already close to the identity (faster, avoids diagonalization), or computes the exact inverse square root via diagonalization of O (stable, more expensive). The returned matrix has the same shape as W_nn and its columns are orthonormal within numerical tolerance: the overlap of the returned columns with their Hermitian conjugates is (approximately) the identity.", "tools": [{"function": {"description": "gpaw.xc.sic.ortho: Orthogonalize column vectors of a matrix using the symmetric Löwdin procedure.\n\nPerforms Symmetric-Löwdin orthogonalization of the column vectors contained in a square numpy.ndarray. This function is intended for use within GPAW's self-interaction-correction (gpaw.xc.sic) code paths where a set of (typically localized) orbital-like column vectors must be transformed into an orthonormal set before further processing. The routine computes the Hermitian overlap matrix O = W_nn @ W_nn.T.conj() and either applies a second-order perturbative approximation of O^{-1/2} when O is already close to the identity (faster, avoids diagonalization), or computes the exact inverse square root via diagonalization of O (stable, more expensive). The returned matrix has the same shape as W_nn and its columns are orthonormal within numerical tolerance: the overlap of the returned columns with their Hermitian conjugates is (approximately) the identity.", "name": "gpaw_xc_sic_ortho", "parameters": {"properties": {"W_nn": {"type": "array", "items": {"type": "float"}, "description": "Square input array whose column vectors are to be orthogonalized. In the source code this array is treated as having ndim = W_nn.shape[1] and the Hermitian overlap is computed as O_nn = W_nn @ W_nn.T.conj(), so W_nn must be laid out such that this product yields an (ndim x ndim) Hermitian matrix. The practical significance in GPAW is that W_nn typically contains orbital or projector coefficients that must be orthonormalized for stable self-interaction-correction or related operations.", "default": ""}, "maxerr": {"type": "float", "description": "Threshold for choosing the perturbative (approximate) orthogonalization instead of explicit diagonalization of the overlap. If the L1 deviation from identity of the overlap matrix, computed as sum(abs(O_nn - I)), is strictly less than maxerr, the routine uses the perturbative Symmetric-Löwdin approximation X = 1.5*I - 0.5*O_nn to obtain an approximate O^{-1/2} and avoid diagonalization. If the deviation is greater than or equal to maxerr, the routine diagonalizes O_nn with a Hermitian eigensolver (eigh), forms nsqrt_n = diag(1/sqrt(eigenvalues)), and constructs X = U @ nsqrt_n @ U.T.conj() to obtain the exact inverse square root. The default value 1e-10 is chosen to balance performance and numerical stability in typical GPAW calculations; set this parameter smaller to force diagonalization more often, or larger to favor the perturbative shortcut.", "default": 1e-10}}, "required": ["W_nn", "maxerr"], "type": "any"}}, "type": "function"}], "query": "We’re validating a SIC localized-orbital preprocessing stage under realistic numerical drift. A small batch of candidate coefficient matrices (each square, columns are orbital-like vectors) comes from different SCF snapshots. Before downstream SIC steps, Symmetric-Löwdin orthogonalize only those replicates whose column-set overlap is sufficiently well-conditioned: include a replicate if its overlap matrix O = W @ W^\\u2020 has all diagonal elements within 2% of 1.0 and its largest absolute off-diagonal element is \\u2264 0.30 (others are treated as contaminated by mixing and are not forwarded). For accepted replicates, set the shortcut tolerance maxerr adaptively from the matrix itself: use maxerr = 1e-5 when the largest absolute off-diagonal element of W is < 0.05, otherwise use maxerr = 1e-8. Use this on the following raw SIC-candidate matrices:\n\nA: [[1.0, 0.2, -0.1], [0.2, 0.9, 0.3], [-0.1, 0.3, 0.8]]\nB: [[1.0, 0.01, 0.0, 0.0], [0.01, 1.0, 0.02, 0.0], [0.0, 0.02, 1.0, 0.03], [0.0, 0.0, 0.03, 1.0]]\nC: [[1.0, 0.6, 0.0], [0.6, 1.0, 0.0], [0.0, 0.0, 1.0]]\n\nReturn the orthonormalized coefficient matrices for the accepted replicates, preserving input order among those that pass.", "answers": "[{\"name\":\"gpaw_xc_sic_ortho\",\"arguments\":{\"W_nn\":[[1.0,0.2,-0.1],[0.2,0.9,0.3],[-0.1,0.3,0.8]],\"maxerr\":1e-08}},{\"name\":\"gpaw_xc_sic_ortho\",\"arguments\":{\"W_nn\":[[1.0,0.01,0.0,0.0],[0.01,1.0,0.02,0.0],[0.0,0.02,1.0,0.03],[0.0,0.0,0.03,1.0]],\"maxerr\":1e-05}}]"}
{"func_name": "gpaw_xc_xc_string_to_dict", "func_desc": "Parse an exchange–correlation (XC) specification string used by GPAW into a dictionary.\n    \n    This function is used within the GPAW DFT codebase to interpret the xc argument (exchange–correlation functional specification) supplied when configuring a GPAW calculator (for example, the common usage xc='PBE' or xc='B3LYP:alpha=0.2'). It converts a compact, colon-separated specification of the form 'name:key1=value1:key2=value2:...' into a Python dictionary that downstream GPAW code (and LibXC/backends) can consume to select the functional and its numeric parameters.", "tools": [{"function": {"description": "Parse an exchange–correlation (XC) specification string used by GPAW into a dictionary.\n\nThis function is used within the GPAW DFT codebase to interpret the xc argument (exchange–correlation functional specification) supplied when configuring a GPAW calculator (for example, the common usage xc='PBE' or xc='B3LYP:alpha=0.2'). It converts a compact, colon-separated specification of the form 'name:key1=value1:key2=value2:...' into a Python dictionary that downstream GPAW code (and LibXC/backends) can consume to select the functional and its numeric parameters.", "name": "gpaw_xc_xc_string_to_dict", "parameters": {"properties": {"string": {"type": "string", "description": "The XC specification string. The first token (substring before the first ':') is interpreted as the functional name and stored under the 'name' key in the returned dictionary. Subsequent tokens must have the form 'key=value' (exactly one '=' per token). Values are interpreted as integers if int(...) succeeds, otherwise as floats if float(...) succeeds, and otherwise left as the original string. Note that this function does not strip whitespace from tokens or validate key names; any leading or trailing whitespace in tokens will be preserved. Example valid inputs include 'PBE', 'PBE:beta=1', or 'MYFUNC:mix=0.25:order=2'. An empty input string yields {'name': ''}. Tokens that do not follow the 'key=value' pattern (including empty tokens produced by consecutive or trailing colons) will cause a ValueError due to the required split/unpack step.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "We’re triaging XC strings from a mixed GPAW campaign log before launching a hybrid-functional benchmark. The raw tokens below include true hybrid candidates, semilocal controls, and a few malformed/operator-typo artifacts:\n\nXC tokens (in arrival order):\n1) \"B3LYP:alpha=0.20:omega=0.15:backend=libxc\"\n2) \"PBE\"\n3) \"B3LYP:alpha=0.2:omega=0.3:version=rev06\"\n4) \"B3LYP:alpha=0.25\" \n5) \"B3LYP::alpha=0.2\"\n6) \"B3LYP:alpha=twenty:omega=0.1\"\n7) \"B3LYP:alpha=0.00:omega=0.0\"\n\nFor the hybrid cross-backend consistency check, parse only the XC specifications that (a) declare the B3LYP functional, (b) provide both numeric alpha and omega parameters, and (c) have alpha strictly between 0 and 1 with omega > 0. Return the parsed dictionaries for the qualifying specifications in the same order they appear in the list.", "answers": "[{\"name\":\"gpaw_xc_xc_string_to_dict\",\"arguments\":{\"string\":\"B3LYP:alpha=0.20:omega=0.15:backend=libxc\"}},{\"name\":\"gpaw_xc_xc_string_to_dict\",\"arguments\":{\"string\":\"B3LYP:alpha=0.2:omega=0.3:version=rev06\"}}]"}
{"func_name": "guacamol_utils_data_remove_duplicates", "func_desc": "Removes duplicate elements from a list while preserving the original ordering of the first occurrences.\n    \n    This function is used in the GuacaMol data-processing pipeline (for example when preparing standardized SMILES training/validation/test sets) to ensure that each molecule or entry appears only once while keeping the original ordering used in the source file. For duplicates, the first occurrence is kept and any later occurrences are ignored. The operation is non-destructive with respect to the input: the input list is not modified and a new list is returned. The function relies on a Python set for membership checks, so elements are expected to be hashable (see Failure modes below). The implementation provides O(n) average-time complexity and O(n) additional memory where n is the length of the input list.", "tools": [{"function": {"description": "Removes duplicate elements from a list while preserving the original ordering of the first occurrences.\n\nThis function is used in the GuacaMol data-processing pipeline (for example when preparing standardized SMILES training/validation/test sets) to ensure that each molecule or entry appears only once while keeping the original ordering used in the source file. For duplicates, the first occurrence is kept and any later occurrences are ignored. The operation is non-destructive with respect to the input: the input list is not modified and a new list is returned. The function relies on a Python set for membership checks, so elements are expected to be hashable (see Failure modes below). The implementation provides O(n) average-time complexity and O(n) additional memory where n is the length of the input list.", "name": "guacamol_utils_data_remove_duplicates", "parameters": {"properties": {"list_with_duplicates": {"type": "array", "items": {"type": "float"}, "description": "A list that possibly contains duplicate entries. In the GuacaMol context this is typically a list of molecular identifiers such as SMILES strings read from a dataset file. Each element in the list is treated as a distinct item according to Python equality semantics; the first element that compares equal to a later one is the one that is preserved. Elements are expected to be hashable (for example, strings, integers, tuples). If elements are unhashable (for example, lists or dicts), the function will raise a TypeError when attempting to add them to the internal set.", "default": ""}}, "required": ["list_with_duplicates"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a consolidated GuacaMol SMILES corpus from three upstream exports, but only molecules that look like plausible organic drug-like candidates should enter the deduplication stage. For each cohort below, first apply a pre-sieve that keeps only SMILES strings that contain at least one carbon atom (uppercase 'C' or aromatic 'c') and have no triple bonds (i.e., the SMILES must not contain '#', since those are flagged as non-target gases/linear artifacts in this run). After that pre-sieve, run the standard GuacaMol ordering-preserving deduplication on the remaining entries for each cohort, returning a clean list per cohort.\n\nCohort A (ChEMBL+ZINC overlap check): [\"CCO\", \"CCN\", \"CCO\", \"c1ccccc1\", \"CCN\", \"O=C=O\"]\nCohort B (pre train/valid cleanup): [\"CCO\",\"c1ccccc1\",\"CCO\",\"O=C=O\",\"c1ccccc1\",\"CCN\",\"O=C=O\"]\nCohort C (GuacaMol-style with N#N present): [\"CCO\",\"c1ccccc1\",\"CCO\",\"O=C=O\",\"c1ccccc1\",\"N#N\",\"CCN\",\"N#N\"]", "answers": "[{\"name\":\"guacamol_utils_data_remove_duplicates\",\"arguments\":{\"list_with_duplicates\":[\"CCO\",\"CCN\",\"CCO\",\"c1ccccc1\",\"CCN\"]}},{\"name\":\"guacamol_utils_data_remove_duplicates\",\"arguments\":{\"list_with_duplicates\":[\"CCO\",\"c1ccccc1\",\"CCO\",\"c1ccccc1\",\"CCN\"]}},{\"name\":\"guacamol_utils_data_remove_duplicates\",\"arguments\":{\"list_with_duplicates\":[\"CCO\",\"c1ccccc1\",\"CCO\",\"c1ccccc1\",\"CCN\"]}}]"}
{"func_name": "guacamol_utils_math_arithmetic_mean", "func_desc": "guacamol.utils.math.arithmetic_mean computes the arithmetic mean of a list of float values. This function is a small pure utility used in the GuacaMol benchmarking toolkit for de novo molecular design to aggregate numeric results such as per-molecule scores, per-run metrics, or other scalar quantities that must be averaged when evaluating distribution-learning or goal-directed generation methods.", "tools": [{"function": {"description": "guacamol.utils.math.arithmetic_mean computes the arithmetic mean of a list of float values. This function is a small pure utility used in the GuacaMol benchmarking toolkit for de novo molecular design to aggregate numeric results such as per-molecule scores, per-run metrics, or other scalar quantities that must be averaged when evaluating distribution-learning or goal-directed generation methods.\n", "name": "guacamol_utils_math_arithmetic_mean", "parameters": {"properties": {"values": {"type": "array", "items": {"type": "float"}, "description": "A list of floating-point values to average. Each element represents a scalar measurement used in GuacaMol benchmarks (for example, a model score, similarity metric, or other numeric per-molecule quantity). The function uses the implementation sum(values) / len(values), so values must be a non-empty list of floats as required by the calling benchmark code; there are no defaults or in-place side effects.", "default": ""}}, "required": ["values"], "type": "any"}}, "type": "function"}], "query": "In our GuacaMol benchmarking QA pass, we need cohort-level aggregation that is robust to obvious instrumentation artifacts and protocol-driven heterogeneity. We have three measurement streams (each intended to be 5 replicates) and we only want to average values that are physically plausible normalized scores in [0, 1]. Additionally, for the goal-directed batch, treat any near-perfect score (>=0.90) as a likely docking saturation artifact and exclude it from the cohort mean. \n\nStreams:\n1) Per-molecule validity readouts (5 replicates): [0.92, 0.87, 0.95, 0.90, 1.02]\n2) Goal-directed per-molecule scores (5 candidates): [0.71, 0.83, 0.65, 0.90, -0.10]\n3) Per-run validation scores from 5 independent runs: [0.71, 0.76, 0.69, 0.74, 0.78]\n\nCompute the arithmetic mean for each stream after applying the above rules, using guacamol.utils.math.arithmetic_mean on the retained values for each stream.", "answers": "[{\"name\":\"guacamol_utils_math_arithmetic_mean\",\"arguments\":{\"values\":[0.92,0.87,0.95,0.9]}},{\"name\":\"guacamol_utils_math_arithmetic_mean\",\"arguments\":{\"values\":[0.71,0.83,0.65]}},{\"name\":\"guacamol_utils_math_arithmetic_mean\",\"arguments\":{\"values\":[0.71,0.76,0.69,0.74,0.78]}}]"}
{"func_name": "guacamol_utils_math_geometric_mean", "func_desc": "Compute the geometric mean of a list of numeric values.\n    \n    This function is a small numeric utility used in the GuacaMol benchmarking codebase to aggregate multiplicative scores (for example, combining per-objective or per-component scores in goal-directed benchmark scoring). It converts the input sequence to a NumPy array, computes the product of all elements, and returns the n-th root of that product (product ** (1/len(values))). The operation is commutative with respect to the input order and has no side effects on external state.", "tools": [{"function": {"description": "Compute the geometric mean of a list of numeric values.\n\nThis function is a small numeric utility used in the GuacaMol benchmarking codebase to aggregate multiplicative scores (for example, combining per-objective or per-component scores in goal-directed benchmark scoring). It converts the input sequence to a NumPy array, computes the product of all elements, and returns the n-th root of that product (product ** (1/len(values))). The operation is commutative with respect to the input order and has no side effects on external state.", "name": "guacamol_utils_math_geometric_mean", "parameters": {"properties": {"values": {"type": "array", "items": {"type": "float"}, "description": "A Python list of floating-point numbers representing the values to be aggregated by geometric mean. In the GuacaMol context, these are typically scalar scores for a molecule or component scores that should be combined multiplicatively. Each element is treated as a factor in the product; the function performs no validation or clipping of elements before aggregation.", "default": ""}}, "required": ["values"], "type": "any"}}, "type": "function"}], "query": "We’re re-running the multiplicative aggregation step for a messy goal-directed benchmark export where each candidate is a replicate with multiple per-objective/component success scores in [0,1]. Before aggregation, apply the standard assay hygiene rule: only objectives that behave like proper probabilities are allowed into the multiplicative score (i.e., finite numeric values strictly greater than 0 and less than or equal to 1). For each candidate, drop any objective entries that fail that criterion (these correspond to censored/failed readouts), then compute the GuacaMol-style aggregated score as the geometric mean of the remaining objectives.\n\nProcess these three candidates:\n1) Candidate A raw objective list: [0.85, 0.9, 0.95, 1.0]\n2) Candidate B raw component list (QED, similarity, SA, activity, assay_artifact_flag): [0.82, 0.61, 0.74, 0.53, 0]\n3) Candidate C raw component list (novelty, QED, synthetic_accessibility_score, calibration_probe): [0.82, 0.71, 0.64, 1.2]\n\nReturn the geometric-mean aggregated score for each candidate after applying the filtering rule.", "answers": "[{\"name\":\"guacamol_utils_math_geometric_mean\",\"arguments\":{\"values\":[0.85,0.9,0.95,1.0]}},{\"name\":\"guacamol_utils_math_geometric_mean\",\"arguments\":{\"values\":[0.82,0.61,0.74,0.53]}},{\"name\":\"guacamol_utils_math_geometric_mean\",\"arguments\":{\"values\":[0.82,0.71,0.64]}}]"}
{"func_name": "lungmask_utils_bbox_3D", "func_desc": "Compute bounding box of a 3D labelmap used in lungmask for cropping and postprocessing of segmentation volumes.\n    \n    This function inspects a multi-dimensional numpy labelmap (typically a 3D segmentation volume produced or consumed by the lungmask package) and computes per-axis minimum and maximum indices that enclose all non-zero labels. The bounding box is expanded by the integer margin on each axis and clipped to the labelmap extents. In the lungmask workflow this is commonly used to crop volumes to the lung region before further processing (for example to reduce computation during model inference, to crop input for visualization, or to restrict fusion operations between models such as LTRCLobes and R231). For numpy arrays following the package convention, the first axis is slices (z), the second is chest-to-back (y), and the third is right-to-left (x), so for a 3D labelmap the returned array corresponds to [zmin, zmax, ymin, ymax, xmin, xmax]. The returned upper bounds are exclusive (suitable for Python slicing).", "tools": [{"function": {"description": "Compute bounding box of a 3D labelmap used in lungmask for cropping and postprocessing of segmentation volumes.\n\nThis function inspects a multi-dimensional numpy labelmap (typically a 3D segmentation volume produced or consumed by the lungmask package) and computes per-axis minimum and maximum indices that enclose all non-zero labels. The bounding box is expanded by the integer margin on each axis and clipped to the labelmap extents. In the lungmask workflow this is commonly used to crop volumes to the lung region before further processing (for example to reduce computation during model inference, to crop input for visualization, or to restrict fusion operations between models such as LTRCLobes and R231). For numpy arrays following the package convention, the first axis is slices (z), the second is chest-to-back (y), and the third is right-to-left (x), so for a 3D labelmap the returned array corresponds to [zmin, zmax, ymin, ymax, xmin, xmax]. The returned upper bounds are exclusive (suitable for Python slicing).", "name": "lungmask_utils_bbox_3D", "parameters": {"properties": {"labelmap": {"type": "array", "items": {"type": "float"}, "description": "Input labelmap. A numpy array containing integer or boolean labels where non-zero values indicate voxels of interest (e.g., lung labels produced by a segmentation model). The array may be 3D (typical) or n-D; the function computes bounds for each axis in order.", "default": ""}, "margin": {"type": "integer", "description": "Margin to add to the bounding box on each axis. This integer is broadcast to all axes (the code creates a per-axis list [margin] * number_of_axes). Positive values expand the box, negative values will shrink it. Defaults to 2.", "default": 2}}, "required": ["labelmap", "margin"], "type": "any"}}, "type": "function"}], "query": "We’re running a lungmask crop QA pass on a mixed bag of synthetic segmentation labelmaps (axis order z,y,x) that intentionally includes acquisition artifacts and postprocessing edge-cases. For each volume, first determine if it contains any foreground at all (any voxel > 0). Only for volumes with foreground, compute a NumPy-slice-ready bounding box [zmin, zmax, ymin, ymax, xmin, xmax] that encloses all foreground voxels, then expand by a safety margin that depends on the spatial extent of the foreground: use margin=3 if the foreground spans more than one z-slice (i.e., max_z_index > min_z_index), otherwise use margin=1. Clip bounds to the array extents; upper bounds must be exclusive.\n\nEvaluate this batch:\nA) Mostly-empty scout volume (should emulate a failed thresholding pass):\n[\n  [[0,0,0],[0,0,0],[0,0,0]],\n  [[0,0,0],[0,0,0],[0,0,0]]\n]\n\nB) Single-slice foreground (thin slab artifact):\n[\n  [[0,0,0,0],[0,0,0,0],[0,0,0,0]],\n  [[0,0,0,0],[0,2,2,0],[0,0,0,0]],\n  [[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n]\n\nC) Multi-slice, disconnected foreground regions (postprocessing stress-test):\n[\n  [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n  [[0,0,0,0,0],[0,1,1,0,0],[0,1,1,0,0],[0,0,0,0,0]],\n  [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,1,1,1]],\n  [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,1,1,1]]\n]\n\nReturn the bounding box for each processed volume (volumes without any foreground produce no bbox output in this QA stage).", "answers": "[{\"name\":\"lungmask_utils_bbox_3D\",\"arguments\":{\"labelmap\":[[[0,0,0,0],[0,0,0,0],[0,0,0,0]],[[0,0,0,0],[0,2,2,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0],[0,0,0,0]]],\"margin\":1}},{\"name\":\"lungmask_utils_bbox_3D\",\"arguments\":{\"labelmap\":[[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,1,1,0,0],[0,1,1,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,1,1,1]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,1,1,1]]],\"margin\":3}}]"}
{"func_name": "lungmask_utils_keep_largest_connected_component", "func_desc": "lungmask.utils.keep_largest_connected_component: Return a binary mask that contains only the largest connected component from an input segmentation label map. This function is used in the lungmask pipeline to remove small disconnected islands (false positive fragments) that can appear in per-slice or per-volume lung segmentation outputs, keeping the primary contiguous lung region used for downstream processing (e.g., volume measurements, lobe assignment, or visualization).", "tools": [{"function": {"description": "lungmask.utils.keep_largest_connected_component: Return a binary mask that contains only the largest connected component from an input segmentation label map. This function is used in the lungmask pipeline to remove small disconnected islands (false positive fragments) that can appear in per-slice or per-volume lung segmentation outputs, keeping the primary contiguous lung region used for downstream processing (e.g., volume measurements, lobe assignment, or visualization).\n", "name": "lungmask_utils_keep_largest_connected_component", "parameters": {"properties": {"mask": {"type": "array", "items": {"type": "float"}, "description": "Input label map produced by a segmentation model or preprocessing step. This numpy.ndarray is expected to contain integer labels or boolean values where non-zero (True) elements are considered foreground. The array may represent a single 2D slice or a 3D volume as used throughout the lungmask package. The function treats all non-zero values as foreground regardless of specific label values (for example, left/right lung labels 1/2 or lobe labels 1-5 are all considered foreground).", "default": ""}}, "required": ["mask"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing post-processing robustness on a mixed batch of lung segmentation outputs where some volumes include multi-class lung labels and sparse single-voxel artifacts. For each candidate label map, first derive a binary lung ROI by treating any non-zero voxel as lung (collapsing all foreground classes), then run the largest-connected-component cleanup to keep only the dominant contiguous lung region. Apply this only to inputs whose collapsed binary ROI contains at least one 2×2 all-foreground patch (a proxy for a plausible anatomical region rather than isolated speckle). Candidate inputs in this run: (A) the 3D prediction label map (0=background, non-zero=lung) [[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,0,0],[0,0,2,0],[0,0,0,0]]]; (B) the single-slice mask [[0,0,0,0,0],[0,1,1,0,0],[0,1,1,0,1],[0,0,0,0,0]]; (C) the 2×3×4 binary volume [[[0,1,1,0],[0,0,0,0],[0,0,0,1]],[[0,1,1,0],[0,0,0,0],[0,0,0,0]]].", "answers": "[{\"name\":\"lungmask_utils_keep_largest_connected_component\",\"arguments\":{\"mask\":[[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]],[[0,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,0]]] }},{\"name\":\"lungmask_utils_keep_largest_connected_component\",\"arguments\":{\"mask\":[[0,0,0,0,0],[0,1,1,0,0],[0,1,1,0,1],[0,0,0,0,0]]}},{\"name\":\"lungmask_utils_keep_largest_connected_component\",\"arguments\":{\"mask\":[[[0,1,1,0],[0,0,0,0],[0,0,0,1]],[[0,1,1,0],[0,0,0,0],[0,0,0,0]]]}}]"}
{"func_name": "lungmask_utils_postprocessing", "func_desc": "lungmask.utils.postprocessing\n    Post-process a labeled lung segmentation volume by remapping small connected components to neighboring labels, keeping only the largest connected component per original label, and removing labels listed in a spare mapping. This function is used in the lungmask pipeline to clean and fuse outputs from different models (for example when fusing LTRCLobes and R231 results), to remove small false-positive regions, and to ensure coherent left/right or lobe labelings as described in the project README (two-label outputs: 1=Right lung, 2=Left lung; five-label lobe outputs: 1..5 correspond to specific lobes).", "tools": [{"function": {"description": "lungmask.utils.postprocessing\nPost-process a labeled lung segmentation volume by remapping small connected components to neighboring labels, keeping only the largest connected component per original label, and removing labels listed in a spare mapping. This function is used in the lungmask pipeline to clean and fuse outputs from different models (for example when fusing LTRCLobes and R231 results), to remove small false-positive regions, and to ensure coherent left/right or lobe labelings as described in the project README (two-label outputs: 1=Right lung, 2=Left lung; five-label lobe outputs: 1..5 correspond to specific lobes).", "name": "lungmask_utils_postprocessing", "parameters": {"properties": {"label_image": {"type": "array", "items": {"type": "float"}, "description": "Label image (integer valued) to be processed. This is the input segmentation volume produced by a lung segmentation model (e.g., U-net(R231) or U-net(LTRCLobes)). Each voxel value represents a label id (0 for background, positive integers for lung parts or lobes). The function expects non-negative integer labels; very large maximum label values increase memory for internal arrays since the algorithm allocates arrays sized by the maximum label. The input array itself is not modified in-place; a new array is produced and returned.", "default": ""}, "spare": {"type": "array", "items": {"type": "float"}, "description": "Labels that are treated as temporary/filler labels: components with these label ids will be remapped to neighboring non-spare labels during post-processing and will not appear in the final returned labeling. This is intended for use in label-fusion workflows (for example when a \"filling\" model supplies candidate regions that should be merged into existing lobes or lungs). Defaults to [] (note: the default is a mutable list object shared across calls; to avoid unexpected persistence between calls, pass an explicit list).", "default": []}, "disable_tqdm": {"type": "boolean", "description": "If True, progress display via tqdm is disabled. If False (default), tqdm will display a progress bar while iterating over connected components. This only affects user-visible progress output and does not change algorithmic behavior.", "default": false}, "skip_below": {"type": "integer", "description": "Threshold for very small connected components. Any connected component with area smaller than this value will not be considered for merging into neighbors and will be removed from the final labeling. This parameter is a runtime/performance optimization and defaults to 3. Components with area >= skip_below can be merged into the neighbor with which they share the largest border (unless that neighbor is also in spare).", "default": 3}}, "required": ["label_image", "spare", "disable_tqdm", "skip_below"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed-provenance lungmask fusion QA step on two 3D labeled CT volumes where auxiliary classes may appear. Treat each volume as a 2-label lung segmentation candidate (1=right, 2=left) but infer cleanup parameters from the intrinsic label content of each volume: (a) identify all labels >2 as auxiliary/spare labels and pass them as the `spare` list; (b) set the small-component removal threshold `skip_below` to 5 voxels if the maximum auxiliary label present is a single-digit value, otherwise set `skip_below` to 20 voxels (multi-digit auxiliary codes indicate a more aggressively filtered secondary model); (c) disable progress reporting. Apply lungmask postprocessing with these per-volume rules to the following volumes: Volume A = [[[0,0,0,0,0],[0,1,1,0,0],[0,1,9,2,0],[0,0,2,2,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,1,1,0,0],[0,1,9,2,0],[0,0,2,2,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,1,0,0],[0,0,9,2,0],[0,0,2,2,0],[0,0,0,0,0]]]; Volume B = [[[0,0,1,1,1],[0,10,1,1,1],[0,10,1,1,0]],[[0,0,1,1,1],[0,10,1,2,2],[0,10,1,2,2]],[[0,0,1,1,1],[0,11,1,2,2],[0,11,1,2,2]]].", "answers": "[{\"name\":\"lungmask_utils_postprocessing\",\"arguments\":{\"label_image\":[[[0,0,0,0,0],[0,1,1,0,0],[0,1,9,2,0],[0,0,2,2,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,1,1,0,0],[0,1,9,2,0],[0,0,2,2,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,1,0,0],[0,0,9,2,0],[0,0,2,2,0],[0,0,0,0,0]]],\"spare\":[9],\"disable_tqdm\":true,\"skip_below\":5}},{\"name\":\"lungmask_utils_postprocessing\",\"arguments\":{\"label_image\":[[[0,0,1,1,1],[0,10,1,1,1],[0,10,1,1,0]],[[0,0,1,1,1],[0,10,1,2,2],[0,10,1,2,2]],[[0,0,1,1,1],[0,11,1,2,2],[0,11,1,2,2]]],\"spare\":[10,11],\"disable_tqdm\":true,\"skip_below\":20}}]"}
{"func_name": "lungmask_utils_preprocess", "func_desc": "Preprocesses a 3D CT volume by intensity clipping, per-slice cropping to the body, and resizing each slice to a fixed in-plane resolution. This function is used in the lungmask pipeline to prepare numpy array volumes for U-net based lung segmentation models (for example U-net(R231) and LTRCLobes) by mapping raw Hounsfield Unit (HU) intensities into a stable range, removing image borders outside the body, and producing uniformly sized 2D slices that the models expect.", "tools": [{"function": {"description": "Preprocesses a 3D CT volume by intensity clipping, per-slice cropping to the body, and resizing each slice to a fixed in-plane resolution. This function is used in the lungmask pipeline to prepare numpy array volumes for U-net based lung segmentation models (for example U-net(R231) and LTRCLobes) by mapping raw Hounsfield Unit (HU) intensities into a stable range, removing image borders outside the body, and producing uniformly sized 2D slices that the models expect.\n", "name": "lungmask_utils_preprocess", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "float"}, "description": "Input CT volume to be preprocessed. Expected to be a 3-dimensional numpy array where the first axis indexes slices (axial planes), the second axis corresponds to chest-to-back, and the third axis corresponds to right-to-left (this ordering is the numpy array convention supported by lungmask as described in the README). Values are expected to be encoded in Hounsfield Units (HU). The function creates an internal copy of this array and does not modify the provided array in-place.", "default": ""}, "resolution": {"type": "array", "items": {"type": "float"}, "description": "Target in-plane size after preprocessing given as [width, height]. Defaults to [192, 192]. The first element is used as the width argument and the second as the height argument when calling the underlying crop_and_resize routine. The function expects a list-like object with exactly two integer-like elements; providing a differently shaped or non-indexable object will raise an indexing or type error.", "default": [192, 192]}}, "required": ["img", "resolution"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our lungmask-style CT preprocessing on a messy benchmarking cohort where some replicates contain heavy metal artifacts or non-chest fields of view. For each 3D HU volume below, decide whether it should enter the U-net inference queue based on an intrinsic plausibility check: only volumes that contain at least one voxel in the soft-tissue window (HU between -300 and +300, inclusive) are considered valid chest acquisitions for this benchmark. For valid volumes, run the standard preprocessing (HU clipping + per-axial-slice body cropping + resize) but apply a branching in-plane target resolution based on the dynamic range of the volume: if the volume contains any value > 1500 HU (suggesting high-density artifact/contrast), resize to 320×320 to preserve detail; otherwise resize to 256×256. Use the provided arrays as-is and generate the exact preprocessing calls for the volumes that pass the plausibility check.\n\nReplicate A (3 slices, 4×4 each):\n[\n  [[-1024, -950, -800, -700], [-500, -200, 50, 200], [300, 600, 1200, 2000], [-1024, -900, -850, -750]],\n  [[-1000, -900, -780, -650], [-450, -150, 20, 180], [320, 650, 1100, 1900], [-1024, -920, -860, -740]],\n  [[-980, -890, -760, -620], [-420, -120, 10, 160], [340, 700, 1050, 1800], [-1024, -930, -870, -730]]\n]\n\nReplicate B (4 slices, 5×6 each):\n[\n  [[0, -150, -800, -1024, 300, 500], [50, -200, -900, -950, 200, 400], [100, -100, -850, -1000, 150, 350], [200, 0, -750, -900, 100, 300], [300, 50, -700, -850, 50, 250]],\n  [[0, -140, -780, -1000, 310, 510], [60, -190, -880, -940, 210, 410], [110, -90, -840, -990, 160, 360], [210, 10, -740, -890, 110, 310], [310, 60, -690, -840, 60, 260]],\n  [[10, -130, -770, -990, 320, 520], [70, -180, -870, -930, 220, 420], [120, -80, -830, -980, 170, 370], [220, 20, -730, -880, 120, 320], [320, 70, -680, -830, 70, 270]],\n  [[20, -120, -760, -980, 330, 530], [80, -170, -860, -920, 230, 430], [130, -70, -820, -970, 180, 380], [230, 30, -720, -870, 130, 330], [330, 80, -670, -820, 80, 280]]\n]\n\nReplicate C (3 slices, 4×4 each):\n[\n  [[-900, -850, -800, -750], [-950, -700, -650, -600], [-980, -720, -680, -640], [-1000, -800, -750, -700]],\n  [[-950, -900, -850, -800], [-970, -650, -600, -550], [-990, -660, -620, -580], [-1020, -820, -770, -720]],\n  [[-920, -880, -840, -800], [-960, -690, -640, -590], [-985, -710, -670, -630], [-1010, -790, -740, -700]]\n]\n\nReturn the preprocessing calls for all and only the valid volumes, with the resolution chosen by the dynamic-range rule above.", "answers": "[{\"name\":\"lungmask_utils_preprocess\",\"arguments\":{\"img\":[[[-1024,-950,-800,-700],[-500,-200,50,200],[300,600,1200,2000],[-1024,-900,-850,-750]],[[-1000,-900,-780,-650],[-450,-150,20,180],[320,650,1100,1900],[-1024,-920,-860,-740]],[[-980,-890,-760,-620],[-420,-120,10,160],[340,700,1050,1800],[-1024,-930,-870,-730]]],\"resolution\":[320,320]}},{\"name\":\"lungmask_utils_preprocess\",\"arguments\":{\"img\":[[[0,-150,-800,-1024,300,500],[50,-200,-900,-950,200,400],[100,-100,-850,-1000,150,350],[200,0,-750,-900,100,300],[300,50,-700,-850,50,250]],[[0,-140,-780,-1000,310,510],[60,-190,-880,-940,210,410],[110,-90,-840,-990,160,360],[210,10,-740,-890,110,310],[310,60,-690,-840,60,260]],[[10,-130,-770,-990,320,520],[70,-180,-870,-930,220,420],[120,-80,-830,-980,170,370],[220,20,-730,-880,120,320],[320,70,-680,-830,70,270]],[[20,-120,-760,-980,330,530],[80,-170,-860,-920,230,430],[130,-70,-820,-970,180,380],[230,30,-720,-870,130,330],[330,80,-670,-820,80,280]]],\"resolution\":[256,256]}}]"}
{"func_name": "lungmask_utils_simple_bodymask", "func_desc": "lungmask.utils.simple_bodymask computes a fast, heuristic binary body mask for a single CT slice by thresholding at -500 Hounsfield units (HU), performing morphological cleanup, keeping the largest connected component, and rescaling the result back to the input resolution. This function is used in the lungmask pipeline to isolate the patient body / chest region on a single 2D CT slice so subsequent lung segmentation steps can focus on the relevant image area and ignore background and small artifacts.", "tools": [{"function": {"description": "lungmask.utils.simple_bodymask computes a fast, heuristic binary body mask for a single CT slice by thresholding at -500 Hounsfield units (HU), performing morphological cleanup, keeping the largest connected component, and rescaling the result back to the input resolution. This function is used in the lungmask pipeline to isolate the patient body / chest region on a single 2D CT slice so subsequent lung segmentation steps can focus on the relevant image area and ignore background and small artifacts.\n", "name": "lungmask_utils_simple_bodymask", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "float"}, "description": "2D CT image representing a single axial slice encoded in Hounsfield units (HU). The array should be a two-dimensional NumPy array (height, width). The function assumes HU intensity values are present; if the input is not encoded in HU (e.g., arbitrary image intensities or externally scaled values), the hard-coded threshold at -500 HU will not give meaningful results.", "default": ""}}, "required": ["img"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a lungmask pre-segmentation QC stage on a mixed minibatch of 2D axial CT slices where some frames are expected to be empty/air-only (gantry air or missing-patient frames). Using the same fast heuristic body/chest masking used upstream in lungmask (binary body mask via -500 HU thresholding + morphological cleanup + keep the largest connected component + rescale to input resolution), run bodymask generation only on slices that contain evidence of patient anatomy: specifically, only slices that contain at least one voxel with HU > -500 (soft tissue / bone / table contamination) should be forwarded into masking; slices that are entirely at or below -500 HU should be treated as non-anatomical and skipped from mask generation. Input candidates (process as independent replicates if they pass the criterion): (A) 4×4 toy HU slice [[-800.0,-750.0,-700.0,-650.0],[-600.0,-550.0,-500.0,-450.0],[-400.0,-350.0,-300.0,-250.0],[-200.0,-150.0,-100.0,-50.0]]; (B) 8×8 axial slice [[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024],[-1024,-900,-800,-700,-700,-800,-900,-1024],[-1024,-850,-200,50,60,-150,-850,-1024],[-1024,-800,0,200,220,20,-800,-1024],[-1024,-820,-50,180,190,-30,-820,-1024],[-1024,-880,-300,-100,-120,-280,-880,-1024],[-1024,-950,-900,-850,-850,-900,-950,-1024],[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024]]; (C) 8×8 axial slice [[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024],[-1024,-980,-950,-920,-910,-940,-990,-1024],[-1024,-850,-300,20,15,-250,-800,-1024],[-1024,-820,-150,60,55,-120,-780,-1024],[-1024,-830,-200,40,35,-180,-790,-1024],[-1024,-900,-700,-650,-640,-680,-920,-1024],[-1024,-1010,-990,-980,-975,-985,-1005,-1024],[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024]]. Return a binary mask for each processed slice suitable for cropping prior to lung segmentation.", "answers": "[{\"name\":\"lungmask_utils_simple_bodymask\",\"arguments\":{\"img\":[[-800.0,-750.0,-700.0,-650.0],[-600.0,-550.0,-500.0,-450.0],[-400.0,-350.0,-300.0,-250.0],[-200.0,-150.0,-100.0,-50.0]]}},{\"name\":\"lungmask_utils_simple_bodymask\",\"arguments\":{\"img\":[[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024],[-1024,-900,-800,-700,-700,-800,-900,-1024],[-1024,-850,-200,50,60,-150,-850,-1024],[-1024,-800,0,200,220,20,-800,-1024],[-1024,-820,-50,180,190,-30,-820,-1024],[-1024,-880,-300,-100,-120,-280,-880,-1024],[-1024,-950,-900,-850,-850,-900,-950,-1024],[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024]]}},{\"name\":\"lungmask_utils_simple_bodymask\",\"arguments\":{\"img\":[[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024],[-1024,-980,-950,-920,-910,-940,-990,-1024],[-1024,-850,-300,20,15,-250,-800,-1024],[-1024,-820,-150,60,55,-120,-780,-1024],[-1024,-830,-200,40,35,-180,-790,-1024],[-1024,-900,-700,-650,-640,-680,-920,-1024],[-1024,-1010,-990,-980,-975,-985,-1005,-1024],[-1024,-1024,-1024,-1024,-1024,-1024,-1024,-1024]]}}]"}
{"func_name": "mace_calculators_lammps_mliap_mace_timer", "func_desc": "mace.calculators.lammps_mliap_mace.timer: Context manager that measures wall-clock elapsed time for a block of code and emits a logging.info entry with the measured duration in milliseconds. This utility is intended for use in the MACE codebase (for example in LAMMPS/MLIAP calculator integration, training, evaluation, preprocessing, or performance debugging) to label and record how long specific operations take (neighbor list construction, model forward passes, data preprocessing, etc.). It uses Python's high-resolution time.perf_counter() to measure elapsed real time and formats the log message as \"Timer - {name}: {elapsed_ms:.3f} ms\".", "tools": [{"function": {"description": "mace.calculators.lammps_mliap_mace.timer: Context manager that measures wall-clock elapsed time for a block of code and emits a logging.info entry with the measured duration in milliseconds. This utility is intended for use in the MACE codebase (for example in LAMMPS/MLIAP calculator integration, training, evaluation, preprocessing, or performance debugging) to label and record how long specific operations take (neighbor list construction, model forward passes, data preprocessing, etc.). It uses Python's high-resolution time.perf_counter() to measure elapsed real time and formats the log message as \"Timer - {name}: {elapsed_ms:.3f} ms\".\n", "name": "mace_calculators_lammps_mliap_mace_timer", "parameters": {"properties": {"name": {"type": "string", "description": "Human-readable label for the timed code block. This string is inserted verbatim into the log message and serves to identify which operation was measured (for example \"forward_pass\", \"neighbor_list_build\", or \"data_preprocess\"). Keep the label concise and descriptive so the log can be correlated with training/evaluation steps recorded when using MACE tools and scripts.", "default": ""}, "enabled": {"type": "boolean", "description": "Flag that controls whether timing and logging are performed. If True (default) the context manager records the start time, yields control to the wrapped block, and upon exit computes elapsed time and logs it via logging.info. If False, the context manager yields immediately without measuring time or producing any log output; this allows conditional disabling of instrumentation without changing call sites.", "default": true}}, "required": ["name", "enabled"], "type": "any"}}, "type": "function"}], "query": "We’re profiling neighbor-list construction in a mixed set of LAMMPS+MLIAP+MACE runs where some replicates include an additional rebuild inside the same timestep due to dynamic box deformation. Instrument timing with MACE’s wall-clock timer using the following protocol:\n\n- For every neighbor-list build invocation, wrap the fine-grained inner construction step with a timer labeled exactly \"neighbor_list_build_step\" (timing enabled).\n- Additionally, only for invocations whose build context indicates an aggregate build cycle (i.e., the build includes both the pre-pass bookkeeping and the final list assembly in a single contiguous block), also wrap the entire block with a timer labeled exactly \"neighbor_list_build\" (timing enabled).\n\nApply this to two observed build invocations in a single profiling trace: one is a simple build consisting only of the inner construction step, and the other is a full aggregate build cycle including bookkeeping + assembly.", "answers": "[{\"name\":\"mace_calculators_lammps_mliap_mace_timer\",\"arguments\":{\"name\":\"neighbor_list_build_step\",\"enabled\":true}},{\"name\":\"mace_calculators_lammps_mliap_mace_timer\",\"arguments\":{\"name\":\"neighbor_list_build_step\",\"enabled\":true}},{\"name\":\"mace_calculators_lammps_mliap_mace_timer\",\"arguments\":{\"name\":\"neighbor_list_build\",\"enabled\":true}}]"}
{"func_name": "mace_tools_tables_utils_custom_key", "func_desc": "mace.tools.tables_utils.custom_key returns a two-element sort key that prioritizes the \"train\" and \"valid\" entries when sorting the keys of a data-loader or results dictionary used during MACE training and evaluation. This ensures that the training set and validation set are evaluated (and therefore reported or plotted) before other datasets (for example test or per-config-type results) in scripts such as the training loop (run_train.py / mace_run_train) and evaluation utilities (mace_eval_configs), where deterministic ordering of dataset evaluation and logging is important.", "tools": [{"function": {"description": "mace.tools.tables_utils.custom_key returns a two-element sort key that prioritizes the \"train\" and \"valid\" entries when sorting the keys of a data-loader or results dictionary used during MACE training and evaluation. This ensures that the training set and validation set are evaluated (and therefore reported or plotted) before other datasets (for example test or per-config-type results) in scripts such as the training loop (run_train.py / mace_run_train) and evaluation utilities (mace_eval_configs), where deterministic ordering of dataset evaluation and logging is important.\n", "name": "mace_tools_tables_utils_custom_key", "parameters": {"properties": {"key": {"type": "string", "description": "The dictionary key/name for a dataset or configuration (for example \"train\", \"valid\", or \"test\"). In the MACE workflow this is typically the identifier used by the data loader or results collection to label different splits or config types. The function expects a string and uses its exact value to determine priority; passing a non-string may lead to type errors or undefined ordering when used as a sort key.", "default": ""}}, "required": ["key"], "type": "any"}}, "type": "function"}], "query": "We are aggregating per-split metrics from a multi-source MACE run where keys arrive from mixed conventions (official dataloader splits, ad-hoc config-type rollups, and a few legacy pipeline artifacts). To keep reporting deterministic across the training loop and downstream evaluation, generate the two-element `custom_key` for every entry in the raw key list below, but only for keys that are plausible dataset split identifiers after normalization (trim surrounding whitespace; accept only lowercase a–z, digits, and underscore; reject anything containing other characters). Raw keys: [\" train\", \"valid \", \"test\", \"config_type_1\", \"CONFIG_TYPE_2\", \"valid/fold0\", \"train#1\", \"\", \"  \", \"ood__set\", \"per-config:type\", \"dev\"]. We will sort using those keys and expect the canonical \"train\" and \"valid\" splits to be prioritized ahead of all other accepted split-like keys.", "answers": "[{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"train\"}},{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"valid\"}},{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"test\"}},{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"config_type_1\"}},{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"ood__set\"}},{\"name\":\"mace_tools_tables_utils_custom_key\",\"arguments\":{\"key\":\"dev\"}}]"}
{"func_name": "medclip_vision_model_window_partition", "func_desc": "medclip.vision_model.window_partition partitions a 4-D image feature tensor into non-overlapping square windows. This operation is used in MedCLIP vision models (for example ViT or Swin-style components) to prepare local image patches for windowed self-attention and downstream contrastive learning between medical images and text (e.g., chest x-ray feature maps). The function reshapes and permutes the input so each output row is one window, which simplifies batched window-wise processing used in MedCLIP training and inference.", "tools": [{"function": {"description": "medclip.vision_model.window_partition partitions a 4-D image feature tensor into non-overlapping square windows. This operation is used in MedCLIP vision models (for example ViT or Swin-style components) to prepare local image patches for windowed self-attention and downstream contrastive learning between medical images and text (e.g., chest x-ray feature maps). The function reshapes and permutes the input so each output row is one window, which simplifies batched window-wise processing used in MedCLIP training and inference.\n", "name": "medclip_vision_model_window_partition", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input feature tensor with shape (B, H, W, C), where B is batch size, H and W are spatial height and width, and C is the number of channels. This tensor is the image-level feature map produced by a vision backbone in MedCLIP. The function expects a 4-D tensor; values, dtype, and device are preserved in the returned tensor. H and W must be integer multiples of window_size (see failure modes below).", "default": ""}, "window_size": {"type": "integer", "description": "Size of the square window (number of pixels/patches per side). This is the same window granularity used by windowed attention components in MedCLIP vision models and must be a positive integer that divides both H and W without remainder.", "default": ""}}, "required": ["x", "window_size"], "type": "any"}}, "type": "function"}], "query": "We’re validating windowed self-attention packing in a MedCLIP-style vision encoder under mixed-resolution feature maps coming from different backbone stages. We have two single-image feature tensors (NHWC). For each tensor, choose the largest square window_size in {2, 4} that evenly tiles both spatial dimensions (H and W) without remainder; then run window_partition to produce a window-major batch (each row = one local window).\n\nFeature-map A (B=1, H=4, W=4, C=2): x = [[[[1.0, 0.1], [2.0, 0.2], [3.0, 0.3], [4.0, 0.4]], [[5.0, 0.5], [6.0, 0.6], [7.0, 0.7], [8.0, 0.8]], [[9.0, 0.9], [10.0, 1.0], [11.0, 1.1], [12.0, 1.2]], [[13.0, 1.3], [14.0, 1.4], [15.0, 1.5], [16.0, 1.6]]]].\n\nFeature-map B (B=1, H=8, W=8, C=3): x = [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2], [1.3, 1.4, 1.5], [1.6, 1.7, 1.8], [1.9, 2.0, 2.1], [2.2, 2.3, 2.4]], [[2.5, 2.6, 2.7], [2.8, 2.9, 3.0], [3.1, 3.2, 3.3], [3.4, 3.5, 3.6], [3.7, 3.8, 3.9], [4.0, 4.1, 4.2], [4.3, 4.4, 4.5], [4.6, 4.7, 4.8]], [[4.9, 5.0, 5.1], [5.2, 5.3, 5.4], [5.5, 5.6, 5.7], [5.8, 5.9, 6.0], [6.1, 6.2, 6.3], [6.4, 6.5, 6.6], [6.7, 6.8, 6.9], [7.0, 7.1, 7.2]], [[7.3, 7.4, 7.5], [7.6, 7.7, 7.8], [7.9, 8.0, 8.1], [8.2, 8.3, 8.4], [8.5, 8.6, 8.7], [8.8, 8.9, 9.0], [9.1, 9.2, 9.3], [9.4, 9.5, 9.6]], [[9.7, 9.8, 9.9], [10.0, 10.1, 10.2], [10.3, 10.4, 10.5], [10.6, 10.7, 10.8], [10.9, 11.0, 11.1], [11.2, 11.3, 11.4], [11.5, 11.6, 11.7], [11.8, 11.9, 12.0]], [[12.1, 12.2, 12.3], [12.4, 12.5, 12.6], [12.7, 12.8, 12.9], [13.0, 13.1, 13.2], [13.3, 13.4, 13.5], [13.6, 13.7, 13.8], [13.9, 14.0, 14.1], [14.2, 14.3, 14.4]], [[14.5, 14.6, 14.7], [14.8, 14.9, 15.0], [15.1, 15.2, 15.3], [15.4, 15.5, 15.6], [15.7, 15.8, 15.9], [16.0, 16.1, 16.2], [16.3, 16.4, 16.5], [16.6, 16.7, 16.8]], [[16.9, 17.0, 17.1], [17.2, 17.3, 17.4], [17.5, 17.6, 17.7], [17.8, 17.9, 18.0], [18.1, 18.2, 18.3], [18.4, 18.5, 18.6], [18.7, 18.8, 18.9], [19.0, 19.1, 19.2]]]].", "answers": "[{\"name\":\"medclip_vision_model_window_partition\",\"arguments\":{\"x\":[[[[1.0,0.1],[2.0,0.2],[3.0,0.3],[4.0,0.4]],[[5.0,0.5],[6.0,0.6],[7.0,0.7],[8.0,0.8]],[[9.0,0.9],[10.0,1.0],[11.0,1.1],[12.0,1.2]],[[13.0,1.3],[14.0,1.4],[15.0,1.5],[16.0,1.6]]]],\"window_size\":4}},{\"name\":\"medclip_vision_model_window_partition\",\"arguments\":{\"x\":[[[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4]],[[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]],[[4.9,5.0,5.1],[5.2,5.3,5.4],[5.5,5.6,5.7],[5.8,5.9,6.0],[6.1,6.2,6.3],[6.4,6.5,6.6],[6.7,6.8,6.9],[7.0,7.1,7.2]],[[7.3,7.4,7.5],[7.6,7.7,7.8],[7.9,8.0,8.1],[8.2,8.3,8.4],[8.5,8.6,8.7],[8.8,8.9,9.0],[9.1,9.2,9.3],[9.4,9.5,9.6]],[[9.7,9.8,9.9],[10.0,10.1,10.2],[10.3,10.4,10.5],[10.6,10.7,10.8],[10.9,11.0,11.1],[11.2,11.3,11.4],[11.5,11.6,11.7],[11.8,11.9,12.0]],[[12.1,12.2,12.3],[12.4,12.5,12.6],[12.7,12.8,12.9],[13.0,13.1,13.2],[13.3,13.4,13.5],[13.6,13.7,13.8],[13.9,14.0,14.1],[14.2,14.3,14.4]],[[14.5,14.6,14.7],[14.8,14.9,15.0],[15.1,15.2,15.3],[15.4,15.5,15.6],[15.7,15.8,15.9],[16.0,16.1,16.2],[16.3,16.4,16.5],[16.6,16.7,16.8]],[[16.9,17.0,17.1],[17.2,17.3,17.4],[17.5,17.6,17.7],[17.8,17.9,18.0],[18.1,18.2,18.3],[18.4,18.5,18.6],[18.7,18.8,18.9],[19.0,19.1,19.2]]]],\"window_size\":4}}]"}
{"func_name": "mendeleev_electronegativity_allred_rochow", "func_desc": "Calculate the electronegativity of an atom according to the Allred and Rochow definition.\n    \n    This function implements the mathematical core of the Allred & Rochow electronegativity scale as used in the mendeleev package's \"Electronegativity scales\" utilities. The returned value is the proportional Allred–Rochow electronegativity computed as zeff / radius**2. In the context of mendeleev, zeff is typically derived from nuclear screening constants or other effective nuclear charge estimates available in the package data tables, and radius should be a corresponding atomic-size value (for example an atomic or covalent radius drawn from the size-related properties in mendeleev). The numerical value and its comparability to literature Allred–Rochow numbers depend on using consistent radius values and units with the source of zeff.", "tools": [{"function": {"description": "Calculate the electronegativity of an atom according to the Allred and Rochow definition.\n\nThis function implements the mathematical core of the Allred & Rochow electronegativity scale as used in the mendeleev package's \"Electronegativity scales\" utilities. The returned value is the proportional Allred–Rochow electronegativity computed as zeff / radius**2. In the context of mendeleev, zeff is typically derived from nuclear screening constants or other effective nuclear charge estimates available in the package data tables, and radius should be a corresponding atomic-size value (for example an atomic or covalent radius drawn from the size-related properties in mendeleev). The numerical value and its comparability to literature Allred–Rochow numbers depend on using consistent radius values and units with the source of zeff.", "name": "mendeleev_electronegativity_allred_rochow", "parameters": {"properties": {"zeff": {"type": "float", "description": "Effective nuclear charge for the atom. This is a dimensionless measure of the net positive charge experienced by the valence electrons after accounting for electron screening. In mendeleev this value is commonly obtained from nuclear screening constants (Slater/Clementi) or computed effective charges and serves as the numerator in the Allred–Rochow formula.", "default": ""}, "radius": {"type": "float", "description": "Value of the radius used in the denominator of the Allred–Rochow expression. In practice this should be an atomic-size measure (for example a covalent or atomic radius available in mendeleev's size-related properties). The value is squared in the formula, so units must be consistent with those used to derive zeff when comparing results to published Allred–Rochow electronegativities.", "default": ""}}, "required": ["zeff", "radius"], "type": "any"}}, "type": "function"}], "query": "We’re QC’ing a mixed atom-property table prior to fitting an Allred–Rochow-like electronegativity proxy for a halogen-oriented benchmark. Each record contains an effective nuclear charge (zeff) and an effective covalent radius (Å), but the radius source is heterogeneous. Compute the proportional Allred–Rochow electronegativity strictly as zeff/radius^2 only for records with physically plausible inputs (zeff > 0, radius > 0) and with radius reported in Å and within the typical covalent-radius window of 0.6–1.2 Å. For any accepted record, if its radius is below 0.75 Å treat it as a compact-species estimate and use a 1.03× radius correction before computing; otherwise use the radius as reported. Raw records: (i) Cl_ref_rep1: zeff=6.2, radius=0.99 Å; (ii) Atom_A: zeff=5.20, radius=0.77 Å; (iii) Atom_B: zeff=6.10, radius=0.71 Å; (iv) Artifact_1: zeff=6.00, radius=0 Å; (v) UnitMix_1: zeff=5.90, radius=71 pm.", "answers": "[{\"name\":\"mendeleev_electronegativity_allred_rochow\",\"arguments\":{\"zeff\":6.2,\"radius\":0.99}},{\"name\":\"mendeleev_electronegativity_allred_rochow\",\"arguments\":{\"zeff\":5.2,\"radius\":0.77}},{\"name\":\"mendeleev_electronegativity_allred_rochow\",\"arguments\":{\"zeff\":6.1,\"radius\":0.7313}}]"}
{"func_name": "mendeleev_electronegativity_interpolate_property", "func_desc": "Interpolate or extrapolate a numeric property for an element using reference element indices and property values.", "tools": [{"function": {"description": "Interpolate or extrapolate a numeric property for an element using reference element indices and property values.\n", "name": "mendeleev_electronegativity_interpolate_property", "parameters": {"properties": {"x": {"type": "integer", "description": "The target independent variable value for which the property will be estimated. In the domain of the mendeleev package this represents an element identifier on the numeric axis used for the reference data (for example an atomic number or any integer index used consistently in x_ref). The function evaluates the property at this single integer x by either interpolating within the provided reference range or by extrapolating beyond it.", "default": ""}, "x_ref": {"type": "array", "items": {"type": "integer"}, "description": "A list of integer independent-variable reference points corresponding to known elements or ordered element indices. These are the x-coordinates of the known data points (for example atomic numbers for which the property in y_ref is known). The list must be non-empty and its values define the inclusive interval used to decide whether to interpolate (x within min(x_ref) .. max(x_ref)) or extrapolate (x outside that interval). The routine converts this sequence to a numpy array internally.", "default": ""}, "y_ref": {"type": "array", "items": {"type": "float"}, "description": "A list of floating-point dependent-variable reference values corresponding one-to-one with x_ref. Each entry is the known property value for the element/index in the same position in x_ref (for example electronegativity values, radii, energies, etc.). The lengths of x_ref and y_ref must match; otherwise numpy functions called internally (np.interp or np.polyfit) will raise an error. The sequence is converted to a numpy array internally.", "default": ""}, "poly_deg": {"type": "integer", "description": "Degree of the polynomial used for extrapolation beyond the provided data points. Default is 1 which selects a linear polynomial fit for extrapolation. This integer controls np.polyfit called on a small slice of the reference data when x lies outside the inclusive range defined by x_ref. Choosing a poly_deg larger than the number of reference points used for the fit or an inappropriate value may cause numpy to emit warnings or raise exceptions (for example from np.polyfit); the caller is responsible for selecting a sensible degree given the available data.", "default": 1}}, "required": ["x", "x_ref", "y_ref", "poly_deg"], "type": "any"}}, "type": "function"}], "query": "We’re running an electronegativity-like property reconstruction pass on a mixed-quality reference table coming from three upstream instruments. Each cohort reports (x_ref, y_ref) pairs, but the table may contain placeholder zeros that indicate masked/failed measurements. For each cohort, first retain only reference pairs with strictly positive y_ref (treat non-positive values as masked) and preserve the original x_ref ordering of the retained points. Then choose the estimation protocol based on where the query x falls relative to the retained x_ref range: if x lies within the min/max of the retained x_ref, use linear interpolation (default settings); if x falls outside that range, extrapolate using a quadratic (degree-2) polynomial fit.\n\nCohort A (halogen-gap QC): estimate at x=17 from x_ref=[9,10,11,12,13,14,15,16,17,18,19,20] with y_ref=[3.98,0.0,0.93,1.31,1.61,1.9,2.19,2.58,3.16,0.0,0.82,1.0].\nCohort B (main-group benchmark axis): estimate at x=10 from x_ref=[1,6,7,8,9] with y_ref=[2.20,2.55,3.04,3.44,3.98].\nCohort C (custom calibration scale): estimate at x=45 from x_ref=[10,20,30,40] with y_ref=[0.82,1.54,1.72,1.33].", "answers": "[{\"name\":\"mendeleev_electronegativity_interpolate_property\",\"arguments\":{\"x\":17,\"x_ref\":[9,11,12,13,14,15,16,17,19,20],\"y_ref\":[3.98,0.93,1.31,1.61,1.9,2.19,2.58,3.16,0.82,1.0]}},{\"name\":\"mendeleev_electronegativity_interpolate_property\",\"arguments\":{\"x\":10,\"x_ref\":[1,6,7,8,9],\"y_ref\":[2.2,2.55,3.04,3.44,3.98],\"poly_deg\":2}},{\"name\":\"mendeleev_electronegativity_interpolate_property\",\"arguments\":{\"x\":45,\"x_ref\":[10,20,30,40],\"y_ref\":[0.82,1.54,1.72,1.33],\"poly_deg\":2}}]"}
{"func_name": "mendeleev_electronegativity_li_xue", "func_desc": "mendeleev.electronegativity.li_xue computes the electronegativity of an atom or ion according to the Li & Xue definition used in the mendeleev package. It implements the formula used in the source code:\n    n_effective(valence_pqn, source=\"zhang\") * sqrt(ionization_energy / RY) * 100.0 / radius, where RY is the Rydberg energy constant imported in this module and n_effective(...) is computed with the \"zhang\" prescription.", "tools": [{"function": {"description": "mendeleev.electronegativity.li_xue computes the electronegativity of an atom or ion according to the Li & Xue definition used in the mendeleev package. It implements the formula used in the source code:\nn_effective(valence_pqn, source=\"zhang\") * sqrt(ionization_energy / RY) * 100.0 / radius, where RY is the Rydberg energy constant imported in this module and n_effective(...) is computed with the \"zhang\" prescription.", "name": "mendeleev_electronegativity_li_xue", "parameters": {"properties": {"ionization_energy": {"type": "float", "description": "First ionization energy (numeric). This value must be expressed in the same energy units as the module-level RY constant (the Rydberg energy) because the implementation divides ionization_energy by RY prior to taking the square root. In the Li & Xue electronegativity model this term represents the ionization energy contribution to orbital binding and therefore to electronegativity. Passing a negative ionization_energy will raise a ValueError from math.sqrt. The caller is responsible for ensuring the value is in the correct units and is physically meaningful (non-negative).", "default": ""}, "radius": {"type": "float", "description": "Effective atomic or ionic radius as a numeric scalar. This radius is used as the denominator in the Li & Xue formula, so it must be non-zero; a zero radius will raise a ZeroDivisionError. Provide the radius in the same length units consistently used elsewhere in the codebase (for example, the same units returned by element attributes such as crystal_radius or ionic_radius) because the formula scales inversely with radius. A larger radius decreases the computed electronegativity in this model.", "default": ""}, "valence_pqn": {"type": "integer", "description": "Valence principal quantum number (n) for the atom's valence shell. This integer is passed to n_effective(valence_pqn, source=\"zhang\") to compute an effective principal quantum number following the \"zhang\" prescription used in this implementation. The effective quantum number scales the overall result; valence_pqn should be a positive integer consistent with the element's valence shell (for example, 1, 2, 3, ...). Supplying a non-integer or nonsensical quantum number may produce an incorrect result or a downstream error from n_effective.", "default": ""}}, "required": ["ionization_energy", "radius", "valence_pqn"], "type": "any"}}, "type": "function"}], "query": "We’re validating a Li–Xue electronegativity pre-processor that ingests heterogeneous ionic radii from mixed provenance. Use the Zhang prescription for n_effective. From the following three candidate records, compute Li–Xue electronegativity only for entries that are physically admissible for the Li–Xue form (positive ionization energy, positive radius, and an integer valence principal quantum number n≥1). For admissible entries, apply an in-pipeline radius harmonization rule: if the radius is greater than 1.80 (same length units throughout), treat it as a neutral-like Shannon radius and add a +0.05 correction before computing electronegativity; otherwise use the radius as reported. Records: (A) Na-like reference: ionization_energy=5.14, radius=1.86, n=3. (B) spreadsheet anion test: ionization_energy=5.14, radius=1.81, n=3. (C) Fe2+ ionic: ionization_energy=16.18, radius=0.78, n=3. Return results in the original record order for those that pass the admissibility gate.", "answers": "[{\"name\":\"mendeleev_electronegativity_li_xue\",\"arguments\":{\"ionization_energy\":5.14,\"radius\":1.9100000000000001,\"valence_pqn\":3}},{\"name\":\"mendeleev_electronegativity_li_xue\",\"arguments\":{\"ionization_energy\":5.14,\"radius\":1.86,\"valence_pqn\":3}},{\"name\":\"mendeleev_electronegativity_li_xue\",\"arguments\":{\"ionization_energy\":16.18,\"radius\":0.78,\"valence_pqn\":3}}]"}
{"func_name": "mendeleev_electronegativity_mulliken", "func_desc": "mendeleev.electronegativity.mulliken computes the absolute (Mulliken) electronegativity for an element or species by averaging its ionization energy and electron affinity. This function is used in the mendeleev package to provide one of the standardized electronegativity scales (Mulliken) described in the project README and is useful for comparing tendencies of elements to attract electrons when analyzing periodic trends, generating periodic table visualizations, or computing derived chemical descriptors.", "tools": [{"function": {"description": "mendeleev.electronegativity.mulliken computes the absolute (Mulliken) electronegativity for an element or species by averaging its ionization energy and electron affinity. This function is used in the mendeleev package to provide one of the standardized electronegativity scales (Mulliken) described in the project README and is useful for comparing tendencies of elements to attract electrons when analyzing periodic trends, generating periodic table visualizations, or computing derived chemical descriptors.\n", "name": "mendeleev_electronegativity_mulliken", "parameters": {"properties": {"ionization_energy": {"type": "float", "description": "The ionization energy I of the element or species. This is the energy required to remove an electron and is provided as a numeric energy value. The function accepts None at runtime (the implementation checks for None), in which case the function returns None because ionization energy is required to define the Mulliken value. Practically, supply ionization_energy in the same energy units as electron_affinity (for example both in eV or both in kJ/mol) so the average has a consistent physical meaning. Providing a non-numeric type will typically raise a TypeError during arithmetic operations.", "default": ""}, "electron_affinity": {"type": "float", "description": "The electron affinity A of the element or species. This is the energy change when an electron is added. If electron_affinity is None, the function falls back to using only the ionization_energy and returns half of that value (I / 2). As with ionization_energy, supply this value in the same units as ionization_energy; supplying a non-numeric type will typically raise a TypeError.", "default": ""}}, "required": ["ionization_energy", "electron_affinity"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed-provenance electronegativity panel for a multi-domain descriptor library (periodic-trend visualization + electrolyte polarity model). The raw ingest contains physically plausible records, artifacts, and a speculative species. Using the Mulliken definition (absolute electronegativity in eV), compute values only for entries whose electron affinity is non-negative and whose first ionization energy is at least an order of magnitude larger than the electron affinity (I/A ≥ 10), consistent with the screening we apply to avoid transient-anion artifacts. Dataset (I in eV, A in eV): O: I=13.6181, A=1.4611; S: I=10.36, A=2.077; Li: I=5.139, A=0.548; X*: I=8.7, A=1.3. Return the Mulliken electronegativity calls for the retained records.", "answers": "[{\"name\":\"mendeleev_electronegativity_mulliken\",\"arguments\":{\"ionization_energy\":13.6181,\"electron_affinity\":1.4611}}]"}
{"func_name": "mendeleev_electronegativity_n_effective", "func_desc": "mendeleev.electronegativity.n_effective returns the effective principal quantum number (n*) used by the mendeleev package for approximate atomic orbital and electronegativity calculations. This scalar value is taken from published parameter sets (Slater or Zhang) and is used in empirical formulas that require an effective principal quantum number to represent screening and orbital contraction effects in atoms and ions.\n    \n    This function looks up a published effective principal quantum number for a given principal quantum number and source. The implementation currently supports two named sources: 'slater' and 'zhang'. Slater values are taken from J. A. Pople and D. L. Beveridge, \"Approximate Molecular Orbital Theory\", McGraw-Hill, 1970. Zhang values are taken from Zhang, Y. (1982). Electronegativities of elements in valence states and their applications. Inorganic Chemistry, 21(11), 3886–3889. The function performs no I/O and has no side effects beyond returning a numeric value or None; it will raise an exception if an unknown source is requested.", "tools": [{"function": {"description": "mendeleev.electronegativity.n_effective returns the effective principal quantum number (n*) used by the mendeleev package for approximate atomic orbital and electronegativity calculations. This scalar value is taken from published parameter sets (Slater or Zhang) and is used in empirical formulas that require an effective principal quantum number to represent screening and orbital contraction effects in atoms and ions.\n\nThis function looks up a published effective principal quantum number for a given principal quantum number and source. The implementation currently supports two named sources: 'slater' and 'zhang'. Slater values are taken from J. A. Pople and D. L. Beveridge, \"Approximate Molecular Orbital Theory\", McGraw-Hill, 1970. Zhang values are taken from Zhang, Y. (1982). Electronegativities of elements in valence states and their applications. Inorganic Chemistry, 21(11), 3886–3889. The function performs no I/O and has no side effects beyond returning a numeric value or None; it will raise an exception if an unknown source is requested.", "name": "mendeleev_electronegativity_n_effective", "parameters": {"properties": {"n": {"type": "integer", "description": "Principal quantum number. This is the hydrogen-like shell index (1 for K-shell, 2 for L-shell, etc.) used to select the effective principal quantum number from the chosen published dataset. Valid integer n values are determined by the selected source: for 'slater' the implemented keys are 1, 2, 3, 4, 5, 6; for 'zhang' the implemented keys are 1, 2, 3, 4, 5, 6, 7. If an n value outside the implemented keys is supplied for a supported source, the function returns None (no side effects).", "default": ""}, "source": {"type": "string", "description": "Identifier of the published dataset to use; must be one of 'slater' or 'zhang'. Default is 'slater'. 'slater' selects values from Pople & Beveridge (1970) used for Slater-type screening approximations; 'zhang' selects values from Zhang (1982) used in empirical electronegativity assignments. If an unknown source string is provided, the function raises a ValueError listing the available sources.", "default": "slater"}}, "required": ["n", "source"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mixed-quality set of inferred valence-shell assignments coming out of our MO screening preprocessor before fitting electronegativity terms. The raw stream reports principal quantum numbers as: [5, 3, 5, 4, 0, -1, 8, 3.0, 4.5, null, \"5\", 2]. For consistency with the Zhang (1982) parametrization used by mendeleev, compute n* only for entries that represent physically admissible *integer* principal quantum numbers supported by our model’s valence manifold (n in {3,4,5}). Treat numeric strings as parseable if they map cleanly to an integer in that set; ignore everything else as upstream artifacts. Preserve the original stream order for the retained entries and query mendeleev with source='zhang' for each retained n.", "answers": "[{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":5,\"source\":\"zhang\"}},{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":3,\"source\":\"zhang\"}},{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":5,\"source\":\"zhang\"}},{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":4,\"source\":\"zhang\"}},{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":3,\"source\":\"zhang\"}},{\"name\":\"mendeleev_electronegativity_n_effective\",\"arguments\":{\"n\":5,\"source\":\"zhang\"}}]"}
{"func_name": "mendeleev_fetch_fetch_ionic_radii", "func_desc": "Fetch a pandas.DataFrame of ionic radii for all elements and ions available in the package data.\n    \n    This function, mendeleev.fetch.fetch_ionic_radii, is used by the mendeleev package to retrieve size-related properties (ionic radii and crystal radii) from the internal \"ionicradii\" data table (part of the mendeleev data assets). It is intended for downstream analysis and visualization of element and ion size trends (for example, comparing radii across coordination numbers or plotting periodic trends) and returns a pivoted table keyed by atomic number and ionic charge with coordination numbers as columns.", "tools": [{"function": {"description": "Fetch a pandas.DataFrame of ionic radii for all elements and ions available in the package data.\n\nThis function, mendeleev.fetch.fetch_ionic_radii, is used by the mendeleev package to retrieve size-related properties (ionic radii and crystal radii) from the internal \"ionicradii\" data table (part of the mendeleev data assets). It is intended for downstream analysis and visualization of element and ion size trends (for example, comparing radii across coordination numbers or plotting periodic trends) and returns a pivoted table keyed by atomic number and ionic charge with coordination numbers as columns.", "name": "mendeleev_fetch_fetch_ionic_radii", "parameters": {"properties": {"radius": {"type": "string", "description": "The radius column to return from the underlying \"ionicradii\" table. Must be either 'ionic_radius' (the default) to obtain ionic radii as reported in the data source, or 'crystal_radius' to obtain crystal radii when available. The parameter selects which numeric radius values (as provided in the mendeleev data assets) will populate the pivoted table.", "default": "ionic_radius"}}, "required": ["radius"], "type": "any"}}, "type": "function"}], "query": "We’re doing a reproducibility and anomaly-screening pass for an ion-size curation workflow before building coordination-dependent models. Pull the **pivoted crystal-radius** table (atomic number + ionic charge index; coordination numbers as columns). Then run a second independent fetch for a replicate snapshot. Using the two snapshots, we’ll retain only ion entries that have at least **three** non-null coordination-number radii reported (to avoid sparsely characterized/speculative species) for downstream trend fitting; the fetch step should provide the full pivoted tables needed for this filtering and consistency checks.", "answers": "[{\"name\":\"mendeleev_fetch_fetch_ionic_radii\",\"arguments\":{\"radius\":\"crystal_radius\"}},{\"name\":\"mendeleev_fetch_fetch_ionic_radii\",\"arguments\":{\"radius\":\"crystal_radius\"}}]"}
{"func_name": "mendeleev_mendeleev_ids_to_attr", "func_desc": "Convert element identifiers (atomic numbers, atomic symbols, or English element names) to a list of attribute values for the corresponding Element objects from the mendeleev periodic-table API.\n    \n    This function is used in the mendeleev package to map user-supplied element identifiers into concrete element properties that are commonly used when accessing the periodic table data programmatically (for example in the CLI, tutorials, or when preparing data for pandas/bokeh visualizations). It resolves identifiers via the package's element() helper, obtains the requested attribute from each resolved Element using getattr, and returns the attribute values in a list preserving the input order. The default attribute is \"atomic_number\", which yields a list of atomic numbers for the provided identifiers.", "tools": [{"function": {"description": "Convert element identifiers (atomic numbers, atomic symbols, or English element names) to a list of attribute values for the corresponding Element objects from the mendeleev periodic-table API.\n\nThis function is used in the mendeleev package to map user-supplied element identifiers into concrete element properties that are commonly used when accessing the periodic table data programmatically (for example in the CLI, tutorials, or when preparing data for pandas/bokeh visualizations). It resolves identifiers via the package's element() helper, obtains the requested attribute from each resolved Element using getattr, and returns the attribute values in a list preserving the input order. The default attribute is \"atomic_number\", which yields a list of atomic numbers for the provided identifiers.", "name": "mendeleev_mendeleev_ids_to_attr", "parameters": {"properties": {"ids": {"type": "array", "items": {"type": "float"}, "description": "One or multiple element identifiers to resolve. Accepted identifier forms are documented in the mendeleev README and include atomic numbers (int), atomic symbols (str, e.g., \"Fe\"), and English element names (str, e.g., \"Iron\"). If a list or tuple is provided, each item may be any supported identifier type and the function returns a list with one entry per item. If a single int or str is provided, the function returns a single-element list containing the requested attribute for that element. This parameter is the primary input used to select which element properties are returned and may be mixed (e.g., [26, \"Fe\", \"Oxygen\"]) to obtain attributes for multiple elements in one call.", "default": ""}, "attr": {"type": "string", "description": "The name of the attribute to retrieve from each resolved Element object. This must be the exact attribute name as exposed on mendeleev.Element instances (for example, \"atomic_number\", \"name\", \"symbol\", \"thermal_conductivity\", \"isotopes\", etc., as listed in the package documentation). The default value is \"atomic_number\". The attribute is obtained via Python's getattr, so its meaning and type follow how the attribute is implemented on the Element object (it may be a scalar like an int/float or a complex object such as a list of Isotope objects).", "default": "atomic_number"}}, "required": ["ids", "attr"], "type": "any"}}, "type": "function"}], "query": "We’re curating three messy cohorts of element identifiers coming from OCR’d lab notebooks and legacy ELN exports (mixed atomic numbers, symbols, and English names, with inconsistent casing/whitespace). For downstream joins, we need periodic-table resolution with cohort-specific rules and order preservation.\n\nCohort A (wet-chem quick-check): start from the raw list [8, \"Fe\", \"Gold\", 17]. Resolve each identifier and export the atomic symbols, but only for entries corresponding to nonmetals under standard classification; keep their relative order as they appeared in the raw list.\n\nCohort B (materials viz feature pack): start from [26, \"Cu\", \"Oxygen\", 14, \"Au\"]. For entries that are metals, export two parallel arrays over the same filtered ordered list: (1) official element names and (2) thermal conductivities.\n\nCohort C (lookup-table reconciliation): start from [\"Fe\", 8, \"Gold\", \"Na\"]. For entries that are solids at STP, export the official element names in the filtered order.", "answers": "[{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[8,\"Fe\",\"Gold\",17],\"attr\":\"is_nonmetal\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[8,\"Fe\",\"Gold\",17],\"attr\":\"symbol\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[26,\"Cu\",\"Oxygen\",14,\"Au\"],\"attr\":\"is_metal\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[26,\"Cu\",\"Oxygen\",14,\"Au\"],\"attr\":\"name\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[26,\"Cu\",\"Oxygen\",14,\"Au\"],\"attr\":\"thermal_conductivity\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[\"Fe\",8,\"Gold\",\"Na\"],\"attr\":\"phase\"}},{\"name\":\"mendeleev_mendeleev_ids_to_attr\",\"arguments\":{\"ids\":[\"Fe\",8,\"Gold\",\"Na\"],\"attr\":\"name\"}}]"}
{"func_name": "mendeleev_models_fetch_by_group", "func_desc": "mendeleev.models.fetch_by_group retrieves specified Element attributes for every element in a given periodic-table group.", "tools": [{"function": {"description": "mendeleev.models.fetch_by_group retrieves specified Element attributes for every element in a given periodic-table group.\n", "name": "mendeleev_models_fetch_by_group", "parameters": {"properties": {"properties": {"type": "array", "items": {"type": "string"}, "description": "One or more attribute names of the Element model to retrieve for each element in the group. Typical attributes are those listed in the package README under \"Basic properties\" and \"Physical properties\" (for example, \"atomic_number\", \"name\", \"atomic_weight\", \"melting_point\", \"ionization_energies\"). The function also accepts a single attribute as a str (the implementation checks isinstance(properties, str) and will convert it to a one-item list). The order of attributes in this sequence determines the order of values in each returned row. If \"atomic_number\" is not included in this sequence it will be automatically prepended so that results include the atomic number and are ordered reliably.", "default": ""}, "group": {"type": "integer", "description": "Periodic-table group number used to filter elements (Element.group_id == group). This corresponds to the chemical group/column in the periodic table; the default value is 18 (the noble gases). Use this parameter to request data for all elements that belong to the specified group.", "default": 18}}, "required": ["properties", "group"], "type": "any"}}, "type": "function"}], "query": "We’re validating a halogen (periodic-table group 17) extract for a regulatory-facing report where different subpanels require different fields. Pull the full group-17 roster once for master indexing using standardized identifiers only (atomic number, symbol, name). Then produce a second extract intended for mass-balance calculations that includes atomic weights in addition to the identifiers, but only keep entries whose atomic weight is numerically defined (i.e., present and finite) so they can be used in downstream computations.", "answers": "[{\"name\":\"mendeleev_models_fetch_by_group\",\"arguments\":{\"properties\":[\"atomic_number\",\"symbol\",\"name\"],\"group\":17}},{\"name\":\"mendeleev_models_fetch_by_group\",\"arguments\":{\"properties\":[\"atomic_number\",\"symbol\",\"name\",\"atomic_weight\"],\"group\":17}}]"}
{"func_name": "mendeleev_models_with_uncertainty", "func_desc": "mendeleev.models.with_uncertainty formats a numeric value together with its measurement uncertainty into a human-readable string using scientific notation conventions commonly used in the mendeleev package (for printing element properties, isotope masses, atomic weights and other numeric material/chemical properties in CLI, tables and web views).", "tools": [{"function": {"description": "mendeleev.models.with_uncertainty formats a numeric value together with its measurement uncertainty into a human-readable string using scientific notation conventions commonly used in the mendeleev package (for printing element properties, isotope masses, atomic weights and other numeric material/chemical properties in CLI, tables and web views).\n", "name": "mendeleev_models_with_uncertainty", "parameters": {"properties": {"value": {"type": "float", "description": "The measured or computed numeric value to format. In the source code this is treated as a float but the function also explicitly accepts None for the case where no value is available; if value is None and uncertainty is not None the function will raise an exception when attempting to format. The value is the primary quantity shown to users in periodic-table outputs, reports and visualizations.", "default": ""}, "uncertainty": {"type": "float", "description": "The absolute uncertainty of the value, given as a float. The function accepts None to indicate an unknown/absent uncertainty (in which case the function falls back to fixed-point formatting using the digits parameter). If uncertainty is 0.0 the function treats the quantity as exact and returns a fixed-point representation. If uncertainty is a positive non-zero float the function computes the number of significant digits to display using scientific rounding; if uncertainty is negative (other than -0.0) a ValueError from the underlying math.log10 call will be raised. This parameter represents measurement or tabulated uncertainty associated with element or isotope properties in mendeleev data views.", "default": ""}, "digits": {"type": "integer", "description": "The number of digits after the decimal point to print when uncertainty is None or equals 0.0. The default is 5. This parameter must be an integer appropriate for Python string-format precision; negative values or non-integer types will raise a ValueError or TypeError from the formatting operation. In mendeleev contexts this controls fallback display precision for properties that lack an associated uncertainty.", "default": 5}}, "required": ["value", "uncertainty", "digits"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing numeric property display across a mixed isotope/element dataset coming from multiple instruments. Each record has a nominal value and an uncertainty field that may be missing or effectively unusable. Apply this formatting protocol: for records with a strictly positive, finite absolute uncertainty, format using mendeleev-style value±uncertainty. Otherwise (uncertainty missing/NULL, non-finite, or not strictly positive), format the value without ± using a fixed decimal fallback where the number of digits is determined by magnitude: use 4 digits after the decimal when the absolute value is >= 1e-3, and 6 digits after the decimal when the absolute value is < 1e-3. Process this raw cohort: (A) 55.845 with uncertainty 0.003; (B) 55.845 with uncertainty NULL; (C) 35.453 with uncertainty 0.002; (D) 55.845 with uncertainty 0.0023.", "answers": "[{\"name\":\"mendeleev_models_with_uncertainty\",\"arguments\":{\"value\":55.845,\"uncertainty\":0.003}},{\"name\":\"mendeleev_models_with_uncertainty\",\"arguments\":{\"value\":55.845,\"uncertainty\":null,\"digits\":4}},{\"name\":\"mendeleev_models_with_uncertainty\",\"arguments\":{\"value\":35.453,\"uncertainty\":0.002}},{\"name\":\"mendeleev_models_with_uncertainty\",\"arguments\":{\"value\":55.845,\"uncertainty\":0.0023}}]"}
{"func_name": "molmass_from_oligo", "func_desc": "Return the chemical formula for a polymer composed of unmodified (deoxy)nucleotides derived from a DNA or RNA sequence. This function constructs a Hill-notation style chemical formula string that represents the polymer used in molmass molecular-mass and composition calculations. Each strand produced by this function includes a 5' monophosphate; single-stranded polymers have one appended water unit (H2O) and double-stranded polymers have two appended water units ((H2O)2) as part of the returned grouped formula. The resulting string is suitable for feeding into molmass.Formula or other molmass parsers to compute average/monoisotopic mass, elemental composition, and isotopic spectra. The function also accepts and preserves an optional ionic charge suffix in the input sequence (for example \"G_2+\"), which is parsed and appended to the returned formula.", "tools": [{"function": {"description": "Return the chemical formula for a polymer composed of unmodified (deoxy)nucleotides derived from a DNA or RNA sequence. This function constructs a Hill-notation style chemical formula string that represents the polymer used in molmass molecular-mass and composition calculations. Each strand produced by this function includes a 5' monophosphate; single-stranded polymers have one appended water unit (H2O) and double-stranded polymers have two appended water units ((H2O)2) as part of the returned grouped formula. The resulting string is suitable for feeding into molmass.Formula or other molmass parsers to compute average/monoisotopic mass, elemental composition, and isotopic spectra. The function also accepts and preserves an optional ionic charge suffix in the input sequence (for example \"G_2+\"), which is parsed and appended to the returned formula.\n", "name": "molmass_from_oligo", "parameters": {"properties": {"sequence": {"type": "string", "description": "DNA or RNA sequence for the polymer. Whitespace in the sequence is ignored before parsing. A trailing ionic charge suffix may be included in the sequence (examples in the package use forms such as \"_2+\"); the function uses the package's charge-parsing helper to extract and reapply that charge to the returned formula. The sequence is interpreted using the nucleotide sets appropriate for the selected dtype; invalid or unrecognized characters will cause the underlying parsing routines to raise an exception.", "default": ""}, "dtype": {"type": "string", "description": "Nucleic acid sequence type, case-insensitive. One of 'ssdna', 'dsdna', 'ssrna', or 'dsrna'. This parameter determines two orthogonal choices: whether to use deoxyribonucleotide monomer formulas (for 'ssdna' and 'dsdna') or ribonucleotide monomer formulas (for 'ssrna' and 'dsrna'), and whether to produce a single-stranded polymer (prefix 'ss') or a double-stranded polymer (prefix 'ds'). For double-stranded types the complementary strand is generated and the final formula groups both strands together and appends two water units.", "default": "ssdna"}}, "required": ["sequence", "dtype"], "type": "any"}}, "type": "function"}], "query": "We’re validating oligo-to-formula generation against a mixed strand-state RNA panel where strand state is determined from the construct itself rather than being provided. Given these raw RNA inputs (each may include an ionic charge suffix that must be preserved exactly): [\"AUGCUGA_3-\", \"AUGCUGA_2-\", \"AUGC_2-\"]. Treat an entry as a duplex construct if (after stripping any charge suffix) its RNA sequence length is at least 6 nt; otherwise treat it as single-stranded. For each entry, generate the molmass-compatible Hill-notation polymer chemical formula for an unmodified RNA polymer that includes a 5′ monophosphate and the correct terminal hydration grouping for the inferred strand state ((H2O)2 for duplex, (H2O) for single-stranded), while preserving any ionic charge suffix exactly as provided.", "answers": "[{\"name\":\"molmass_from_oligo\",\"arguments\":{\"sequence\":\"AUGCUGA_3-\",\"dtype\":\"dsrna\"}},{\"name\":\"molmass_from_oligo\",\"arguments\":{\"sequence\":\"AUGCUGA_2-\",\"dtype\":\"dsrna\"}},{\"name\":\"molmass_from_oligo\",\"arguments\":{\"sequence\":\"AUGC_2-\",\"dtype\":\"ssrna\"}}]"}
{"func_name": "molmass_join_charge", "func_desc": "Return a chemical formula string with an ionic charge suffix suitable for use\n    in molmass calculations, web output, and textual representations of charged\n    species.", "tools": [{"function": {"description": "Return a chemical formula string with an ionic charge suffix suitable for use\nin molmass calculations, web output, and textual representations of charged\nspecies.", "name": "molmass_join_charge", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula without charge. This is the core formula\nstring describing the molecular composition (for example \"C8H10N4O2\")\nthat will be augmented with an ionic charge notation. The string is\ninserted either inside square brackets (when separator is the empty\nstring) or left as-is (when a non-empty separator such as '_' is\nprovided). The function does not modify element ordering or validate\nchemical correctness of the formula; it only modifies the textual\nrepresentation to include the charge.", "default": ""}, "charge": {"type": "integer", "description": "Charge number of the species. Zero indicates a neutral\nspecies and leaves the input formula unchanged. Non-zero integers are\nformatted as a charge suffix where the sign character is '+' for\npositive charges and '-' for negative charges. The magnitude '1' is\nomitted when formatting a single charge (e.g. 1 -> '+', -1 -> '-'),\nwhile magnitudes greater than 1 are shown before the sign\n(e.g. 2 -> '2+', -2 -> '2-'). This integer value is used directly to\nproduce the textual suffix appended to the formula and is the standard\nionic notation used elsewhere in the molmass library for charged ions.", "default": ""}, "separator": {"type": "string", "description": "Character separating the formula from the formatted charge.\nAllowed values are the empty string '' (the default) or '_' as used in\nmolmass code and outputs. If separator is the empty string, the input\nformula is wrapped in square brackets and the formatted charge is\nappended immediately after the closing bracket (for example\n'[Formula]2+'); if separator is '_' the separator is inserted between\nthe formula and the formatted charge without adding brackets\n(for example 'Formula_2-'). The default empty string produces the\nbracketed representation commonly used for ionic formulas.", "default": ""}}, "required": ["formula", "charge", "separator"], "type": "any"}}, "type": "function"}], "query": "We’re cleaning up a mixed anion panel exported from three instruments where the charge state is sometimes implicit. Each record is a base formula plus a measured integer charge (negative for anions). Generate standardized strings only for species that are chemically plausible oxyanions (formula begins with S, P, N, or C and contains at least two oxygen atoms) and that are actually charged (charge ≠ 0). For each qualifying record, emit two textual representations: (1) a molmass-ready string using the default bracketed style, and (2) a report-label variant using the molmass-style underscore separator between the formula and charge. Raw panel: [('SO4', -2), ('SO4', 0), ('S2O3', -2), ('NO3', -1), ('ClO4', -1), ('PO4', -3), ('CO3', -2), ('HCO3', -1), ('SO3', -2), ('SO4', -1)].", "answers": "[{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO4\",\"charge\":-2}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO4\",\"charge\":-2,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"S2O3\",\"charge\":-2}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"S2O3\",\"charge\":-2,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"NO3\",\"charge\":-1}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"NO3\",\"charge\":-1,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"PO4\",\"charge\":-3}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"PO4\",\"charge\":-3,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"CO3\",\"charge\":-2}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"CO3\",\"charge\":-2,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO3\",\"charge\":-2}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO3\",\"charge\":-2,\"separator\":\"_\"}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO4\",\"charge\":-1}},{\"name\":\"molmass_join_charge\",\"arguments\":{\"formula\":\"SO4\",\"charge\":-1,\"separator\":\"_\"}}]"}
{"func_name": "molmass_split_charge", "func_desc": "molmass.split_charge: Extract the chemical formula with any trailing ionic charge notation removed and return the cleaned formula plus the integer net charge. This utility is used throughout the molmass library to accept user-supplied chemical formulas that include appended ionic charge annotations (for example \"H2O+\", \"[Fe(CN)6]3-\", \"M_2+\" or \"[[M]]2-\") so the core formula parsing and mass calculations operate on the neutral/formula part while the ionic charge is handled separately by mass/charge computations.", "tools": [{"function": {"description": "molmass.split_charge: Extract the chemical formula with any trailing ionic charge notation removed and return the cleaned formula plus the integer net charge. This utility is used throughout the molmass library to accept user-supplied chemical formulas that include appended ionic charge annotations (for example \"H2O+\", \"[Fe(CN)6]3-\", \"M_2+\" or \"[[M]]2-\") so the core formula parsing and mass calculations operate on the neutral/formula part while the ionic charge is handled separately by mass/charge computations.\n", "name": "molmass_split_charge", "parameters": {"properties": {"formula": {"type": "string", "description": "Chemical formula string that may include an appended ionic charge notation. The string may contain bracketed formulas (leading and trailing square brackets), an underscore '_' delimiter separating a numeric count from a trailing sign, explicit numeric charge counts, or repeated '+' and '-' characters. This function expects the common molmass and RDKit-style charge annotations appended to the formula (examples: 'Formula', 'Formula+', 'Formula+2', '[Formula]2-', 'Formula_2-'). The function does not validate chemical correctness of the formula content (element symbols, counts, or parentheses); it only analyzes and removes trailing charge notation.", "default": ""}}, "required": ["formula"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a messy batch of user-entered precursor annotations from an MS pipeline. Each entry may include bracketed coordination chemistry and may append a molmass-style trailing ionic charge token. As a QC step before neutral-mass parsing, run the split-charge preprocessing only for candidates that appear to be explicitly ion-annotated (i.e., they end with a charge sign, optionally preceded by an integer magnitude). Preserve the original batch order of the surviving candidates and include technical replicates when the exact same ion string appears multiple times. Raw batch: [\"H2O+\", \"[Fe(CN)6]3-\", \"CH3COO-\", \"C6H12O6\", \"NaCl\", \"[Fe(CN)6]3-\", \"M_2+\", \"[[M]]2-\", \"[Fe(CN)6]3-\"].", "answers": "[{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"H2O+\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"[Fe(CN)6]3-\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"CH3COO-\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"[Fe(CN)6]3-\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"M_2+\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"[[M]]2-\"}},{\"name\":\"molmass_split_charge\",\"arguments\":{\"formula\":\"[Fe(CN)6]3-\"}}]"}
{"func_name": "molmass_web_webbrowser", "func_desc": "molmass.web.webbrowser: Open a URL in the system default web browser after a short delay.\n    \n    This helper function is used by the molmass web application to launch the web-based user interface (for example when running the package with the web server option). It schedules a background timer that calls the standard library webbrowser.open() on the provided URL after the specified delay. The short delay is intended to give a locally started web server time to bind to its port before the browser attempts to connect.", "tools": [{"function": {"description": "molmass.web.webbrowser: Open a URL in the system default web browser after a short delay.\n\nThis helper function is used by the molmass web application to launch the web-based user interface (for example when running the package with the web server option). It schedules a background timer that calls the standard library webbrowser.open() on the provided URL after the specified delay. The short delay is intended to give a locally started web server time to bind to its port before the browser attempts to connect.", "name": "molmass_web_webbrowser", "parameters": {"properties": {"url": {"type": "string", "description": "URL to open in the web browser. In the molmass context this is typically the local address served by the molmass web application (for example \"http://127.0.0.1:5000/\"), but any valid URL string accepted by the Python standard library webbrowser.open() may be supplied. The function will pass this string verbatim to webbrowser.open(); it does not validate or canonicalize the URL.", "default": ""}, "delay": {"type": "float", "description": "Delay in seconds before opening the web browser. A non-negative float; the default is 1.0 which is used to allow the molmass Flask server (if used) to start. The function schedules the open call with threading.Timer and returns immediately; the delay only affects when the background thread calls webbrowser.open().", "default": 1.0}}, "required": ["url", "delay"], "type": "any"}}, "type": "function"}], "query": "For the MolMass web-UI smoke test, we’re cycling through a mixed set of candidate local endpoints produced by today’s dev session: [\"http://127.0.0.1:5000/\", \"http://localhost:5000/\", \"http://127.0.0.1:5000\", \"http://127.0.0.1:8000/\", \"http://0.0.0.0:5000/\", \"http://127.0.0.1:5000/?_ts=rep1\", \"http://127.0.0.1:5001/\", \"http://[::1]:5000/\"]. Treat this as a two-replicate launch validation where only loopback-hosted endpoints using port 5000 and a proper root-path form are considered valid for automated UI launch. Apply a dynamic warm-up rule per replicate: use a 2.5 s delay for the baseline root endpoint, and for any other valid endpoint in the set, use a delay equal to baseline + 0.5 s. Launch the browser for each valid endpoint once per replicate under those rules.", "answers": "[{\"name\":\"molmass_web_webbrowser\",\"arguments\":{\"url\":\"http://127.0.0.1:5000/\",\"delay\":2.5}},{\"name\":\"molmass_web_webbrowser\",\"arguments\":{\"url\":\"http://localhost:5000/\",\"delay\":3.0}},{\"name\":\"molmass_web_webbrowser\",\"arguments\":{\"url\":\"http://127.0.0.1:5000/\",\"delay\":2.5}},{\"name\":\"molmass_web_webbrowser\",\"arguments\":{\"url\":\"http://localhost:5000/\",\"delay\":3.0}}]"}
{"func_name": "monai_auto3dseg_utils_list_to_python_fire_arg_str", "func_desc": "Convert a Python list into a single argument string formatted for use with python-fire.\n    \n    This utility is part of MONAI's auto3dseg utilities and is used in automated 3D segmentation workflows to serialize a list of values (for example, device indices, file paths, or other hyperparameter lists commonly used in medical imaging experiments) into one command-line argument that python-fire can receive. The function obtains each element's textual form via str(), joins those textual elements with commas, and wraps the whole result in single quotes so it can be passed as a single shell/CLI token to python-fire.", "tools": [{"function": {"description": "Convert a Python list into a single argument string formatted for use with python-fire.\n\nThis utility is part of MONAI's auto3dseg utilities and is used in automated 3D segmentation workflows to serialize a list of values (for example, device indices, file paths, or other hyperparameter lists commonly used in medical imaging experiments) into one command-line argument that python-fire can receive. The function obtains each element's textual form via str(), joins those textual elements with commas, and wraps the whole result in single quotes so it can be passed as a single shell/CLI token to python-fire.", "name": "monai_auto3dseg_utils_list_to_python_fire_arg_str", "parameters": {"properties": {"args": {"type": "array", "items": {"type": "float"}, "description": "The list of values to convert into a python-fire argument string. Each element is converted by calling str(element). The input list is not modified by this function. Typical usage in the MONAI auto3dseg domain includes serializing lists of numeric IDs, file paths, or configuration tokens so they can be passed through a python-fire CLI; the function does not restrict the element types beyond requiring that they be stringable via str(). There are no default values. Side effects: none. The function does not escape commas or quotes inside individual element strings.", "default": ""}}, "required": ["args"], "type": "any"}}, "type": "function"}], "query": "We’re assembling python-fire CLI tokens for an Auto3DSeg launch script from a messy set of scheduler-provided GPU manifests and run annotations. Each cohort is a single list that must be serialized into one shell-safe python-fire argument (elements converted with str(), comma-joined, wrapped in single quotes).\n\nCohort A (training): Start from the raw GPU manifest `[0, 1, 1, 3, -1, \"3\", \"gpu2\"]`. Keep only unique, non-negative integer device indices that are explicitly present as integers (ignore string-coded entries and any negative/sentinel values), then append two experiment tags: `\"liver_seg_v2\"` and `\"ablation_study\"`.\n\nCohort B (baseline): Use the same cleaned GPU index list as Cohort A, but without any tags.\n\nCohort C (stress-test): Start from `[0, 1, 3, 5, \"5\", 999]`. Keep only unique, non-negative integer device indices within the realistic single-node range (0–7 inclusive), preserving ascending order.\n\nGenerate the python-fire compatible single-argument strings for these three cohorts.", "answers": "[{\"name\":\"monai_auto3dseg_utils_list_to_python_fire_arg_str\",\"arguments\":{\"args\":[0,1,3,\"liver_seg_v2\",\"ablation_study\"]}},{\"name\":\"monai_auto3dseg_utils_list_to_python_fire_arg_str\",\"arguments\":{\"args\":[0,1,3]}},{\"name\":\"monai_auto3dseg_utils_list_to_python_fire_arg_str\",\"arguments\":{\"args\":[0,1,3,5]}}]"}
{"func_name": "monai_data_meta_obj_set_track_meta", "func_desc": "monai.data.meta_obj.set_track_meta sets whether MONAI associates metadata with data objects used throughout the MONAI medical imaging data pipeline.\n    \n    This function configures a module-level boolean flag that controls whether metadata (for example, spatial affine, voxel spacing, orientation, original filename, channel dimension, and other dataset-specific attributes commonly preserved in medical imaging workflows) is tracked and attached to data objects by using MONAI's MetaObj subclasses. When tracking is enabled, MONAI returns enhanced objects that carry metadata alongside the raw data; when tracking is disabled, MONAI returns standard data containers (for example, torch.Tensor and numpy.ndarray) with empty or no metadata. By default this flag is True; most users should leave metadata tracking enabled to preserve spatial and provenance information required for preprocessing, resampling, visualization, and evaluation in healthcare imaging workflows.", "tools": [{"function": {"description": "monai.data.meta_obj.set_track_meta sets whether MONAI associates metadata with data objects used throughout the MONAI medical imaging data pipeline.\n\nThis function configures a module-level boolean flag that controls whether metadata (for example, spatial affine, voxel spacing, orientation, original filename, channel dimension, and other dataset-specific attributes commonly preserved in medical imaging workflows) is tracked and attached to data objects by using MONAI's MetaObj subclasses. When tracking is enabled, MONAI returns enhanced objects that carry metadata alongside the raw data; when tracking is disabled, MONAI returns standard data containers (for example, torch.Tensor and numpy.ndarray) with empty or no metadata. By default this flag is True; most users should leave metadata tracking enabled to preserve spatial and provenance information required for preprocessing, resampling, visualization, and evaluation in healthcare imaging workflows.", "name": "monai_data_meta_obj_set_track_meta", "parameters": {"properties": {"val": {"type": "boolean", "description": "Flag indicating whether to enable metadata tracking. If True, metadata will be associated with data objects via MetaObj subclasses and related MONAI APIs that construct enhanced objects. If False, MONAI will return standard data objects without metadata. This parameter directly sets the module-global _TRACK_META variable inside monai.data.meta_obj and thus affects subsequent MONAI data creation and transformation functions that consult that flag.", "default": ""}}, "required": ["val"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed MONAI pipeline across three imaging cohorts with heterogeneous spatial/provenance requirements: (1) a CT intensity-normalization cohort where downstream consumers never read affine/spacing/orientation and we want maximal throughput; (2) a 3D brain MRI segmentation cohort that must be exportable to inference with spatially consistent outputs, so metadata must be preserved for orientation/spacing-aware post-processing; (3) an exploratory 3D MRI preprocessing experiment where we are stress-testing augmentations and logging only tensor statistics (no provenance needed). For each cohort run, set MONAI’s global MetaObj tracking flag according to whether the cohort’s downstream steps require spatial metadata to remain attached to tensors/arrays.", "answers": "[{\"name\":\"monai_data_meta_obj_set_track_meta\",\"arguments\":{\"val\":false}},{\"name\":\"monai_data_meta_obj_set_track_meta\",\"arguments\":{\"val\":true}},{\"name\":\"monai_data_meta_obj_set_track_meta\",\"arguments\":{\"val\":false}}]"}
{"func_name": "monai_data_utils_collate_meta_tensor", "func_desc": "Collate a sequence of meta-tensor containers into a batched meta-tensor structure suitable for MONAI data pipelines.\n    \n    This function is used in MONAI (a PyTorch-based medical imaging framework) to assemble per-sample data items produced by a Dataset into a single batch that preserves both tensor data and associated metadata (for example spatial affine, origin, orientation stored in MONAI MetaTensor/MetaObj objects). It recursively inspects the first element of the provided sequence to determine how to collate: if elements are MONAI MetaObj instances, it delegates to collate_meta_tensor_fn to produce a single batched meta-tensor; if elements are mapping/dictionary-like it builds a dictionary whose values are the collated results for each key; if elements are tuples/lists it returns a list of collated results for each position; otherwise it falls back to torch.utils.data.dataloader.default_collate to produce a conventional tensor batch. This behavior enables DataLoader-style batching while preserving domain-specific metadata required for healthcare imaging workflows and downstream models.", "tools": [{"function": {"description": "Collate a sequence of meta-tensor containers into a batched meta-tensor structure suitable for MONAI data pipelines.\n\nThis function is used in MONAI (a PyTorch-based medical imaging framework) to assemble per-sample data items produced by a Dataset into a single batch that preserves both tensor data and associated metadata (for example spatial affine, origin, orientation stored in MONAI MetaTensor/MetaObj objects). It recursively inspects the first element of the provided sequence to determine how to collate: if elements are MONAI MetaObj instances, it delegates to collate_meta_tensor_fn to produce a single batched meta-tensor; if elements are mapping/dictionary-like it builds a dictionary whose values are the collated results for each key; if elements are tuples/lists it returns a list of collated results for each position; otherwise it falls back to torch.utils.data.dataloader.default_collate to produce a conventional tensor batch. This behavior enables DataLoader-style batching while preserving domain-specific metadata required for healthcare imaging workflows and downstream models.", "name": "monai_data_utils_collate_meta_tensor", "parameters": {"properties": {"batch": {"type": "array", "items": {"type": "float"}, "description": "A list (sequence) of per-sample objects to collate into a batch. Each element in this list is expected to be one of:\n- A MetaObj instance (MONAI metadata wrapper): when the first element is a MetaObj, the function will call collate_meta_tensor_fn(batch) to combine all MetaObj items into a single batched meta-tensor that preserves per-sample metadata (affine, spacing, keys used in imaging pipelines).\n- A Mapping (e.g., dict): when the first element is mapping-like, the function assumes all elements share the same keys and returns a dict where each key maps to the result of recursively collating the list of values for that key. Note: if a later element is missing a key present in the first element, a KeyError will be raised when attempting to access d[k].\n- A tuple or list: when the first element is a tuple/list, the function returns a list whose i-th entry is the result of recursively collating the i-th entries from each element of batch. This preserves ordered container structures commonly used to store (image, label) pairs or multi-modal inputs.\n- Any other object type: if none of the above cases match (and no MetaObj is found during the recursive checks), the function falls back to torch.utils.data.dataloader.default_collate(batch) to produce a standard PyTorch-style batched tensor or nested structure.\nThe function requires that the provided batch is a Sequence; if the argument is not a Sequence the function raises NotImplementedError. The function performs recursive inspection starting from the first element and recurses into mappings and sequences to locate MetaObj instances; if it reaches leaf objects without finding MetaObj it uses default_collate. No mutation of the input list is performed by this function, but the collated outputs may be new tensors/containers allocated in memory.", "default": ""}}, "required": ["batch"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our MONAI DataLoader collation in a more realistic mixed-quality ingestion setting. Below are three acquisition cohorts, each containing multiple candidate per-sample dictionaries coming out of a Dataset. Build a batched structure using the meta-tensor-aware collation, but only for samples that meet all of the following QC gates (apply gates per cohort independently):\n\nQC gates:\n1) The sample must have an `image` field.\n2) Spatial metadata must be usable: `meta.spacing` (or `image.meta.spacing` for nested `{data, meta}` containers) must contain only positive numeric values.\n3) For segmentation-style samples with nested `{data, meta}` containers, `image.meta.spacing` must be identical to `label.meta.spacing`.\n4) `patient_id` (or `patient_info.id`) must be a non-empty string.\n\nAfter QC, collate the remaining samples per cohort into a single batch, preserving nested structures and metadata alignment.\n\nCohort A (2D toy classification; simple meta; 3 candidates):\n- sample 1: {\"image\": [[1.0, 2.0],[3.0, 4.0]], \"label\": 0, \"meta\": {\"patient_id\": \"P001\", \"spacing\": [1.0, 1.0]}}\n- sample 2: {\"image\": [[5.0, 6.0],[7.0, 8.0]], \"label\": 1, \"meta\": {\"patient_id\": \"P002\", \"spacing\": [1.0, 1.0]}}\n- sample 3: {\"image\": [[9.0, 10.0],[11.0, 12.0]], \"label\": 1, \"meta\": {\"patient_id\": \"P003\", \"spacing\": [1.0, -1.0]}}\n\nCohort B (3D brain MRI segmentation-style nested containers with affine + patient_info; 3 candidates):\n- sample 1: {\"image\": {\"data\": [[[[0.1, 0.2], [0.3, 0.4]]]], \"meta\": {\"spacing\": [1.0, 1.0, 1.2], \"affine\": [[1.0, 0.0, 0.0, 0.0],[0.0, 1.0, 0.0, 0.0],[0.0, 0.0, 1.0, 0.0],[0.0, 0.0, 0.0, 1.0]]}}, \"label\": {\"data\": [[[[0, 1], [1, 0]]]], \"meta\": {\"spacing\": [1.0, 1.0, 1.2]}}, \"patient_info\": {\"id\": \"P001\", \"age\": 65}}\n- sample 2: {\"image\": {\"data\": [[[[0.5, 0.6], [0.7, 0.8]]]], \"meta\": {\"spacing\": [0.9, 0.9, 1.0], \"affine\": [[1.0, 0.0, 0.0, 1.0],[0.0, 1.0, 0.0, 2.0],[0.0, 0.0, 1.0, 3.0],[0.0, 0.0, 0.0, 1.0]]}}, \"label\": {\"data\": [[[[1, 1], [0, 0]]]], \"meta\": {\"spacing\": [0.9, 0.9, 1.0]}}, \"patient_info\": {\"id\": \"P002\", \"age\": 70}}\n- sample 3: {\"image\": {\"data\": [[[[0.9, 1.0], [1.1, 1.2]]]], \"meta\": {\"spacing\": [1.0, 1.0, 1.2], \"affine\": [[1.0, 0.0, 0.0, 0.0],[0.0, 1.0, 0.0, 0.0],[0.0, 0.0, 1.0, 0.0],[0.0, 0.0, 0.0, 1.0]]}}, \"label\": {\"data\": [[[[0, 0], [1, 1]]]], \"meta\": {\"spacing\": [1.0, 1.0, 1.0]}}, \"patient_info\": {\"id\": \"P003\", \"age\": 58}}\n\nCohort C (single-channel image tensors with 2D label maps + simple meta; 3 candidates):\n- sample 1: {\"image\": [[[0.0, 1.0],[2.0, 3.0]]], \"label\": [[0, 1],[1, 0]], \"meta\": {\"patient_id\": \"P001\", \"spacing\": [1.0, 1.0, 1.5]}}\n- sample 2: {\"image\": [[[4.0, 5.0],[6.0, 7.0]]], \"label\": [[1, 1],[0, 0]], \"meta\": {\"patient_id\": \"P002\", \"spacing\": [1.0, 1.0, 1.5]}}\n- sample 3: {\"image\": [[[8.0, 9.0],[10.0, 11.0]]], \"label\": [[0, 0],[1, 1]], \"meta\": {\"patient_id\": \"\", \"spacing\": [1.0, 1.0, 1.5]}}\n\nProduce one collated/batched output per cohort from only the QC-passing samples using MONAI’s meta-tensor-aware collation behavior.", "answers": "[{\"name\":\"monai_data_utils_collate_meta_tensor\",\"arguments\":{\"batch\":[{\"image\":[[1.0,2.0],[3.0,4.0]],\"label\":0,\"meta\":{\"patient_id\":\"P001\",\"spacing\":[1.0,1.0]}},{\"image\":[[5.0,6.0],[7.0,8.0]],\"label\":1,\"meta\":{\"patient_id\":\"P002\",\"spacing\":[1.0,1.0]}}]}},{\"name\":\"monai_data_utils_collate_meta_tensor\",\"arguments\":{\"batch\":[{\"image\":{\"data\":[[[[0.1,0.2],[0.3,0.4]]]],\"meta\":{\"spacing\":[1.0,1.0,1.2],\"affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]}},\"label\":{\"data\":[[[[0,1],[1,0]]]],\"meta\":{\"spacing\":[1.0,1.0,1.2]}},\"patient_info\":{\"id\":\"P001\",\"age\":65}},{\"image\":{\"data\":[[[[0.5,0.6],[0.7,0.8]]]],\"meta\":{\"spacing\":[0.9,0.9,1.0],\"affine\":[[1.0,0.0,0.0,1.0],[0.0,1.0,0.0,2.0],[0.0,0.0,1.0,3.0],[0.0,0.0,0.0,1.0]]}},\"label\":{\"data\":[[[[1,1],[0,0]]]],\"meta\":{\"spacing\":[0.9,0.9,1.0]}},\"patient_info\":{\"id\":\"P002\",\"age\":70}}]}},{\"name\":\"monai_data_utils_collate_meta_tensor\",\"arguments\":{\"batch\":[{\"image\":[[[0.0,1.0],[2.0,3.0]]],\"label\":[[0,1],[1,0]],\"meta\":{\"patient_id\":\"P001\",\"spacing\":[1.0,1.0,1.5]}},{\"image\":[[[4.0,5.0],[6.0,7.0]]],\"label\":[[1,1],[0,0]],\"meta\":{\"patient_id\":\"P002\",\"spacing\":[1.0,1.0,1.5]}}]}}]"}
{"func_name": "monai_data_utils_remove_extra_metadata", "func_desc": "Remove extra metadata keys from a MONAI metadata dictionary in-place.\n    \n    This function is part of the monai.data.utils utilities and is used in MONAI preprocessing and data handling workflows to remove keys that are considered \"extra metadata\" according to MONAI conventions. It determines which keys to remove by calling get_extra_metadata_keys(), then delegates the actual removal to remove_keys(data=meta, keys=keys). Typical uses include cleaning a sample's metadata before serialization, logging, or passing the metadata into training/evaluation components so that large, temporary, or implementation-specific entries do not propagate through the pipeline.", "tools": [{"function": {"description": "Remove extra metadata keys from a MONAI metadata dictionary in-place.\n\nThis function is part of the monai.data.utils utilities and is used in MONAI preprocessing and data handling workflows to remove keys that are considered \"extra metadata\" according to MONAI conventions. It determines which keys to remove by calling get_extra_metadata_keys(), then delegates the actual removal to remove_keys(data=meta, keys=keys). Typical uses include cleaning a sample's metadata before serialization, logging, or passing the metadata into training/evaluation components so that large, temporary, or implementation-specific entries do not propagate through the pipeline.", "name": "monai_data_utils_remove_extra_metadata", "parameters": {"properties": {"meta": {"type": "any", "description": "A dictionary containing metadata for a medical imaging sample or batch, following MONAI's metadata conventions. This mapping is modified in-place: any top-level keys in meta that match the set returned by get_extra_metadata_keys() will be removed from this same dict object. The function does not return a new dict. The values and nested structures of remaining keys are preserved. If meta is empty, the function performs no changes. The caller should provide a dict; passing an object that is not a dict may raise a TypeError or other exception from underlying operations.", "default": ""}}, "required": ["meta"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed-provenance MONAI dataset for long-term reproducibility where only *export-ready* samples should have transient MONAI runtime state stripped before JSON serialization. Treat the following three metadata dicts as independent replicates, but apply hygiene conditionally: run the MONAI in-place extra-metadata scrub only for samples that show evidence of having passed through a runtime pipeline (e.g., they carry transform/application trace fields or patch/window extraction bookkeeping). Samples that look like “raw acquisition-only” provenance (geometry + identifiers without pipeline trace) should remain unchanged. Apply this rule to these per-sample metadata objects: (1) {\"filename_or_obj\":\"patient_001_ct.nii.gz\",\"spacing\":[1.0,1.0,2.0],\"original_affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,2.0,0.0],[0.0,0.0,0.0,1.0]],\"patient_id\":\"P001\",\"series_uid\":\"1.2.840.113619.2.55.3.604688432.123.1599769480.467\",\"transform_history\":[{\"name\":\"LoadImaged\",\"version\":\"1.3.0\"},{\"name\":\"Spacingd\",\"pixdim\":[1.0,1.0,2.0]}],\"cache_source\":\"/tmp/monai_cache/patient_001.pkl\",\"loader_info\":{\"worker_id\":3,\"epoch\":5},\"intensity_stats\":{\"mean\":42.7,\"std\":12.3}}; (2) {\"filename_or_obj\":\"/data/Task01/imagesTr/patient001.nii.gz\",\"affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]],\"spacing\":[1.25,1.25,2.5],\"original_channel_dim\":-1,\"patch_index\":[0,64,64],\"applied_operations\":[{\"class\":\"Spacingd\",\"orig_spacing\":[1.0,1.0,1.0],\"new_spacing\":[1.25,1.25,2.5]}],\"extra_info\":{\"note\":\"debug run\"}}; (3) {\"filename_or_obj\":\"/data/brain_t1w_001.nii.gz\",\"affine\":[[1.0,0.0,0.0,-96.0],[0.0,1.0,0.0,-132.0],[0.0,0.0,1.0,-78.0],[0.0,0.0,0.0,1.0]],\"spatial_shape\":[192,256,160],\"original_channel_dim\":-1,\"patch_index\":[0,2,1],\"extra_info\":{\"loaded_by\":\"ITKReader\",\"tmp_cache_id\":\"b7c2\"}}.", "answers": "[{\"name\":\"monai_data_utils_remove_extra_metadata\",\"arguments\":{\"meta\":{\"filename_or_obj\":\"patient_001_ct.nii.gz\",\"spacing\":[1.0,1.0,2.0],\"original_affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,2.0,0.0],[0.0,0.0,0.0,1.0]],\"patient_id\":\"P001\",\"series_uid\":\"1.2.840.113619.2.55.3.604688432.123.1599769480.467\",\"transform_history\":[{\"name\":\"LoadImaged\",\"version\":\"1.3.0\"},{\"name\":\"Spacingd\",\"pixdim\":[1.0,1.0,2.0]}],\"cache_source\":\"/tmp/monai_cache/patient_001.pkl\",\"loader_info\":{\"worker_id\":3,\"epoch\":5},\"intensity_stats\":{\"mean\":42.7,\"std\":12.3}}}},{\"name\":\"monai_data_utils_remove_extra_metadata\",\"arguments\":{\"meta\":{\"filename_or_obj\":\"/data/Task01/imagesTr/patient001.nii.gz\",\"affine\":[[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]],\"spacing\":[1.25,1.25,2.5],\"original_channel_dim\":-1,\"patch_index\":[0,64,64],\"applied_operations\":[{\"class\":\"Spacingd\",\"orig_spacing\":[1.0,1.0,1.0],\"new_spacing\":[1.25,1.25,2.5]}],\"extra_info\":{\"note\":\"debug run\"}}}},{\"name\":\"monai_data_utils_remove_extra_metadata\",\"arguments\":{\"meta\":{\"filename_or_obj\":\"/data/brain_t1w_001.nii.gz\",\"affine\":[[1.0,0.0,0.0,-96.0],[0.0,1.0,0.0,-132.0],[0.0,0.0,1.0,-78.0],[0.0,0.0,0.0,1.0]],\"spatial_shape\":[192,256,160],\"original_channel_dim\":-1,\"patch_index\":[0,2,1],\"extra_info\":{\"loaded_by\":\"ITKReader\",\"tmp_cache_id\":\"b7c2\"}}}}]"}
{"func_name": "monai_data_utils_remove_keys", "func_desc": "monai.data.utils.remove_keys removes one or more keys from a mapping in-place without returning a value.\n    \n    This utility function is intended for MONAI data-processing workflows (for example, preprocessing and transform pipelines in medical imaging) where sample dictionaries carry images, labels, and metadata and specific keys need to be discarded before further processing or saving. The function iterates over the provided list of keys and calls dict.pop(key, None) for each key so that missing keys are ignored silently. Because it modifies the input mapping directly, callers should not expect a new dictionary or any return value.", "tools": [{"function": {"description": "monai.data.utils.remove_keys removes one or more keys from a mapping in-place without returning a value.\n\nThis utility function is intended for MONAI data-processing workflows (for example, preprocessing and transform pipelines in medical imaging) where sample dictionaries carry images, labels, and metadata and specific keys need to be discarded before further processing or saving. The function iterates over the provided list of keys and calls dict.pop(key, None) for each key so that missing keys are ignored silently. Because it modifies the input mapping directly, callers should not expect a new dictionary or any return value.", "name": "monai_data_utils_remove_keys", "parameters": {"properties": {"data": {"type": "any", "description": "The dictionary to be modified. In MONAI this is typically a sample or batch dictionary containing imaging tensors and associated metadata (e.g., image, label, meta_dict). The function operates on this object in-place; no copy is made. If an object that does not implement dict.pop is provided, a runtime exception (for example, AttributeError or TypeError) may be raised by the interpreter.", "default": ""}, "keys": {"type": "array", "items": {"type": "string"}, "description": "An ordered collection of string keys to remove from data. For each key in this list, the function attempts to delete that key from data using data.pop(key, None). If a key is not present in data, it is ignored and no error is raised. The operation is performed in the order of this list; duplicate entries in keys will cause repeated pop attempts but have no additional effect after the first removal.", "default": ""}}, "required": ["data", "keys"], "type": "any"}}, "type": "function"}], "query": "We’re preparing heterogeneous MONAI sample dictionaries for two downstream consumers: (1) a training dataloader that should not carry augmentation byproducts or interactive debugging payloads, and (2) a de-identified imaging-only export where only geometry- and tensor-relevant fields should remain. Treat each sample as a raw product of a transform pipeline: remove any key whose value is a nested mapping that looks like a debugging/provenance bundle (e.g., contains step/note or loaded_from/timestamp-style fields), and also remove any key whose name indicates an intermediate augmentation artifact (e.g., mask/crop artifact keys produced during random spatial ops). For the export sample, additionally enforce de-identification by stripping direct subject identifiers while keeping imaging tensors and acquisition geometry (affine/spacing) intact. Apply this logic in-place to the following two samples: (A) {\"image\":\"tensor_placeholder\",\"label\":\"tensor_label\",\"meta_dict\":{\"spacing\":[1.0,1.0,1.5]},\"temp_crop_mask\":\"tensor_mask\",\"debug_info\":{\"step\":5,\"note\":\"after random flip\"}} and (B) {\"image\":\"ct_scan_tensor\",\"label\":\"tumor_mask_tensor\",\"affine\":\"4x4_matrix\",\"spacing\":[1.0,1.0,2.5],\"patient_id\":\"P001\",\"debug_info\":{\"loaded_from\":\"disk\",\"timestamp\":\"2025-03-01T10:15:00\"}}.", "answers": "[{\"name\":\"monai_data_utils_remove_keys\",\"arguments\":{\"data\":{\"image\":\"tensor_placeholder\",\"label\":\"tensor_label\",\"meta_dict\":{\"spacing\":[1.0,1.0,1.5]},\"temp_crop_mask\":\"tensor_mask\",\"debug_info\":{\"step\":5,\"note\":\"after random flip\"}},\"keys\":[\"debug_info\",\"temp_crop_mask\"]}},{\"name\":\"monai_data_utils_remove_keys\",\"arguments\":{\"data\":{\"image\":\"ct_scan_tensor\",\"label\":\"tumor_mask_tensor\",\"affine\":\"4x4_matrix\",\"spacing\":[1.0,1.0,2.5],\"patient_id\":\"P001\",\"debug_info\":{\"loaded_from\":\"disk\",\"timestamp\":\"2025-03-01T10:15:00\"}},\"keys\":[\"debug_info\",\"patient_id\"]}}]"}
{"func_name": "monai_handlers_utils_stopping_fn_from_metric", "func_desc": "Returns a stopping function that reads a metric value by name from an Ignite Engine's state for use with ignite.handlers.EarlyStopping.\n    \n    This helper is used in MONAI training and evaluation workflows (medical imaging deep learning) to construct the score function required by ignite.handlers.EarlyStopping. MONAI engines (ignite.engine.Engine) commonly populate engine.state.metrics with validation or training metrics (for example \"val_loss\", \"val_dice\") after each epoch or iteration. The callable returned by this function simply accesses engine.state.metrics[metric_name] and returns that value, so the EarlyStopping handler can decide whether to stop based on that metric.", "tools": [{"function": {"description": "Returns a stopping function that reads a metric value by name from an Ignite Engine's state for use with ignite.handlers.EarlyStopping.\n\nThis helper is used in MONAI training and evaluation workflows (medical imaging deep learning) to construct the score function required by ignite.handlers.EarlyStopping. MONAI engines (ignite.engine.Engine) commonly populate engine.state.metrics with validation or training metrics (for example \"val_loss\", \"val_dice\") after each epoch or iteration. The callable returned by this function simply accesses engine.state.metrics[metric_name] and returns that value, so the EarlyStopping handler can decide whether to stop based on that metric.", "name": "monai_handlers_utils_stopping_fn_from_metric", "parameters": {"properties": {"metric_name": {"type": "string", "description": "The name of the metric key to retrieve from engine.state.metrics. This string should match a metric key that MONAI or user code has stored in the Ignite Engine's state.metrics mapping (for example \"val_loss\" or \"val_mean_dice\"). This parameter has no default and must be provided; it determines which metric the returned stopping function will read when called.", "default": ""}}, "required": ["metric_name"], "type": "any"}}, "type": "function"}], "query": "In our MONAI/Ignite cardiac MRI segmentation study, we’re running a heterogeneous validation suite across multiple folds and experimental variants. Each validator engine logs a different set of metrics depending on what was enabled for that run (some runs log Dice as a single aggregated key, others log per-class Dice and derive an aggregate under a different name). To standardize EarlyStopping across the batch, build the score function per run using this protocol:\n\n- If an engine’s metric namespace includes an aggregated Dice key, use the aggregated Dice key for stopping.\n- Prefer a mean/aggregate Dice over any class-specific Dice keys (e.g., avoid keys that look like per-class Dice such as containing \"class\" or an index).\n- If multiple aggregate candidates exist, select the one matching the most common aggregated-Dice key across the batch.\n\nHere are the metric key sets observed after the first validation epoch for each run:\n1) baseline fold-0: {\"val_loss\", \"val_mean_dice\", \"val_dice_lv\", \"val_dice_myo\", \"val_dice_rv\"}\n2) ablation fold-0: {\"val_loss\", \"val_dice\", \"val_dice_class0\", \"val_dice_class1\", \"val_dice_class2\"}\n3) replicate fold-0: {\"val_loss\", \"val_mean_dice\", \"val_dice_bg\", \"val_dice_lv\"}\n\nGenerate the EarlyStopping score/stopping function for each run accordingly so that all runs stop on the same aggregate Dice definition whenever possible.", "answers": "[{\"name\":\"monai_handlers_utils_stopping_fn_from_metric\",\"arguments\":{\"metric_name\":\"val_mean_dice\"}},{\"name\":\"monai_handlers_utils_stopping_fn_from_metric\",\"arguments\":{\"metric_name\":\"val_dice\"}},{\"name\":\"monai_handlers_utils_stopping_fn_from_metric\",\"arguments\":{\"metric_name\":\"val_mean_dice\"}}]"}
{"func_name": "monai_losses_perceptual_medicalnet_intensity_normalisation", "func_desc": "monai.losses.perceptual.medicalnet_intensity_normalisation: Normalize a medical image volume to zero mean and unit variance following the MedicalNet preprocessing convention.\n    \n    This function implements the intensity normalization used in the MedicalNet project (see referenced source in original implementation). It computes the global arithmetic mean and standard deviation of all voxels in the provided n-dimensional medical image volume (for example, a 3D MRI or CT scan stored as a NumPy array) and returns a new array where each voxel intensity is shifted and scaled to have zero mean and unit variance. This normalization is commonly applied as a preprocessing step in deep learning workflows for medical imaging (as in MONAI) to stabilize training, make network weights more comparable across inputs, and reduce sensitivity to absolute intensity scales between studies or scanners.", "tools": [{"function": {"description": "monai.losses.perceptual.medicalnet_intensity_normalisation: Normalize a medical image volume to zero mean and unit variance following the MedicalNet preprocessing convention.\n\nThis function implements the intensity normalization used in the MedicalNet project (see referenced source in original implementation). It computes the global arithmetic mean and standard deviation of all voxels in the provided n-dimensional medical image volume (for example, a 3D MRI or CT scan stored as a NumPy array) and returns a new array where each voxel intensity is shifted and scaled to have zero mean and unit variance. This normalization is commonly applied as a preprocessing step in deep learning workflows for medical imaging (as in MONAI) to stabilize training, make network weights more comparable across inputs, and reduce sensitivity to absolute intensity scales between studies or scanners.", "name": "monai_losses_perceptual_medicalnet_intensity_normalisation", "parameters": {"properties": {"volume": {"type": "array", "items": {"type": "float"}, "description": "An n-dimensional NumPy array containing the medical image intensities to normalize. The array represents voxel or pixel intensity values across spatial dimensions (and optionally channels). This parameter is used as the source data from which the mean and standard deviation are computed globally (over all elements of the array). The function does not modify this input in-place; it reads the values to compute statistics and produces a separate normalized array as output.", "default": ""}}, "required": ["volume"], "type": "any"}}, "type": "function"}], "query": "We’re running a MedicalNet-compatible intensity normalization step on a messy mini-cohort of candidate MRI-derived n-D volumes. Each candidate may include acquisition artifacts. Apply MedicalNet global z-score normalization (mean/std over all voxels, including any channel dimension) only to volumes that (i) contain no non-finite voxels and (ii) have non-degenerate intensity dispersion, defined as having at least 10 unique voxel intensities across the entire array. The raw candidates are: (A) a full 3D brain MRI volume with shape (1, 4, 4, 4) and voxel values [[[[0.0, 15.2, 23.5, 40.1], [5.0, 18.3, 27.6, 35.0], [10.2, 20.0, 30.5, 45.3], [2.5, 12.0, 22.0, 32.0]], [[3.0, 14.5, 25.0, 36.5], [6.5, 19.0, 28.0, 38.0], [11.0, 21.5, 31.0, 41.0], [4.0, 13.0, 24.0, 34.0]], [[1.0, 16.0, 26.0, 37.0], [7.0, 17.5, 29.0, 39.5], [9.5, 22.5, 33.0, 43.0], [3.5, 11.5, 21.0, 31.5]], [[2.0, 8.0, 18.0, 28.0], [6.0, 16.0, 26.0, 36.0], [12.0, 24.0, 32.0, 44.0], [5.5, 9.0, 20.0, 30.0]]]]; (B) a 2×2×3 brain MRI patch with intensities [[[120.0, 130.0, 110.0], [95.0, 100.0, 105.0]], [[140.0, 150.0, 135.0], [125.0, 115.0, 118.0]]]; (C) another 2×2×3 brain MRI subvolume with intensities [[[120.0, 135.0, 150.0], [160.0, 170.0, 180.0]], [[190.0, 200.0, 210.0], [220.0, 230.0, 240.0]]]. Return the normalized outputs for the qualifying volumes in their original shapes.", "answers": "[{\"name\":\"monai_losses_perceptual_medicalnet_intensity_normalisation\",\"arguments\":{\"volume\":[[[[0.0,15.2,23.5,40.1],[5.0,18.3,27.6,35.0],[10.2,20.0,30.5,45.3],[2.5,12.0,22.0,32.0]],[[3.0,14.5,25.0,36.5],[6.5,19.0,28.0,38.0],[11.0,21.5,31.0,41.0],[4.0,13.0,24.0,34.0]],[[1.0,16.0,26.0,37.0],[7.0,17.5,29.0,39.5],[9.5,22.5,33.0,43.0],[3.5,11.5,21.0,31.5]],[[2.0,8.0,18.0,28.0],[6.0,16.0,26.0,36.0],[12.0,24.0,32.0,44.0],[5.5,9.0,20.0,30.0]]]]}},{\"name\":\"monai_losses_perceptual_medicalnet_intensity_normalisation\",\"arguments\":{\"volume\":[[[120.0,130.0,110.0],[95.0,100.0,105.0]],[[140.0,150.0,135.0],[125.0,115.0,118.0]]]}},{\"name\":\"monai_losses_perceptual_medicalnet_intensity_normalisation\",\"arguments\":{\"volume\":[[[120.0,135.0,150.0],[160.0,170.0,180.0]],[[190.0,200.0,210.0],[220.0,230.0,240.0]]]}}]"}
{"func_name": "monai_metrics_froc_compute_froc_score", "func_desc": "Compute the CAMELYON-style FROC score (average sensitivity at predefined false positive rates per image).\n    \n    This function is modified from the official CAMELYON16 challenge evaluation code and implements the challenge's second evaluation metric: the average sensitivity (true positive rate) evaluated at a set of predefined false positive rates per whole-slide image. It is intended for use in medical imaging detection pipelines (for example, lesion/metastasis detection on whole-slide histopathology images) where model outputs are aggregated at multiple detection thresholds to produce per-threshold average false positives per image and corresponding sensitivities. The function linearly interpolates the provided sensitivity curve at the requested false-positive-per-image thresholds and returns the arithmetic mean of those interpolated sensitivities. The implementation reverses the input arrays before interpolation to satisfy numpy.interp's requirement that the interpolation x-coordinates be in increasing order.", "tools": [{"function": {"description": "Compute the CAMELYON-style FROC score (average sensitivity at predefined false positive rates per image).\n\nThis function is modified from the official CAMELYON16 challenge evaluation code and implements the challenge's second evaluation metric: the average sensitivity (true positive rate) evaluated at a set of predefined false positive rates per whole-slide image. It is intended for use in medical imaging detection pipelines (for example, lesion/metastasis detection on whole-slide histopathology images) where model outputs are aggregated at multiple detection thresholds to produce per-threshold average false positives per image and corresponding sensitivities. The function linearly interpolates the provided sensitivity curve at the requested false-positive-per-image thresholds and returns the arithmetic mean of those interpolated sensitivities. The implementation reverses the input arrays before interpolation to satisfy numpy.interp's requirement that the interpolation x-coordinates be in increasing order.", "name": "monai_metrics_froc_compute_froc_score", "parameters": {"properties": {"fps_per_image": {"type": "array", "items": {"type": "float"}, "description": "A one-dimensional numeric array containing the average number of false positives per image computed at a series of detection thresholds. Each element corresponds to a particular detection threshold and represents the expected false positives per whole-slide image for that threshold. This array must have the same length as total_sensitivity and represent the x-axis values for interpolation. In typical MONAI/CAMELYON workflows these values are produced by aggregating per-image false positive counts across a validation/test set at multiple detection thresholds.", "default": ""}, "total_sensitivity": {"type": "array", "items": {"type": "float"}, "description": "A one-dimensional numeric array of the same length as fps_per_image containing the sensitivities (true positive rates) corresponding to each detection threshold. Each element is the fraction of true lesions correctly detected at the matching threshold. This array provides the y-axis values for interpolation. Both fps_per_image and total_sensitivity are reversed internally (via [::-1]) before interpolation so that numpy.interp receives increasing x-coordinates.", "default": ""}, "eval_thresholds": {"type": "any", "description": "A tuple of numeric false-positive-per-image target rates at which the function will evaluate (by linear interpolation) the sensitivity curve defined by fps_per_image and total_sensitivity. Defaults to (0.25, 0.5, 1, 2, 4, 8), which is the canonical set used in the CAMELYON16 challenge for reporting the averaged sensitivity metric. The tuple elements are treated as the x-coordinates at which to sample the sensitivity curve.", "default": [0.25, 0.5, 1, 2, 4, 8]}}, "required": ["fps_per_image", "total_sensitivity", "eval_thresholds"], "type": "any"}}, "type": "function"}], "query": "We’re auditing CAMELYON-style FROC in a multi-cohort validation where the per-threshold summaries come from different aggregation scripts and may include non-monotonic or duplicated operating points. For each cohort below, first sanitize the per-threshold curve by retaining only operating points whose fps/image values fall within the standard CAMELYON operating window [0.25, 8] (inclusive). If multiple operating points share the same fps/image after this sieve, keep the one with the highest sensitivity at that fps/image (treat it as the best-achievable sensitivity for that false-positive burden). Then compute the CAMELYON16-style FROC score as the mean sensitivity linearly interpolated at targets [0.25, 0.5, 1, 2, 4, 8].\n\nCohort A (pilot, 20 WSIs):\n- fps/image = [8.0, 4.0, 2.0, 1.0, 0.5, 0.25, 0.10, 16.0, 1.0]\n- sensitivity = [0.92, 0.88, 0.83, 0.78, 0.70, 0.60, 0.40, 0.95, 0.80]\n\nCohort B (follow-up, 48 WSIs):\n- fps/image = [8.0, 4.0, 2.0, 1.0, 0.5, 0.25, 0.25, 12.0]\n- sensitivity = [0.92, 0.89, 0.84, 0.78, 0.70, 0.60, 0.62, 0.93]\n\nReport the FROC score separately for each cohort after applying the above sanitization rules.", "answers": "[{\"name\":\"monai_metrics_froc_compute_froc_score\",\"arguments\":{\"fps_per_image\":[8.0,4.0,2.0,1.0,0.5,0.25],\"total_sensitivity\":[0.92,0.88,0.83,0.8,0.7,0.6],\"eval_thresholds\":[0.25,0.5,1,2,4,8]}},{\"name\":\"monai_metrics_froc_compute_froc_score\",\"arguments\":{\"fps_per_image\":[8.0,4.0,2.0,1.0,0.5,0.25],\"total_sensitivity\":[0.92,0.89,0.84,0.78,0.7,0.62],\"eval_thresholds\":[0.25,0.5,1,2,4,8]}}]"}
{"func_name": "monai_networks_layers_factories_adaptive_avgpooling_factory", "func_desc": "monai.networks.layers.factories.adaptive_avgpooling_factory returns the PyTorch Adaptive Average Pooling class corresponding to a requested spatial dimensionality. This factory function is used within MONAI (a PyTorch-based medical imaging framework) to select the correct adaptive average pooling layer type when building neural network architectures that operate on 1D signals, 2D image slices, or 3D volumes (common modalities in healthcare imaging workflows).", "tools": [{"function": {"description": "monai.networks.layers.factories.adaptive_avgpooling_factory returns the PyTorch Adaptive Average Pooling class corresponding to a requested spatial dimensionality. This factory function is used within MONAI (a PyTorch-based medical imaging framework) to select the correct adaptive average pooling layer type when building neural network architectures that operate on 1D signals, 2D image slices, or 3D volumes (common modalities in healthcare imaging workflows).\n", "name": "monai_networks_layers_factories_adaptive_avgpooling_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "Desired spatial dimension of the adaptive average pooling layer. This integer selects which PyTorch class is returned: 1 selects nn.AdaptiveAvgPool1d for 1D signals, 2 selects nn.AdaptiveAvgPool2d for 2D images (e.g., slice-based processing), and 3 selects nn.AdaptiveAvgPool3d for 3D volumes (e.g., CT/MRI volumes). The parameter is required and has no default. The argument is used directly as an index (dim - 1) into the internal tuple of classes; therefore it must be an integer in the set {1, 2, 3} to succeed.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re integrating adaptive global average pooling into a mixed-modality neuroimaging pipeline where the upstream tensor shapes are inconsistent across acquisitions and preprocessing. Each incoming feature map is described only by its spatial shape (channel/batch already stripped): `[(256,), (96, 96), (64, 64, 48), (1, 128, 128), (48, 48, 48, 12), (80, 80, 0), (32, 32, 32)]`. For each feature map, infer the spatial dimensionality as the count of strictly positive spatial extents, and select the corresponding PyTorch Adaptive Average Pooling layer class via MONAI’s factory. This is used to route 1D spectroscopy traces, 2D slice models, and 3D volumetric branches through the correct pooling module while ignoring malformed spatial shapes that cannot represent a real image volume.", "answers": "[{\"name\":\"monai_networks_layers_factories_adaptive_avgpooling_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_adaptive_avgpooling_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_adaptive_avgpooling_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_adaptive_avgpooling_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_adaptive_avgpooling_factory\",\"arguments\":{\"dim\":3}}]"}
{"func_name": "monai_networks_layers_factories_adaptive_maxpooling_factory", "func_desc": "Factory that returns the PyTorch adaptive max pooling class corresponding to a specified spatial dimension (1, 2, or 3). This function is used by MONAI's pooling factory (registered as Pool.factory_function(\"adaptivemax\")) to map a simple dimension identifier into the concrete PyTorch nn.AdaptiveMaxPoolXd class so network-building code in medical imaging workflows can instantiate the appropriate adaptive max-pooling module for 1D signals, 2D images (slices), or 3D volumes.\n    \n    This factory does not create a layer instance; it returns the class object for the appropriate AdaptiveMaxPool module. The returned class can be instantiated as a normal PyTorch module (for example, returned_type(output_size)) and then used in model definitions to perform spatial down-sampling via maximum pooling in MONAI models for healthcare imaging.", "tools": [{"function": {"description": "Factory that returns the PyTorch adaptive max pooling class corresponding to a specified spatial dimension (1, 2, or 3). This function is used by MONAI's pooling factory (registered as Pool.factory_function(\"adaptivemax\")) to map a simple dimension identifier into the concrete PyTorch nn.AdaptiveMaxPoolXd class so network-building code in medical imaging workflows can instantiate the appropriate adaptive max-pooling module for 1D signals, 2D images (slices), or 3D volumes.\n\nThis factory does not create a layer instance; it returns the class object for the appropriate AdaptiveMaxPool module. The returned class can be instantiated as a normal PyTorch module (for example, returned_type(output_size)) and then used in model definitions to perform spatial down-sampling via maximum pooling in MONAI models for healthcare imaging.", "name": "monai_networks_layers_factories_adaptive_maxpooling_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "desired dimension of the adaptive max pooling layer. This integer selects which PyTorch class is returned: 1 selects torch.nn.AdaptiveMaxPool1d for 1D signals, 2 selects torch.nn.AdaptiveMaxPool2d for 2D images (typical for slice-based medical imaging or 2D CNNs), and 3 selects torch.nn.AdaptiveMaxPool3d for 3D volumes (typical for volumetric medical imaging such as CT/MRI). The parameter is required and must be an integer; it is interpreted directly by indexing a tuple of the three classes in the order (1d, 2d, 3d).", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating downsampling across a mixed neuroimaging + interventional dataset where each incoming study is tagged with a nominal spatial dimensionality inferred from its tensor layout (some are true 3D volumes, some are 2D slice stacks, and some are 1D physiological traces mistakenly routed into the same queue). Given the following batch of studies, select the correct PyTorch adaptive max-pooling **class type** via MONAI’s adaptive-max factory for each study based on its inferred spatial dim, using this rule: treat a study as 3D only if it has three spatial axes (D,H,W); treat it as 2D if it has (H,W) only; treat it as 1D if it has a single spatial axis (L). Return the pooling class type for each study in the same order.\n\nBatch:\n1) CT_A: tensor shape (B=1, C=1, D=128, H=256, W=256)\n2) MRI_B: tensor shape (B=2, C=1, D=1, H=192, W=192)\n3) EKG_C: tensor shape (B=8, C=12, L=5000)\n4) US_D: tensor shape (B=4, C=1, H=512, W=512)\n5) PET_E: tensor shape (B=1, C=1, D=64, H=128, W=128)", "answers": "[{\"name\":\"monai_networks_layers_factories_adaptive_maxpooling_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_adaptive_maxpooling_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_adaptive_maxpooling_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_adaptive_maxpooling_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_adaptive_maxpooling_factory\",\"arguments\":{\"dim\":3}}]"}
{"func_name": "monai_networks_layers_factories_constant_pad_factory", "func_desc": "monai.networks.layers.factories.constant_pad_factory: Factory that returns the PyTorch nn.ConstantPad class for a specified spatial dimensionality (1, 2, or 3).\n    \n    This factory is used within MONAI preprocessing and network construction to select the appropriate constant-padding layer type for medical imaging data of different spatial dimensionalities. In the MONAI domain, 1D is typically used for sequential signals, 2D for slice-based images (e.g., X-rays or individual MRI/CT slices), and 3D for volumetric medical scans (e.g., full CT or MRI volumes). The function performs no tensor allocation or layer instantiation itself; it returns the class object (for example, nn.ConstantPad3d) so callers can instantiate a layer with specific padding and fill value (e.g., nn.ConstantPad3d(padding, value)) according to the PyTorch API.", "tools": [{"function": {"description": "monai.networks.layers.factories.constant_pad_factory: Factory that returns the PyTorch nn.ConstantPad class for a specified spatial dimensionality (1, 2, or 3).\n\nThis factory is used within MONAI preprocessing and network construction to select the appropriate constant-padding layer type for medical imaging data of different spatial dimensionalities. In the MONAI domain, 1D is typically used for sequential signals, 2D for slice-based images (e.g., X-rays or individual MRI/CT slices), and 3D for volumetric medical scans (e.g., full CT or MRI volumes). The function performs no tensor allocation or layer instantiation itself; it returns the class object (for example, nn.ConstantPad3d) so callers can instantiate a layer with specific padding and fill value (e.g., nn.ConstantPad3d(padding, value)) according to the PyTorch API.", "name": "monai_networks_layers_factories_constant_pad_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "desired dimension of the constant padding layer. Must be 1, 2, or 3. The integer selects which PyTorch class is returned: 1 -> nn.ConstantPad1d, 2 -> nn.ConstantPad2d, 3 -> nn.ConstantPad3d. This parameter controls the spatial dimensionality of padding applied in downstream MONAI pipelines and network layers.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a MONAI preprocessing graph that ingests a mixed-modality acquisition bundle where spatial dimensionality is inferred from each record’s voxel grid (ignoring the channel axis). Each record includes `id`, `voxel_grid_shape` (either `[L]`, `[H,W]`, or `[D,H,W]`), and a `qc_flag` from upstream DICOM parsing. Raw bundle:\n\n1) `{id: 'brain_mri_repA', voxel_grid_shape: [176, 256, 256], qc_flag: 'ok'}`\n2) `{id: 'xray_scout', voxel_grid_shape: [1024, 1024], qc_flag: 'ok'}`\n3) `{id: 'ecg_leadII', voxel_grid_shape: [5000], qc_flag: 'ok'}`\n4) `{id: 'chest_ct_repB', voxel_grid_shape: [320, 512, 512], qc_flag: 'ok'}`\n5) `{id: 'brain_mri_repC', voxel_grid_shape: [176, 256, 256], qc_flag: 'ok'}`\n6) `{id: 'corrupted_series', voxel_grid_shape: [0, 512, 512], qc_flag: 'parse_error'}`\n\nFor benchmarking, select the constant-padding operator class via `monai.networks.layers.factories.constant_pad_factory` only for records that (a) pass QC (`qc_flag == 'ok'`) and (b) represent true 3D volumetric scans (i.e., `voxel_grid_shape` has three positive spatial extents). Return the PyTorch ConstantPad *class* type chosen for each qualifying record (do not instantiate layers; padding widths/fill values will be applied downstream).", "answers": "[{\"name\":\"monai_networks_layers_factories_constant_pad_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_constant_pad_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_constant_pad_factory\",\"arguments\":{\"dim\":3}}]"}
{"func_name": "monai_networks_layers_factories_conv_factory", "func_desc": "monai.networks.layers.factories.conv_factory: Return the PyTorch convolution module class appropriate for a specified spatial dimensionality used in MONAI medical imaging networks.\n    \n    This factory function maps an integer spatial dimensionality used in medical imaging deep learning workflows to the corresponding torch.nn convolution class. In MONAI, selecting the correct convolution dimensionality is essential when designing networks for different data modalities: 1 for temporal or 1D signal data, 2 for 2D image slices (e.g., individual radiology slices), and 3 for volumetric image data (e.g., CT or MRI volumes). The function is registered as the \"conv\" factory and is intended to be used when building or configuring network layers so that subsequent layer construction uses the appropriate nn.ConvXd class for the chosen dimension. The function performs no in-place side effects; it only returns a class object.", "tools": [{"function": {"description": "monai.networks.layers.factories.conv_factory: Return the PyTorch convolution module class appropriate for a specified spatial dimensionality used in MONAI medical imaging networks.\n\nThis factory function maps an integer spatial dimensionality used in medical imaging deep learning workflows to the corresponding torch.nn convolution class. In MONAI, selecting the correct convolution dimensionality is essential when designing networks for different data modalities: 1 for temporal or 1D signal data, 2 for 2D image slices (e.g., individual radiology slices), and 3 for volumetric image data (e.g., CT or MRI volumes). The function is registered as the \"conv\" factory and is intended to be used when building or configuring network layers so that subsequent layer construction uses the appropriate nn.ConvXd class for the chosen dimension. The function performs no in-place side effects; it only returns a class object.", "name": "monai_networks_layers_factories_conv_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "Desired spatial dimensionality of the convolutional layer. Must be 1, 2, or 3 to select between torch.nn.Conv1d, torch.nn.Conv2d, and torch.nn.Conv3d respectively. This integer controls which convolution class is returned so callers can instantiate the returned class with the usual convolution constructor arguments (for example, channels and kernel size) appropriate to that dimensionality.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed-modality MONAI training suite where each specimen can be a temporal physiologic trace (1D), a single-slice radiology frame (2D), or a full volumetric MRI acquisition (3D). The raw manifest below is messy (some entries have anisotropic voxel spacing and some are derived 2.5D slice-stacks), so infer the spatial dimensionality per entry from the tensor layout and only instantiate convolution factories for entries that are true spatial images (i.e., have 3 to 5 tensor dimensions including batch/channel, where the count of spatial axes is exactly 1, 2, or 3). Use the number of spatial axes to select dim (exclude anything that doesn’t map cleanly).\n\nManifest (tensor shapes):\n- spec_A: (B=2, C=1, L=4096)\n- spec_B: (B=4, C=1, H=256, W=256)\n- spec_C: (B=1, C=2, D=96, H=96, W=64)\n- spec_D: (B=8, C=1)\n- spec_E: (B=1, C=1, T=12, H=192, W=192)\n\nFor every retained specimen, call MONAI’s convolution factory with the inferred dim and return the selected PyTorch convolution class per specimen in the same order as the manifest (skipping non-mappable entries).", "answers": "[{\"name\":\"monai_networks_layers_factories_conv_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_conv_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_conv_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_conv_factory\",\"arguments\":{\"dim\":3}}]"}
{"func_name": "monai_networks_layers_factories_dropout_factory", "func_desc": "monai.networks.layers.factories.dropout_factory returns the PyTorch dropout layer class corresponding to a specified spatial dimensionality used in MONAI network construction for medical imaging models. In MONAI this factory is registered via the Dropout.factory_function decorator under the name \"dropout\" and is used to select the correct dropout class when building or configuring networks for 1D, 2D, or 3D imaging tasks (for example, time-series, slice-based, or volumetric medical image models) so that callers can instantiate the appropriate nn.Dropout, nn.Dropout2d, or nn.Dropout3d layer.", "tools": [{"function": {"description": "monai.networks.layers.factories.dropout_factory returns the PyTorch dropout layer class corresponding to a specified spatial dimensionality used in MONAI network construction for medical imaging models. In MONAI this factory is registered via the Dropout.factory_function decorator under the name \"dropout\" and is used to select the correct dropout class when building or configuring networks for 1D, 2D, or 3D imaging tasks (for example, time-series, slice-based, or volumetric medical image models) so that callers can instantiate the appropriate nn.Dropout, nn.Dropout2d, or nn.Dropout3d layer.\n", "name": "monai_networks_layers_factories_dropout_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "Desired spatial dimension index for the dropout layer. This argument selects which PyTorch dropout class to return: 1 selects nn.Dropout (standard, element-wise dropout suitable for generic 1D features), 2 selects nn.Dropout2d (channel-wise/spatial dropout typically used for 2D image feature maps), and 3 selects nn.Dropout3d (channel-wise/spatial dropout for 3D volumetric feature maps). The parameter must be an integer; the function uses straightforward tuple indexing (types[dim - 1]) to perform the selection.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a mixed-modality MONAI training suite from a messy registry of candidate model jobs. Each job includes its intended spatial sampling and must be wired with the correct PyTorch dropout layer via MONAI’s dropout factory. Registry (unordered):\n- job_id=ts_ecg_qc: modality=ECG, tensor_layout=(B,C,T) with T=5000 (1D temporal)\n- job_id=ct_liver_seg_v1: modality=CT, voxel_patch=(96,96,96) (full volumetric)\n- job_id=mr_brain_slice_v2: modality=MR, slice_patch=(256,256) (2D slice-based)\n- job_id=ct_unet3d_rep2: modality=CT, voxel_patch=(128,128,64) (volumetric anisotropic but still 3D)\n- job_id=pathology_tile: modality=WSI, tile=(512,512) (2D)\n\nProtocol: for each job, infer the required dropout dimensionality from the patch/tensor shape (use 1D for temporal vectors, 2D for planar tiles/slices, 3D for volumetric patches) and query the dropout-layer factory with that inferred spatial dimensionality. Report the resulting dropout class for every job in the registry.", "answers": "[{\"name\":\"monai_networks_layers_factories_dropout_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_dropout_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_dropout_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_dropout_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_dropout_factory\",\"arguments\":{\"dim\":2}}]"}
{"func_name": "monai_networks_layers_factories_instance_factory", "func_desc": "monai.networks.layers.factories.instance_factory selects and returns the appropriate PyTorch Instance Normalization layer class for a specified spatial dimensionality used in MONAI medical-imaging networks.\n    \n    This factory function is registered via Norm.factory_function(\"instance\") and is intended for use in MONAI model construction and layer factory patterns where the normalization layer class must be chosen based on the spatial dimension of the data (e.g., 1D biomedical signals, 2D image slices, or 3D volumetric scans). The function does not create an instance of the normalization layer; it returns the corresponding nn.InstanceNormXd class so the caller can instantiate it with the required arguments (for example, num_features, affine, track_running_stats) according to PyTorch's API. The mapping is: dim == 1 -> torch.nn.InstanceNorm1d, dim == 2 -> torch.nn.InstanceNorm2d, dim == 3 -> torch.nn.InstanceNorm3d.", "tools": [{"function": {"description": "monai.networks.layers.factories.instance_factory selects and returns the appropriate PyTorch Instance Normalization layer class for a specified spatial dimensionality used in MONAI medical-imaging networks.\n\nThis factory function is registered via Norm.factory_function(\"instance\") and is intended for use in MONAI model construction and layer factory patterns where the normalization layer class must be chosen based on the spatial dimension of the data (e.g., 1D biomedical signals, 2D image slices, or 3D volumetric scans). The function does not create an instance of the normalization layer; it returns the corresponding nn.InstanceNormXd class so the caller can instantiate it with the required arguments (for example, num_features, affine, track_running_stats) according to PyTorch's API. The mapping is: dim == 1 -> torch.nn.InstanceNorm1d, dim == 2 -> torch.nn.InstanceNorm2d, dim == 3 -> torch.nn.InstanceNorm3d.", "name": "monai_networks_layers_factories_instance_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "Desired spatial dimension for the instance normalization layer. This integer must be 1, 2, or 3, corresponding respectively to InstanceNorm1d, InstanceNorm2d, and InstanceNorm3d. The value determines which PyTorch nn.InstanceNormXd class is returned and therefore which normalization semantics are applied when processing 1D signals, 2D medical image slices, or 3D medical image volumes in MONAI pipelines.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re validating normalization-layer selection for a mixed-modality neuroimaging study before model construction. The raw batch contains acquisitions with heterogeneous spatial support and some malformed metadata:\n\n- Cohort records (each has a declared spatial_dim and an example tensor layout):\n  1) peripheral-nerve ultrasound M-mode trace: spatial_dim=1, layout N,C,L\n  2) histopathology tile: spatial_dim=2, layout N,C,H,W\n  3) abdominal CT volume: spatial_dim=3, layout N,C,D,H,W\n  4) fetal Doppler waveform: spatial_dim=\"1\" (string), layout N,C,L\n  5) brain MRI volume: spatial_dim=3, layout N,C,D,H,W\n  6) microscopy z-stack exported as spatial_dim=4, layout N,C,Z,D,H,W\n  7) QA phantom with missing spatial_dim: spatial_dim=null, layout unknown\n  8) diffusion MRI volume flagged for later instantiation with num_features=32 and affine=True: spatial_dim=3, layout N,C,D,H,W\n\nUsing MONAI’s Norm.factory_function(\"instance\") selection behavior, pick (but do not instantiate) the correct PyTorch InstanceNorm layer *class* for every record whose spatial_dim can be interpreted as an integer in {1,2,3}.", "answers": "[{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_instance_factory\",\"arguments\":{\"dim\":3}}]"}
{"func_name": "monai_networks_layers_factories_instance_nvfuser_factory", "func_desc": "monai.networks.layers.factories.instance_nvfuser_factory returns a normalization layer class optimized for 3D instance normalization when available (apex.normalization.InstanceNorm3dNVFuser) and falls back to the appropriate torch.nn.InstanceNorm{1,2,3} classes for other dimensionalities or when the NVFuser implementation is not available. This factory is used in MONAI to select an efficient InstanceNorm layer for medical imaging deep learning pipelines, where 3D volumetric data (e.g., CT or MRI) often benefits from a specialized, CUDA-accelerated implementation.\n    \n    This function examines the requested spatial dimensionality and either:\n    - returns the NVIDIA APEX NVFuser accelerated class apex.normalization.InstanceNorm3dNVFuser when dim == 3 and the NVFuser implementation is installed and importable; or\n    - returns the corresponding torch.nn.InstanceNorm1d or torch.nn.InstanceNorm2d class when dim is 1 or 2; or\n    - returns torch.nn.InstanceNorm3d when dim == 3 but the NVFuser implementation is not installed or not importable.\n    \n    Behavioral notes, practical significance, and side effects:\n    This factory returns the layer class itself (not an instantiated layer). Callers must instantiate the returned class with appropriate constructor arguments (for example, affine and track_running_stats) consistent with torch.nn.InstanceNorm* semantics. The NVFuser implementation (apex.normalization.InstanceNorm3dNVFuser) is a customized autograd implementation provided by NVIDIA APEX that can be faster on CUDA for 3D volumes; it requires a CUDA-enabled environment and is not supported on Windows. Because the NVFuser variant uses custom autograd logic, it is currently not compatible with TorchScript; if TorchScript compatibility is required, use torch.nn.InstanceNorm3d instead.\n    \n    When the factory chooses a non-NVFuser fallback, it issues a Python warning via warnings.warn to inform the user about the fallback. If dim != 3 and dim is in {1, 2}, a warning indicates which torch.nn.InstanceNorm class will be used. If NVFuser is not installed or not importable, a warning indicates that torch.nn.InstanceNorm3d will be used instead. The function uses optional_import to import the NVFuser class; the returned value is the first element of optional_import(...)[0], i.e., the class object.\n    \n    Failure modes and limits:\n    The function assumes dim is an integer representing spatial dimensionality. It is designed for dim values 1, 2, or 3. If dim is outside the range 1..3, the function will attempt to index an internal tuple and will raise an IndexError; callers should validate dim before calling if there is any chance it is outside this range. The NVFuser path requires that apex.normalization.InstanceNorm3dNVFuser be installed and importable; otherwise the function falls back to torch.nn.InstanceNorm3d. The NVFuser implementation is not TorchScript compatible and requires CUDA on a non-Windows OS; attempting to use it in a CPU-only environment, on Windows, or with TorchScript will fail or produce unsupported behavior.\n    \n    Installation reference:\n    If you intend to use the NVFuser implementation, install NVIDIA APEX per its repository instructions: https://github.com/NVIDIA/apex#installation", "tools": [{"function": {"description": "monai.networks.layers.factories.instance_nvfuser_factory returns a normalization layer class optimized for 3D instance normalization when available (apex.normalization.InstanceNorm3dNVFuser) and falls back to the appropriate torch.nn.InstanceNorm{1,2,3} classes for other dimensionalities or when the NVFuser implementation is not available. This factory is used in MONAI to select an efficient InstanceNorm layer for medical imaging deep learning pipelines, where 3D volumetric data (e.g., CT or MRI) often benefits from a specialized, CUDA-accelerated implementation.\n\nThis function examines the requested spatial dimensionality and either:\n- returns the NVIDIA APEX NVFuser accelerated class apex.normalization.InstanceNorm3dNVFuser when dim == 3 and the NVFuser implementation is installed and importable; or\n- returns the corresponding torch.nn.InstanceNorm1d or torch.nn.InstanceNorm2d class when dim is 1 or 2; or\n- returns torch.nn.InstanceNorm3d when dim == 3 but the NVFuser implementation is not installed or not importable.\n\nBehavioral notes, practical significance, and side effects:\nThis factory returns the layer class itself (not an instantiated layer). Callers must instantiate the returned class with appropriate constructor arguments (for example, affine and track_running_stats) consistent with torch.nn.InstanceNorm* semantics. The NVFuser implementation (apex.normalization.InstanceNorm3dNVFuser) is a customized autograd implementation provided by NVIDIA APEX that can be faster on CUDA for 3D volumes; it requires a CUDA-enabled environment and is not supported on Windows. Because the NVFuser variant uses custom autograd logic, it is currently not compatible with TorchScript; if TorchScript compatibility is required, use torch.nn.InstanceNorm3d instead.\n\nWhen the factory chooses a non-NVFuser fallback, it issues a Python warning via warnings.warn to inform the user about the fallback. If dim != 3 and dim is in {1, 2}, a warning indicates which torch.nn.InstanceNorm class will be used. If NVFuser is not installed or not importable, a warning indicates that torch.nn.InstanceNorm3d will be used instead. The function uses optional_import to import the NVFuser class; the returned value is the first element of optional_import(...)[0], i.e., the class object.\n\nFailure modes and limits:\nThe function assumes dim is an integer representing spatial dimensionality. It is designed for dim values 1, 2, or 3. If dim is outside the range 1..3, the function will attempt to index an internal tuple and will raise an IndexError; callers should validate dim before calling if there is any chance it is outside this range. The NVFuser path requires that apex.normalization.InstanceNorm3dNVFuser be installed and importable; otherwise the function falls back to torch.nn.InstanceNorm3d. The NVFuser implementation is not TorchScript compatible and requires CUDA on a non-Windows OS; attempting to use it in a CPU-only environment, on Windows, or with TorchScript will fail or produce unsupported behavior.\n\nInstallation reference:\nIf you intend to use the NVFuser implementation, install NVIDIA APEX per its repository instructions: https://github.com/NVIDIA/apex#installation", "name": "monai_networks_layers_factories_instance_nvfuser_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "Spatial dimensionality requested for the InstanceNorm layer. This integer selects which normalization class to return: 1 returns torch.nn.InstanceNorm1d, 2 returns torch.nn.InstanceNorm2d, and 3 attempts to return the faster apex.normalization.InstanceNorm3dNVFuser class when available; if the NVFuser class is not available, torch.nn.InstanceNorm3d is returned. The value must be 1, 2, or 3; values outside this set will cause an IndexError due to internal indexing.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing normalization-layer selection across a mixed-modality imaging study where each cohort exports tensors with varying spatial dimensionality depending on the acquisition and preprocessing stage. The raw manifest below contains study blocks with an inferred `spatial_dim` (number of spatial axes in the feature map, excluding N and C). Use MONAI’s `instance_nvfuser_factory(dim=...)` to retrieve the appropriate **InstanceNorm layer class** for each block **only when the block is eligible for instance normalization**: process blocks whose `spatial_dim` is an integer in {1,2,3} and whose `intent` is one of {\"train\", \"inference\"}. For blocks tagged `deployment=\"torchscript\"`, force a non-NVFuser path by using `dim=2` if the block’s `spatial_dim` is 3 (we’ll later swap in a TorchScript-safe norm), otherwise keep the original spatial_dim. For all other eligible blocks, use their native `spatial_dim`.\n\nManifest:\n1) brainMRI_siteA: intent=train, deployment=eager, spatial_dim=3 (NCDHW)\n2) lungCT_siteB_rep1: intent=train, deployment=eager, spatial_dim=3 (NCDHW)\n3) lungCT_siteB_rep2: intent=inference, deployment=torchscript, spatial_dim=3 (NCDHW)\n4) qa_phantom_sliceCheck: intent=qc, deployment=eager, spatial_dim=2 (NCHW)\n5) spectroscopy_trace: intent=train, deployment=eager, spatial_dim=1 (NCW)\n6) corrupted_header_unknown: intent=inference, deployment=eager, spatial_dim=4\n\nReturn only the layer class objects selected by the factory (do not instantiate layers).", "answers": "[{\"name\":\"monai_networks_layers_factories_instance_nvfuser_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_instance_nvfuser_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_instance_nvfuser_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_instance_nvfuser_factory\",\"arguments\":{\"dim\":1}}]"}
{"func_name": "monai_networks_layers_factories_replication_pad_factory", "func_desc": "Replication padding layer class selector for 1D, 2D, and 3D spatial tensors used by MONAI's layer factory.\n    \n    This function is used in MONAI (Medical Open Network for AI) to map a requested spatial dimensionality to the corresponding PyTorch replication padding layer class when the \"replicationpad\" pad type is requested from the Pad factory. In medical imaging workflows within MONAI, replication padding is commonly applied to multi-dimensional image tensors to extend boundaries by copying edge values; this helper returns the appropriate nn.ReplicationPad class so the caller or factory can instantiate a padding layer with concrete padding sizes for preprocessing or network layers.", "tools": [{"function": {"description": "Replication padding layer class selector for 1D, 2D, and 3D spatial tensors used by MONAI's layer factory.\n\nThis function is used in MONAI (Medical Open Network for AI) to map a requested spatial dimensionality to the corresponding PyTorch replication padding layer class when the \"replicationpad\" pad type is requested from the Pad factory. In medical imaging workflows within MONAI, replication padding is commonly applied to multi-dimensional image tensors to extend boundaries by copying edge values; this helper returns the appropriate nn.ReplicationPad class so the caller or factory can instantiate a padding layer with concrete padding sizes for preprocessing or network layers.", "name": "monai_networks_layers_factories_replication_pad_factory", "parameters": {"properties": {"dim": {"type": "integer", "description": "desired spatial dimensionality for the replication padding layer. Valid values are 1, 2, or 3 corresponding to nn.ReplicationPad1d, nn.ReplicationPad2d, and nn.ReplicationPad3d respectively. This parameter has no default and must be provided. If dim is outside the supported set (for example less than 1 or greater than 3), the function will raise an IndexError from the internal tuple lookup; if dim is not an integer, a TypeError or similar will occur when used as an index. The caller (or the MONAI Pad factory) is responsible for supplying a correct integer dimension as part of constructing padding layers for multi-dimensional medical image tensors.", "default": ""}}, "required": ["dim"], "type": "any"}}, "type": "function"}], "query": "We’re validating a heterogeneous MONAI padding plan across a mixed-modality neuroimaging batch before scaling up.\n\nEach entry below is a candidate tensor spec coming from different stages (raw MRI volumes, derived 2D QC maps, a 1D intensity profile, and some metadata artifacts). For entries that represent real spatial tensors intended to receive edge-value replication padding, select the correct PyTorch replication padding *layer class* by passing the tensor’s spatial dimensionality to the MONAI replication padding layer class selector.\n\nUse these rules:\n- Treat only entries with `pad_type == \"replicationpad\"` as eligible.\n- If `spatial_shape` is present, `dim` is the number of spatial axes (length of `spatial_shape`).\n- If `spatial_shape` is missing or contains non-positive/unknown sizes, infer `dim` from `spatial_rank` if it is an integer in {1,2,3}.\n\nBatch specs:\n1) id=\"sub-001_T1w\", pad_type=\"replicationpad\", spatial_shape=(240, 240, 155)\n2) id=\"sub-001_QC_montage\", pad_type=\"replicationpad\", spatial_shape=(512, 512)\n3) id=\"sub-001_line_profile\", pad_type=\"replicationpad\", spatial_shape=(1024,)\n4) id=\"sub-002_T2w\", pad_type=\"constant\", spatial_shape=(256, 256, 160)\n5) id=\"sub-003_mask\", pad_type=\"replicationpad\", spatial_shape=(0, 256, 128), spatial_rank=3\n6) id=\"sub-004_meta\", pad_type=\"replicationpad\", spatial_shape=None, spatial_rank=2\n\nReturn the replication padding layer class selection for every eligible entry in the same order they appear.", "answers": "[{\"name\":\"monai_networks_layers_factories_replication_pad_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_replication_pad_factory\",\"arguments\":{\"dim\":2}},{\"name\":\"monai_networks_layers_factories_replication_pad_factory\",\"arguments\":{\"dim\":1}},{\"name\":\"monai_networks_layers_factories_replication_pad_factory\",\"arguments\":{\"dim\":3}},{\"name\":\"monai_networks_layers_factories_replication_pad_factory\",\"arguments\":{\"dim\":2}}]"}
{"func_name": "monai_networks_layers_factories_split_args", "func_desc": "monai.networks.layers.factories.split_args normalizes an argument specification into a (type, kwargs) pair suitable for MONAI factory-style layer construction utilities.\n    \n    This function is part of MONAI's layer factory utilities used by network/layer factories to accept flexible type specifications from configuration or code. It accepts either a single string naming a layer/type (for example a key used with monai.networks.layers.Act) or an explicit pair consisting of a name or callable that identifies the object to instantiate and a dict of keyword arguments to pass to that object's constructor. The normalized output is intended for direct use with factory mappings that construct PyTorch/MONAI layer objects from a type-specifier and keyword arguments.", "tools": [{"function": {"description": "monai.networks.layers.factories.split_args normalizes an argument specification into a (type, kwargs) pair suitable for MONAI factory-style layer construction utilities.\n\nThis function is part of MONAI's layer factory utilities used by network/layer factories to accept flexible type specifications from configuration or code. It accepts either a single string naming a layer/type (for example a key used with monai.networks.layers.Act) or an explicit pair consisting of a name or callable that identifies the object to instantiate and a dict of keyword arguments to pass to that object's constructor. The normalized output is intended for direct use with factory mappings that construct PyTorch/MONAI layer objects from a type-specifier and keyword arguments.", "name": "monai_networks_layers_factories_split_args", "parameters": {"properties": {"args": {"type": "any", "description": "input arguments to be parsed. This must be either:\n- a string that names the desired type (for example \"PRELU\" as used by monai.networks.layers.Act), in which case the function returns that string and an empty dict of kwargs; or\n- a two-element tuple (name_obj, name_args) where name_obj is either a string naming the object type or a callable (for example a class or factory function) and name_args is a dict of keyword arguments to pass when constructing the object. The tuple form is used to provide explicit constructor parameters (for example (\"PRELU\", {\"num_parameters\": 1, \"init\": 0.25}) when instantiating an activation with specific settings).", "default": ""}}, "required": ["args"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a YAML-driven MONAI UNet activation block where collaborators mixed naming conventions and occasionally over-specified parameters. Given the following raw activation specs (each intended to be a factory-style (type, kwargs) tuple), normalize only those entries whose kwargs indicate an in-place capable activation (i.e., `inplace` is present and truthy) OR whose parameterization is explicitly learnable but stable (i.e., `num_parameters` is present and positive, and `init` is within [0.2, 0.25], inclusive). For each retained entry, run MONAI’s `split_args` to produce the canonical (type, kwargs) pair.\n\nRaw specs (order preserved):\n1) (\"leakyrelu\", {negative_slope: 0.02, inplace: true})\n2) (\"PRELU\", {num_parameters: 1, init: 0.25})\n3) (\"PRELU\", {num_parameters: 1, init: 0.2})", "answers": "[{\"name\":\"monai_networks_layers_factories_split_args\",\"arguments\":{\"args\":[\"leakyrelu\",{\"negative_slope\":0.02,\"inplace\":true}]}},{\"name\":\"monai_networks_layers_factories_split_args\",\"arguments\":{\"args\":[\"PRELU\",{\"num_parameters\":1,\"init\":0.25}]}},{\"name\":\"monai_networks_layers_factories_split_args\",\"arguments\":{\"args\":[\"PRELU\",{\"num_parameters\":1,\"init\":0.2}]}}]"}
{"func_name": "monai_networks_nets_mednext_create_mednext", "func_desc": "Create a configured MedNeXt model instance for medical imaging tasks by selecting one of the predefined model-size variants. This factory constructs a MedNeXt encoder-decoder network (used in MONAI for deep learning on healthcare imaging data) with variant-specific expansion ratios and block counts optimized for different model capacities, and with shared common settings (residual connections enabled, group normalization, no global response normalization, and 32 initial filters). The created MedNeXt is intended for use in segmentation or classification pipelines that operate on multi-dimensional medical images (e.g., 2D or 3D volumes) and integrates with MONAI training and inference workflows.", "tools": [{"function": {"description": "Create a configured MedNeXt model instance for medical imaging tasks by selecting one of the predefined model-size variants. This factory constructs a MedNeXt encoder-decoder network (used in MONAI for deep learning on healthcare imaging data) with variant-specific expansion ratios and block counts optimized for different model capacities, and with shared common settings (residual connections enabled, group normalization, no global response normalization, and 32 initial filters). The created MedNeXt is intended for use in segmentation or classification pipelines that operate on multi-dimensional medical images (e.g., 2D or 3D volumes) and integrates with MONAI training and inference workflows.\n", "name": "monai_networks_nets_mednext_create_mednext", "parameters": {"properties": {"variant": {"type": "string", "description": "The MedNeXt variant to create. Must be one of 'S', 'B', 'M', or 'L' (case-insensitive). Each letter selects a predefined architecture size and complexity: 'S' (small) constructs a lightweight model with encoder_expansion_ratio=2, decoder_expansion_ratio=2, bottleneck_expansion_ratio=2, blocks_down=(2, 2, 2, 2), blocks_bottleneck=2, blocks_up=(2, 2, 2, 2); 'B' (base) constructs a medium model with encoder_expansion_ratio=(2, 3, 4, 4), decoder_expansion_ratio=(4, 4, 3, 2), bottleneck_expansion_ratio=4, blocks_down=(2, 2, 2, 2), blocks_bottleneck=2, blocks_up=(2, 2, 2, 2); 'M' (medium) increases depth and capacity with encoder_expansion_ratio=(2, 3, 4, 4), decoder_expansion_ratio=(4, 4, 3, 2), bottleneck_expansion_ratio=4, blocks_down=(3, 4, 4, 4), blocks_bottleneck=4, blocks_up=(4, 4, 4, 3); 'L' (large) constructs the highest-capacity model with encoder_expansion_ratio=(3, 4, 8, 8), decoder_expansion_ratio=(8, 8, 4, 3), bottleneck_expansion_ratio=8, blocks_down=(3, 4, 8, 8), blocks_bottleneck=8, blocks_up=(8, 8, 4, 3). Choosing a larger variant increases model parameters and representational capacity, which can improve accuracy on complex medical imaging tasks at the cost of greater memory and compute.", "default": ""}, "spatial_dims": {"type": "integer", "description": "Number of spatial dimensions for the network convolutions and tensor operations. Defaults to 3. In practice, set this to 2 for 2D medical images (e.g., X-ray or slice-based tasks) or 3 for volumetric data (e.g., CT, MRI), and MONAI will configure convolutional and normalization layers accordingly.", "default": 3}, "in_channels": {"type": "integer", "description": "Number of input channels in the input tensor. Defaults to 1. For example, set to 1 for single-channel grayscale volumes (typical in many medical imaging modalities) or to higher values when the input contains multi-channel data (e.g., multi-contrast MRI).", "default": 1}, "out_channels": {"type": "integer", "description": "Number of output channels produced by the network's final layer. Defaults to 2. In segmentation workflows, out_channels commonly equals the number of segmentation labels or classes; in other tasks it represents the dimensionality of the model output expected by downstream loss/metric computations.", "default": 2}, "kernel_size": {"type": "integer", "description": "Kernel size for convolutions used throughout the network. Defaults to 3. This integer determines the spatial support of convolutional filters and therefore influences the receptive field and local context captured by the model.", "default": 3}, "deep_supervision": {"type": "boolean", "description": "Whether to enable deep supervision (intermediate auxiliary outputs during training). Defaults to False. When True, the model exposes intermediate outputs from decoder stages that can be used to compute auxiliary losses during training to improve gradient flow and training stability for deep architectures; when False, only the final output is produced.", "default": false}}, "required": ["variant", "deep_supervision", "spatial_dims", "kernel_size", "out_channels", "in_channels"], "type": "any"}}, "type": "function"}], "query": "We’re setting up an ablation for a BraTS-like 3D multi-modal brain tumor MRI segmentation study where the dataset is messy and not all subjects have identical acquisition completeness. Below is a manifest of candidate cohorts (each cohort is a planned training run family, not a single subject):\n\nCohort manifest:\n1) {\"cohort_id\":\"cohort_A\",\"modalities\":[\"T1\",\"T1c\",\"T2\",\"FLAIR\"],\"task\":\"segmentation\",\"label_schema\":\"WT/TC/ET\"}\n2) {\"cohort_id\":\"cohort_B\",\"modalities\":[\"T1\",\"T2\",\"FLAIR\"],\"task\":\"segmentation\",\"label_schema\":\"WT/TC/ET\"}\n3) {\"cohort_id\":\"cohort_C\",\"modalities\":[\"T1\",\"T1c\",\"T2\",\"FLAIR\"],\"task\":\"segmentation\",\"label_schema\":\"WT/TC/ET\",\"note\":\"ok\"}\n4) {\"cohort_id\":\"cohort_D\",\"modalities\":[\"T1\",\"T1c\",\"T2\",\"FLAIR\"],\"task\":\"segmentation\",\"label_schema\":\"BraTS_full_5class\"}\n\nFor each cohort that is a 3D segmentation task with a BraTS-style 3-class schema (WT/TC/ET) and has a complete 4-modality input (T1, T1c, T2, FLAIR), instantiate MedNeXt backbones under a branching-capacity protocol: if the cohort’s input has 4 modalities, create BOTH a base-capacity and a medium-capacity model for benchmarking; otherwise, no model should be instantiated for that cohort. All instantiated models must share identical training-time settings: spatial_dims=3, in_channels determined by the number of modalities in that cohort, out_channels=3, kernel_size=3 (3×3×3), and deep_supervision enabled.", "answers": "[{\"name\":\"monai_networks_nets_mednext_create_mednext\",\"arguments\":{\"variant\":\"B\",\"spatial_dims\":3,\"in_channels\":4,\"out_channels\":3,\"kernel_size\":3,\"deep_supervision\":true}},{\"name\":\"monai_networks_nets_mednext_create_mednext\",\"arguments\":{\"variant\":\"M\",\"spatial_dims\":3,\"in_channels\":4,\"out_channels\":3,\"kernel_size\":3,\"deep_supervision\":true}},{\"name\":\"monai_networks_nets_mednext_create_mednext\",\"arguments\":{\"variant\":\"B\",\"spatial_dims\":3,\"in_channels\":4,\"out_channels\":3,\"kernel_size\":3,\"deep_supervision\":true}},{\"name\":\"monai_networks_nets_mednext_create_mednext\",\"arguments\":{\"variant\":\"M\",\"spatial_dims\":3,\"in_channels\":4,\"out_channels\":3,\"kernel_size\":3,\"deep_supervision\":true}}]"}
{"func_name": "monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args", "func_desc": "get_medicalnet_pretrained_resnet_args: Return the configuration tuple (bias_downsample, shortcut_type) that matches the MedicalNet pretrained ResNet weight conventions for a given ResNet depth.\n    \n    This function is used in MONAI (a PyTorch-based medical imaging deep learning framework) when constructing ResNet variants that will load pretrained weights from MedicalNet/model zoo. It encodes the known conventions for whether the downsample convolution uses a bias term and which residual shortcut variant (\"A\" or \"B\") was used when the MedicalNet weights were produced. Using the values returned by this function ensures the model architecture matches the weight parameter shapes and naming expected by the pretrained checkpoint, avoiding mismatches when loading weights for medical imaging tasks (classification, segmentation, or feature extraction).", "tools": [{"function": {"description": "get_medicalnet_pretrained_resnet_args: Return the configuration tuple (bias_downsample, shortcut_type) that matches the MedicalNet pretrained ResNet weight conventions for a given ResNet depth.\n\nThis function is used in MONAI (a PyTorch-based medical imaging deep learning framework) when constructing ResNet variants that will load pretrained weights from MedicalNet/model zoo. It encodes the known conventions for whether the downsample convolution uses a bias term and which residual shortcut variant (\"A\" or \"B\") was used when the MedicalNet weights were produced. Using the values returned by this function ensures the model architecture matches the weight parameter shapes and naming expected by the pretrained checkpoint, avoiding mismatches when loading weights for medical imaging tasks (classification, segmentation, or feature extraction).", "name": "monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args", "parameters": {"properties": {"resnet_depth": {"type": "integer", "description": "The ResNet depth identifier (for example 18, 34, 50, 101, 152, 200) that specifies which ResNet variant is being constructed. This integer is used to select the MedicalNet weight convention: ResNet-18 and ResNet-34 use the alternative shortcut/type and bias convention encoded by this function, while other common depths use the default convention. The parameter is expected to be an integer corresponding to the network depth; passing a value of a different type may lead to unexpected membership behavior or TypeError if the object is unhashable.", "default": ""}}, "required": ["resnet_depth"], "type": "any"}}, "type": "function"}], "query": "We’re aggregating pretrained-backbone metadata from multiple MONAI 3D ResNet experiments before launching a CT tumor-classification study. The run registry contains mixed ResNet depths from different cohorts and some nonconformant entries (e.g., missing, non-integer, or unsupported depths). Use MedicalNet pretrained-weight conventions to determine the correct instantiation tuple (bias_downsample, shortcut_type) only for registry entries whose ResNet depth is a valid integer and corresponds to the canonical MedicalNet-supported depths for MONAI ResNet backbones. Registry depths: [34, 18, 34, 50, null, \"34\", 101, -34, 0, 200]. Return the convention tuple per valid entry in the same order they appear after filtering.", "answers": "[{\"name\":\"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\",\"arguments\":{\"resnet_depth\":34}},{\"name\":\"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\",\"arguments\":{\"resnet_depth\":18}},{\"name\":\"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\",\"arguments\":{\"resnet_depth\":34}},{\"name\":\"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\",\"arguments\":{\"resnet_depth\":50}},{\"name\":\"monai_networks_nets_resnet_get_medicalnet_pretrained_resnet_args\",\"arguments\":{\"resnet_depth\":101}}]"}
{"func_name": "monai_networks_nets_swin_unetr_window_reverse", "func_desc": "monai.networks.nets.swin_unetr.window_reverse reconstructs a tensor of local windowed features back into the original spatial layout used by Swin Transformer style models (for example within the Swin-UNETR architecture used in medical imaging in MONAI). It reverses the window partitioning performed during windowed self-attention so that per-window feature vectors are rearranged into a contiguous volume or image that matches the original spatial dimensions.\n    \n    This function is used in the Swin-UNETR pipeline to reassemble processed local windows (the output of window-based attention or processing) into a full 2D image or 3D volume so downstream modules (decoders, up-samplers, loss computation) can operate on the restored spatial grid.", "tools": [{"function": {"description": "monai.networks.nets.swin_unetr.window_reverse reconstructs a tensor of local windowed features back into the original spatial layout used by Swin Transformer style models (for example within the Swin-UNETR architecture used in medical imaging in MONAI). It reverses the window partitioning performed during windowed self-attention so that per-window feature vectors are rearranged into a contiguous volume or image that matches the original spatial dimensions.\n\nThis function is used in the Swin-UNETR pipeline to reassemble processed local windows (the output of window-based attention or processing) into a full 2D image or 3D volume so downstream modules (decoders, up-samplers, loss computation) can operate on the restored spatial grid.", "name": "monai_networks_nets_swin_unetr_window_reverse", "parameters": {"properties": {"windows": {"type": "array", "items": {"type": "float"}, "description": "A tensor containing features extracted or processed per local window. This tensor is expected to be the result of a corresponding window partition operation and to be laid out so that it can be reshaped into blocks for each batch and spatial grid of windows. The last dimension is treated as the feature/channel dimension and is preserved. The tensor's device and dtype are preserved by this operation. If the memory layout or total number of elements does not match the expected grouping implied by window_size and dims, a runtime error (view/reshape mismatch) will occur.", "default": ""}, "window_size": {"type": "any", "description": "A tuple of integers specifying the size of each local window along each spatial axis. For a 3D volume this is (ws_d, ws_h, ws_w); for a 2D image this is (ws_h, ws_w). The lengths of window_size and the spatial portion of dims must be consistent: when dims has length 4 (b, d, h, w) window_size must have three elements; when dims has length 3 (b, h, w) window_size must have two elements. Mismatched lengths or values that do not evenly divide the corresponding spatial dimensions in dims will produce a runtime error during reshape.", "default": ""}, "dims": {"type": "any", "description": "A tuple describing the target output spatial dimensions including the batch size. For 3D volumes provide (b, d, h, w); for 2D images provide (b, h, w). The function branches based on len(dims): when len(dims) == 4 it reconstructs a 3D volume of shape (b, d, h, w, C); when len(dims) == 3 it reconstructs a 2D image of shape (b, h, w, C). dims must exactly match the spatial dimensions from which windows were originally partitioned; otherwise the reshaping logic will fail and raise a runtime exception (an undefined local variable or view/reshape error).", "default": ""}}, "required": ["windows", "window_size", "dims"], "type": "any"}}, "type": "function"}], "query": "We’re running a reconstruction QA on heterogeneous Swin-UNETR window-attention outputs coming from two scanners. The raw dumps include a mix of 2D and 3D samples, and we only want to run `window_reverse` on samples whose spatial axes are exactly tiled by the proposed `window_size` (i.e., every spatial dimension is evenly divisible by its corresponding window edge length). For each sample that passes this tiling criterion, reverse the window partitioning back into its original spatial grid.\n\nRaw batch (each has batch size B=1):\n\n1) 3D MRI feature map (candidate): intended spatial dims (D,H,W)=(4,4,4) with C=2 channels. Proposed window_size=(2,2,2). Windows tensor contains 8 windows total (2×2×2); each window has 8 tokens (2×2×2); each token has 2 channels:\n[[[0.0,0.1],[1.0,1.1],[2.0,2.1],[3.0,3.1],[4.0,4.1],[5.0,5.1],[6.0,6.1],[7.0,7.1]],[[8.0,8.1],[9.0,9.1],[10.0,10.1],[11.0,11.1],[12.0,12.1],[13.0,13.1],[14.0,14.1],[15.0,15.1]],[[16.0,16.1],[17.0,17.1],[18.0,18.1],[19.0,19.1],[20.0,20.1],[21.0,21.1],[22.0,22.1],[23.0,23.1]],[[24.0,24.1],[25.0,25.1],[26.0,26.1],[27.0,27.1],[28.0,28.1],[29.0,29.1],[30.0,30.1],[31.0,31.1]],[[32.0,32.1],[33.0,33.1],[34.0,34.1],[35.0,35.1],[36.0,36.1],[37.0,37.1],[38.0,38.1],[39.0,39.1]],[[40.0,40.1],[41.0,41.1],[42.0,42.1],[43.0,43.1],[44.0,44.1],[45.0,45.1],[46.0,46.1],[47.0,47.1]],[[48.0,48.1],[49.0,49.1],[50.0,50.1],[51.0,51.1],[52.0,52.1],[53.0,53.1],[54.0,54.1],[55.0,55.1]],[[56.0,56.1],[57.0,57.1],[58.0,58.1],[59.0,59.1],[60.0,60.1],[61.0,61.1],[62.0,62.1],[63.0,63.1]]]\n\n2) 2D medical image feature map (candidate): intended spatial dims (H,W)=(4,4) with C=3 channels. Proposed window_size=(2,2). Windows are 4 windows total (2×2); each window has 4 tokens (2×2); tokens listed sequentially with 3 channels:\n[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4],[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]]\n\nReturn the reconstructed full feature map/volume for every sample that satisfies the tiling criterion.", "answers": "[{\"name\":\"monai_networks_nets_swin_unetr_window_reverse\",\"arguments\":{\"windows\":[[[0.0,0.1],[1.0,1.1],[2.0,2.1],[3.0,3.1],[4.0,4.1],[5.0,5.1],[6.0,6.1],[7.0,7.1]],[[8.0,8.1],[9.0,9.1],[10.0,10.1],[11.0,11.1],[12.0,12.1],[13.0,13.1],[14.0,14.1],[15.0,15.1]],[[16.0,16.1],[17.0,17.1],[18.0,18.1],[19.0,19.1],[20.0,20.1],[21.0,21.1],[22.0,22.1],[23.0,23.1]],[[24.0,24.1],[25.0,25.1],[26.0,26.1],[27.0,27.1],[28.0,28.1],[29.0,29.1],[30.0,30.1],[31.0,31.1]],[[32.0,32.1],[33.0,33.1],[34.0,34.1],[35.0,35.1],[36.0,36.1],[37.0,37.1],[38.0,38.1],[39.0,39.1]],[[40.0,40.1],[41.0,41.1],[42.0,42.1],[43.0,43.1],[44.0,44.1],[45.0,45.1],[46.0,46.1],[47.0,47.1]],[[48.0,48.1],[49.0,49.1],[50.0,50.1],[51.0,51.1],[52.0,52.1],[53.0,53.1],[54.0,54.1],[55.0,55.1]],[[56.0,56.1],[57.0,57.1],[58.0,58.1],[59.0,59.1],[60.0,60.1],[61.0,61.1],[62.0,62.1],[63.0,63.1]]],\"window_size\":[2,2,2],\"dims\":[1,4,4,4]}},{\"name\":\"monai_networks_nets_swin_unetr_window_reverse\",\"arguments\":{\"windows\":[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9],[1.0,1.1,1.2],[1.3,1.4,1.5],[1.6,1.7,1.8],[1.9,2.0,2.1],[2.2,2.3,2.4],[2.5,2.6,2.7],[2.8,2.9,3.0],[3.1,3.2,3.3],[3.4,3.5,3.6],[3.7,3.8,3.9],[4.0,4.1,4.2],[4.3,4.4,4.5],[4.6,4.7,4.8]],\"window_size\":[2,2],\"dims\":[1,4,4]}}]"}
{"func_name": "monai_networks_nets_vista3d_vista3d132", "func_desc": "monai.networks.nets.vista3d.vista3d132 returns a configured VISTA3D model instance implementing the exact network configuration used in the paper at https://arxiv.org/abs/2406.05285. This factory function builds a 3D image encoder (SegResNetDS2) and two task heads (PointMappingSAM and ClassMappingClassify), wires them into a VISTA3D model, and returns the assembled model for use in medical imaging workflows (for example, 3D segmentation, point-based mapping, and classification in healthcare imaging datasets). The implementation treats class indices larger than 132 as zero-shot (i.e., out-of-support classes are handled as unseen by the model).", "tools": [{"function": {"description": "monai.networks.nets.vista3d.vista3d132 returns a configured VISTA3D model instance implementing the exact network configuration used in the paper at https://arxiv.org/abs/2406.05285. This factory function builds a 3D image encoder (SegResNetDS2) and two task heads (PointMappingSAM and ClassMappingClassify), wires them into a VISTA3D model, and returns the assembled model for use in medical imaging workflows (for example, 3D segmentation, point-based mapping, and classification in healthcare imaging datasets). The implementation treats class indices larger than 132 as zero-shot (i.e., out-of-support classes are handled as unseen by the model).\n", "name": "monai_networks_nets_vista3d_vista3d132", "parameters": {"properties": {"encoder_embed_dim": {"type": "integer", "description": "Hidden embedding dimension used throughout the encoder and head feature interfaces. Practically, this value controls the number of output channels produced by the SegResNetDS2 encoder (passed as out_channels and init_filters) and the feature_size consumed by both PointMappingSAM and ClassMappingClassify. A larger encoder_embed_dim increases model capacity and memory usage; the default 48 is the value used in the referenced VISTA3D132 configuration. This value must be a positive integer; non-positive values or values that are incompatible with downstream deployment hardware may cause runtime errors (for example, allocation failures or shape mismatches when loading pretrained weights).", "default": 48}, "in_channels": {"type": "integer", "description": "Number of input channels expected by the 3D image encoder (SegResNetDS2). In medical imaging, common values are 1 for single-channel modalities (e.g., CT or MRI intensity volumes) or 3 for multi-channel inputs. This argument is forwarded directly to SegResNetDS2(in_channels=...), so the tensor passed to the returned model during inference or training must have the same channel dimension. Mismatched channel counts will raise tensor-shape errors at runtime.", "default": 1}}, "required": ["encoder_embed_dim", "in_channels"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a mixed abdominal CT cohort for a single shared VISTA3D evaluation pipeline (segmentation + point-mapping + classification) on a 24GB GPU node. Each study is single-channel (in_channels=1), but the annotation dictionaries are messy: some studies contain only in-support organ class indices (1–132), while others include at least one label index >132 (treated as zero-shot by VISTA3D132). Instantiate two VISTA3D132 replicas with the exact paper configuration, choosing the encoder embedding width per study as follows: if a study contains any out-of-support class index (>132), use a memory-reduced encoder_embed_dim=32 to maximize throughput during zero-shot stress tests; otherwise, use a higher-capacity encoder_embed_dim=64 for in-support benchmarking. Apply this rule to the following per-study label inventories: Study_A:[1,2,5,12,77], Study_B:[3,9,140], Study_C:[4,8,16,33,120,132], Study_D:[2,7,133,200].", "answers": "[{\"name\":\"monai_networks_nets_vista3d_vista3d132\",\"arguments\":{\"encoder_embed_dim\":64,\"in_channels\":1}},{\"name\":\"monai_networks_nets_vista3d_vista3d132\",\"arguments\":{\"encoder_embed_dim\":32,\"in_channels\":1}},{\"name\":\"monai_networks_nets_vista3d_vista3d132\",\"arguments\":{\"encoder_embed_dim\":64,\"in_channels\":1}},{\"name\":\"monai_networks_nets_vista3d_vista3d132\",\"arguments\":{\"encoder_embed_dim\":32,\"in_channels\":1}}]"}
{"func_name": "monai_transforms_lazy_utils_is_compatible_apply_kwargs", "func_desc": "monai.transforms.lazy.utils.is_compatible_apply_kwargs checks whether two mappings of keyword arguments are compatible for combination by a lazy transform \"apply\" operation in MONAI's transform pipeline.\n    \n    This function is intended for use within MONAI's lazy transform system (used in medical imaging preprocessing and compositional transform APIs described in the project README) to decide if two per-call kwargs dictionaries can be merged or applied together when composing transforms. Each argument dictionary is expected to represent keyword arguments that would be passed to an individual transform's apply method during lazy execution (for example, per-item options carried through a composed sequence of transforms). The current implementation is a predicate function used by higher-level code that composes or merges kwargs before invoking apply.", "tools": [{"function": {"description": "monai.transforms.lazy.utils.is_compatible_apply_kwargs checks whether two mappings of keyword arguments are compatible for combination by a lazy transform \"apply\" operation in MONAI's transform pipeline.\n\nThis function is intended for use within MONAI's lazy transform system (used in medical imaging preprocessing and compositional transform APIs described in the project README) to decide if two per-call kwargs dictionaries can be merged or applied together when composing transforms. Each argument dictionary is expected to represent keyword arguments that would be passed to an individual transform's apply method during lazy execution (for example, per-item options carried through a composed sequence of transforms). The current implementation is a predicate function used by higher-level code that composes or merges kwargs before invoking apply.", "name": "monai_transforms_lazy_utils_is_compatible_apply_kwargs", "parameters": {"properties": {"kwargs_1": {"type": "any", "description": "First mapping of keyword arguments intended for a transform's apply call. This parameter represents per-transform or per-item options produced earlier in a lazy pipeline. The function expects a dict as provided by calling code; no mutation is performed on this object by the function itself.", "default": ""}, "kwargs_2": {"type": "any", "description": "Second mapping of keyword arguments intended for a transform's apply call. This parameter represents additional per-transform or per-item options that might be merged with kwargs_1 when composing transforms. The function expects a dict as provided by calling code; no mutation is performed on this object by the function itself.", "default": ""}}, "required": ["kwargs_1", "kwargs_2"], "type": "any"}}, "type": "function"}], "query": "In a MONAI lazy-execution reproducibility audit, we’re trying to merge per-call `apply` kwargs across a heterogeneous preprocessing graph where some operators are deterministic (safe to merge if all deterministic fields agree) and some carry stochastic or geometry-defining state (only mergeable when the geometry-defining fields are identical and no extra per-call state is introduced). Given these candidate kwargs-pairs collected from different stages/replicates, run `is_compatible_apply_kwargs` only on pairs that (a) have matching keys for geometry-defining parameters when present (e.g., `start`/`end` ROI bounds), and (b) do not introduce any additional per-call state keys (e.g., `random_state`) in either mapping. For the eligible pairs, assess compatibility for merging into a single lazy `apply` call.\n\nCandidate pairs:\n1) Resampling/interpolation: kwargs_A={mode:\"bilinear\", padding_mode:\"border\", align_corners:false}, kwargs_B={mode:\"nearest\", padding_mode:\"border\", align_corners:false}\n2) Spatial ROI crop (replicate-to-replicate): kwargs_A={start:[10,20,5], end:[50,80,45], padding_mode:\"constant\"}, kwargs_B={start:[10,20,5], end:[50,80,45], padding_mode:\"reflect\", random_state:12345}\n3) Spatial ROI crop (session-to-session): kwargs_A={start:[0,0,0], end:[64,64,32], padding_mode:\"constant\"}, kwargs_B={start:[0,0,0], end:[64,64,32], padding_mode:\"constant\"}\n4) Resampling/interpolation (replicate-to-replicate): kwargs_A={mode:\"bilinear\", padding_mode:\"zeros\", align_corners:true}, kwargs_B={mode:\"bilinear\", padding_mode:\"zeros\", align_corners:true}\n\nReturn the compatibility decision for each eligible pair.", "answers": "[{\"name\":\"monai_transforms_lazy_utils_is_compatible_apply_kwargs\",\"arguments\":{\"kwargs_1\":{\"mode\":\"bilinear\",\"padding_mode\":\"border\",\"align_corners\":false},\"kwargs_2\":{\"mode\":\"nearest\",\"padding_mode\":\"border\",\"align_corners\":false}}},{\"name\":\"monai_transforms_lazy_utils_is_compatible_apply_kwargs\",\"arguments\":{\"kwargs_1\":{\"start\":[0,0,0],\"end\":[64,64,32],\"padding_mode\":\"constant\"},\"kwargs_2\":{\"start\":[0,0,0],\"end\":[64,64,32],\"padding_mode\":\"constant\"}}},{\"name\":\"monai_transforms_lazy_utils_is_compatible_apply_kwargs\",\"arguments\":{\"kwargs_1\":{\"mode\":\"bilinear\",\"padding_mode\":\"zeros\",\"align_corners\":true},\"kwargs_2\":{\"mode\":\"bilinear\",\"padding_mode\":\"zeros\",\"align_corners\":true}}}]"}
{"func_name": "monai_transforms_lazy_utils_requires_interp", "func_desc": "monai.transforms.lazy.utils.requires_interp checks whether a given affine transformation matrix can be implemented by simple axis operations (flip, permutation, pad/slice) or whether it requires voxel-wise interpolation during resampling.\n    \n    This function is used in MONAI preprocessing and lazy transform code to decide whether a spatial transform represented by an affine matrix can be realized by cheap array operations (no interpolation, e.g., memory-only permutation/flip) or must be performed with interpolation (resampling voxels), which is more computationally expensive and can change image intensities. The function inspects the translation column and the top-left submatrix of the affine matrix to determine if the transform is an integer-translation plus axis-permutation/flip (returns a mapping) or requires interpolation (returns None). Internally the input is converted to a NumPy array for numeric checks; the function does not modify the provided matrix.", "tools": [{"function": {"description": "monai.transforms.lazy.utils.requires_interp checks whether a given affine transformation matrix can be implemented by simple axis operations (flip, permutation, pad/slice) or whether it requires voxel-wise interpolation during resampling.\n\nThis function is used in MONAI preprocessing and lazy transform code to decide whether a spatial transform represented by an affine matrix can be realized by cheap array operations (no interpolation, e.g., memory-only permutation/flip) or must be performed with interpolation (resampling voxels), which is more computationally expensive and can change image intensities. The function inspects the translation column and the top-left submatrix of the affine matrix to determine if the transform is an integer-translation plus axis-permutation/flip (returns a mapping) or requires interpolation (returns None). Internally the input is converted to a NumPy array for numeric checks; the function does not modify the provided matrix.", "name": "monai_transforms_lazy_utils_requires_interp", "parameters": {"properties": {"matrix": {"type": "array", "items": {"type": "float"}, "description": "The affine matrix to check. This is the (N+1)x(N+1) homogeneous affine matrix typically used in spatial transforms for medical images in MONAI, where the top-left N x N submatrix encodes axis scaling/rotation/flip and the last column encodes translation. The function uses the matrix values to determine whether the transform corresponds exactly (within tolerance) to axis flips/permutations and integer translations. The matrix is converted to a NumPy array internally for numeric comparisons; the function does not mutate the original matrix argument.", "default": ""}, "atol": {"type": "float", "description": "Absolute tolerance used for numerical comparisons. This tolerance is applied when checking whether translation components are close to integers and whether submatrix entries are close to -1, 0, or 1. The default is AFFINE_TOL (0.001 in the signature), meaning values within this absolute difference are treated as exact integers or exact -1/0/1 in the decision logic.", "default": 0.001}}, "required": ["matrix", "atol"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed-quality set of candidate 3D scanner-to-canonical affines exported from multiple sites for MONAI lazy preprocessing. Some affines are “axis-only” (pure axis permutation/flip plus integer-voxel translation) and can be executed lazily via permute/flip + pad/slice, while others will trigger resampling interpolation. Run requires_interp on every affine that is plausibly a rigid axis-only operation under its site-specific tolerance: use a stricter atol (1e−6) for matrices whose translation entries are exact integers in the export, and a looser atol (1e−4) for matrices whose translation entries show small decimal export noise. Evaluate these candidate affines:\n\nA) [[0, 1, 0, 5], [1, 0, 0, -2], [0, 0, -1, 0], [0, 0, 0, 1]]\nB) [[0, 1, 0, 10.00003], [1, 0, 0, -4.99997], [0, 0, -1, 0], [0, 0, 0, 1]]\nC) [[0, -1, 0, 0], [1, 0, 0, 10.00002], [0, 0, 1, -5.00001], [0, 0, 0, 1]]\nD) [[0.9999, 0.01, 0, 2], [-0.01, 0.9999, 0, 3], [0, 0, 1, 0], [0, 0, 0, 1]]\n\nFor each matrix you evaluate, report whether it qualifies as an axis-permutation/flip with integer translation (and the mapping if returned) or whether it requires interpolation (no mapping).", "answers": "[{\"name\":\"monai_transforms_lazy_utils_requires_interp\",\"arguments\":{\"matrix\":[[0,1,0,5],[1,0,0,-2],[0,0,-1,0],[0,0,0,1]],\"atol\":1e-06}},{\"name\":\"monai_transforms_lazy_utils_requires_interp\",\"arguments\":{\"matrix\":[[0,1,0,10.00003],[1,0,0,-4.99997],[0,0,-1,0],[0,0,0,1]],\"atol\":0.0001}},{\"name\":\"monai_transforms_lazy_utils_requires_interp\",\"arguments\":{\"matrix\":[[0,-1,0,0],[1,0,0,10.00002],[0,0,1,-5.00001],[0,0,0,1]],\"atol\":0.0001}},{\"name\":\"monai_transforms_lazy_utils_requires_interp\",\"arguments\":{\"matrix\":[[0.9999,0.01,0,2],[-0.01,0.9999,0,3],[0,0,1,0],[0,0,0,1]],\"atol\":0.0001}}]"}
{"func_name": "monai_transforms_spatial_functional_convert_points_to_box", "func_desc": "Converts a set of corner points for rectangular (2D) or cuboid (3D) regions into axis-aligned bounding boxes.\n    \n    This function is intended for medical imaging workflows (see MONAI README) where annotations or detections are often provided as the corner points of a rectangle in 2D or a cuboid in 3D. Given a batch of such corner points, the function computes the axis-aligned bounding box that encloses each set of corners by taking the elementwise minimum and maximum along the corner dimension and concatenating them. The result is suitable for downstream preprocessing, cropping, augmentation, or evaluation steps that expect boxes in (min_coords, max_coords) form.", "tools": [{"function": {"description": "Converts a set of corner points for rectangular (2D) or cuboid (3D) regions into axis-aligned bounding boxes.\n\nThis function is intended for medical imaging workflows (see MONAI README) where annotations or detections are often provided as the corner points of a rectangle in 2D or a cuboid in 3D. Given a batch of such corner points, the function computes the axis-aligned bounding box that encloses each set of corners by taking the elementwise minimum and maximum along the corner dimension and concatenating them. The result is suitable for downstream preprocessing, cropping, augmentation, or evaluation steps that expect boxes in (min_coords, max_coords) form.", "name": "monai_transforms_spatial_functional_convert_points_to_box", "parameters": {"properties": {"points": {"type": "array", "items": {"type": "float"}, "description": "Numeric array containing the corner coordinates for one or more boxes.\nFor 3D cuboids, the expected shape is (N, 8, 3) corresponding to N boxes, each with 8 corner points\nand 3 coordinates (x, y, z) per corner. For 2D rectangles, the expected shape is (N, 4, 2)\ncorresponding to N boxes, each with 4 corner points and 2 coordinates (x, y) per corner.\nThe first dimension N represents the number of boxes in the batch. The function delegates the\nelementwise min/max reduction to MONAI's utils that unify NumPy/PyTorch operations, so the\nnumeric semantics follow those underlying implementations.\nPractical significance: callers typically provide model predictions or annotated corners here;\nthe function produces a compact axis-aligned box representation used widely in medical image\npreprocessing and metric calculations.\nBehavior and failure modes: if the input does not have one of the documented shapes or is not a\nnumeric numpy.ndarray, the underlying reduction calls will raise an exception (for example,\ndue to an invalid dimension index). The function does not validate coordinate ordering beyond\ntaking per-coordinate minima and maxima; it assumes corners represent valid rectangle/cuboid corners.", "default": ""}}, "required": ["points"], "type": "any"}}, "type": "function"}], "query": "Run the ROI standardization stage for a mixed-source CT lesion cropping benchmark where corner annotations are noisy and occasionally degenerate. You are given three incoming batches of candidate 3D ROIs (each candidate ROI is represented by 8 corner points in (x, y, z)). For each batch independently, first perform a geometric QC pass and retain only those ROIs whose 8 corners span a non-degenerate cuboid in voxel space: the axis-aligned extent must be strictly positive along all three axes (dx>0, dy>0, dz>0) when computed from the per-axis min and max across the 8 corners. Convert only the QC-passing ROIs into (min_coords, max_coords) axis-aligned boxes for downstream cropping using the points-to-box conversion.\n\nBatch 1 (Cohort A, manual annotations; batch shape (2, 8, 3)):\n- ROI 1 corners: [[12.5, 40.0, 8.0], [12.5, 55.0, 8.0], [30.0, 40.0, 8.0], [30.0, 55.0, 8.0], [12.5, 40.0, 20.0], [12.5, 55.0, 20.0], [30.0, 40.0, 20.0], [30.0, 55.0, 20.0]]\n- ROI 2 corners: [[100.0, 120.0, 30.0], [115.0, 120.0, 30.0], [100.0, 140.0, 30.0], [115.0, 140.0, 30.0], [100.0, 120.0, 45.0], [115.0, 120.0, 45.0], [100.0, 140.0, 45.0], [115.0, 140.0, 45.0]]\n\nBatch 2 (Cohort B, model predictions; batch shape (2, 8, 3)):\n- ROI 1 corners: [[12.0, 25.0, 40.0], [20.0, 25.0, 40.0], [12.0, 35.0, 40.0], [20.0, 35.0, 40.0], [12.0, 25.0, 55.0], [20.0, 25.0, 55.0], [12.0, 35.0, 55.0], [20.0, 35.0, 55.0]]\n- ROI 2 corners (suspected collapsed-z artifact): [[100.0, 80.0, 10.0], [120.0, 80.0, 10.0], [100.0, 95.0, 10.0], [120.0, 95.0, 10.0], [100.0, 80.0, 10.0], [120.0, 80.0, 10.0], [100.0, 95.0, 10.0], [120.0, 95.0, 10.0]]\n\nBatch 3 (validation replicate; batch shape (2, 8, 3)):\n- ROI 1 corners: [[30.0, 50.0, 10.0], [40.0, 50.0, 10.0], [30.0, 60.0, 10.0], [40.0, 60.0, 10.0], [30.0, 50.0, 20.0], [40.0, 50.0, 20.0], [30.0, 60.0, 20.0], [40.0, 60.0, 20.0]]\n- ROI 2 corners: [[80.0, 120.0, 40.0], [100.0, 120.0, 40.0], [80.0, 140.0, 40.0], [100.0, 140.0, 40.0], [80.0, 120.0, 60.0], [100.0, 120.0, 60.0], [80.0, 140.0, 60.0], [100.0, 140.0, 60.0]]", "answers": "[{\"name\":\"monai_transforms_spatial_functional_convert_points_to_box\",\"arguments\":{\"points\":[[[12.5,40.0,8.0],[12.5,55.0,8.0],[30.0,40.0,8.0],[30.0,55.0,8.0],[12.5,40.0,20.0],[12.5,55.0,20.0],[30.0,40.0,20.0],[30.0,55.0,20.0]],[[100.0,120.0,30.0],[115.0,120.0,30.0],[100.0,140.0,30.0],[115.0,140.0,30.0],[100.0,120.0,45.0],[115.0,120.0,45.0],[100.0,140.0,45.0],[115.0,140.0,45.0]]]}} ,{\"name\":\"monai_transforms_spatial_functional_convert_points_to_box\",\"arguments\":{\"points\":[[[12.0,25.0,40.0],[20.0,25.0,40.0],[12.0,35.0,40.0],[20.0,35.0,40.0],[12.0,25.0,55.0],[20.0,25.0,55.0],[12.0,35.0,55.0],[20.0,35.0,55.0]]]}} ,{\"name\":\"monai_transforms_spatial_functional_convert_points_to_box\",\"arguments\":{\"points\":[[[30.0,50.0,10.0],[40.0,50.0,10.0],[30.0,60.0,10.0],[40.0,60.0,10.0],[30.0,50.0,20.0],[40.0,50.0,20.0],[30.0,60.0,20.0],[40.0,60.0,20.0]],[[80.0,120.0,40.0],[100.0,120.0,40.0],[80.0,140.0,40.0],[100.0,140.0,40.0],[80.0,120.0,60.0],[100.0,120.0,60.0],[80.0,140.0,60.0],[100.0,140.0,60.0]]]}}]"}
{"func_name": "monai_transforms_spatial_functional_flip", "func_desc": "monai.transforms.spatial.functional.flip flips image data along specified spatial axes for use in MONAI preprocessing and data-augmentation pipelines for medical imaging. This function implements the flip eagerly (applies torch.flip to the tensor data) or lazily (registers a transform as metadata to be applied later in a TraceableTransform workflow) depending on the lazy flag. The function assumes channel-first tensors (channel dimension first) as used throughout MONAI transform utilities and constructs/updates an affine-like transform matrix describing the flip so downstream traceable metadata can track the spatial change.", "tools": [{"function": {"description": "monai.transforms.spatial.functional.flip flips image data along specified spatial axes for use in MONAI preprocessing and data-augmentation pipelines for medical imaging. This function implements the flip eagerly (applies torch.flip to the tensor data) or lazily (registers a transform as metadata to be applied later in a TraceableTransform workflow) depending on the lazy flag. The function assumes channel-first tensors (channel dimension first) as used throughout MONAI transform utilities and constructs/updates an affine-like transform matrix describing the flip so downstream traceable metadata can track the spatial change.\n", "name": "monai_transforms_spatial_functional_flip", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "float"}, "description": "Input image tensor to be flipped. In MONAI this is expected to be channel-first (shape convention C x D x H x W... for multi-dimensional medical images). The implementation also recognizes MetaTensor instances (a MONAI wrapper around torch.Tensor) and will read pending shape/rank and copy/produce metadata when MetaTensor is provided. The tensor contents are the image voxel/intensity values used for training, inference, or preprocessing.", "default": ""}, "sp_axes": {"type": "any", "description": "Spatial axes along which to flip. By convention this refers to spatial axes relative to the channel-first layout (i.e., axes corresponding to the dimensions after the channel dimension). If None, the function will flip over all spatial axes (behavior documented in the original implementation). Negative axis indices are permitted and count from the last spatial axis toward the first. A tuple of integers flips on each axis specified in the tuple. The provided axes are mapped internally to the tensor axes that include the channel dimension using MONAI's map_spatial_axes utility before the flip is performed.", "default": ""}, "lazy": {"type": "boolean", "description": "When False (eager mode), the function performs the flip immediately by calling torch.flip and returns the flipped tensor (or a MetaTensor with updated metadata). When True (lazy mode), the function does not modify the image data; instead it records the flip as traceable metadata via TraceableTransform.track_transform_meta and returns that metadata or a MetaTensor carrying that metadata so the actual pixel/voxel-level flip can be applied later in a composed transform pipeline. Use lazy=True when building transform pipelines that should accumulate metadata for later application (e.g., for efficient I/O or delayed application), and lazy=False when you need the flipped image data immediately (e.g., for model input).", "default": ""}, "transform_info": {"type": "any", "description": "Dictionary that accumulates transform-specific information across composed transforms in a pipeline. This dictionary is passed to TraceableTransform.track_transform_meta and is used to propagate, merge, or annotate metadata for downstream transforms, inverse operations, or logging. The function will update transform_info implicitly via the metadata object it creates; callers typically pass a mutable dict maintained by pipeline orchestration code.", "default": ""}}, "required": ["img", "sp_axes", "lazy", "transform_info"], "type": "any"}}, "type": "function"}], "query": "We’re doing augmentation QC on a mixed MONAI cohort where some tensors arrive with inconsistent spatial metadata. Use eager execution (lazy=false) and always propagate the provided transform_info for traceability. Apply a flip only to samples whose transform_info indicates they are in an augmentation-capable stage (pipeline_stage is either \"augmentation\" or \"augment\"). For eligible samples, choose flip axes based on intrinsic tensor spatial rank: if the tensor is 3D spatial (channel-first with three spatial dims), flip the last two spatial axes; if it is 2D spatial, flip only the last spatial axis using negative indexing. Run this rule over the following raw inputs in one pass: (1) subject patient_001 with tensor [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]]]] and transform_info={\"pipeline_stage\":\"augmentation\",\"subject_id\":\"patient_001\",\"transforms_applied\":[\"normalize_intensity\"]}; (2) case augtest_001 with tensor [[[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0], [8.0, 9.0, 10.0, 11.0]]] and transform_info={\"case_id\":\"augtest_001\",\"stage\":\"train\",\"notes\":\"eager horizontal flip over W axis\"}; (3) source ct_patch_001 with tensor [[[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]], [[6.0, 7.0, 8.0], [9.0, 10.0, 11.0]]]] and transform_info={\"source\":\"ct_patch_001\",\"pipeline_stage\":\"augment\",\"applied\":[]}.", "answers": "[{\"name\":\"monai_transforms_spatial_functional_flip\",\"arguments\":{\"img\":[[[[0.1,0.2],[0.3,0.4]],[[0.5,0.6],[0.7,0.8]]]],\"sp_axes\":[1,2],\"lazy\":false,\"transform_info\":{\"pipeline_stage\":\"augmentation\",\"subject_id\":\"patient_001\",\"transforms_applied\":[\"normalize_intensity\"]}}},{\"name\":\"monai_transforms_spatial_functional_flip\",\"arguments\":{\"img\":[[[[0.0,1.0,2.0],[3.0,4.0,5.0]],[[6.0,7.0,8.0],[9.0,10.0,11.0]]]],\"sp_axes\":[1,2],\"lazy\":false,\"transform_info\":{\"source\":\"ct_patch_001\",\"pipeline_stage\":\"augment\",\"applied\":[]}}}]"}
{"func_name": "monai_transforms_spatial_functional_rotate90", "func_desc": "Functional implementation of rotate90 used by MONAI for spatial rotations of channel-first medical image tensors. This function composes an affine rotation (90-degree increments) and optionally performs the numerical rotation on the tensor data or records the rotation lazily as transform metadata for later application. It is commonly used in preprocessing pipelines for healthcare imaging (2D slices or 3D volumes) where reproducible metadata tracking is required for downstream transforms, visualization, or inverse mapping.", "tools": [{"function": {"description": "Functional implementation of rotate90 used by MONAI for spatial rotations of channel-first medical image tensors. This function composes an affine rotation (90-degree increments) and optionally performs the numerical rotation on the tensor data or records the rotation lazily as transform metadata for later application. It is commonly used in preprocessing pipelines for healthcare imaging (2D slices or 3D volumes) where reproducible metadata tracking is required for downstream transforms, visualization, or inverse mapping.\n", "name": "monai_transforms_spatial_functional_rotate90", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "float"}, "description": "Input image tensor to be rotated. The function assumes a channel-first layout (channels as dimension 0), so spatial dimensions are expected at dims 1..N. If img is a MONAI MetaTensor, the function will use MetaTensor APIs to read pending shape/rank and may return a MetaTensor with updated metadata; otherwise a plain torch.Tensor is used and metadata (when lazy) is returned as a separate object. Practical significance: callers should provide itk-like channel-first medical image tensors (e.g., CxHxW or CxDxHxW) so that spatial axes indexing and metadata bookkeeping align with MONAI conventions.", "default": ""}, "axes": {"type": "any", "description": "Two integers selecting the spatial plane to rotate. These indices refer to the dimensions of a channel-first tensor (for example, (1, 2) typically selects height and width for a 2D image). Negative values are allowed and count from the last axis toward the first axis. Internally the implementation also derives a zero-based spatial-axis pair (axes - 1) for shape bookkeeping; users must therefore supply axes consistent with channel-first indexing. Practical significance: use this to specify which two spatial axes (slice/row/column or depth/row/column) are rotated.", "default": ""}, "k": {"type": "integer", "description": "Number of times to rotate by 90 degrees. Each increment represents an additional 90-degree rotation applied in the plane specified by axes. A value of 0 (or any integer congruent to 0 mod 4) results in an identity rotation, but metadata will still be updated/tracked. Practical significance: use small integer counts to perform common rotations (k=1 for 90°, k=2 for 180°, k=3 for 270°).", "default": ""}, "lazy": {"type": "boolean", "description": "Flag indicating lazy (metadata-only) versus eager (data-modifying) operation. If False (the typical/eager mode), the function applies torch.rot90 to the tensor data and returns the rotated tensor (with metadata attached when a MetaTensor was given). If True (lazy mode, default behavior in higher-level pipelines), the function does not modify tensor data; instead it computes and returns the transformation metadata (or a MetaTensor whose metadata describes the pending rotation). Practical significance: lazy=True is used in pipelines that accumulate transforms for batched/incremental application or for later resolution on a different device. Note: the original implementation documents lazy default as False; higher-level transforms in MONAI may call this function with lazy=True when composing transformations.", "default": ""}, "transform_info": {"type": "any", "description": "A dictionary containing the relevant information pertaining to an already-applied transform chain; this dictionary is passed to the internal TraceableTransform.track_transform_meta call to merge/augment transform history. The function returns or attaches updated transform metadata that includes the new rotation parameters under extra_info. Practical significance: this allows callers to maintain a chained record of spatial operations (important for inverse transforms, label mapping, or exporting transform provenance).", "default": ""}}, "required": ["img", "axes", "k", "lazy", "transform_info"], "type": "any"}}, "type": "function"}], "query": "We’re doing an orientation QA pass before training, but the cohort metadata is messy and we need a rule-driven pipeline that decides rotation per-volume. Use the same toy single-channel tensor as a stand-in for every volume (C×D×H×W = 1×2×2×3): img = [[[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]], [[6.0, 7.0, 8.0], [9.0, 10.0, 11.0]]]].\n\nFor each record below, perform an eager (lazy=false) in-plane axial rotation in (H,W) (axes=(2,3)) and append/propagate the provided transform metadata.\n\nRotation policy (apply per record):\n- If the record indicates a suspected left–right flip (field `qc_flags` contains the token `suspected_LR_flip`), rotate by 180° (k=2).\n- Otherwise, if the acquisition `stage` is `train`, rotate by 90° (k=1).\n- Otherwise, rotate by 270° (k=3) to match the expected viewing convention for non-training exports.\n\nRaw cohort records:\n1) {id:'case_017_mri', stage:'val', qc_flags:['suspected_LR_flip','motion_ok'], applied_ops:[{name:'normalize_intensity', params:{method:'zscore'}}, {name:'center_crop', params:{roi_size:[1,2,3]}}]}\n2) {subject_id:'BRATS_0123', stage:'train', qc_flags:['motion_ok'], applied_operations:[{name:'spacing', pixdim:[1.0,1.0,1.2]}, {name:'normalize_intensity', channel_wise:true}]}\n3) {id:'siteB_0007', stage:'test', qc_flags:['motion_ok'], applied_ops:[{name:'spacing', pixdim:[0.9,0.9,1.0]}, {name:'bias_field_correction', method:'n4'}]}\n\nReturn the rotate90 calls needed for all records that can be rotated under this policy while carrying through their existing transform history fields unchanged (including their original key names such as `applied_ops` vs `applied_operations`).", "answers": "[{\"name\":\"monai_transforms_spatial_functional_rotate90\",\"arguments\":{\"img\":[[[[0.0,1.0,2.0],[3.0,4.0,5.0]],[[6.0,7.0,8.0],[9.0,10.0,11.0]]]],\"axes\":[2,3],\"k\":2,\"lazy\":false,\"transform_info\":{\"id\":\"case_017_mri\",\"stage\":\"val\",\"qc_flags\":[\"suspected_LR_flip\",\"motion_ok\"],\"applied_ops\":[{\"name\":\"normalize_intensity\",\"params\":{\"method\":\"zscore\"}},{\"name\":\"center_crop\",\"params\":{\"roi_size\":[1,2,3]}}]}}},{\"name\":\"monai_transforms_spatial_functional_rotate90\",\"arguments\":{\"img\":[[[[0.0,1.0,2.0],[3.0,4.0,5.0]],[[6.0,7.0,8.0],[9.0,10.0,11.0]]]],\"axes\":[2,3],\"k\":1,\"lazy\":false,\"transform_info\":{\"subject_id\":\"BRATS_0123\",\"stage\":\"train\",\"qc_flags\":[\"motion_ok\"],\"applied_operations\":[{\"name\":\"spacing\",\"pixdim\":[1.0,1.0,1.2]},{\"name\":\"normalize_intensity\",\"channel_wise\":true}]}}},{\"name\":\"monai_transforms_spatial_functional_rotate90\",\"arguments\":{\"img\":[[[[0.0,1.0,2.0],[3.0,4.0,5.0]],[[6.0,7.0,8.0],[9.0,10.0,11.0]]]],\"axes\":[2,3],\"k\":3,\"lazy\":false,\"transform_info\":{\"id\":\"siteB_0007\",\"stage\":\"test\",\"qc_flags\":[\"motion_ok\"],\"applied_ops\":[{\"name\":\"spacing\",\"pixdim\":[0.9,0.9,1.0]},{\"name\":\"bias_field_correction\",\"method\":\"n4\"}]}}}]"}
{"func_name": "monai_transforms_utils_check_boundaries", "func_desc": "Check boundaries for Signal transforms.\n    \n    Validate that the provided boundaries argument is a list of exactly two float values used by MONAI Signal transforms. In the MONAI medical-imaging preprocessing domain, Signal transforms operate on 1D signals or time-series (for example, signals derived from imaging modalities or physiologic traces). This utility enforces that the transform receives a well-formed interval or pair of limits (lower and upper boundary) so downstream windowing, clipping, or scaling operations behave deterministically and consistently across a preprocessing pipeline.", "tools": [{"function": {"description": "Check boundaries for Signal transforms.\n\nValidate that the provided boundaries argument is a list of exactly two float values used by MONAI Signal transforms. In the MONAI medical-imaging preprocessing domain, Signal transforms operate on 1D signals or time-series (for example, signals derived from imaging modalities or physiologic traces). This utility enforces that the transform receives a well-formed interval or pair of limits (lower and upper boundary) so downstream windowing, clipping, or scaling operations behave deterministically and consistently across a preprocessing pipeline.", "name": "monai_transforms_utils_check_boundaries", "parameters": {"properties": {"boundaries": {"type": "array", "items": {"type": "float"}, "description": "A list containing exactly two float values that represent the lower and upper boundary used by Signal transforms. The list must have length 2 and each element must be of Python type float. There is no default value; callers must pass this argument explicitly. This parameter's practical significance is to define the numeric interval applied by signal-level operations (e.g., clipping or windowing) in MONAI preprocessing workflows. If callers provide a value of a different type (for example, tuple, int-only values, nested lists, or strings), or a list with length not equal to 2, validation will fail.", "default": ""}}, "required": ["boundaries"], "type": "any"}}, "type": "function"}], "query": "We’re QC’ing a mixed ECG-derived amplitude-window specification file before feeding it into a MONAI Signal clipping+normalization stage. Each record is a candidate `boundaries` value and may come from different exporters, so formatting is inconsistent. Treat a record as a valid clipping window only if it represents exactly two finite float limits in strictly increasing [lower, upper] order (lower < upper), suitable for deterministic clipping. From the raw candidate list below, run MONAI boundary validation only on those windows that meet the scientific/format criterion; ignore anything with swapped/degenerate limits, non-floats, NaNs/Infs, nested structure, or wrong length.\n\nRaw candidate windows:\n1) [-1.5, 1.5]\n2) [-0.75, 1.25]\n3) [1.25, -0.75]\n4) [-1.0, -1.0]\n5) [-2, 2]\n6) [-1.5, 1.5, 2.0]\n7) \"-1.5,1.5\"\n8) [null, 1.0]\n9) [NaN, 1.0]\n10) [[-1.0, 1.0]]", "answers": "[{\"name\":\"monai_transforms_utils_check_boundaries\",\"arguments\":{\"boundaries\":[-1.5,1.5]}},{\"name\":\"monai_transforms_utils_check_boundaries\",\"arguments\":{\"boundaries\":[-0.75,1.25]}}]"}
{"func_name": "monai_transforms_utils_create_t__rm_ims_get_stacked_before_after", "func_desc": "monai.transforms.utils_create_transform_ims.get_stacked_before_after returns the processed (stacked) representations of the provided \"before\" and \"after\" image arrays by delegating each array to get_stacked_2d_ims. This function is used in MONAI transform/visualization pipelines for medical imaging to produce 2D visual summaries of 2D or 3D image inputs so that \"before\" and \"after\" states of a transform can be compared side-by-side or inspected.", "tools": [{"function": {"description": "monai.transforms.utils_create_transform_ims.get_stacked_before_after returns the processed (stacked) representations of the provided \"before\" and \"after\" image arrays by delegating each array to get_stacked_2d_ims. This function is used in MONAI transform/visualization pipelines for medical imaging to produce 2D visual summaries of 2D or 3D image inputs so that \"before\" and \"after\" states of a transform can be compared side-by-side or inspected.\n", "name": "monai_transforms_utils_create_t__rm_ims_get_stacked_before_after", "parameters": {"properties": {"before": {"type": "array", "items": {"type": "float"}, "description": "The input image array representing the \"before\" state in a transform or preprocessing pipeline. In the MONAI medical-imaging context this is typically a 2D image or a 3D volume (for example, a single-channel CT/MR volume). When a 3D array is provided, it is expected that get_stacked_2d_ims will convert the volume into a single stacked 2D representation (for visualization). This function assumes that the spatial dimensions of this array match the corresponding dimensions of the `after` array; if they do not match, downstream processing in get_stacked_2d_ims may raise an exception.", "default": ""}, "after": {"type": "array", "items": {"type": "float"}, "description": "The input image array representing the \"after\" state produced by a transform. This has the same intended format and role as `before`: typically a 2D image or 3D volume from medical-imaging workflows. When 3D, get_stacked_2d_ims will be applied to produce a stacked 2D visualization. The function requires that `after` and `before` have compatible spatial sizes; mismatched sizes are not handled here and will likely cause an error from the underlying stacking routine.", "default": ""}, "is_label": {"type": "boolean", "description": "Flag indicating whether the provided arrays are label maps (segmentation masks) rather than continuous intensity images. Default is False. This flag is forwarded to get_stacked_2d_ims so that label-specific handling (for example, preserving discrete labels or using label-appropriate visualization rules) can be applied by that helper. The boolean controls processing semantics but no additional validation of label content is performed here.", "default": false}}, "required": ["before", "after", "is_label"], "type": "any"}}, "type": "function"}], "query": "We’re doing a QC pass on a mixed-modality MONAI preprocessing run where some transform pairs are valid intensity images and others are likely bad captures. For each candidate pair below, generate the stacked 2D before/after summaries (treat as intensity images, not label masks) only when the before/after volumes are structurally comparable: both must be 3D arrays with identical (D,H,W) and must be numerically finite throughout. Use the following raw candidates:\n\n1) CT skull-stripping sanity check (candidate A)\n- before: [[[0.0, 12.3, 15.2], [8.1, 9.4, 10.0]], [[5.2, 7.8, 9.1], [3.4, 4.5, 6.7]]]\n- after:  [[[0.0, 11.0, 14.0], [7.5, 9.0, 9.8]], [[4.8, 7.3, 8.9], [3.0, 4.0, 6.2]]]\n\n2) MR normalization debug (candidate B)\n- before: [[[10,12],[14,16]],[[18,20],[22,24]],[[26,28],[30,32]]]\n- after:  [[[-1.0,-0.8],[-0.6,-0.4]],[[-0.2,0.0],[0.2,0.4]],[[0.6,0.8],[1.0,1.2]]]\n\n3) Artifacted export (candidate C)\n- before: [[[1.0, 2.0],[3.0, 4.0]]]\n- after:  [[[1.0, 2.0],[3.0, 4.0]], [[5.0, 6.0],[7.0, 8.0]]]\n\nProduce stacked before/after visual representations for all candidates that satisfy the structural comparability and finiteness criteria.", "answers": "[{\"name\":\"monai_transforms_utils_create_t__rm_ims_get_stacked_before_after\",\"arguments\":{\"before\":[[[0.0,12.3,15.2],[8.1,9.4,10.0]],[[5.2,7.8,9.1],[3.4,4.5,6.7]]],\"after\":[[[0.0,11.0,14.0],[7.5,9.0,9.8]],[[4.8,7.3,8.9],[3.0,4.0,6.2]]],\"is_label\":false}},{\"name\":\"monai_transforms_utils_create_t__rm_ims_get_stacked_before_after\",\"arguments\":{\"before\":[[[10,12],[14,16]],[[18,20],[22,24]],[[26,28],[30,32]]],\"after\":[[[-1.0,-0.8],[-0.6,-0.4]],[[-0.2,0.0],[0.2,0.4]],[[0.6,0.8],[1.0,1.2]]],\"is_label\":false}}]"}
{"func_name": "monai_transforms_utils_img_bounds", "func_desc": "monai.transforms.utils.img_bounds computes the bounding indices of non-zero content along the first two axes of a NumPy array. It is intended for use in MONAI preprocessing pipelines (medical imaging) to find the first and last rows/columns (axis 0 and axis 1) that contain any foreground (non-zero) values so callers can derive a tight in-plane bounding box for cropping or centering operations.\n    \n    This function inspects axis 0 and axis 1 of the provided image array using numpy.any to detect non-zero elements. It returns a 1D numpy array of four integer indices: the minimum and maximum index where axis 0 contains any non-zero elements, followed by the minimum and maximum index where axis 1 contains any non-zero elements.", "tools": [{"function": {"description": "monai.transforms.utils.img_bounds computes the bounding indices of non-zero content along the first two axes of a NumPy array. It is intended for use in MONAI preprocessing pipelines (medical imaging) to find the first and last rows/columns (axis 0 and axis 1) that contain any foreground (non-zero) values so callers can derive a tight in-plane bounding box for cropping or centering operations.\n\nThis function inspects axis 0 and axis 1 of the provided image array using numpy.any to detect non-zero elements. It returns a 1D numpy array of four integer indices: the minimum and maximum index where axis 0 contains any non-zero elements, followed by the minimum and maximum index where axis 1 contains any non-zero elements.", "name": "monai_transforms_utils_img_bounds", "parameters": {"properties": {"img": {"type": "array", "items": {"type": "float"}, "description": "Input image array used by MONAI preprocessing utilities. The array may be multi-dimensional; this function evaluates non-zero presence along the first two dimensions (axis 0 and axis 1). Elements are interpreted with NumPy truthiness (zero values are treated as background, non-zero values as foreground). This parameter is required and is not modified by the function.", "default": ""}}, "required": ["img"], "type": "any"}}, "type": "function"}], "query": "I’m doing an in-plane foreground localization QC step before cropping in a MONAI-style preprocessing pipeline, but the batch contains mixed modalities and occasional acquisition artifacts. I have two 2D arrays (same pixel grid). For each array, first decide whether it contains any plausible foreground: treat values as foreground only if they are strictly positive (to guard against negative bias-field corrected intensities), and only run bounds extraction on slices where the count of strictly-positive pixels is at least 4 (to avoid single-voxel speckle triggering a crop). For every slice that passes this QC gate, compute the tight foreground bounds along axes 0 and 1 (min_row, max_row, min_col, max_col) using the foreground definition above.\n\nSamples:\n1) Bias-corrected MRI slice (may contain negative values): [[0, 0, 0, 0, 0, 0], [0, 5, 8, 0, 0, 0], [0, 7, 10, 3, 0, 0], [0, 0, 4, 2, 0, 0], [0, 0, 0, 0, 0, 0]]\n2) Tumor mask candidate (binary, but still gate it the same way to catch empty masks): [[0,0,0,0,0,0],[0,0,1,1,0,0],[0,0,1,1,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0]]", "answers": "[{\"name\":\"monai_transforms_utils_img_bounds\",\"arguments\":{\"img\":[[0,0,0,0,0,0],[0,5,8,0,0,0],[0,7,10,3,0,0],[0,0,4,2,0,0],[0,0,0,0,0,0]]}},{\"name\":\"monai_transforms_utils_img_bounds\",\"arguments\":{\"img\":[[0,0,0,0,0,0],[0,0,1,1,0,0],[0,0,1,1,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0]]}}]"}
{"func_name": "monai_transforms_utils_pytorch_numpy_unification_unravel_indices", "func_desc": "Compute unravelled coordinates for a sequence of flat indices and return them\n    as a stacked NumPy array or PyTorch tensor. This utility is used in MONAI's\n    pytorch/numpy unification helpers (monai.transforms.utils_pytorch_numpy_unification)\n    to convert flattened indices (for example, indices produced by argmax over a\n    flattened image or a flattened region of interest) into multi-dimensional\n    coordinates that correspond to positions in medical imaging volumes or tensors\n    used in MONAI transforms and post-processing.", "tools": [{"function": {"description": "Compute unravelled coordinates for a sequence of flat indices and return them\nas a stacked NumPy array or PyTorch tensor. This utility is used in MONAI's\npytorch/numpy unification helpers (monai.transforms.utils_pytorch_numpy_unification)\nto convert flattened indices (for example, indices produced by argmax over a\nflattened image or a flattened region of interest) into multi-dimensional\ncoordinates that correspond to positions in medical imaging volumes or tensors\nused in MONAI transforms and post-processing.", "name": "monai_transforms_utils_pytorch_numpy_unification_unravel_indices", "parameters": {"properties": {"idx": {"type": "array", "items": {"type": "float"}, "description": "A list (sequence) of indices to unravel. Each element of this\nlist should be an index or an array/tensor of indices compatible with\nthe helper unravel_index implementation used by MONAI. Typical\npractical usage is a list of Python integer scalars, NumPy integer\narrays, or torch.Tensor objects containing integer indices. The order\nof elements in this list is preserved in the output: the i-th element\nof the returned stacked result corresponds to the i-th element of\nthis list. If the first element of idx is a torch.Tensor, the function\nuses torch.stack and returns a torch.Tensor; otherwise it uses\nnumpy.stack and returns a numpy.ndarray. Passing an empty list will\nraise IndexError because the routine inspects idx[0] to determine the\nnumeric library to use.", "default": ""}, "shape": {"type": "any", "description": "The shape of the target array or tensor for which indices\nare interpreted. This tuple enumerates the lengths of each axis in the\nsame order as used in the array/tensor (the axis order in the returned\ncoordinates matches the order of this tuple). In practical MONAI\nworkflows, shape is typically the spatial shape of an image/tensor\n(for example the shape of a multi-dimensional medical image or model\noutput). Values in idx must be valid flat indices for an array/tensor\nwith this shape; otherwise the underlying unravel_index implementation\nwill raise an error.", "default": ""}}, "required": ["idx", "shape"], "type": "any"}}, "type": "function"}], "query": "We’re triaging peak detections from two MONAI post-processing streams before writing QC overlays. The raw peak extractor produced flat indices that may include padding artifacts (negative) and occasional overflow when ROI windows extend beyond the tensor (indices ≥ total voxel count). For each volume, recover voxel coordinates only for indices that fall within the valid flattened range implied by the original shape.\n\nStream 1 (brain MRI logits): original volume shape is (128, 128, 64). Candidate flat indices: [-3, 10500, 250000, 500000, 1048576]. Unravel only the indices that correspond to valid voxels in this volume.\n\nStream 2 (small phantom scan): original volume shape is (4, 5, 6). Candidate flat indices: [0, 17, 83, 119, 120]. Unravel only the indices that correspond to valid voxels in this volume.\n\nReturn coordinates in the standard axis order used by unraveling with the provided shapes.", "answers": "[{\"name\":\"monai_transforms_utils_pytorch_numpy_unification_unravel_indices\",\"arguments\":{\"idx\":[10500,250000,500000],\"shape\":[128,128,64]}},{\"name\":\"monai_transforms_utils_pytorch_numpy_unification_unravel_indices\",\"arguments\":{\"idx\":[0,17,83,119],\"shape\":[4,5,6]}}]"}
{"func_name": "monai_transforms_utils_rand_choice", "func_desc": "monai.transforms.utils.rand_choice returns a boolean that is True with probability prob and False otherwise. It is intended for use in MONAI transform pipelines and data-augmentation logic to make stochastic decisions (for example, whether to apply a particular spatial or intensity augmentation during preprocessing or training of medical imaging models).", "tools": [{"function": {"description": "monai.transforms.utils.rand_choice returns a boolean that is True with probability prob and False otherwise. It is intended for use in MONAI transform pipelines and data-augmentation logic to make stochastic decisions (for example, whether to apply a particular spatial or intensity augmentation during preprocessing or training of medical imaging models).\n", "name": "monai_transforms_utils_rand_choice", "parameters": {"properties": {"prob": {"type": "float", "description": "The probability of returning True. This parameter represents the desired probability that the function yields a True outcome; the default value 0.5 implements a 50/50 chance. Internally the function draws a single pseudorandom float from Python's standard library random.random() (which produces values in [0.0, 1.0)) and returns True when that draw is less than or equal to prob. Typical usage in MONAI is to pass a float in the closed interval [0.0, 1.0] where 0.0 means almost always False (True only if random.random() happens to be exactly 0.0, an extremely unlikely event) and 1.0 means always True. If prob > 1.0 the comparison will always be True; if prob < 0.0 the comparison will always be False. If prob is not a numeric type comparable to a float, the comparison may raise a TypeError.", "default": 0.5}}, "required": ["prob"], "type": "any"}}, "type": "function"}], "query": "We’re running one training iteration of a 3D brain MRI segmentation pipeline where augmentation gating must reflect cohort heterogeneity. For each incoming volume, first classify it by QC metadata: if the acquisition is flagged as motion-corrupted OR the bias-field nonuniformity score is above the cohort median, treat it as “unstable”; otherwise treat it as “stable”. Apply the following stochastic gating rules, with independent decisions per rule:\n\n1) Heavy intensity augmentation: use prob=0.1 for unstable volumes and prob=0.35 for stable volumes.\n2) Gaussian noise augmentation: use prob=0.45 for unstable volumes and prob=0.2 for stable volumes.\n\nCohort for this iteration (use the provided metadata as-is):\n- sub-001: motion_flag=false, bias_score=0.62\n- sub-002: motion_flag=true,  bias_score=0.41\n- sub-003: motion_flag=false, bias_score=0.18\n- sub-004: motion_flag=false, bias_score=0.62\n- sub-005: motion_flag=true,  bias_score=0.95\n\nReturn the boolean gating decision for each applicable augmentation module per subject for this iteration (two independent rand_choice calls per subject, with prob determined by the subject’s stability class).", "answers": "[{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.35}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.2}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.1}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.45}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.35}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.2}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.35}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.2}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.1}},{\"name\":\"monai_transforms_utils_rand_choice\",\"arguments\":{\"prob\":0.45}}]"}
{"func_name": "monai_utils_component_store_is_variable", "func_desc": "Check whether a given string is a valid Python variable name and is not a reserved Python keyword.\n    \n    This utility is used in MONAI (a PyTorch-based framework for medical imaging deep learning) to validate candidate identifiers that may be used as variable names, attribute names, configuration keys, or registry/component names (for example when building or populating a component store of models, transforms, losses, or other reusable objects). Ensuring a name is both a valid identifier and not a keyword helps avoid syntax errors, prevents accidental shadowing of language keywords, and supports safe dynamic creation of attributes or generated code in MONAI workflows.", "tools": [{"function": {"description": "Check whether a given string is a valid Python variable name and is not a reserved Python keyword.\n\nThis utility is used in MONAI (a PyTorch-based framework for medical imaging deep learning) to validate candidate identifiers that may be used as variable names, attribute names, configuration keys, or registry/component names (for example when building or populating a component store of models, transforms, losses, or other reusable objects). Ensuring a name is both a valid identifier and not a keyword helps avoid syntax errors, prevents accidental shadowing of language keywords, and supports safe dynamic creation of attributes or generated code in MONAI workflows.", "name": "monai_utils_component_store_is_variable", "parameters": {"properties": {"name": {"type": "string", "description": "The string to test as a potential Python variable/identifier. This function evaluates the exact semantics of Python identifiers by calling str.isidentifier() and also checks against Python reserved words via keyword.iskeyword(). The parameter is expected to be a Python str representing a candidate name used in contexts such as registry keys, dynamic attribute names, or configuration fields within MONAI; callers should provide a str. Passing a non-str value may result in an AttributeError or TypeError because the underlying checks rely on string methods and the standard keyword module.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed batch of MONAI component-store identifiers coming from multiple sources (human-authored YAML keys, auto-generated sweep names, and tags lifted from notebook prototypes). Before committing, run the identifier-safety validator on only those candidates that look like intended Python-style registry keys: after trimming surrounding whitespace, the string must already be lowercase ASCII and use only underscores as separators (no hyphens/spaces), and it must include at least one underscore (to avoid overly generic single-token names). From this raw batch, validate the candidates that meet those criteria: [' lung_ct_segmentation_v1 ', 'rand_flip3d', 'RandFlip3D', '3d_rand_flip', 'lung-ct-segmentation-v1', 'class', 'postproc__v2', ''].", "answers": "[{\"name\":\"monai_utils_component_store_is_variable\",\"arguments\":{\"name\":\"lung_ct_segmentation_v1\"}},{\"name\":\"monai_utils_component_store_is_variable\",\"arguments\":{\"name\":\"3d_rand_flip\"}},{\"name\":\"monai_utils_component_store_is_variable\",\"arguments\":{\"name\":\"postproc__v2\"}}]"}
{"func_name": "monai_utils_misc_list_to_dict", "func_desc": "monai.utils.misc.list_to_dict: Convert a list of \"key=value\" string items into a Python dictionary suitable for simple configuration parsing in MONAI workflows (for example, parsing command-line or bundle parameter overrides used in training/evaluation pipelines).\n    \n    This function accepts a sequence of text tokens where each token is expected to represent either a bare key (\"a\") or a key/value assignment (\"key=value\"). It is commonly used in MONAI to convert simple string-based parameter specifications into native Python types so they can be applied to configuration dictionaries for model training, inference, or pre-/post-processing pipelines.\n    \n    Behavior:\n    - Each element in items is split at the first \"=\" into a key and a value. If an element contains no \"=\", the value for that key becomes None. This supports shorthand flags or presence indicators.\n    - Surrounding whitespace and single-quote characters are removed from both keys and values by stripping the characters \" \n\r\t'\".\n    - After splitting and stripping, the function attempts to convert the textual value to a native Python object using ast.literal_eval (so numeric literals, lists, dicts, tuples, and quoted strings will become their Python equivalents).\n    - If ast.literal_eval raises a ValueError, the function then attempts to interpret the value as a boolean using distutils.util._strtobool; if that succeeds the boolean value is returned.\n    - If both conversions fail, the original stripped string is used as the value.\n    - Duplicate keys are considered an error: the function raises KeyError when the same key is encountered more than once.\n    - If items is an empty list (no elements), an empty dictionary is returned.\n    \n    Limitations and failure modes:\n    - The parameter type is list; passing a non-list is not documented and may lead to unexpected behavior. An empty list yields {}.\n    - ast.literal_eval exceptions other than ValueError (for example, SyntaxError) are not explicitly caught by the implementation and will propagate to the caller.\n    - The boolean conversion relies on distutils.util._strtobool semantics; values not recognized by that helper will not be converted to booleans and will fall back to the raw string.\n    - Keys and values have only the characters \" \n\r\t'\" stripped; other surrounding characters are preserved.", "tools": [{"function": {"description": "monai.utils.misc.list_to_dict: Convert a list of \"key=value\" string items into a Python dictionary suitable for simple configuration parsing in MONAI workflows (for example, parsing command-line or bundle parameter overrides used in training/evaluation pipelines).\n\nThis function accepts a sequence of text tokens where each token is expected to represent either a bare key (\"a\") or a key/value assignment (\"key=value\"). It is commonly used in MONAI to convert simple string-based parameter specifications into native Python types so they can be applied to configuration dictionaries for model training, inference, or pre-/post-processing pipelines.\n\nBehavior:\n- Each element in items is split at the first \"=\" into a key and a value. If an element contains no \"=\", the value for that key becomes None. This supports shorthand flags or presence indicators.\n- Surrounding whitespace and single-quote characters are removed from both keys and values by stripping the characters \" \n     '\".\n- After splitting and stripping, the function attempts to convert the textual value to a native Python object using ast.literal_eval (so numeric literals, lists, dicts, tuples, and quoted strings will become their Python equivalents).\n- If ast.literal_eval raises a ValueError, the function then attempts to interpret the value as a boolean using distutils.util._strtobool; if that succeeds the boolean value is returned.\n- If both conversions fail, the original stripped string is used as the value.\n- Duplicate keys are considered an error: the function raises KeyError when the same key is encountered more than once.\n- If items is an empty list (no elements), an empty dictionary is returned.\n\nLimitations and failure modes:\n- The parameter type is list; passing a non-list is not documented and may lead to unexpected behavior. An empty list yields {}.\n- ast.literal_eval exceptions other than ValueError (for example, SyntaxError) are not explicitly caught by the implementation and will propagate to the caller.\n- The boolean conversion relies on distutils.util._strtobool semantics; values not recognized by that helper will not be converted to booleans and will fall back to the raw string.\n- Keys and values have only the characters \" \n     '\" stripped; other surrounding characters are preserved.", "name": "monai_utils_misc_list_to_dict", "parameters": {"properties": {"items": {"type": "array", "items": {"type": "float"}, "description": "A list of strings representing key or key=value entries. Each element should be a string such as \"lr=0.001\", \"use_amp=True\", or \"tag\". Keys are parsed into dictionary keys (strings). Values are parsed into Python objects when possible via ast.literal_eval, then boolean-converted via _strtobool if literal_eval fails, and otherwise left as the stripped string. If an element contains no \"=\", the corresponding dictionary value is None. Supplying an empty list returns an empty dict. This function is typically used in MONAI to parse simple configuration overrides for training, evaluation, or preprocessing pipelines.", "default": ""}}, "required": ["items"], "type": "any"}}, "type": "function"}], "query": "We’re aggregating raw MONAI bundle override tokens coming from two acquisition sites into cohort-specific native-typed config dicts, but the CLI dumps are noisy. For each cohort, first apply a token-quality sieve: only accept tokens that either (a) are bare flags matching ^[A-Za-z_][A-Za-z0-9_.]*$ or (b) are key/value assignments where the key matches ^[A-Za-z_][A-Za-z0-9_.]*$ and the value is non-empty after stripping whitespace and single quotes. Discard anything with an empty key, missing value after '=', or any leading comment marker '#'. Then, for each cohort, run MONAI’s list_to_dict on the surviving tokens so that literals (ints/floats/tuples/lists/dicts/quoted strings) are parsed via literal_eval when possible, boolean-like strings are converted via strtobool, and bare flags become None.\n\nCohort A raw tokens (site-1 liver CT training): [\"trainer.max_epochs=250\", \"optim.lr=0.0003\", \"amp=True\", \"roi_size=(96,96,64)\", \"class_weights=[1.0,0.5,2.0]\", \"post_pred.transforms={'type':'AsDiscrete','argmax':True}\", \"run_name='liver_ct_v2'\", \"dry_run\", \"=42\", \"badkey=\", \"#scratch=1\", \"  \"]\n\nCohort B raw tokens (site-2 debug run): [\"batch_size=16\", \"lr=0.0005\", \"use_amp=true\", \"roi_size=[128,128,64]\", \"loss='DiceLoss'\", \"debug\", \"notes=\", \"#ignored\", \"=oops\"]", "answers": "[{\"name\":\"monai_utils_misc_list_to_dict\",\"arguments\":{\"items\":[\"trainer.max_epochs=250\",\"optim.lr=0.0003\",\"amp=True\",\"roi_size=(96,96,64)\",\"class_weights=[1.0,0.5,2.0]\",\"post_pred.transforms={'type':'AsDiscrete','argmax':True}\",\"run_name='liver_ct_v2'\",\"dry_run\"]}},{\"name\":\"monai_utils_misc_list_to_dict\",\"arguments\":{\"items\":[\"batch_size=16\",\"lr=0.0005\",\"use_amp=true\",\"roi_size=[128,128,64]\",\"loss='DiceLoss'\",\"debug\"]}}]"}
{"func_name": "monai_utils_module_damerau_levenshtein_distance", "func_desc": "Calculates the Damerau–Levenshtein distance between two strings, returning the minimum number of single-character edits (insertions, deletions, substitutions, or adjacent transpositions) required to transform s1 into s2. In the MONAI context this routine is used for spelling correction and string similarity tasks that arise in medical-imaging workflows (for example, normalizing metadata keys, matching label names, correcting typographical errors in annotations or clinical text before model training and analysis).\n    \n    This implementation follows the Damerau–Levenshtein definition (see https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance) and is designed to be a small, dependency-free utility for preprocessing and validation steps in MONAI pipelines. It performs an exact computation using a dynamic programming table (implemented as a dict) and treats an adjacent transposition of two characters as a single edit operation.", "tools": [{"function": {"description": "Calculates the Damerau–Levenshtein distance between two strings, returning the minimum number of single-character edits (insertions, deletions, substitutions, or adjacent transpositions) required to transform s1 into s2. In the MONAI context this routine is used for spelling correction and string similarity tasks that arise in medical-imaging workflows (for example, normalizing metadata keys, matching label names, correcting typographical errors in annotations or clinical text before model training and analysis).\n\nThis implementation follows the Damerau–Levenshtein definition (see https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance) and is designed to be a small, dependency-free utility for preprocessing and validation steps in MONAI pipelines. It performs an exact computation using a dynamic programming table (implemented as a dict) and treats an adjacent transposition of two characters as a single edit operation.", "name": "monai_utils_module_damerau_levenshtein_distance", "parameters": {"properties": {"s1": {"type": "string", "description": "The source string to be transformed. In MONAI usage this typically represents a label, metadata key, filename, or token extracted from clinical/annotation text whose spelling or format should be compared against a canonical or target string. The function uses Python string semantics (len, indexing); passing a non-str value will raise a TypeError when the function attempts string operations.", "default": ""}, "s2": {"type": "string", "description": "The target string to compare against s1. In MONAI workflows this represents the canonical form, expected label, or corrected token to which s1 should be compared or aligned. Like s1, this must be a Python str; non-str inputs will cause exceptions.", "default": ""}}, "required": ["s1", "s2"], "type": "any"}}, "type": "function"}], "query": "In a MONAI pre-ingest QC step we’re consolidating noisy free-text into a controlled vocabulary. We have a small set of candidate→canonical mappings coming from two sources: (A) label tokens extracted from segmentation headers, and (B) protocol strings parsed from DICOM StudyDescription.\n\nFor each candidate→canonical pair below, compute the Damerau–Levenshtein distance as an edit-effort score **only for pairs that look like they’re intended to be the same concept**, operationalized as: after lowercasing and removing spaces and punctuation, both strings must share the same first character and their length difference must be ≤ 2.\n\nPairs (candidate, canonical):\n1) (\"livre\", \"liver\")\n2) (\"abdomin pelivs ct w contrast\", \"abdomen pelvis ct w/ contrast\")\n3) (\"kidney_left\", \"left kidney\")\n4) (\"spine\", \"spleen\")\n\nReturn the distances for all pairs that pass the heuristic gate.", "answers": "[{\"name\":\"monai_utils_module_damerau_levenshtein_distance\",\"arguments\":{\"s1\":\"livre\",\"s2\":\"liver\"}},{\"name\":\"monai_utils_module_damerau_levenshtein_distance\",\"arguments\":{\"s1\":\"abdomin pelivs ct w contrast\",\"s2\":\"abdomen pelvis ct w/ contrast\"}}]"}
{"func_name": "monai_utils_module_get_package_version", "func_desc": "Get the installed version string for a given Python package name, or return a provided default message when the package is not available or does not expose a version.\n    \n    This function is used throughout MONAI (a PyTorch-based framework for medical imaging AI) to detect and report versions of optional or required third-party packages (for example, imaging libraries, dataset handlers, or utilities). It attempts to import the package by name and then read its __version__ attribute. The practical significance is to enable reproducible experiments, compatibility checks, diagnostic messages, and runtime guards within MONAI code paths that depend on specific dependency versions.", "tools": [{"function": {"description": "Get the installed version string for a given Python package name, or return a provided default message when the package is not available or does not expose a version.\n\nThis function is used throughout MONAI (a PyTorch-based framework for medical imaging AI) to detect and report versions of optional or required third-party packages (for example, imaging libraries, dataset handlers, or utilities). It attempts to import the package by name and then read its __version__ attribute. The practical significance is to enable reproducible experiments, compatibility checks, diagnostic messages, and runtime guards within MONAI code paths that depend on specific dependency versions.", "name": "monai_utils_module_get_package_version", "parameters": {"properties": {"dep_name": {"type": "string", "description": "The import name of the dependency to check (for example, \"torch\" or \"nibabel\"). This parameter identifies which installed Python package MONAI should attempt to load to obtain its version string. Providing the exact import name is required because the function calls an import helper to locate the package by this identifier.", "default": ""}, "default": {"type": "string", "description": "The string to return when the package cannot be loaded or when the loaded package does not have a __version__ attribute. The default value used in the implementation is \"NOT INSTALLED or UNKNOWN VERSION.\" and is intended to make missing-version situations explicit in logs, configuration reports, or error messages produced by MONAI workflows.", "default": "NOT INSTALLED or UNKNOWN VERSION."}}, "required": ["dep_name", "default"], "type": "any"}}, "type": "function"}], "query": "For a MONAI reproducibility/compatibility audit on heterogeneous deployment targets, we need to log dependency versions across two reporting streams. Use this raw dependency manifest (names may include vendor prefixes, case drift, and hyphen/underscore variants): `['torch', 'torchvision', 'torchaudio', 'pytorch-lightning', 'monai', 'nibabel', 'SimpleITK', 'opencv-python', 'skimage', 'pydicom', 'cupy', 'tqdm', 'numpy', 'scipy', 'pandas', 'matplotlib', 'tensorboard', 'einops', 'itk', 'h5py', 'yaml', 'PIL', 'nvidia-cudnn-cu12']`.\n\nApply a two-channel logging protocol driven by intrinsic package-name properties:\n1) For the MONAI runtime diagnostic channel, query version only for dependencies that are plausibly importable as Python modules (i.e., the name is already a valid import identifier after normalizing to lowercase and converting hyphens to underscores). Use the default message `\"<CANONICAL_NAME> NOT INSTALLED or UNKNOWN VERSION.\"`, where `<CANONICAL_NAME>` is the canonicalized import name uppercased.\n2) For the compatibility-check channel, query version only for entries that are either clearly GPU/accelerator-related (contain `cuda`, `cudnn`, `nvidia`, or `cupy`), or are directly in the PyTorch stack (canonicalized name starts with `torch`). Use the default message `\"NOT INSTALLED or UNKNOWN VERSION.\"`.\n\nRun both channels across the manifest and return the list of version-query calls needed to implement this audit.", "answers": "[{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"torch\",\"default\":\"TORCH NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"torchaudio\",\"default\":\"TORCHAUDIO NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"monai\",\"default\":\"MONAI NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"nibabel\",\"default\":\"NIBABEL NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"simpleitk\",\"default\":\"SIMPLEITK NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"skimage\",\"default\":\"SKIMAGE NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"pydicom\",\"default\":\"PYDICOM NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"cupy\",\"default\":\"CUPY NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"tqdm\",\"default\":\"TQDM NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"numpy\",\"default\":\"NUMPY NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"scipy\",\"default\":\"SCIPY NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"pandas\",\"default\":\"PANDAS NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"matplotlib\",\"default\":\"MATPLOTLIB NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"tensorboard\",\"default\":\"TENSORBOARD NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"einops\",\"default\":\"EINOPS NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"itk\",\"default\":\"ITK NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"h5py\",\"default\":\"H5PY NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"torch\",\"default\":\"NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"torchvision\",\"default\":\"NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"torchaudio\",\"default\":\"NOT INSTALLED or UNKNOWN VERSION.\"}},{\"name\":\"monai_utils_module_get_package_version\",\"arguments\":{\"dep_name\":\"cupy\",\"default\":\"NOT INSTALLED or UNKNOWN VERSION.\"}}]"}
{"func_name": "monai_utils_module_version_geq", "func_desc": "Returns True if version `lhs` is later than or equal to version `rhs` according to a best-effort comparison of version strings used by MONAI to gate features and enforce dependency compatibility (for example, checking PyTorch or other package versions before enabling code paths). The function first normalizes inputs by casting them to Python strings, then attempts to use the packaging.version.Version class when available for a robust semantic comparison. If packaging.version is unavailable, it falls back to an element-wise comparison produced by parse_version_strs. This function is therefore useful within MONAI to programmatically decide whether the running environment satisfies a minimum or specific version requirement without raising on common invalid-version formats (it prefers a permissive result in that case).", "tools": [{"function": {"description": "Returns True if version `lhs` is later than or equal to version `rhs` according to a best-effort comparison of version strings used by MONAI to gate features and enforce dependency compatibility (for example, checking PyTorch or other package versions before enabling code paths). The function first normalizes inputs by casting them to Python strings, then attempts to use the packaging.version.Version class when available for a robust semantic comparison. If packaging.version is unavailable, it falls back to an element-wise comparison produced by parse_version_strs. This function is therefore useful within MONAI to programmatically decide whether the running environment satisfies a minimum or specific version requirement without raising on common invalid-version formats (it prefers a permissive result in that case).\n", "name": "monai_utils_module_version_geq", "parameters": {"properties": {"lhs": {"type": "string", "description": "Left-hand side version string to evaluate as the candidate or runtime version (e.g., the installed package version). The function casts this value to str at the start, so non-string inputs will be converted. In MONAI this is typically the version of a dependency or the library itself and is treated as the version that should be \"later or equal\" for the function to return True.", "default": ""}, "rhs": {"type": "string", "description": "Right-hand side version string representing the requirement or baseline version to compare against (e.g., a minimum required dependency version). This is also cast to str and represents the version that lhs is tested to be greater than or equal to.", "default": ""}}, "required": ["lhs", "rhs"], "type": "any"}}, "type": "function"}], "query": "We’re preparing an environment-compatibility sweep across heterogeneous PyTorch installs observed in the cluster. Given the raw torch version strings reported by nodes: ['2.0.1+cu118', '2.0.1', '2.1.0rc2', '2.1.0', '2.1.1+cpu', '2.1', '2.1.0.post1', '2.1.0.dev20231010', 'nightly-2.2.0', 'unknown', ''], decide which nodes can enter the next experiment cohort under two feature gates:\n\n1) CUDA AMP workflow gate: evaluate only builds that look like CUDA-tagged wheels (contain '+cu'). For those, require >= '2.1.0'.\n2) MONAI feature-gated code path: evaluate only stable releases (i.e., version strings that do not include any pre/dev/nightly markers such as 'rc', 'dev', or 'nightly'). For those, require >= '2.1.0'.\n\nFor every version string that qualifies under either gate’s inclusion rule, run the best-effort version comparison and return whether it meets the threshold for that gate.", "answers": "[{\"name\":\"monai_utils_module_version_geq\",\"arguments\":{\"lhs\":\"2.0.1+cu118\",\"rhs\":\"2.1.0\"}},{\"name\":\"monai_utils_module_version_geq\",\"arguments\":{\"lhs\":\"2.1.1+cpu\",\"rhs\":\"2.1.0\"}},{\"name\":\"monai_utils_module_version_geq\",\"arguments\":{\"lhs\":\"2.1.0\",\"rhs\":\"2.1.0\"}},{\"name\":\"monai_utils_module_version_geq\",\"arguments\":{\"lhs\":\"2.1\",\"rhs\":\"2.1.0\"}},{\"name\":\"monai_utils_module_version_geq\",\"arguments\":{\"lhs\":\"2.1.0.post1\",\"rhs\":\"2.1.0\"}}]"}
{"func_name": "monai_utils_module_version_leq", "func_desc": "monai.utils.module.version_leq determines whether one version string (lhs) denotes an earlier or equal release than another version string (rhs).\n    \n    This function is used within MONAI for simple dependency and compatibility checks (for example, to determine whether a runtime or dependency version such as PyTorch satisfies a minimum or maximum required version). It accepts arbitrary inputs but first coerces them to Python str. It attempts to use the standard packaging.version.Version objects for robust semantic version comparison when the packaging library is available; if packaging.version is not available it falls back to a deterministic, segment-wise textual/numeric comparison implemented by parse_version_strs.\n    \n    Behavior details and practical significance:\n    - The function returns True when lhs represents a version earlier than or equal to rhs, and False when lhs represents a later version than rhs. This boolean result is suitable for gating feature availability or enforcing dependency constraints in MONAI workflows.\n    - Inputs are coerced to strings at the start (lhs and rhs are converted via str(lhs) and str(rhs)), so callers may pass objects that implement __str__.\n    - When the optional packaging.version module is present (imported via optional_import), the function constructs packaging.version.Version(lhs) and packaging.version.Version(rhs) and returns the result of the <= operator on those Version objects. This provides standard semantic-version semantics when available and is the preferred comparison path.\n    - If packaging.version.Version raises packaging.version.InvalidVersion for the provided strings, the implementation treats this as a conservative success and returns True. This behavior avoids blocking execution for unknown or nonstandard version formats when attempting to ensure compatibility in MONAI environments.\n    - If packaging.version is not available (optional_import indicates absence), the function uses parse_version_strs(lhs, rhs) as a fallback. In the fallback:\n        - The two version strings are parsed into corresponding sequences of segments (as returned by parse_version_strs).\n        - The function iterates segment-wise over zipped segments from both sequences. For the first pair of unequal segments:\n            - If both segments are integers, they are compared numerically (l < r).\n            - Otherwise, the segments are compared lexicographically as strings (f\"{l}\" < f\"{r}\").\n        - If all compared segments are equal for the length of the shorter parsed sequence, the function returns True. Practically, this means versions that are identical up to the shorter prefix are considered earlier-or-equal (for example, \"1.2\" is treated as <= \"1.2.0\" in the fallback logic).\n    - The function has no side effects: it does not perform I/O, modify global state, or change its arguments. It is deterministic for the same inputs and available environment (presence or absence of packaging.version).\n    \n    Failure modes and error handling:\n    - The function never intentionally raises exceptions for normal comparison of strings. If packaging.version is available but either lhs or rhs cannot be parsed into a valid packaging.version.Version, packaging.version.InvalidVersion is caught and the function returns True.\n    - If parse_version_strs is unable to parse the provided strings and itself raises an exception, that exception will propagate; callers who expect to handle malformed custom version formats may need to validate inputs or handle exceptions from parse_version_strs.\n    - The function does not validate that inputs conform to any specific versioning scheme beyond what packaging.version or parse_version_strs accept.", "tools": [{"function": {"description": "monai.utils.module.version_leq determines whether one version string (lhs) denotes an earlier or equal release than another version string (rhs).\n\nThis function is used within MONAI for simple dependency and compatibility checks (for example, to determine whether a runtime or dependency version such as PyTorch satisfies a minimum or maximum required version). It accepts arbitrary inputs but first coerces them to Python str. It attempts to use the standard packaging.version.Version objects for robust semantic version comparison when the packaging library is available; if packaging.version is not available it falls back to a deterministic, segment-wise textual/numeric comparison implemented by parse_version_strs.\n\nBehavior details and practical significance:\n- The function returns True when lhs represents a version earlier than or equal to rhs, and False when lhs represents a later version than rhs. This boolean result is suitable for gating feature availability or enforcing dependency constraints in MONAI workflows.\n- Inputs are coerced to strings at the start (lhs and rhs are converted via str(lhs) and str(rhs)), so callers may pass objects that implement __str__.\n- When the optional packaging.version module is present (imported via optional_import), the function constructs packaging.version.Version(lhs) and packaging.version.Version(rhs) and returns the result of the <= operator on those Version objects. This provides standard semantic-version semantics when available and is the preferred comparison path.\n- If packaging.version.Version raises packaging.version.InvalidVersion for the provided strings, the implementation treats this as a conservative success and returns True. This behavior avoids blocking execution for unknown or nonstandard version formats when attempting to ensure compatibility in MONAI environments.\n- If packaging.version is not available (optional_import indicates absence), the function uses parse_version_strs(lhs, rhs) as a fallback. In the fallback:\n    - The two version strings are parsed into corresponding sequences of segments (as returned by parse_version_strs).\n    - The function iterates segment-wise over zipped segments from both sequences. For the first pair of unequal segments:\n        - If both segments are integers, they are compared numerically (l < r).\n        - Otherwise, the segments are compared lexicographically as strings (f\"{l}\" < f\"{r}\").\n    - If all compared segments are equal for the length of the shorter parsed sequence, the function returns True. Practically, this means versions that are identical up to the shorter prefix are considered earlier-or-equal (for example, \"1.2\" is treated as <= \"1.2.0\" in the fallback logic).\n- The function has no side effects: it does not perform I/O, modify global state, or change its arguments. It is deterministic for the same inputs and available environment (presence or absence of packaging.version).\n\nFailure modes and error handling:\n- The function never intentionally raises exceptions for normal comparison of strings. If packaging.version is available but either lhs or rhs cannot be parsed into a valid packaging.version.Version, packaging.version.InvalidVersion is caught and the function returns True.\n- If parse_version_strs is unable to parse the provided strings and itself raises an exception, that exception will propagate; callers who expect to handle malformed custom version formats may need to validate inputs or handle exceptions from parse_version_strs.\n- The function does not validate that inputs conform to any specific versioning scheme beyond what packaging.version or parse_version_strs accept.", "name": "monai_utils_module_version_leq", "parameters": {"properties": {"lhs": {"type": "string", "description": "Left-hand side version string to compare. In MONAI this is typically the currently observed or candidate version (for example, the installed PyTorch or dependency version). The function will coerce lhs to str and interpret it as a version identifier; it returns True if lhs is earlier than or equal to rhs according to packaging.version semantics when available, otherwise by the fallback segment-wise comparison.", "default": ""}, "rhs": {"type": "string", "description": "Right-hand side version string to compare. In MONAI this is typically the target, minimum, or maximum version to compare against (for example, a required or tested dependency version). The function will coerce rhs to str and interpret it as a version identifier; it serves as the comparison target such that the function returns True when lhs is earlier-or-equal to this value.", "default": ""}}, "required": ["lhs", "rhs"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a multi-site MONAI imaging training sweep where dependency strings come from heterogeneous telemetry (pip freeze, conda, and container tags). For each reported cohort record, run an upper-bound compatibility gate by checking whether the observed version is earlier-or-equal to that cohort’s validated cap using `monai.utils.module.version_leq(lhs, rhs)`. Apply this policy: only evaluate records whose observed version string is parseable as a strict dotted numeric release (exactly three dot-separated integer components, e.g., X.Y.Z). For the remaining records, evaluate them all. Dataset (cohort, observed, cap):\n- runtime/pytorch: observed `2.1.0+cu121`, cap `2.1.3`\n- runtime/pytorch: observed `2.1.0`, cap `2.1.3`\n- runtime/pytorch: observed `2.2.0rc1`, cap `2.1.3`\n- framework/monai: observed `1.3.0`, cap `1.4.0`\n- framework/monai: observed `1.4`, cap `1.4.0`\n- framework/monai: observed `v1.3.1`, cap `1.4.0`\nReturn the boolean gate result for each evaluated record (keep record order after applying the sieve).", "answers": "[{\"name\":\"monai_utils_module_version_leq\",\"arguments\":{\"lhs\":\"2.1.0\",\"rhs\":\"2.1.3\"}},{\"name\":\"monai_utils_module_version_leq\",\"arguments\":{\"lhs\":\"1.3.0\",\"rhs\":\"1.4.0\"}}]"}
{"func_name": "mordred__util_to_ordinal", "func_desc": "Convert an integer to a short English ordinal string used for human-readable labels.\n    \n    This utility function in mordred._util is a small pure helper used by the mordred molecular descriptor calculator to produce human-friendly ordinal labels (for example when formatting descriptor positions, log messages, CLI output, or report fields). It maps the integers 1, 2 and 3 to the English words \"first\", \"second\" and \"third\" respectively; for any other integer it returns a numeric ordinal using the pattern \"<n>-th\" (for example \"4-th\" or \"104-th\"). The function has no side effects and does not modify external state.", "tools": [{"function": {"description": "Convert an integer to a short English ordinal string used for human-readable labels.\n\nThis utility function in mordred._util is a small pure helper used by the mordred molecular descriptor calculator to produce human-friendly ordinal labels (for example when formatting descriptor positions, log messages, CLI output, or report fields). It maps the integers 1, 2 and 3 to the English words \"first\", \"second\" and \"third\" respectively; for any other integer it returns a numeric ordinal using the pattern \"<n>-th\" (for example \"4-th\" or \"104-th\"). The function has no side effects and does not modify external state.", "name": "mordred__util_to_ordinal", "parameters": {"properties": {"n": {"type": "integer", "description": "The integer to convert to an ordinal string. This parameter is the positional index or rank that will be rendered as an English ordinal for presentation purposes (e.g., descriptor number or step index). The function expects an int as specified by the signature; passing non-int values is not supported by the documented API and may produce unintended results.", "default": ""}}, "required": ["n"], "type": "any"}}, "type": "function"}], "query": "In our HTS descriptor-reporting pipeline, we need consistent human-readable ordinal tags for only the stages that will actually appear in the final audit trail. From the raw stage identifiers `[0, 1, 2, 3, 7, 999]`, generate ordinal labels only for stage IDs that are positive and within the configured operational window (≤ 120). Separately, for descriptor-position field naming, the descriptor indices are recorded as zero-based offsets, so convert the one-based index corresponding to offset `103` into the same ordinal label format for use as a report column header.", "answers": "[{\"name\":\"mordred__util_to_ordinal\",\"arguments\":{\"n\":1}},{\"name\":\"mordred__util_to_ordinal\",\"arguments\":{\"n\":2}},{\"name\":\"mordred__util_to_ordinal\",\"arguments\":{\"n\":3}},{\"name\":\"mordred__util_to_ordinal\",\"arguments\":{\"n\":7}},{\"name\":\"mordred__util_to_ordinal\",\"arguments\":{\"n\":104}}]"}
{"func_name": "ncbi_genome_download_core_get_genus_label", "func_desc": "ncbi_genome_download.core.get_genus_label returns the genus (first token of the organism name) for an NCBI assembly summary entry. This function is used by the ncbi-genome-download tool to extract the genus string from an assembly entry's organism_name field so that entries can be grouped, filtered, or matched against user-specified genera (for example the --genera option described in the README).", "tools": [{"function": {"description": "ncbi_genome_download.core.get_genus_label returns the genus (first token of the organism name) for an NCBI assembly summary entry. This function is used by the ncbi-genome-download tool to extract the genus string from an assembly entry's organism_name field so that entries can be grouped, filtered, or matched against user-specified genera (for example the --genera option described in the README).\n", "name": "ncbi_genome_download_core_get_genus_label", "parameters": {"properties": {"entry": {"type": "any", "description": "A single assembly summary entry as provided by the NCBI assembly summary files or by upstream parsing code in ncbi_genome_download. The entry is expected to be a mapping that contains the key 'organism_name' whose value is the taxonomic organism name string (for example \"Escherichia coli\"). The practical role of this parameter is to supply the full organism name from which the genus is extracted; the function does not validate or enrich taxonomy data beyond simple string parsing.", "default": ""}}, "required": ["entry"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing genus-based stratification for an assembly-summary ingestion stage where organism_name strings are messy and may include nonstandard whitespace or bracketed annotations. Given the following raw assembly entries, compute the genus label exactly as ncbi-genome-download would (genus = first whitespace-delimited token of organism_name) for entries that look like standard binomials (i.e., organism_name begins with an alphabetic genus token and contains at least one additional token). Use the entries as-is (no pre-cleaning beyond what the tool inherently does when taking the first token): (1) organism_name \"Staphylococcus aureus subsp. aureus\" with assembly_accession GCF_000013425.1, assembly_level Complete Genome, strain N315; (2) organism_name \"Staphylococcus  aureus subsp. aureus USA300_FPR3757\" (note the double-space after the genus); (3) organism_name \"Candidatus Liberibacter asiaticus\"; (4) organism_name \"[Lactobacillus] plantarum WCFS1\"; (5) organism_name \"Lactobacillus plantarum WCFS1\"; (6) organism_name \"  Bacillus subtilis 168\" (leading whitespace); (7) organism_name \"Escherichia\" (single-token label).", "answers": "[{\"name\":\"ncbi_genome_download_core_get_genus_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Staphylococcus aureus subsp. aureus\",\"assembly_accession\":\"GCF_000013425.1\",\"assembly_level\":\"Complete Genome\",\"strain\":\"N315\"}}},{\"name\":\"ncbi_genome_download_core_get_genus_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Staphylococcus  aureus subsp. aureus USA300_FPR3757\"}}},{\"name\":\"ncbi_genome_download_core_get_genus_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Candidatus Liberibacter asiaticus\"}}},{\"name\":\"ncbi_genome_download_core_get_genus_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Lactobacillus plantarum WCFS1\"}}}]"}
{"func_name": "ncbi_genome_download_core_get_species_label", "func_desc": "ncbi_genome_download.core.get_species_label: Extract the species label (the species epithet) from an NCBI assembly summary entry.\n\nThis function is used in the ncbi-genome-download tool to derive a simple species identifier from the assembly summary \"organism_name\" field provided by NCBI. The typical \"organism_name\" value is a scientific name string such as \"Escherichia coli str. K-12\" or \"Streptomyces coelicolor A3(2)\". The function performs a simple whitespace split of that string and returns the second token (index 1), which represents the species epithet in common two-word scientific names. This lightweight parsing is useful for creating human-readable directory labels, grouping downloaded assemblies by species, or generating short species tags when building NCBI mirror-like directory structures as described in the project README.", "tools": [{"function": {"description": "ncbi_genome_download.core.get_species_label: Extract the species label (the species epithet) from an NCBI assembly summary entry.\n\nThis function is used in the ncbi-genome-download tool to derive a simple species identifier from the assembly summary \"organism_name\" field provided by NCBI. The typical \"organism_name\" value is a scientific name string such as \"Escherichia coli str. K-12\" or \"Streptomyces coelicolor A3(2)\". The function performs a simple whitespace split of that string and returns the second token (index 1), which represents the species epithet in common two-word scientific names. This lightweight parsing is useful for creating human-readable directory labels, grouping downloaded assemblies by species, or generating short species tags when building NCBI mirror-like directory structures as described in the project README.", "name": "ncbi_genome_download_core_get_species_label", "parameters": {"properties": {"entry": {"type": "any", "description": "A single assembly summary entry represented as a dictionary (as produced by parsing NCBI assembly_summary files in ncbi-genome-download). This dictionary MUST contain the key 'organism_name' whose value is expected to be the organism scientific name as a string. The function splits entry['organism_name'] on spaces and returns the second whitespace-separated token as the species label. If the provided entry is missing 'organism_name' or that value is not a string, attempting to call this function will raise the underlying Python exception (for example, KeyError if the key is absent or AttributeError/TypeError if the value does not support split). If 'organism_name' is present but contains fewer than two whitespace-separated tokens (for example an empty string or a single-word name), the function returns the placeholder string 'sp.' to indicate that no distinct species epithet could be extracted.", "default": ""}}, "required": ["entry"], "type": "any"}}, "type": "function"}], "query": "We’re auditing organism_name cleanliness in an NCBI assembly_summary ingest step before building species-level directory labels. Given a mixed batch of raw organism_name strings, generate a short species directory label using the same lightweight parsing rule (split on whitespace and take the second token), but only for records that look like canonical binomials at the start of the field: the first two whitespace-delimited tokens must both be purely alphabetic (A–Z/a–z), with no punctuation, digits, brackets, quotes, or symbols. Apply the label extraction to all entries that meet that criterion.\n\nBatch organism_name values:\n1) \"Escherichia coli str. K-12 substr. MG1655\"\n2) \"Streptomyces coelicolor A3(2)\"\n3) \"Candidatus Liberibacter asiaticus\"\n4) \"Staphylococcus aureus subsp. aureus N315\"\n5) \"[Escherichia] coli\"\n6) \"Bacillus sp.\"\n7) \"Vibrio cholerae O1 biovar El Tor\"\n8) \"Mycobacterium tuberculosis H37Rv\"", "answers": "[{\"name\":\"ncbi_genome_download_core_get_species_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Escherichia coli str. K-12 substr. MG1655\"}}},{\"name\":\"ncbi_genome_download_core_get_species_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Candidatus Liberibacter asiaticus\"}}},{\"name\":\"ncbi_genome_download_core_get_species_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Staphylococcus aureus subsp. aureus N315\"}}},{\"name\":\"ncbi_genome_download_core_get_species_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Vibrio cholerae O1 biovar El Tor\"}}},{\"name\":\"ncbi_genome_download_core_get_species_label\",\"arguments\":{\"entry\":{\"organism_name\":\"Mycobacterium tuberculosis H37Rv\"}}}]"}
{"func_name": "ncbi_genome_download_core_get_strain", "func_desc": "ncbi_genome_download.core.get_strain: Extract a strain identifier string from a single NCBI assembly summary entry.\n\nThis function is used by the ncbi-genome-download tool to derive a human-meaningful strain label from an assembly summary entry returned by NCBI. The derived strain label is typically used for naming output files or organizing downloaded genomes by strain. The function examines the entry in a prioritized order to find the most specific available strain information: it first inspects the 'infraspecific_name' field (handling values of the form \"key=value\" by taking the value after the final '='), then the 'isolate' field, and finally, when the entry's 'organism_name' consists of more than two whitespace-separated words and the viral flag is False, it treats all words from the third onward as the strain (joining them with single spaces). If none of these produce a non-empty string, the function falls back to returning the assembly accession from the 'assembly_accession' field. Empty strings in the checked fields are treated as absent. The viral parameter disables the organism_name-based extraction because viral organism names should not be parsed in this way for strain derivation. The function performs no I/O and has no side effects; it operates purely on the provided dictionary and returns a string.", "tools": [{"function": {"description": "ncbi_genome_download.core.get_strain: Extract a strain identifier string from a single NCBI assembly summary entry.\n\nThis function is used by the ncbi-genome-download tool to derive a human-meaningful strain label from an assembly summary entry returned by NCBI. The derived strain label is typically used for naming output files or organizing downloaded genomes by strain. The function examines the entry in a prioritized order to find the most specific available strain information: it first inspects the 'infraspecific_name' field (handling values of the form \"key=value\" by taking the value after the final '='), then the 'isolate' field, and finally, when the entry's 'organism_name' consists of more than two whitespace-separated words and the viral flag is False, it treats all words from the third onward as the strain (joining them with single spaces). If none of these produce a non-empty string, the function falls back to returning the assembly accession from the 'assembly_accession' field. Empty strings in the checked fields are treated as absent. The viral parameter disables the organism_name-based extraction because viral organism names should not be parsed in this way for strain derivation. The function performs no I/O and has no side effects; it operates purely on the provided dictionary and returns a string.", "name": "ncbi_genome_download_core_get_strain", "parameters": {"properties": {"entry": {"type": "any", "description": "Assembly summary entry as a Python dictionary corresponding to one line/record from an NCBI assembly summary file. This dictionary is expected to contain at least the keys 'infraspecific_name', 'isolate', 'organism_name', and 'assembly_accession' (access via entry['key'] is performed). The practical significance is that these keys are the standard columns from NCBI assembly metadata used to determine strain-level labels for genome downloads; if any of these keys are missing, a KeyError will be raised by the function.", "default": ""}, "viral": {"type": "boolean", "description": "Flag indicating whether the entry represents a viral genome. When False (default), the function will attempt to extract a strain from 'organism_name' when that name contains more than two words by joining words from the third onward. When True, the function will not use 'organism_name' for strain extraction, which is important in the viral domain where organism_name tokenization does not reliably represent strain. This parameter has no side effects beyond controlling the extraction logic.", "default": false}}, "required": ["entry", "viral"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed “assembly_summary” ingest for downstream genome packaging, where strain labeling must be derived deterministically but only from records that are usable for non-viral bacterial folder naming. Treat an entry as usable if either (a) its viral flag is False, or (b) its organism_name suggests a cellular lineage (i.e., contains at least two whitespace-separated tokens) and it has a non-empty assembly_accession. For each usable record, derive the strain identifier using the standard ncbi-genome-download prioritization: first take the most specific non-empty infraspecific_name (if it contains one or more '=' characters, keep only the substring after the final '='); otherwise use a non-empty isolate; otherwise, when viral is False and organism_name has more than two whitespace-separated words, use words 3+ joined by single spaces; otherwise fall back to assembly_accession. Apply this to the following raw cohort entries (as-is, including artifacts such as empty strings and additional '=' tokens):\n\n1) entry_id=\"A01\": infraspecific_name=\"strain=O157:H7\", isolate=\"\", organism_name=\"Escherichia coli O157:H7 str. Sakai\", assembly_accession=\"GCF_000008865.1\", viral=False\n2) entry_id=\"B02\": infraspecific_name=\"strain=O157:H7\", isolate=\"EC-O157-Ref\", organism_name=\"Escherichia coli O157:H7 str. Sakai\", assembly_accession=\"GCF_000008865.1\", viral=False\n3) entry_id=\"C03\": infraspecific_name=\"\", isolate=\"\", organism_name=\"Escherichia coli\", assembly_accession=\"GCF_000999999.9\", viral=False\n4) entry_id=\"D04\": infraspecific_name=\"bio_sample=strain=K-12 substr. MG1655\", isolate=\"\", organism_name=\"Escherichia coli K-12 substr. MG1655\", assembly_accession=\"GCF_000005845.2\", viral=False\n5) entry_id=\"E05\": infraspecific_name=\"\", isolate=\"\", organism_name=\"Influenza A virus (A/Puerto Rico/8/1934(H1N1))\", assembly_accession=\"GCF_000000000.0\", viral=True\n\nReturn the derived strain identifiers for the usable records (in the same order they appear after applying the usability rule).", "answers": "[{\"name\":\"ncbi_genome_download_core_get_strain\",\"arguments\":{\"entry\":{\"infraspecific_name\":\"strain=O157:H7\",\"isolate\":\"\",\"organism_name\":\"Escherichia coli O157:H7 str. Sakai\",\"assembly_accession\":\"GCF_000008865.1\"},\"viral\":false}},{\"name\":\"ncbi_genome_download_core_get_strain\",\"arguments\":{\"entry\":{\"infraspecific_name\":\"strain=O157:H7\",\"isolate\":\"EC-O157-Ref\",\"organism_name\":\"Escherichia coli O157:H7 str. Sakai\",\"assembly_accession\":\"GCF_000008865.1\"},\"viral\":false}},{\"name\":\"ncbi_genome_download_core_get_strain\",\"arguments\":{\"entry\":{\"infraspecific_name\":\"\",\"isolate\":\"\",\"organism_name\":\"Escherichia coli\",\"assembly_accession\":\"GCF_000999999.9\"},\"viral\":false}},{\"name\":\"ncbi_genome_download_core_get_strain\",\"arguments\":{\"entry\":{\"infraspecific_name\":\"bio_sample=strain=K-12 substr. MG1655\",\"isolate\":\"\",\"organism_name\":\"Escherichia coli K-12 substr. MG1655\",\"assembly_accession\":\"GCF_000005845.2\"},\"viral\":false}}]"}
{"func_name": "nnunetv2_evaluation_find_best_con___plans_to_trained_models_dict", "func_desc": "nnunetv2.evaluation.find_best_configuration.dumb_trainer_config_plans_to_trained_models_dict: Create a deterministic enumeration of trained-model descriptors by taking the Cartesian product of the provided trainer identifiers, configuration names, and plan identifiers. This helper is used in the evaluation pipeline to produce a simple list of candidate trained-model specifications that downstream code (for example: model lookup, evaluation, or selection logic in find_best_configuration) can iterate over when empirically determining the best U-Net configuration for a dataset. The name \"dumb\" reflects that this function performs no validation, filtering, or existence checks — it only composes the inputs into dictionaries.", "tools": [{"function": {"description": "nnunetv2.evaluation.find_best_configuration.dumb_trainer_config_plans_to_trained_models_dict: Create a deterministic enumeration of trained-model descriptors by taking the Cartesian product of the provided trainer identifiers, configuration names, and plan identifiers. This helper is used in the evaluation pipeline to produce a simple list of candidate trained-model specifications that downstream code (for example: model lookup, evaluation, or selection logic in find_best_configuration) can iterate over when empirically determining the best U-Net configuration for a dataset. The name \"dumb\" reflects that this function performs no validation, filtering, or existence checks — it only composes the inputs into dictionaries.\n", "name": "nnunetv2_evaluation_find_best_con___plans_to_trained_models_dict", "parameters": {"properties": {"trainers": {"type": "array", "items": {"type": "string"}, "description": "A list of trainer identifiers (strings). In the nnU-Net domain these typically correspond to trainer classes or trainer name strings that implement the training loop and augmentation/optimization choices (for example variants of nnUNetTrainer). Each entry represents one distinct training strategy that should be considered when evaluating model performance.", "default": ""}, "configs": {"type": "array", "items": {"type": "string"}, "description": "A list of configuration names (strings). In nnU-Net these are the U-Net configuration labels such as '2d', '3d_fullres', and '3d_lowres' (see README description of created configurations). Each configuration encodes a different network topology / dimensionality choice that will be evaluated for the given dataset.", "default": ""}, "plans": {"type": "array", "items": {"type": "string"}, "description": "A list of plan identifiers or plan file names (strings). Plans encode dataset-specific preprocessing and topology decisions (patch sizes, spacing handling, pooling behavior) that nnU-Net derives during dataset analysis. Each plan string identifies one such set of choices to be paired with trainers and configs.", "default": ""}}, "required": ["trainers", "configs", "plans"], "type": "any"}}, "type": "function"}], "query": "I’m assembling a cross-dataset nnU-Net v2 model-selection sweep for a heterogeneous benchmark cohort with raw identifiers: [\"Dataset105_ProstateMRI\", \"Dataset042_Liver\", \"Dataset042_Liver_resampledIso1mm\", \"Dataset105_ProstateMRI-TEST\", \"Dataset999_Phantom\"]. For each entry, first interpret it as a candidate dataset only if it matches the canonical nnU-Net dataset naming convention (\"Dataset\" + three digits + \"_\" + modality/organ token) with no extra suffixes. Then enumerate trained-model descriptors via a deterministic Cartesian product, but use a branching protocol based on the dataset’s organ token: for ProstateMRI-like datasets, sweep trainers [\"nnUNetTrainer\", \"nnUNetTrainerDA5\"], configurations [\"2d\", \"3d_fullres\"], and plans [\"nnUNetPlans\", \"nnUNetPlans_iso1mm\"]; for Liver-like datasets, sweep trainers [\"nnUNetTrainer\"], configurations [\"3d_fullres\"], and plans [\"nnUNetPlans_iso1mm\"]. This is only for generating candidate descriptors for downstream evaluation; no validation or existence checks beyond the naming-canonicality rule.", "answers": "[{\"name\":\"nnunetv2_evaluation_find_best_con___plans_to_trained_models_dict\",\"arguments\":{\"trainers\":[\"nnUNetTrainer\",\"nnUNetTrainerDA5\"],\"configs\":[\"2d\",\"3d_fullres\"],\"plans\":[\"nnUNetPlans\",\"nnUNetPlans_iso1mm\"]}},{\"name\":\"nnunetv2_evaluation_find_best_con___plans_to_trained_models_dict\",\"arguments\":{\"trainers\":[\"nnUNetTrainer\"],\"configs\":[\"3d_fullres\"],\"plans\":[\"nnUNetPlans_iso1mm\"]}}]"}
{"func_name": "nnunetv2_utilities_collate_outputs_collate_outputs", "func_desc": "nnunetv2.utilities.collate_outputs.collate_outputs: Collate a list of per-step output dictionaries produced by nnU-Net training/validation steps into a single dictionary that aggregates values across the list. This function is used by the default train_step and validation_step implementations in nnU-Net to combine outputs (for example per-batch losses, numpy prediction arrays, or lists of identifiers) from multiple executions into a single structure suitable for epoch-level logging, metric computation, or further postprocessing in the semantic segmentation training pipeline.\n    \n    This collator expects a homogeneous list of dictionaries where each dictionary has the same keys and the same kind of value for each key. It is intentionally minimal: it handles three specific value types in a reproducible way and raises an error for other types. It performs no in-place modification of the input list; it returns a newly constructed dictionary. Because nnU-Net is applied to biomedical image segmentation tasks, typical use-cases include collating per-batch scalar losses into a list of loss values, stacking numpy prediction arrays into a batch axis for metric computation, or concatenating lists of case identifiers or file names produced during validation.", "tools": [{"function": {"description": "nnunetv2.utilities.collate_outputs.collate_outputs: Collate a list of per-step output dictionaries produced by nnU-Net training/validation steps into a single dictionary that aggregates values across the list. This function is used by the default train_step and validation_step implementations in nnU-Net to combine outputs (for example per-batch losses, numpy prediction arrays, or lists of identifiers) from multiple executions into a single structure suitable for epoch-level logging, metric computation, or further postprocessing in the semantic segmentation training pipeline.\n\nThis collator expects a homogeneous list of dictionaries where each dictionary has the same keys and the same kind of value for each key. It is intentionally minimal: it handles three specific value types in a reproducible way and raises an error for other types. It performs no in-place modification of the input list; it returns a newly constructed dictionary. Because nnU-Net is applied to biomedical image segmentation tasks, typical use-cases include collating per-batch scalar losses into a list of loss values, stacking numpy prediction arrays into a batch axis for metric computation, or concatenating lists of case identifiers or file names produced during validation.", "name": "nnunetv2_utilities_collate_outputs_collate_outputs", "parameters": {"properties": {"outputs": {"type": "array", "items": {"type": "any"}, "description": "A non-empty list of dictionaries. Each dictionary corresponds to the outputs produced by a single train_step or validation_step call (for example from one batch, one device, or one sample). All dictionaries must have the same set of keys and each key must hold a value of one of the supported types described below. Typical dictionary entries in nnU-Net include scalars such as loss or metric values, numpy.ndarray predictions or probabilities, and lists such as lists of case identifiers or file names.", "default": ""}}, "required": ["outputs"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing nnU-Net’s epoch aggregation under realistic validation noise where only “metric-safe” steps should contribute to epoch-level logging. You have two back-to-back validation cohorts (LiTS then LIDC), but each cohort includes a mixture of well-formed and artifact-contaminated step outputs.\n\nCollation rule for each cohort:\n- Consider a step “metric-safe” only if (i) the loss is a finite scalar within [0, 1], (ii) the probabilities tensor contains only values in [0, 1], and (iii) along the last axis of the probabilities tensor, values form a proper softmax (sum to 1 at every spatial location within a tolerance of 1e-6).\n- Collate only the metric-safe steps using `nnunetv2.utilities.collate_outputs.collate_outputs` (losses collected across steps, probability arrays stacked on a new leading axis, and case_ids concatenated in order).\n\nCohort A (LiTS) raw step outputs (5 steps, mixed quality):\n1) loss=0.412, probabilities=[[0.1,0.9],[0.8,0.2]], case_ids=['LITS_001']\n2) loss=0.387, probabilities=[[0.05,0.95],[0.7,0.3]], case_ids=['LITS_002']\n3) loss=0.455, probabilities=[[0.2,0.8],[0.6,0.4]], case_ids=['LITS_003']\n4) loss=0.510, probabilities=[[0.1,0.95],[0.8,0.2]], case_ids=['LITS_004']\n5) loss=1.240, probabilities=[[0.3,0.7],[0.4,0.6]], case_ids=['LITS_005']\n\nCohort B (LIDC) raw step outputs (5 steps, mixed quality):\n1) loss=0.421, probabilities=[[[0.1,0.9],[0.8,0.2]],[[0.6,0.4],[0.3,0.7]]], case_ids=['LIDC_0001']\n2) loss=0.387, probabilities=[[[0.2,0.8],[0.7,0.3]],[[0.55,0.45],[0.25,0.75]]], case_ids=['LIDC_0002']\n3) loss=0.402, probabilities=[[[0.15,0.85],[0.75,0.25]],[[0.58,0.42],[0.28,0.72]]], case_ids=['LIDC_0003']\n4) loss=0.399, probabilities=[[[0.12,0.88],[0.76,0.26]],[[0.60,0.40],[0.30,0.69]]], case_ids=['LIDC_0004']\n5) loss=nan, probabilities=[[[0.11,0.89],[0.79,0.21]],[[0.61,0.39],[0.31,0.69]]], case_ids=['LIDC_0005']\n\nFor each cohort, filter by the metric-safe rule and then collate the remaining step dictionaries.", "answers": "[{\"name\":\"nnunetv2_utilities_collate_outputs_collate_outputs\",\"arguments\":{\"outputs\":[{\"loss\":0.412,\"probabilities\":[[0.1,0.9],[0.8,0.2]],\"case_ids\":[\"LITS_001\"]},{\"loss\":0.387,\"probabilities\":[[0.05,0.95],[0.7,0.3]],\"case_ids\":[\"LITS_002\"]},{\"loss\":0.455,\"probabilities\":[[0.2,0.8],[0.6,0.4]],\"case_ids\":[\"LITS_003\"]}]}},{\"name\":\"nnunetv2_utilities_collate_outputs_collate_outputs\",\"arguments\":{\"outputs\":[{\"loss\":0.421,\"probabilities\":[[[0.1,0.9],[0.8,0.2]],[[0.6,0.4],[0.3,0.7]]],\"case_ids\":[\"LIDC_0001\"]},{\"loss\":0.387,\"probabilities\":[[[0.2,0.8],[0.7,0.3]],[[0.55,0.45],[0.25,0.75]]],\"case_ids\":[\"LIDC_0002\"]},{\"loss\":0.402,\"probabilities\":[[[0.15,0.85],[0.75,0.25]],[[0.58,0.42],[0.28,0.72]]],\"case_ids\":[\"LIDC_0003\"]}]} }]"}
{"func_name": "periodictable_core_get_data_path", "func_desc": "periodictable.core.get_data_path locates and returns the filesystem directory path for the tables belonging to a named extension used by the periodictable package. This function is used by extension modules (for example the 'xsf' extension) to find the packaged data files that provide element tables and supporting data for mass, density, x-ray and neutron scattering, and neutron activation calculations described in the package README.", "tools": [{"function": {"description": "periodictable.core.get_data_path locates and returns the filesystem directory path for the tables belonging to a named extension used by the periodictable package. This function is used by extension modules (for example the 'xsf' extension) to find the packaged data files that provide element tables and supporting data for mass, density, x-ray and neutron scattering, and neutron activation calculations described in the package README.\n", "name": "periodictable_core_get_data_path", "parameters": {"properties": {"data": {"type": "string", "description": "Name of the extension data directory to locate. This is the directory name as used by extension modules (for example, 'xsf' for the xsf extension). The function treats this name as a literal subdirectory to search for in a series of candidate locations; it does not validate or normalize the name beyond joining it to candidate base paths.", "default": ""}}, "required": ["data"], "type": "any"}}, "type": "function"}], "query": "We’re validating extension-table discovery across a mixed-resolution diffraction workflow where some jobs request multiple table families, and others contain malformed extension identifiers from an upstream YAML. Given the requested extension labels from two independent subsystems (CLI loader and plugin loader):\n\n- CLI requested labels: ['xsf', 'XSF', 'xsf ', 'xsf/v2', '', null]\n- Plugin requested labels: ['xsf', 'xsf\\n', 'xsf\\t', 'Xsf', 'xrf']\n\nResolve the packaged data directory only for labels that canonically identify the x-ray scattering factor tables shipped with periodictable (i.e., the xsf extension after normalizing case and surrounding whitespace). Treat each subsystem as an independent replicate set, and report the resolved directory for every label in those sets that maps to the xsf extension.", "answers": "[{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}},{\"name\":\"periodictable_core_get_data_path\",\"arguments\":{\"data\":\"xsf\"}}]"}
{"func_name": "periodictable_cromermann_fxrayatstol", "func_desc": "Calculate x-ray scattering factors at a specified sin(theta)/lambda (stol) for an element or ion using the Cromer–Mann form-factor data in the periodictable package.\n    \n    This function is used in x-ray scattering calculations (see package README) to obtain the atomic scattering factor f(sin(theta)/lambda) for a resolved element or ion symbol. It resolves and normalizes the provided element/ion symbol, optionally applies an explicit integer ion charge override, looks up the Cromer–Mann parameters for that species via getCMformula, and evaluates the form factor at the provided stol value(s) by calling the resulting object's atstol method. The returned value is intended for use in computing x-ray scattering amplitudes and cross sections in crystallography and related applications.", "tools": [{"function": {"description": "Calculate x-ray scattering factors at a specified sin(theta)/lambda (stol) for an element or ion using the Cromer–Mann form-factor data in the periodictable package.\n\nThis function is used in x-ray scattering calculations (see package README) to obtain the atomic scattering factor f(sin(theta)/lambda) for a resolved element or ion symbol. It resolves and normalizes the provided element/ion symbol, optionally applies an explicit integer ion charge override, looks up the Cromer–Mann parameters for that species via getCMformula, and evaluates the form factor at the provided stol value(s) by calling the resulting object's atstol method. The returned value is intended for use in computing x-ray scattering amplitudes and cross sections in crystallography and related applications.", "name": "periodictable_cromermann_fxrayatstol", "parameters": {"properties": {"symbol": {"type": "string", "description": "Symbol of an element or ion to evaluate, e.g., \"Ca\" or \"Ca2+\". This input may include ionic suffixes such as \"+\", \"-\", \"3+\" etc. If the symbol ends with a single sign character and no trailing digit (for example \"Na+\" or \"Cl-\"), the function will normalize it to an explicit 1 charge (e.g., \"Na1+\", \"Cl1-\") before lookup. The resolved symbol determines which Cromer–Mann parameters are used for the scattering factor lookup and thus directly controls the physical species whose scattering is returned.", "default": ""}, "stol": {"type": "float", "description": "The value(s) of sin(theta)/lambda at which to evaluate the x-ray form factor, given in inverse angstroms (1/Å). The function accepts a scalar float for a single evaluation or a sequence (e.g., list or numpy array) of floats for vectorized evaluation; when a sequence is supplied, the function will return a numpy.ndarray of matching shape containing the scattering factor for each stol. These values are forwarded to the underlying cmf.atstol(stol) routine which computes the Cromer–Mann form factor.", "default": ""}, "charge": {"type": "integer", "nullable": true, "description": "Optional integer ion charge override. If provided (not None), this integer will override any ionic suffix present in symbol. The function strips trailing digits and sign characters from symbol and then appends an explicit charge suffix corresponding to this integer (formatted so that 2 becomes \"2+\", -1 becomes \"1-\", etc.). If charge is None (the default), the function uses any valence suffix already present in symbol (after the normalization rule described above).", "default": null}}, "required": ["symbol", "stol", "charge"], "type": "any"}}, "type": "function"}], "query": "We’re validating the Cromer–Mann scattering-factor stage using a messy beamline annotation dump where species/charge are inconsistently recorded. Treat the following as raw labels from different instruments: [\"Fe3+\", \"O\", \"Fe\", \"Fe(III)\"] with corresponding stol points [0.15, 0.25, 0.35, 0.30] 1/Å. Compute f(stol) only for records whose stol lies inside the calibration window [0.12, 0.33]. For iron-bearing records, standardize the chemical state to ferric (+3) if (and only if) the label is explicitly ferric (e.g., contains \"3+\" or \"III\"); implement this as an explicit integer charge override while keeping the symbol normalized to the base element. For non-iron records that pass the stol window, use the resolved symbol as-is with no charge override.", "answers": "[{\"name\":\"periodictable_cromermann_fxrayatstol\",\"arguments\":{\"symbol\":\"Fe\",\"stol\":0.15,\"charge\":3}},{\"name\":\"periodictable_cromermann_fxrayatstol\",\"arguments\":{\"symbol\":\"O\",\"stol\":0.25,\"charge\":null}},{\"name\":\"periodictable_cromermann_fxrayatstol\",\"arguments\":{\"symbol\":\"Fe\",\"stol\":0.3,\"charge\":3}}]"}
{"func_name": "periodictable_magnetic_ff_formfactor_0", "func_desc": "periodictable.magnetic_ff.formfactor_0: Compute the scattering potential for the magnetic form factor j0 at the supplied q values for use in neutron and x-ray scattering calculations.\n    \n    This function evaluates the analytic form used in the package to represent the magnetic form factor labeled j0. In the context of the periodictable package (which provides scattering and cross-section data for x-ray and neutron calculations), the returned scattering potential is a scalar or array giving the form factor contribution at each supplied momentum-transfer-like value q. The implementation converts q to an internal squared variable s_sq = (q/(4*pi))**2 and evaluates the sum of three Gaussian-like exponential terms plus a constant offset using coefficients supplied in j0.", "tools": [{"function": {"description": "periodictable.magnetic_ff.formfactor_0: Compute the scattering potential for the magnetic form factor j0 at the supplied q values for use in neutron and x-ray scattering calculations.\n\nThis function evaluates the analytic form used in the package to represent the magnetic form factor labeled j0. In the context of the periodictable package (which provides scattering and cross-section data for x-ray and neutron calculations), the returned scattering potential is a scalar or array giving the form factor contribution at each supplied momentum-transfer-like value q. The implementation converts q to an internal squared variable s_sq = (q/(4*pi))**2 and evaluates the sum of three Gaussian-like exponential terms plus a constant offset using coefficients supplied in j0.", "name": "periodictable_magnetic_ff_formfactor_0", "parameters": {"properties": {"j0": {"type": "any", "description": "A 7-tuple of numeric coefficients (A, a, B, b, C, c, D) that define the j0 form factor. A, B, and C are multiplicative amplitudes for the three exponential terms; a, b, and c are the corresponding exponential decay coefficients applied to s_sq; D is a constant offset added to the sum. These coefficients come from the package's magnetic form factor data tables and are used directly in the expression A*exp(-a*s_sq) + B*exp(-b*s_sq) + C*exp(-c*s_sq) + D. The tuple must have exactly seven elements; otherwise Python will raise a ValueError during unpacking.", "default": ""}, "q": {"type": "array", "items": {"type": "float"}, "description": "Array-like input of scalar q values (e.g., momentum-transfer magnitudes used in scattering computations). The function calls numpy.asarray(q) internally, so q may be a numpy.ndarray, list, tuple, or scalar convertible to a numpy array. The computation is performed elementwise and preserves the shape of the converted array; if q is a scalar the result will be a scalar-like numpy value. Invalid numeric entries in q (NaN, +/-Inf, non-numeric types that cannot be converted) will propagate or raise an error from numpy.asarray or the subsequent arithmetic.", "default": ""}}, "required": ["j0", "q"], "type": "any"}}, "type": "function"}], "query": "We’re doing a magnetic neutron-scattering QA pass where the raw momentum-transfer grids contain a mix of physically admissible points and acquisition artifacts. For each coefficient set (A, a, B, b, C, c, D), compute the magnetic form-factor j0 only on the subset of q values that are finite, non-negative, and lie within the instrument’s calibrated window defined as: q must be \u001e 4.0 \u0015\u001b\u001e. Use the standard analytic representation with coefficients ordered as (A, a, B, b, C, c, D).\n\nCoefficient set S1 = (0.3972, 13.2442, 0.6295, 4.9034, -0.0314, 0.3496, 0.0044) with raw q-grid: [0.0, 0.5, 1.0, 2.0, 4.0, -0.25, null].\n\nCoefficient set S2 (3d transition-metal ion) = (2.35, 15.0, 1.20, 5.5, 0.75, 1.8, 0.05) with raw q-grid: [0.0, 1.5, 3.0, 4.5, 6.0, -1.0].\n\nReturn the resulting scattering-potential arrays for S1 and S2 on their respective retained q-grids, preserving the original order of retained points.", "answers": "[{\"name\":\"periodictable_magnetic_ff_formfactor_0\",\"arguments\":{\"j0\":[0.3972,13.2442,0.6295,4.9034,-0.0314,0.3496,0.0044],\"q\":[0.0,0.5,1.0,2.0,4.0]}},{\"name\":\"periodictable_magnetic_ff_formfactor_0\",\"arguments\":{\"j0\":[2.35,15.0,1.2,5.5,0.75,1.8,0.05],\"q\":[0.0,1.5,3.0]}}]"}
{"func_name": "periodictable_nsf_neutron_wavelength", "func_desc": "periodictable.nsf.neutron_wavelength: Convert neutron energy (meV) to neutron wavelength (Å) for use in neutron scattering calculations.\n    \n    Converts a neutron energy value or array of energies, expressed in millielectronvolts (meV), to the corresponding neutron wavelength in angstroms (Å) using the non-relativistic relationship between kinetic energy and wavelength suitable for thermal and cold neutron scattering work. This function is used within the periodictable package's neutron-scattering utilities (nsf) to compute wavelengths needed for scattering cross section, scattering length density, and other neutron-beam calculations that rely on wavelength as an input parameter. The conversion is based on the formula λ = sqrt(h^2 / (2 m_n E)), where h is the Planck constant and m_n is the neutron mass; the implementation uses a module-level ENERGY_FACTOR equal to h^2 / (2 m_n) with units chosen so that E is given in meV and the result is in Å.", "tools": [{"function": {"description": "periodictable.nsf.neutron_wavelength: Convert neutron energy (meV) to neutron wavelength (Å) for use in neutron scattering calculations.\n\nConverts a neutron energy value or array of energies, expressed in millielectronvolts (meV), to the corresponding neutron wavelength in angstroms (Å) using the non-relativistic relationship between kinetic energy and wavelength suitable for thermal and cold neutron scattering work. This function is used within the periodictable package's neutron-scattering utilities (nsf) to compute wavelengths needed for scattering cross section, scattering length density, and other neutron-beam calculations that rely on wavelength as an input parameter. The conversion is based on the formula λ = sqrt(h^2 / (2 m_n E)), where h is the Planck constant and m_n is the neutron mass; the implementation uses a module-level ENERGY_FACTOR equal to h^2 / (2 m_n) with units chosen so that E is given in meV and the result is in Å.", "name": "periodictable_nsf_neutron_wavelength", "parameters": {"properties": {"energy": {"type": "array", "items": {"type": "float"}, "description": "Neutron energy values in millielectronvolts (meV). This argument provides the kinetic energy(s) of neutrons for which the wavelength will be computed. The function accepts a numpy.ndarray containing one or more energies; a scalar energy may be provided as a 0-D numpy array (numpy.asarray is applied internally). The array's numeric dtype should represent positive energy values in meV; negative or zero energies are outside the physical domain for this classical conversion and will produce NaN/inf or runtime warnings when evaluated.", "default": ""}}, "required": ["energy"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a reflectometry incident-energy plan from three proposal spreadsheets that contain mixed units and occasional placeholder values. Each cohort below is a raw list of incident-energy readings; interpret each numeric entry using this rule: values < 0 are electronic baselines and should be ignored; values between 0 and 0.2 are recorded in eV (convert to meV by multiplying by 1000); values >= 0.2 are recorded directly in meV. After normalizing units and filtering baselines, convert the resulting energies to neutron wavelengths (Å), preserving the within-cohort ordering of the surviving points.\n\nCohort A raw energies: [-1, 0.005, 2.5, 5.0, 12.0]\nCohort B raw energies: [5.0, 0.0125, 25.0, 80.0, -99]\nCohort C raw energies: [0.0018, 5.0, 12.5, 0, 25.0]\n\nReturn the wavelength arrays for cohorts A–C after this normalization step.", "answers": "[{\"name\":\"periodictable_nsf_neutron_wavelength\",\"arguments\":{\"energy\":[5.0,2.5,5.0,12.0]}},{\"name\":\"periodictable_nsf_neutron_wavelength\",\"arguments\":{\"energy\":[5.0,12.5,25.0,80.0]}},{\"name\":\"periodictable_nsf_neutron_wavelength\",\"arguments\":{\"energy\":[1.8,5.0,12.5,0.0,25.0]}}]"}
{"func_name": "periodictable_nsf_neutron_wavelength_from_velocity", "func_desc": "periodictable.nsf.neutron_wavelength_from_velocity converts a neutron velocity (speed) to its de Broglie wavelength for use in neutron scattering and related calculations.\n    \n    This function implements the physical relation lambda = h/(m_n v) (wavelength equals Planck's constant divided by the neutron momentum m_n v) and, in the package implementation, returns VELOCITY_FACTOR / velocity where VELOCITY_FACTOR encodes the Planck constant, neutron mass, and the conversion to Angstrom units. In the context of this package (periodictable), this conversion is used in neutron scattering calculations (for example, computing wavelength-dependent scattering lengths, cross sections, Bragg conditions, and scattering length density) as described in the project README and the neutron data sources it relies on.", "tools": [{"function": {"description": "periodictable.nsf.neutron_wavelength_from_velocity converts a neutron velocity (speed) to its de Broglie wavelength for use in neutron scattering and related calculations.\n\nThis function implements the physical relation lambda = h/(m_n v) (wavelength equals Planck's constant divided by the neutron momentum m_n v) and, in the package implementation, returns VELOCITY_FACTOR / velocity where VELOCITY_FACTOR encodes the Planck constant, neutron mass, and the conversion to Angstrom units. In the context of this package (periodictable), this conversion is used in neutron scattering calculations (for example, computing wavelength-dependent scattering lengths, cross sections, Bragg conditions, and scattering length density) as described in the project README and the neutron data sources it relies on.", "name": "periodictable_nsf_neutron_wavelength_from_velocity", "parameters": {"properties": {"velocity": {"type": "float", "description": "Neutron velocity or speeds, expressed in metres per second (m/s). Provide a single floating-point value for a scalar velocity or a vector of velocities to obtain elementwise wavelengths. The argument must represent SI velocities; the function performs simple arithmetic division and does not perform automatic unit conversion beyond this expectation.", "default": ""}}, "required": ["velocity"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mixed neutron beamline log where the velocity readback sometimes includes nonphysical values and duplicated channels from the same pulse. Use the following raw velocity readings (m/s): [2200.0, 2215.0, 0.0, -35.0, 1980.0, 4500.0, 2200.0, 1.0e-9]. Treat each reading as a candidate thermal/epithermal neutron event and convert to de Broglie wavelength (Å) only for physically meaningful forward-traveling neutrons that are within the plausible instrument TOF window of 500–5000 m/s, and for any duplicated speeds keep only a single conversion (one per unique speed). The resulting wavelengths will be used downstream for (i) calibrating at the thermal reference neighborhood and (ii) checking whether any converted wavelength falls near the 1.8 Å Bragg-condition target.", "answers": "[{\"name\":\"periodictable_nsf_neutron_wavelength_from_velocity\",\"arguments\":{\"velocity\":2200.0}},{\"name\":\"periodictable_nsf_neutron_wavelength_from_velocity\",\"arguments\":{\"velocity\":2215.0}},{\"name\":\"periodictable_nsf_neutron_wavelength_from_velocity\",\"arguments\":{\"velocity\":1980.0}},{\"name\":\"periodictable_nsf_neutron_wavelength_from_velocity\",\"arguments\":{\"velocity\":4500.0}}]"}
{"func_name": "periodictable_util_parse_uncertainty", "func_desc": "Parse a numeric value with an uncertainty notation and return the numeric value and its 1-sigma uncertainty.\n    \n    This function is used throughout the periodictable package to interpret numeric fields that include uncertainties in data tables (for example isotopic masses, densities, and scattering factors used in x-ray and neutron calculations). It accepts several common textual forms produced in tabulated scientific data and converts them into a canonical (value, uncertainty) pair of Python floats so downstream code can perform arithmetic, unit conversions, and statistical combination of uncertainties.", "tools": [{"function": {"description": "Parse a numeric value with an uncertainty notation and return the numeric value and its 1-sigma uncertainty.\n\nThis function is used throughout the periodictable package to interpret numeric fields that include uncertainties in data tables (for example isotopic masses, densities, and scattering factors used in x-ray and neutron calculations). It accepts several common textual forms produced in tabulated scientific data and converts them into a canonical (value, uncertainty) pair of Python floats so downstream code can perform arithmetic, unit conversions, and statistical combination of uncertainties.", "name": "periodictable_util_parse_uncertainty", "parameters": {"properties": {"s": {"type": "string", "description": "Input string containing a numeric value with optional uncertainty. Accepted forms are:\n- A bare numeric value, e.g. \"23.0035\", which is interpreted as value with zero uncertainty.\n  - The parenthetical form \"value(unc)\", e.g. \"23.0035(12)\", \"23(1)\", \"23.0(1.0)\" or \"23(1.0)\". The substring inside the parentheses is interpreted as the uncertainty. If the uncertainty has no decimal point but the value does, the parser aligns the significance by inserting leading zeros into the uncertainty (so \"23.0035(12)\" yields uncertainty 0.0012). Any characters after the closing parenthesis are ignored when extracting the uncertainty.\n  - A bracketed nominal form \"[nominal]\" which is equivalent to a bare value with zero uncertainty.\n  - A bracketed range \"[low,high]\" which denotes a uniform (rectangular) distribution between low and high; the function returns the midpoint as the nominal value and the equivalent 1-sigma uncertainty computed as (high - low) / sqrt(12), matching the standard conversion from a rectangular distribution to its 1-sigma width.\nThe input must be a string. Exponential notation containing an exponent after a parenthetical uncertainty (for example \"1.032(4)E10\") is not supported by this parser and will not be interpreted as intended.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed-format scrape from legacy nuclear and materials reference tables where numeric fields may include 1-sigma uncertainty in parentheses, may be unit-tagged, and may include clearly non-numeric placeholders from OCR. From the following raw fields, extract (value, 1σ) only for records that present a parenthesized uncertainty immediately adjacent to a numeric token (allowing optional leading sign and optional trailing unit text). For isotopic-mass style fields (no units), run two independent parsing replicates per qualifying record to cross-check that the digitization stream and the uncertainty-normalization stream yield identical 1σ interpretation. For material-property fields, run a single parse per qualifying record, ignoring any trailing units.\n\nRaw fields:\n- \"23.0035(12)\"\n- \"7.874(15) g/cm^3\"\n- \"7.874 g/cm^3\" \n- \"—\" \n- \"23.0035(12\" \n- \"-12.0(3)\"\n\nReturn the function-call plan for the qualifying fields under these rules.", "answers": "[{\"name\":\"periodictable_util_parse_uncertainty\",\"arguments\":{\"s\":\"23.0035(12)\"}},{\"name\":\"periodictable_util_parse_uncertainty\",\"arguments\":{\"s\":\"23.0035(12)\"}},{\"name\":\"periodictable_util_parse_uncertainty\",\"arguments\":{\"s\":\"7.874(15) g/cm^3\"}},{\"name\":\"periodictable_util_parse_uncertainty\",\"arguments\":{\"s\":\"-12.0(3)\"}},{\"name\":\"periodictable_util_parse_uncertainty\",\"arguments\":{\"s\":\"-12.0(3)\"}}]"}
{"func_name": "periodictable_xsf_xray_wavelength", "func_desc": "Convert X-ray photon energy to wavelength for use in x-ray scattering and optics calculations.\n    \n    This function, periodictable.xsf.xray_wavelength, converts photon energy values expressed in kilo-electronvolts (keV) to wavelengths expressed in Angstroms (Å). It is used in the periodictable package’s x-ray scattering and optics modules (xsf) where wavelength is required for computing scattering factors, cross sections, or optical properties derived from energy-dependent datasets (for example, values sourced from the LBL Center for X-ray Optics as used by this package). The conversion implements the physical relation λ = h c / E using the module’s Planck constant, electron volt, and speed of light constants, and returns numeric wavelength values that follow the input shape semantics of numpy.asarray.", "tools": [{"function": {"description": "Convert X-ray photon energy to wavelength for use in x-ray scattering and optics calculations.\n\nThis function, periodictable.xsf.xray_wavelength, converts photon energy values expressed in kilo-electronvolts (keV) to wavelengths expressed in Angstroms (Å). It is used in the periodictable package’s x-ray scattering and optics modules (xsf) where wavelength is required for computing scattering factors, cross sections, or optical properties derived from energy-dependent datasets (for example, values sourced from the LBL Center for X-ray Optics as used by this package). The conversion implements the physical relation λ = h c / E using the module’s Planck constant, electron volt, and speed of light constants, and returns numeric wavelength values that follow the input shape semantics of numpy.asarray.", "name": "periodictable_xsf_xray_wavelength", "parameters": {"properties": {"energy": {"type": "array", "items": {"type": "float"}, "description": "Photon energy value(s) in kilo-electronvolts (keV). This argument is the input energy to convert; it may be a NumPy array of any shape (the function calls numpy.asarray on the input so array-like sequences are accepted). Scalar numeric values (e.g., a Python float) are accepted in practice and will be converted to a zero-dimensional NumPy array internally, but the documented type is numpy.ndarray to match the function signature. Each element represents an X-ray photon energy in keV; the practical significance is that these energies feed downstream x-ray scattering and cross-section computations in the periodictable library.", "default": ""}}, "required": ["energy"], "type": "any"}}, "type": "function"}], "query": "We’re validating an X-ray optics model against a mixed beamtime plan that includes both fundamental energies and accidental harmonic settings. Given the proposed monochromator energies in keV: [6.5, 8.0, 10.5, 12.0, 13.0, 16.0, 21.0, 24.0, 31.5]. Convert only those energies that are consistent with a fundamental line in the 6–14 keV window (inclusive) into wavelengths (Å) for scattering-factor calculations. For a metrology cross-check, also convert the beamline’s nominal calibration line at 12.398 keV to wavelength (Å) as an independent benchmark.", "answers": "[{\"name\":\"periodictable_xsf_xray_wavelength\",\"arguments\":{\"energy\":[6.5,8.0,10.5,12.0,13.0]}},{\"name\":\"periodictable_xsf_xray_wavelength\",\"arguments\":{\"energy\":[12.398]}}]"}
{"func_name": "pubchempy_get_all_sources", "func_desc": "Return a list of all current depositors (source names) for the specified PubChem domain (for example, substances or assays). This function is a thin helper used by PubChemPy to query the PubChem REST API \"sources\" endpoint for metadata about who submitted records; it is useful in workflows that need to audit, filter, or cross-reference contributors to PubChem datasets.", "tools": [{"function": {"description": "Return a list of all current depositors (source names) for the specified PubChem domain (for example, substances or assays). This function is a thin helper used by PubChemPy to query the PubChem REST API \"sources\" endpoint for metadata about who submitted records; it is useful in workflows that need to audit, filter, or cross-reference contributors to PubChem datasets.\n", "name": "pubchempy_get_all_sources", "parameters": {"properties": {"domain": {"type": "string", "description": "The PubChem data domain to query for depositors. In practice this identifies which set of depositors to return (the original implementation uses this to request either substance-related depositors or assay-related depositors). The default value is \"substance\", which returns depositors of substance records. The value is passed verbatim to the internal PubChem API helper and is used to construct the \"sources\" endpoint request; do not rely on automatic validation of domain values by this function.", "default": "substance"}}, "required": ["domain"], "type": "any"}}, "type": "function"}], "query": "We are preparing a provenance audit for a PubChem BioAssay ingestion job where the upstream data feed is messy: the \"domain\" field arrives from three different internal subsystems with inconsistent casing, whitespace, and occasional synonym labels. Given these three replicate domain tokens: (1) \" Assay \", (2) \"bioassay\", and (3) \"ASSAY\", run a stability check by querying PubChem for depositor/source names only for those tokens that normalize to the PubChem BioAssay domain. Use the normalized PubChem domain value in the actual API calls so that the three replicates should be comparable for endpoint stability monitoring.", "answers": "[{\"name\":\"pubchempy_get_all_sources\",\"arguments\":{\"domain\":\"assay\"}},{\"name\":\"pubchempy_get_all_sources\",\"arguments\":{\"domain\":\"assay\"}},{\"name\":\"pubchempy_get_all_sources\",\"arguments\":{\"domain\":\"assay\"}}]"}
{"func_name": "pyEQL_equilibrium_alpha", "func_desc": "pyEQL.equilibrium.alpha computes the acid-base distribution coefficient alpha_n for an acid at a given pH. This function is used in aqueous chemistry speciation routines (for example in pyEQL Solution objects and related calculations) to determine the fraction of a total acid pool present in a specific deprotonation state. The computation follows the classical formulation from Stumm & Morgan (Aquatic Chemistry) using pKa values (negative base-10 logarithms of Ka) and the hydrogen ion activity [H+] = 10**(-pH). The function sorts pKa values, constructs the sequence of terms corresponding to each protonation state, and returns the fraction (term_n / sum_of_all_terms). The result is used downstream for species-specific properties (activities, transport coefficients) and bulk properties derived from speciation.", "tools": [{"function": {"description": "pyEQL.equilibrium.alpha computes the acid-base distribution coefficient alpha_n for an acid at a given pH. This function is used in aqueous chemistry speciation routines (for example in pyEQL Solution objects and related calculations) to determine the fraction of a total acid pool present in a specific deprotonation state. The computation follows the classical formulation from Stumm & Morgan (Aquatic Chemistry) using pKa values (negative base-10 logarithms of Ka) and the hydrogen ion activity [H+] = 10**(-pH). The function sorts pKa values, constructs the sequence of terms corresponding to each protonation state, and returns the fraction (term_n / sum_of_all_terms). The result is used downstream for species-specific properties (activities, transport coefficients) and bulk properties derived from speciation.\n", "name": "pyEQL_equilibrium_alpha", "parameters": {"properties": {"n": {"type": "integer", "description": "The number of protons that have been lost by the desired form of the acid (the subscript in alpha_n). For domain context, n=0 corresponds to the fully protonated form, n=1 to the singly deprotonated form (e.g., HCO3- for carbonic acid when n=1), etc. This integer selects which alpha fraction to return and must be non-negative and less than or equal to the number of dissociable protons implied by pKa_list.", "default": ""}, "pH": {"type": "float", "description": "The solution pH used to compute hydrogen ion activity via [H+] = 10**(-pH). This controls the protonation equilibrium and therefore the partitioning among protonation states; pH may be provided as an integer or float. Practical significance: small changes in pH near pKa values produce large changes in alpha values and are important for speciation in natural and engineered waters.", "default": ""}, "pKa_list": {"type": "array", "items": {"type": "float"}, "description": "The acid dissociation constants expressed as pKa = -log10(Ka) for each dissociation step of the acid, provided as a sequence of numbers. The list is sorted internally (ascending) before computation so the caller need not pre-sort. The length of pKa_list defines the number of dissociable protons (num_protons = len(pKa_list)). There must be at least n pKa values (i.e., len(pKa_list) >= n) and at least one pKa value is required. These values are used to compute Ka = 10**(-pKa) and thus the multiplicative terms in the distribution coefficient formula.", "default": ""}}, "required": ["n", "pH", "pKa_list"], "type": "any"}}, "type": "function"}], "query": "We’re doing carbonate-speciation QA/QC on a mixed field dataset where metadata quality varies by replicate. For each record below, compute the acid–base distribution coefficient for the singly deprotonated state (n=1, HCO3−) at that record’s pH using the provided pKa list. Only include records where (i) pH is within environmentally plausible freshwater bounds (6.0–9.5) and (ii) the pKa list contains exactly two finite numeric values (discarding entries with missing, non-numeric, or placeholder values). Records (unordered): R1: pH=8.30, pKa=[6.35, 10.33]; R2: pH=8.30, pKa=[6.35, 10.33]; R3: pH=5.40, pKa=[6.35, 10.33]; R4: pH=8.72, pKa=[6.35, null]; R5: pH=9.20, pKa=[10.33, 6.35]; R6: pH=7.10, pKa=[6.35, 10.33]. Report the bicarbonate alpha for each retained record as independent replicates.", "answers": "[{\"name\":\"pyEQL_equilibrium_alpha\",\"arguments\":{\"n\":1,\"pH\":8.3,\"pKa_list\":[6.35,10.33]}},{\"name\":\"pyEQL_equilibrium_alpha\",\"arguments\":{\"n\":1,\"pH\":8.3,\"pKa_list\":[6.35,10.33]}},{\"name\":\"pyEQL_equilibrium_alpha\",\"arguments\":{\"n\":1,\"pH\":9.2,\"pKa_list\":[10.33,6.35]}},{\"name\":\"pyEQL_equilibrium_alpha\",\"arguments\":{\"n\":1,\"pH\":7.1,\"pKa_list\":[6.35,10.33]}}]"}
{"func_name": "pyEQL_utils_format_solutes_dict", "func_desc": "pyEQL.utils.format_solutes_dict formats a dictionary of solutes into the string-valued form expected by the pyEQL Solution class for constructing an aqueous electrolyte Solution from specified solute amounts.", "tools": [{"function": {"description": "pyEQL.utils.format_solutes_dict formats a dictionary of solutes into the string-valued form expected by the pyEQL Solution class for constructing an aqueous electrolyte Solution from specified solute amounts.\n", "name": "pyEQL_utils_format_solutes_dict", "parameters": {"properties": {"solute_dict": {"type": "any", "description": "A mapping of solute identifiers to their numeric amounts. In the pyEQL context these keys are chemical species labels used by Solution (for example \"Na+\" or \"Cl-\") and the values are numeric quantities (e.g., int or float) representing the amount of each solute in the same units. Example form: {\"Na+\": 0.5, \"Cl-\": 0.9}. The function requires that all numeric values in this dictionary are expressed in the same physical units (see units). If solute_dict is not a dict, the function raises a TypeError.", "default": ""}, "units": {"type": "string", "description": "A units string to append to every numeric value in solute_dict to produce a quantity string understood by Solution and pyEQL's units-aware calculations (pint-compatible unit strings such as \"mol/kg\", \"mol/L\", \"mg/L\", etc.). This argument must be a string; the function performs no unit parsing or validation itself beyond string concatenation, so the caller should supply a units string compatible with downstream pyEQL/pint usage.", "default": ""}}, "required": ["solute_dict", "units"], "type": "any"}}, "type": "function"}], "query": "We’re triaging three aqueous electrolyte cohorts before constructing pyEQL Solution objects, but the field sheets are inconsistent. Each cohort is a dict of measured ionic species amounts and may include non-dissolved/neutral analytes or artifacts. Pre-process each cohort by keeping only entries that look like dissolved ions (species strings that contain an explicit charge sign, e.g., '+' or '-') and are physically plausible (strictly positive amounts). Then convert the retained solutes into the string-valued solute mapping expected by `pyEQL.Solution`, preserving the cohort’s original unit basis. Cohort A (groundwater replicate, mol/kg): {'Na+': 0.015, 'Cl-': 0.014, 'Ca2+': 0.002, 'SiO2(aq)': 0.0003}. Cohort B (lab calibration standard, mol/L): {'Na+': 0.150, 'Cl-': 0.150, 'Ca2+': 0.010, 'H2O': 55.5}. Cohort C (bracketing replicate, mol/kg): {'Na+': 0.1, 'Cl-': 0.1, 'Ca2+': 0.02, 'NO3-': 0.0}. Output the formatted solutes mapping for each cohort after applying the sieve rules.", "answers": "[{\"name\":\"pyEQL_utils_format_solutes_dict\",\"arguments\":{\"solute_dict\":{\"Na+\":0.015,\"Cl-\":0.014,\"Ca2+\":0.002},\"units\":\"mol/kg\"}},{\"name\":\"pyEQL_utils_format_solutes_dict\",\"arguments\":{\"solute_dict\":{\"Na+\":0.15,\"Cl-\":0.15,\"Ca2+\":0.01},\"units\":\"mol/L\"}},{\"name\":\"pyEQL_utils_format_solutes_dict\",\"arguments\":{\"solute_dict\":{\"Na+\":0.1,\"Cl-\":0.1,\"Ca2+\":0.02},\"units\":\"mol/kg\"}}]"}
{"func_name": "pyEQL_utils_interpret_units", "func_desc": "pyEQL.utils.interpret_units translates commonly used environmental unit abbreviations (for example, \"ppm\") into strings that the pint library can understand and use in pyEQL's units-aware calculations for aqueous solution properties.\n    \n    This function is used throughout pyEQL when parsing user-provided unit strings (for concentrations, amounts, and other solution properties) so they can be passed to a pint UnitRegistry for numeric conversions and arithmetic. It provides a small, explicit mapping for a handful of common environmental shorthand notations to practical units used in water chemistry modeling. The mapping is case-sensitive and limited to the explicit keys implemented; unrecognized inputs are returned unchanged so callers can decide how to handle them.", "tools": [{"function": {"description": "pyEQL.utils.interpret_units translates commonly used environmental unit abbreviations (for example, \"ppm\") into strings that the pint library can understand and use in pyEQL's units-aware calculations for aqueous solution properties.\n\nThis function is used throughout pyEQL when parsing user-provided unit strings (for concentrations, amounts, and other solution properties) so they can be passed to a pint UnitRegistry for numeric conversions and arithmetic. It provides a small, explicit mapping for a handful of common environmental shorthand notations to practical units used in water chemistry modeling. The mapping is case-sensitive and limited to the explicit keys implemented; unrecognized inputs are returned unchanged so callers can decide how to handle them.", "name": "pyEQL_utils_interpret_units", "parameters": {"properties": {"unit": {"type": "string", "description": "The input unit string to translate. This should be the exact string provided by the user or upstream code (case-sensitive). Typical expected inputs include environmental shorthand such as \"ppm\", \"ppb\", \"ppt\", and \"m\" (where \"m\" here denotes molal). The function treats these specific lowercase strings as special cases and maps them to pint-compatible equivalents: \"m\" -> \"mol/kg\" (molality), \"ppm\" -> \"mg/L\", \"ppb\" -> \"ug/L\", and \"ppt\" -> \"ng/L\". If the caller provides a unit string not listed here or with different casing (for example \"PPM\" or \"M\"), the function will not perform a translation and will simply return the original string.", "default": ""}}, "required": ["unit"], "type": "any"}}, "type": "function"}], "query": "We’re merging nitrate concentration metadata from three ingestion channels (LIMS export, modeling-script CLI, and a spreadsheet parser) where the reported unit shorthand is heterogeneous and sometimes contains vendor artifacts. For each channel, translate the unit shorthand to a pint-compatible unit string using pyEQL’s environmental shorthand mapping only when the unit token is a clean environmental abbreviation (i.e., strictly alphabetic after trimming surrounding whitespace). Use the raw token as-is otherwise so downstream validation can flag it.\n\nUnit tokens observed (one per channel):\n- LIMS export: \" ppb \"\n- modeling-script CLI: \"PPB\"\n- spreadsheet parser: \"ppb*\"", "answers": "[{\"name\":\"pyEQL_utils_interpret_units\",\"arguments\":{\"unit\":\"ppb\"}},{\"name\":\"pyEQL_utils_interpret_units\",\"arguments\":{\"unit\":\"PPB\"}}]"}
{"func_name": "pybel_io_jgif_map_cbn", "func_desc": "pybel.io.jgif.map_cbn pre-processes a JGIF (JSON Graph Interchange Format) document produced by the CBN to normalize experimental-context evidence attached to edges so the data can be more reliably consumed by PyBEL I/O pipelines. In the biological-network domain (PyBEL), this function standardizes per-evidence annotation keys (for example mapping \"species_common_name\" to a canonical \"Species\" value via a species_map and remapping other keys via annotation_map), strips surrounding whitespace from keys and values, drops empty/placeholder values, and replaces each evidence's EXPERIMENT_CONTEXT mapping with the cleaned, standardized mapping.", "tools": [{"function": {"description": "pybel.io.jgif.map_cbn pre-processes a JGIF (JSON Graph Interchange Format) document produced by the CBN to normalize experimental-context evidence attached to edges so the data can be more reliably consumed by PyBEL I/O pipelines. In the biological-network domain (PyBEL), this function standardizes per-evidence annotation keys (for example mapping \"species_common_name\" to a canonical \"Species\" value via a species_map and remapping other keys via annotation_map), strips surrounding whitespace from keys and values, drops empty/placeholder values, and replaces each evidence's EXPERIMENT_CONTEXT mapping with the cleaned, standardized mapping.\n", "name": "pybel_io_jgif_map_cbn", "parameters": {"properties": {"d": {"type": "any", "description": "Raw JGIF dictionary output from the CBN. This function expects d to follow the JGIF structure used by the CBN: a top-level \"graph\" key containing an \"edges\" list. Each edge may contain a \"metadata\" mapping with an \"evidences\" list; each evidence may contain an EXPERIMENT_CONTEXT mapping of annotation keys to string values. The parameter d is both the input and the object that will be modified in place: the function updates d[\"graph\"][\"edges\"][i][\"metadata\"][\"evidences\"][j][EXPERIMENT_CONTEXT] with a new normalized dictionary for each evidence that contains EXPERIMENT_CONTEXT.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "We’re doing a two-replicate ingest preflight on CBN-exported JGIFs, but only for edges whose evidence context is actually interpretable for comparative biology. For each cohort independently, run the CBN→PyBEL evidence-context normalizer on the whole document, then retain only those edges that have at least one evidence whose EXPERIMENT_CONTEXT, after normalization, contains a resolvable Species annotation (i.e., species fields like species_common_name/synonyms/whitespace-padded variants normalize to the canonical Species key with a non-placeholder value). Evidence contexts should be cleaned as usual (trim keys/values, drop empty/whitespace-only and placeholders like N/A, remap variant keys via the standard annotation and species maps) while leaving all other edge fields (ids, sources/targets, citations, other metadata) unchanged. Use these two raw JGIF dictionaries as the batch inputs (treat as independent replicates):\n\nCohort A:\n{\n  \"graph\": {\n    \"edges\": [\n      {\n        \"id\": \"e1\",\n        \"source\": \"n1\",\n        \"target\": \"n2\",\n        \"metadata\": {\n          \"evidences\": [\n            {\n              \"EXPERIMENT_CONTEXT\": {\n                \" species_common_name \": \"  human  \",\n                \" CellType \": \"  HepG2  \",\n                \" Treatment \": \" N/A \",\n                \"Note\": \"  \"\n              },\n              \"citation\": \"PMID:12345678\"\n            }\n          ]\n        }\n      },\n      {\n        \"id\": \"e2\",\n        \"source\": \"n2\",\n        \"target\": \"n3\",\n        \"metadata\": {\n          \"evidences\": [\n            {\n              \"EXPERIMENT_CONTEXT\": {\n                \"species_common_name\": \" Mus musculus \",\n                \"   tissue  \": \" liver \",\n                \"Condition\": \"\"\n              },\n              \"citation\": \"PMID:87654321\"\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n\nCohort B:\n{\n  \"graph\": {\n    \"edges\": [\n      {\n        \"source\": \"n0\",\n        \"target\": \"n1\",\n        \"metadata\": {\n          \"evidences\": [\n            {\n              \"citation\": \"PMID:12345678\",\n              \"EXPERIMENT_CONTEXT\": {\n                \"species_common_name\": \"  human  \",\n                \"Cell type \": \" HeLa \",\n                \"treatment\": \"   \",\n                \"_placeholder\": \"N/A\"\n              }\n            },\n            {\n              \"citation\": \"PMID:87654321\",\n              \"EXPERIMENT_CONTEXT\": {\n                \"species_common_name\": \"mouse \",\n                \"cell_type\": \"  fibroblast  \"\n              }\n            }\n          ]\n        }\n      },\n      {\n        \"source\": \"n1\",\n        \"target\": \"n2\",\n        \"metadata\": {\n          \"evidences\": [\n            {\n              \"citation\": \"PMID:11223344\",\n              \"EXPERIMENT_CONTEXT\": {\n                \"species_common_name\": \" rat\",\n                \"Cell line\": \"  PC12  \",\n                \"dose\": \"10 uM\",\n                \"extra_field\": \"   \"\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n}", "answers": "[{\"name\":\"pybel_io_jgif_map_cbn\",\"arguments\":{\"d\":{\"graph\":{\"edges\":[{\"id\":\"e1\",\"source\":\"n1\",\"target\":\"n2\",\"metadata\":{\"evidences\":[{\"EXPERIMENT_CONTEXT\":{\" species_common_name \":\"  human  \",\" CellType \":\"  HepG2  \",\" Treatment \":\" N/A \",\"Note\":\"  \"},\"citation\":\"PMID:12345678\"}]}},{\"id\":\"e2\",\"source\":\"n2\",\"target\":\"n3\",\"metadata\":{\"evidences\":[{\"EXPERIMENT_CONTEXT\":{\"species_common_name\":\" Mus musculus \",\"   tissue  \":\" liver \",\"Condition\":\"\"},\"citation\":\"PMID:87654321\"}]}}]}}}},{\"name\":\"pybel_io_jgif_map_cbn\",\"arguments\":{\"d\":{\"graph\":{\"edges\":[{\"source\":\"n0\",\"target\":\"n1\",\"metadata\":{\"evidences\":[{\"citation\":\"PMID:12345678\",\"EXPERIMENT_CONTEXT\":{\"species_common_name\":\"  human  \",\"Cell type \":\" HeLa \",\"treatment\":\"   \",\"_placeholder\":\"N/A\"}},{\"citation\":\"PMID:87654321\",\"EXPERIMENT_CONTEXT\":{\"species_common_name\":\"mouse \",\"cell_type\":\"  fibroblast  \"}}]}},{\"source\":\"n1\",\"target\":\"n2\",\"metadata\":{\"evidences\":[{\"citation\":\"PMID:11223344\",\"EXPERIMENT_CONTEXT\":{\"species_common_name\":\" rat\",\"Cell line\":\"  PC12  \",\"dose\":\"10 uM\",\"extra_field\":\"   \"}}]}}]}}}}]"}
{"func_name": "pybel_manager_cache_manager_not_resource_cachable", "func_desc": "Check if a BEL resource entry should not be cached.\n    \n    This function inspects the dictionary metadata for a BEL resource (as returned by\n    get_bel_resource) and determines whether the resource is considered non-cacheable.\n    Within the PyBEL project this is used by the cache manager to decide whether to\n    persist a downloaded or generated resource to the local cache or other storage.\n    The function expects the resource dictionary to contain a \"Processing\" mapping\n    with an optional \"CacheableFlag\" string value. The function treats the resource\n    as cacheable only when the \"CacheableFlag\" exactly equals one of the\n    case-sensitive strings: \"yes\", \"Yes\", \"True\", or \"true\". Any other value,\n    including a missing \"CacheableFlag\" or None, is interpreted as non-cacheable.", "tools": [{"function": {"description": "Check if a BEL resource entry should not be cached.\n\nThis function inspects the dictionary metadata for a BEL resource (as returned by\nget_bel_resource) and determines whether the resource is considered non-cacheable.\nWithin the PyBEL project this is used by the cache manager to decide whether to\npersist a downloaded or generated resource to the local cache or other storage.\nThe function expects the resource dictionary to contain a \"Processing\" mapping\nwith an optional \"CacheableFlag\" string value. The function treats the resource\nas cacheable only when the \"CacheableFlag\" exactly equals one of the\ncase-sensitive strings: \"yes\", \"Yes\", \"True\", or \"true\". Any other value,\nincluding a missing \"CacheableFlag\" or None, is interpreted as non-cacheable.", "name": "pybel_manager_cache_manager_not_resource_cachable", "parameters": {"properties": {"bel_resource": {"type": "any", "description": "A dictionary representing a BEL resource metadata\nreturned by get_bel_resource. In practice this dict must contain a\n\"Processing\" key whose value is a mapping (dict-like) that may include\nthe \"CacheableFlag\" key. The \"CacheableFlag\" value is expected to be a\nstring indicating whether the resource may be cached; accepted\ncacheable string values are \"yes\", \"Yes\", \"True\", and \"true\".", "default": ""}}, "required": ["bel_resource"], "type": "any"}}, "type": "function"}], "query": "We’re doing a pre-publication BEL resource provenance audit before syncing anything to our shared cache. Treat the incoming batch as messy metadata from multiple pipelines. Run the cacheability check only on resources that look like they could plausibly be stable third‑party assets: the entry must have a non-empty 'URL' or 'SourceURL' field whose value starts with 'https://', and it must include a 'Processing' mapping (to ensure the producer attempted to declare caching intent). For each qualifying resource, classify whether it should NOT be cached using the project rule that considers a resource cacheable only when Processing.CacheableFlag is exactly one of: 'yes', 'Yes', 'True', or 'true'. Batch to triage: (A) {'Name':'EphemeralGeneSet','Version':'2025-12-01','Source':'generated','Processing':{'CacheableFlag':'No','Notes':'Generated per-request; do not persist'},'URL':'https://example.org/resources/ephemeral_geneset'}; (B) {'Name':'Example Pathway Resource','Version':'2026-01-01','URL':'https://example.org/bel/resources/pathways.belns','Processing':{'CacheableFlag':'No','Notes':'Generated on demand; avoid caching to prevent stale data'}}; (C) {'Name':'HGNC Gene Annotations','Version':'2024-03-01','Processing':{'CacheableFlag':'no','GeneratedOn':'2024-03-15T10:23:00Z','SourceURL':'https://resources.example.org/hgnc/genes.bel'}; (D) {'Name':'Legacy Mirrored Resource','Version':'2019-07-20','Processing':{'CacheableFlag':'True'},'URL':'http://insecure.example.org/legacy.belns'}; (E) {'Name':'Partner Drop (missing processing)','Version':'2026-01-15','URL':'https://partner.example.org/drop/latest.belns'}; (F) {'Name':'Curated Ontology Snapshot','Version':'2025-05-01','URL':'https://example.org/ontologies/snapshot_2025_05.belns','Processing':{'CacheableFlag':'true','GeneratedOn':'2025-05-02T00:00:00Z'}}. Return results as “should NOT be cached” for each resource that meets the triage criteria.", "answers": "[{\"name\":\"pybel_manager_cache_manager_not_resource_cachable\",\"arguments\":{\"bel_resource\":{\"Name\":\"EphemeralGeneSet\",\"Version\":\"2025-12-01\",\"Source\":\"generated\",\"Processing\":{\"CacheableFlag\":\"No\",\"Notes\":\"Generated per-request; do not persist\"},\"URL\":\"https://example.org/resources/ephemeral_geneset\"}}},{\"name\":\"pybel_manager_cache_manager_not_resource_cachable\",\"arguments\":{\"bel_resource\":{\"Name\":\"Example Pathway Resource\",\"Version\":\"2026-01-01\",\"URL\":\"https://example.org/bel/resources/pathways.belns\",\"Processing\":{\"CacheableFlag\":\"No\",\"Notes\":\"Generated on demand; avoid caching to prevent stale data\"}}}},{\"name\":\"pybel_manager_cache_manager_not_resource_cachable\",\"arguments\":{\"bel_resource\":{\"Name\":\"HGNC Gene Annotations\",\"Version\":\"2024-03-01\",\"Processing\":{\"CacheableFlag\":\"no\",\"GeneratedOn\":\"2024-03-15T10:23:00Z\",\"SourceURL\":\"https://resources.example.org/hgnc/genes.bel\"}}}},{\"name\":\"pybel_manager_cache_manager_not_resource_cachable\",\"arguments\":{\"bel_resource\":{\"Name\":\"Curated Ontology Snapshot\",\"Version\":\"2025-05-01\",\"URL\":\"https://example.org/ontologies/snapshot_2025_05.belns\",\"Processing\":{\"CacheableFlag\":\"true\",\"GeneratedOn\":\"2025-05-02T00:00:00Z\"}}}}]"}
{"func_name": "pybel_manager_citation_utils_sanitize_date", "func_desc": "Sanitize a variety of free-form publication date strings into an ISO-8601 date string (YYYY-MM-DD) for use in citation metadata and BEL graph versioning within PyBEL.\n    \n    This function is used by PyBEL's citation utilities (pybel.manager.citation_utils) to convert heterogeneous date representations found in BEL documents, external data sources, or citation records into a consistent ISO-8601 format required for downstream storage, comparison, and display (for example, in graph summaries, exports, and database fields).", "tools": [{"function": {"description": "Sanitize a variety of free-form publication date strings into an ISO-8601 date string (YYYY-MM-DD) for use in citation metadata and BEL graph versioning within PyBEL.\n\nThis function is used by PyBEL's citation utilities (pybel.manager.citation_utils) to convert heterogeneous date representations found in BEL documents, external data sources, or citation records into a consistent ISO-8601 format required for downstream storage, comparison, and display (for example, in graph summaries, exports, and database fields).", "name": "pybel_manager_citation_utils_sanitize_date", "parameters": {"properties": {"publication_date": {"type": "string", "description": "The raw publication date string to normalize. This is typically a date extracted from a citation record or BEL document (for example, the \"date\" field when compiling or loading BEL content). The function expects a Python str and will apply a sequence of compiled regular-expression checks and datetime parsing to interpret common forms of publication dates. Recognized forms derived from the implementation include an exact year-month-day with abbreviated month (parsed with \"%Y %b %d\"), year and abbreviated month (\"%Y %b\" -> day defaults to \"01\"), a bare four-digit year (interpreted as YYYY-01-01), seasonal expressions (year plus a season token mapped via the internal season_map to a month), and variants that include hyphenated qualifiers after the day (these are matched and incorporated into the parsing format). The input is not modified in place; it is read and converted. If the string does not match any of the handled patterns or if parsing fails, datetime.strptime can raise ValueError; if a non-str is passed, a TypeError may be raised when regular-expression functions are applied.", "default": ""}}, "required": ["publication_date"], "type": "any"}}, "type": "function"}], "query": "In a PyBEL citation ingest QA/QC run, we’re reconciling publication dates from heterogeneous upstream sources before persisting citation metadata and using dates for BEL graph versioning. The raw `publication_date` fields are: `[\"2019 Fall\", \"2019 Nov\", \"2019-11-15\", \"TBD\", \"2019\", \"2019 Q4\", \"Nov 2019\", \"2019-00-10\"]`. Apply a data-sieve rule: sanitize only records that encode a usable calendar date with at least year+month resolution (including seasonal/quarter encodings and month abbreviations) and ignore entries that are placeholders, year-only, or contain impossible month/day components. Return ISO-8601 strings (YYYY-MM-DD) for the retained records for downstream comparison, database fields, and export.", "answers": "[{\"name\":\"pybel_manager_citation_utils_sanitize_date\",\"arguments\":{\"publication_date\":\"2019 Fall\"}},{\"name\":\"pybel_manager_citation_utils_sanitize_date\",\"arguments\":{\"publication_date\":\"2019 Nov\"}},{\"name\":\"pybel_manager_citation_utils_sanitize_date\",\"arguments\":{\"publication_date\":\"2019 Q4\"}},{\"name\":\"pybel_manager_citation_utils_sanitize_date\",\"arguments\":{\"publication_date\":\"Nov 2019\"}}]"}
{"func_name": "pybel_testing_utils_get_uri_name", "func_desc": "pybel.testing.utils.get_uri_name: Extract the file name or resource name from the given URL string for use when saving, caching, or naming downloaded BEL-related resources in PyBEL testing utilities.\n    \n    This function parses the input URL with urllib.parse.urlparse and returns a short, human-usable name describing the remote resource. In practice within PyBEL (for example when calling urllib.request.urlretrieve as shown in the README), this name is used to generate local filenames for downloaded BEL documents, JSON serializations, or other test resources without performing any network I/O.", "tools": [{"function": {"description": "pybel.testing.utils.get_uri_name: Extract the file name or resource name from the given URL string for use when saving, caching, or naming downloaded BEL-related resources in PyBEL testing utilities.\n\nThis function parses the input URL with urllib.parse.urlparse and returns a short, human-usable name describing the remote resource. In practice within PyBEL (for example when calling urllib.request.urlretrieve as shown in the README), this name is used to generate local filenames for downloaded BEL documents, JSON serializations, or other test resources without performing any network I/O.", "name": "pybel_testing_utils_get_uri_name", "parameters": {"properties": {"url": {"type": "string", "description": "The URL to parse and extract a terminal resource name from. This is expected to be a Python string representing a web resource location (for example, a raw GitHub URL or other HTTP(S) link to a BEL document). The function treats the input purely as text and does not attempt to download or validate the resource. If a non-str value is passed, the caller will encounter a TypeError due to the function signature and the expectation that url is a string.", "default": ""}}, "required": ["url"], "type": "any"}}, "type": "function"}], "query": "I’m hardening our PyBEL test-suite caching layer against messy, real-world URI inputs. Given the following candidate download locations (a mix of raw GitHub content, repository “blob” views, and URLs with query/fragment noise), compute the local cache key using PyBEL’s URI-derived naming helper **only for URIs that actually point to a concrete resource file** (i.e., the parsed URI path ends with a filename that contains an extension). For any such URI, derive the short resource/filename component exactly as PyBEL would use for naming a saved artifact.\n\nCandidate URIs:\n1) https://raw.githubusercontent.com/pybel/pybel-examples/master/examples/bel_corpus.bel\n2) https://github.com/pybel/pybel-examples/raw/master/examples/resources/example.bel.json?download=1\n3) https://raw.githubusercontent.com/pybel/pybel/master/tests/resources/small_corpus.bel#sha256=deadbeef\n4) https://github.com/pybel/pybel-examples/tree/master/examples/resources/\n5) https://raw.githubusercontent.com/pybel/pybel-examples/master/examples/resources/\n6) https://github.com/pybel/pybel/blob/master/README.md", "answers": "[{\"name\":\"pybel_testing_utils_get_uri_name\",\"arguments\":{\"url\":\"https://raw.githubusercontent.com/pybel/pybel-examples/master/examples/bel_corpus.bel\"}},{\"name\":\"pybel_testing_utils_get_uri_name\",\"arguments\":{\"url\":\"https://github.com/pybel/pybel-examples/raw/master/examples/resources/example.bel.json?download=1\"}},{\"name\":\"pybel_testing_utils_get_uri_name\",\"arguments\":{\"url\":\"https://raw.githubusercontent.com/pybel/pybel/master/tests/resources/small_corpus.bel#sha256=deadbeef\"}},{\"name\":\"pybel_testing_utils_get_uri_name\",\"arguments\":{\"url\":\"https://github.com/pybel/pybel/blob/master/README.md\"}}]"}
{"func_name": "pybel_utils_ensure_quotes", "func_desc": "Ensure that a BEL token or label is wrapped in double quotes when it contains non-alphanumeric characters.", "tools": [{"function": {"description": "Ensure that a BEL token or label is wrapped in double quotes when it contains non-alphanumeric characters.\n", "name": "pybel_utils_ensure_quotes", "parameters": {"properties": {"s": {"type": "string", "description": "A text token, typically a node name, label, or identifier string used in BEL statements and PyBEL serialization routines (for example, a gene/protein name, a namespace entry, or an annotation value). This function examines the supplied string to decide whether it is already an acceptable unquoted token (consisting only of characters matched by the module-level regular expression used for unquoted BEL tokens) or whether it must be represented as a quoted string in serialized BEL. The argument must be a Python str as required by the function signature; other types are not accepted by this function.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "We’re running a BEL export preflight on a messy set of protein node labels coming from mixed curation sources (manual entry + OCR). Before serialization, standardize only the labels that should survive as valid BEL tokens under a conservative hygiene rule: treat a label as exportable if, after trimming leading/trailing whitespace, it is non-empty and contains at least one alphanumeric character. For each exportable label, apply the BEL quoting rule exactly once: wrap the token in double quotes only if the trimmed label contains any non-alphanumeric characters (e.g., whitespace, hyphen, parentheses). Run the QC in the incoming order.\n\nRaw labels (in order):\n1) \"  p53 (human)  \"\n2) \"ATP1A1\"\n3) \"TNF-alpha (human)\"\n4) \"   \"\n5) \"---\"", "answers": "[{\"name\":\"pybel_utils_ensure_quotes\",\"arguments\":{\"s\":\"p53 (human)\"}},{\"name\":\"pybel_utils_ensure_quotes\",\"arguments\":{\"s\":\"ATP1A1\"}},{\"name\":\"pybel_utils_ensure_quotes\",\"arguments\":{\"s\":\"TNF-alpha (human)\"}}]"}
{"func_name": "pybel_utils_expand_dict", "func_desc": "pybel.utils.expand_dict expands a flattened dictionary whose keys are composite strings (concatenated nested keys) into a nested dictionary structure by splitting keys on a separator and recursing. In the PyBEL codebase this is useful for rebuilding nested attribute or annotation structures (for example, attributes serialized as concatenated strings in export formats) back into hierarchical Python dicts suitable for BEL graph processing and downstream I/O.", "tools": [{"function": {"description": "pybel.utils.expand_dict expands a flattened dictionary whose keys are composite strings (concatenated nested keys) into a nested dictionary structure by splitting keys on a separator and recursing. In the PyBEL codebase this is useful for rebuilding nested attribute or annotation structures (for example, attributes serialized as concatenated strings in export formats) back into hierarchical Python dicts suitable for BEL graph processing and downstream I/O.\n", "name": "pybel_utils_expand_dict", "parameters": {"properties": {"flat_dict": {"type": "any", "description": "A flattened dictionary to expand. Keys must be strings that represent one or more nested key components concatenated with the separator string sep (for example, \"a_b_c\"). Values are treated as opaque Python objects and are preserved at the leaf positions of the reconstructed structure. This function does not mutate the passed-in flat_dict; it builds and returns a new dict.", "default": ""}, "sep": {"type": "string", "description": "The literal string used to split composite keys into their first component and the remainder. This function uses str.split(sep, 1) so only the first occurrence of sep is split on each recursion step; deeper occurrences are handled by recursive calls. sep must be a non-empty string (an empty sep will cause str.split to raise ValueError). The default separator is \"_\" which is commonly used in PyBEL-generated flattened keys.", "default": "_"}}, "required": ["flat_dict", "sep"], "type": "any"}}, "type": "function"}], "query": "We’re merging flattened metadata coming from two BEL export paths where the delimiter is not explicitly tracked per record. Treat the following as a mixed batch of flattened payloads emitted by different serializers. For each payload, infer the intended key delimiter from the keys themselves: if any key contains a double-colon token, treat the payload as CSV-style and expand using the \"::\" separator; otherwise, treat it as graph-export style and expand using the \".\" separator. Apply expansion only to payloads that actually contain at least one composite key (i.e., at least one key containing the chosen separator); payloads without any composite keys should be left as-is and not sent through rehydration.\n\nPayloads:\n1) {\"evidence.source.pubmed\":12345678,\"evidence.source.pmc\":\"PMC567890\",\"evidence.text\":\"Inhibition of AKT1 leads to decreased cell survival.\",\"annotation.disease.mesh\":\"D009369\",\"annotation.species.ncbi\":9606}\n2) {\"subject::species\":\"Homo sapiens\",\"subject::tissue\":\"liver\",\"experiment::platform\":\"RNA-Seq\",\"experiment::qc::reads_mapped\":0.972,\"experiment::qc::dup_rate\":0.081,\"provenance::source::database\":\"GEO\",\"provenance::source::accession\":\"GSE12345\"}\n3) {\"id\":\"BEL-STATEMENT-00042\",\"status\":\"curated\",\"notes\":\"no structured annotations present\"}\n\nReturn the rehydrated nested dictionaries for the eligible payloads, preserving the original leaf values.", "answers": "[{\"name\":\"pybel_utils_expand_dict\",\"arguments\":{\"flat_dict\":{\"evidence.source.pubmed\":12345678,\"evidence.source.pmc\":\"PMC567890\",\"evidence.text\":\"Inhibition of AKT1 leads to decreased cell survival.\",\"annotation.disease.mesh\":\"D009369\",\"annotation.species.ncbi\":9606},\"sep\":\".\"}},{\"name\":\"pybel_utils_expand_dict\",\"arguments\":{\"flat_dict\":{\"subject::species\":\"Homo sapiens\",\"subject::tissue\":\"liver\",\"experiment::platform\":\"RNA-Seq\",\"experiment::qc::reads_mapped\":0.972,\"experiment::qc::dup_rate\":0.081,\"provenance::source::database\":\"GEO\",\"provenance::source::accession\":\"GSE12345\"},\"sep\":\"::\"}}]"}
{"func_name": "pyriemann_datasets_simulated_make_outliers", "func_desc": "Generate outlier SPD matrices for a Riemannian Gaussian distribution with a fixed mean and dispersion.\n\nThis function is used in the pyRiemann simulated datasets context to create covariance-like symmetric positive definite (SPD) matrices that lie far from a given Riemannian Gaussian center. Such outliers are useful to test robustness of Riemannian classifiers and processing pipelines (for example in biosignal/BCI or remote sensing applications where covariance matrices are processed on the SPD manifold). The generation procedure (used internally by pyRiemann) is: compute the matrix square root of the provided mean, draw random SPD matrices O_i, compute a scalar exponent epsilon from outlier_coeff, sigma and the Riemannian squared distance between O_i and the identity, then map O_i to the tangent-scaled outlier by conjugation outlier = mean_sqrt @ O_i**epsilon @ mean_sqrt. The resulting matrices are SPD and have shape (n_matrices, n_dim, n_dim).", "tools": [{"function": {"description": "Generate outlier SPD matrices for a Riemannian Gaussian distribution with a fixed mean and dispersion.\n\nThis function is used in the pyRiemann simulated datasets context to create covariance-like symmetric positive definite (SPD) matrices that lie far from a given Riemannian Gaussian center. Such outliers are useful to test robustness of Riemannian classifiers and processing pipelines (for example in biosignal/BCI or remote sensing applications where covariance matrices are processed on the SPD manifold). The generation procedure (used internally by pyRiemann) is: compute the matrix square root of the provided mean, draw random SPD matrices O_i, compute a scalar exponent epsilon from outlier_coeff, sigma and the Riemannian squared distance between O_i and the identity, then map O_i to the tangent-scaled outlier by conjugation outlier = mean_sqrt @ O_i**epsilon @ mean_sqrt. The resulting matrices are SPD and have shape (n_matrices, n_dim, n_dim).", "name": "pyriemann_datasets_simulated_make_outliers", "parameters": {"properties": {"n_matrices": {"type": "integer", "description": "Number of outlier matrices to generate. This determines the first dimension of the returned array. Must be a non-negative integer; passing zero returns an array with shape (0, n_dim, n_dim).", "default": ""}, "mean": {"type": "array", "items": {"type": "float"}, "description": "Center of the Riemannian Gaussian distribution. Must be a square ndarray of shape (n_dim, n_dim) representing an SPD matrix (symmetric positive definite). The function uses mean to compute its matrix square root (mean_sqrt = sqrtm(mean)) and to conjugate generated matrices, so mean must be numerically symmetric and positive definite for meaningful results; otherwise a linear algebra error or incorrect output may occur.", "default": ""}, "sigma": {"type": "float", "description": "Dispersion (scale) parameter of the Riemannian Gaussian distribution. A non-negative scalar controlling how far outliers will be placed relative to the distribution spread. sigma participates linearly in the computation of the exponent epsilon; sigma <= 0 will produce degenerate scaling (epsilon may be zero), so provide a positive sigma for standard outlier behaviour.", "default": ""}, "outlier_coeff": {"type": "float", "description": "Coefficient that scales the definition of an outlier in units of sigma. Conceptually, this parameter controls \"how many times the sigma parameter its distance to the mean should be\" (as in the original implementation): larger values create more distant outliers. The default value 10 is the library default used historically in pyRiemann to produce clear outliers.", "default": 10}, "random_state": {"type": "integer", "nullable": true, "description": "Pseudo-random number generator or seed used to draw intermediate SPD matrices. Pass an int for reproducible output across multiple calls, pass an instance of numpy.random.RandomState for a specific RNG state, or pass None to use the global RNG. The provided random_state is forwarded to the internal make_matrices call; therefore it controls reproducibility and also will be advanced (consumes randomness) as a side effect.", "default": null}}, "required": ["n_matrices", "mean", "sigma", "outlier_coeff", "random_state"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an SPD-manifold pipeline with synthetic Riemannian-Gaussian outliers around 3×3 mean covariance centers, but the dataset is messy and we want heterogeneous injection rules.\n\nYou are given a batch of candidate cohort specifications (each with a 3×3 mean center, an RNG seed, and a requested sample count):\n1) cohort_id='A_pilot_BCI': mean=[[1.0,0.2,0.0],[0.2,1.5,0.1],[0.0,0.1,2.0]], requested_n=5, seed=42\n2) cohort_id='B_stress_RS': mean=[[1.0,0.2,0.1],[0.2,0.9,0.0],[0.1,0.0,1.1]], requested_n=50, seed=2024\n3) cohort_id='C_artifact_singular': mean=[[1.0,1.2,0.0],[1.2,1.0,0.0],[0.0,0.0,0.05]], requested_n=20, seed=7\n4) cohort_id='D_near_identity': mean=[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]], requested_n=10, seed=99\n\nPipeline rules:\n- Only generate outliers for cohorts whose mean center is suitable as a covariance-like SPD reference (symmetric with strictly positive eigenvalues).\n- Use a dispersion rule that depends on conditioning of the mean: sigma=0.7 when cond(mean) ≤ 10; sigma=0.4 when cond(mean) > 10.\n- Use an outlier-strength rule tied to spectral anisotropy of the mean: outlier_coeff=12 when (largest_eigenvalue / smallest_eigenvalue) ≥ 2; otherwise outlier_coeff=8.\n- For each eligible cohort, generate exactly requested_n outlier SPD matrices using the provided seed.\n\nReturn the resulting outlier matrices per eligible cohort for downstream robustness evaluation (each output should have shape (requested_n, 3, 3)).", "answers": "[{\"name\":\"pyriemann_datasets_simulated_make_outliers\",\"arguments\":{\"n_matrices\":5,\"mean\":[[1.0,0.2,0.0],[0.2,1.5,0.1],[0.0,0.1,2.0]],\"sigma\":0.7,\"outlier_coeff\":12,\"random_state\":42}},{\"name\":\"pyriemann_datasets_simulated_make_outliers\",\"arguments\":{\"n_matrices\":50,\"mean\":[[1.0,0.2,0.1],[0.2,0.9,0.0],[0.1,0.0,1.1]],\"sigma\":0.7,\"outlier_coeff\":8,\"random_state\":2024}},{\"name\":\"pyriemann_datasets_simulated_make_outliers\",\"arguments\":{\"n_matrices\":10,\"mean\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],\"sigma\":0.7,\"outlier_coeff\":8,\"random_state\":99}}]"}
{"func_name": "pyriemann_stats_multiset_perm_number", "func_desc": "pyriemann.stats.multiset_perm_number returns the number of unique permutations of the provided multiset array.\nThis function is used in pyRiemann statistical utilities to quantify how many distinct orderings exist for a multiset of elements such as class labels or categorical epoch identifiers (for example in EEG/BCI permutation tests or combinatorial counts applied to covariance matrix labels).", "tools": [{"function": {"description": "pyriemann.stats.multiset_perm_number returns the number of unique permutations of the provided multiset array.\nThis function is used in pyRiemann statistical utilities to quantify how many distinct orderings exist for a multiset of elements such as class labels or categorical epoch identifiers (for example in EEG/BCI permutation tests or combinatorial counts applied to covariance matrix labels).", "name": "pyriemann_stats_multiset_perm_number", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional numpy array representing the multiset elements whose unique permutation count is required.\nEach entry is considered an element of the multiset (for example, labels for epochs in a BCI experiment). The algorithm\ncomputes the multiplicity of each distinct element (using numpy.unique and equality comparisons) and uses these multiplicities\nto compute the multinomial denominator. The function expects values that compare equal for identical elements; note that\nNaN values do not compare equal to themselves and therefore are not reliably grouped by this routine. The function uses\nlen(y) and numpy.unique(y), so passing a multi-dimensional array will cause the length of the first axis to be used,\nwhich may produce unintended results. Provide a 1-D numpy.ndarray for correct semantic behavior.", "default": ""}}, "required": ["y"], "type": "any"}}, "type": "function"}], "query": "In an EEG/BCI permutation-test audit, I’m aggregating epoch-label blocks from multiple recording sessions. Some sessions contain artifact-coded epochs (e.g., 'artifact', 'bad', NaN-like placeholders), and other sessions are effectively single-class after cleaning, which makes permutation testing meaningless. For each session’s raw label multiset below, compute the number of unique permutations only for sessions that (i) contain at least two distinct labels after dropping any non-class artifact tokens and (ii) have total epoch count between 6 and 12 inclusive. Use the cleaned label multiset that satisfies those criteria.\n\nRaw session label multisets:\nS1: [1, 1, 1, 2, 2, 3, 3, 4]\nS2: [\"left\", \"left\", \"right\", \"right\", \"right\", \"rest\"]\nS3: [\"rest\", \"rest\", \"rest\", \"rest\", \"artifact\", \"artifact\"]\nS4: [2, 2, 2, 2, 2, 2]\nS5: [\"left\", \"right\", \"bad\", \"left\", \"right\", \"bad\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\"]", "answers": "[{\"name\":\"pyriemann_stats_multiset_perm_number\",\"arguments\":{\"y\":[1,1,1,2,2,3,3,4]}},{\"name\":\"pyriemann_stats_multiset_perm_number\",\"arguments\":{\"y\":[\"left\",\"left\",\"right\",\"right\",\"right\",\"rest\"]}}]"}
{"func_name": "pyriemann_utils_base_ddexpm", "func_desc": "pyriemann.utils.base.ddexpm: Compute the directional derivative of the matrix exponential at a reference SPD/HPD matrix.\n\nComputes the directional derivative of the matrix exponential at a reference symmetric positive definite\n(SPD) or Hermitian positive definite (HPD) matrix Cref in the direction(s) X. This function implements the\nformula used in Riemannian geometry of positive definite matrices (see Matrix Analysis) and is provided for\nalgorithms that require the sensitivity of the matrix exponential with respect to perturbations of a reference\ncovariance or scatter matrix. In the pyRiemann context, Cref typically represents a covariance matrix estimated\nfrom multichannel biosignal data (EEG/MEG/EMG) or remote sensing (hyperspectral/SAR) and X represents one or\nseveral perturbation directions (for example, differences between epoch covariances and the reference). The\nimplementation obtains the eigen-decomposition of Cref, evaluates the first divided difference of the exponential\non the eigenvalues, and reconstructs the directional derivative via conjugation by the eigenvectors.", "tools": [{"function": {"description": "pyriemann.utils.base.ddexpm: Compute the directional derivative of the matrix exponential at a reference SPD/HPD matrix.\n\nComputes the directional derivative of the matrix exponential at a reference symmetric positive definite\n(SPD) or Hermitian positive definite (HPD) matrix Cref in the direction(s) X. This function implements the\nformula used in Riemannian geometry of positive definite matrices (see Matrix Analysis) and is provided for\nalgorithms that require the sensitivity of the matrix exponential with respect to perturbations of a reference\ncovariance or scatter matrix. In the pyRiemann context, Cref typically represents a covariance matrix estimated\nfrom multichannel biosignal data (EEG/MEG/EMG) or remote sensing (hyperspectral/SAR) and X represents one or\nseveral perturbation directions (for example, differences between epoch covariances and the reference). The\nimplementation obtains the eigen-decomposition of Cref, evaluates the first divided difference of the exponential\non the eigenvalues, and reconstructs the directional derivative via conjugation by the eigenvectors.", "name": "pyriemann_utils_base_ddexpm", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Array of one or more direction matrices with shape (..., n, n). Each trailing (n, n) matrix\nis interpreted as a symmetric (real) or Hermitian (complex) perturbation direction in the tangent space\nat the reference matrix. In practical use within pyRiemann, X often contains per-epoch covariance\nperturbations and may have leading batch dimensions for multiple directions computed at once.", "default": ""}, "Cref": {"type": "array", "items": {"type": "float"}, "description": "Reference SPD/HPD matrix with shape (n, n). This square matrix is the point at which the\ndirectional derivative of the matrix exponential is evaluated. The function computes the eigenvalues and\neigenvectors of Cref (via numpy.linalg.eigh) and uses them to form the action of the first divided\ndifference of the exponential on the projected directions.", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "We’re running a QC-aware Riemannian sensitivity pass for 3-channel EEG covariance modeling where some perturbation directions are contaminated by numerical asymmetry (e.g., rounding during covariance differencing). For each job below, compute the directional derivative of the matrix exponential at the provided reference covariance Cref, but only for perturbation directions that are physically admissible as covariance perturbations (i.e., exactly symmetric). If a job contains multiple candidate directions, treat it as a stacked batch and keep only those directions that satisfy the symmetry criterion; if exactly one direction passes, run it as a single-direction call. Jobs: (A) eyes-open reference Cref = [[1.2, 0.1, 0.0],[0.1, 0.9, 0.2],[0.0, 0.2, 1.5]] with candidate directions D1 = [[0.05, -0.01, 0.0],[-0.01, 0.02, 0.01],[0.0, 0.01, -0.03]] and D2 = [[-0.02, 0.00, 0.01],[0.00, 0.03, -0.021],[0.01, -0.02, 0.04]]; (B) independent resting-state reference Cref = [[2.0, 0.3, 0.1],[0.3, 1.5, 0.2],[0.1, 0.2, 1.2]] with candidate directions D3 = [[0.1, -0.02, 0.0],[-0.02, 0.05, 0.01],[0.0, 0.01, 0.08]] and D4 = [[-0.05, 0.01, -0.02],[0.01, -0.03, 0.0],[-0.02, 0.0, 0.04]]. Return the derivative matrices for each executed call to support downstream sensitivity comparisons across cohorts.", "answers": "[{\"name\":\"pyriemann_utils_base_ddexpm\",\"arguments\":{\"X\":[[0.05,-0.01,0.0],[-0.01,0.02,0.01],[0.0,0.01,-0.03]],\"Cref\":[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.5]]}},{\"name\":\"pyriemann_utils_base_ddexpm\",\"arguments\":{\"X\":[[[0.1,-0.02,0.0],[-0.02,0.05,0.01],[0.0,0.01,0.08]],[[-0.05,0.01,-0.02],[0.01,-0.03,0.0],[-0.02,0.0,0.04]]],\"Cref\":[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]]}}]"}
{"func_name": "pyriemann_utils_base_invsqrtm", "func_desc": "pyriemann.utils.base.invsqrtm — Inverse square root of SPD/HPD matrices.\n\nCompute the matrix inverse square root of one or more symmetric positive definite (SPD)\nor Hermitian positive definite (HPD) matrices. This function is used throughout pyRiemann\nfor operations on covariance matrices (for example whitening, normalization, and steps\nin Riemannian geometry-based classification pipelines for biosignals such as EEG/MEG/EMG\nin brain-computer interface applications). Given an input matrix C, the result D is\ndefined by the eigen-decomposition of C: C = V Lambda V^H and D = V Lambda^{-1/2} V^H,\nwhere Lambda is the diagonal matrix of eigenvalues and V the matrix of corresponding\neigenvectors. The elementwise operator applied to the eigenvalues is 1 / sqrt(lambda),\nimplemented via a call to the internal matrix operator used by pyRiemann.", "tools": [{"function": {"description": "pyriemann.utils.base.invsqrtm — Inverse square root of SPD/HPD matrices.\n\nCompute the matrix inverse square root of one or more symmetric positive definite (SPD)\nor Hermitian positive definite (HPD) matrices. This function is used throughout pyRiemann\nfor operations on covariance matrices (for example whitening, normalization, and steps\nin Riemannian geometry-based classification pipelines for biosignals such as EEG/MEG/EMG\nin brain-computer interface applications). Given an input matrix C, the result D is\ndefined by the eigen-decomposition of C: C = V Lambda V^H and D = V Lambda^{-1/2} V^H,\nwhere Lambda is the diagonal matrix of eigenvalues and V the matrix of corresponding\neigenvectors. The elementwise operator applied to the eigenvalues is 1 / sqrt(lambda),\nimplemented via a call to the internal matrix operator used by pyRiemann.", "name": "pyriemann_utils_base_invsqrtm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices to invert-square-root. Input must be\nat least a 2-D ndarray representing either a single n-by-n matrix or a stack/batch\nof matrices with leading dimensions indicated by \"...\". For real-valued matrices,\nC is expected to be symmetric positive definite (SPD); for complex-valued matrices,\nC is expected to be Hermitian positive definite (HPD). The function does not modify\nC in place; it reads C and returns a new ndarray with the same trailing (n, n)\nshape. Typical use in pyRiemann is to process covariance matrices estimated from\nmultichannel time series (see package README and estimation modules).", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "We’re running the whitening stage for a Riemannian EEG-BCI preprocessing pass on the [C3, Cz, C4] channel-triplet covariance estimates collected across multiple time windows, but the batch contains common acquisition artifacts. Treat the following 3×3 covariance matrices as candidate SPD inputs (some may be invalid due to numerical issues):\n\nC_baseline = [[2.0, 0.5, 0.3], [0.5, 1.5, 0.4], [0.3, 0.4, 1.2]]\nC1 = [[1.2, 0.3, 0.1],[0.3, 0.9, 0.2],[0.1, 0.2, 1.1]]\nC2 = [[2.0, 0.4, 0.0],[0.4, 1.5, 0.3],[0.0, 0.3, 1.8]]\nC3 = [[1.7, 0.2, 0.1],[0.2, 1.3, 0.4],[0.1, 0.4, 1.6]]\nC_artifact = [[1.0, 2.0, 0.0],[2.0, 1.0, 0.0],[0.0, 0.0, 0.2]]\nC_dropout = [[0.0, 0.0, 0.0],[0.0, 1.1, 0.2],[0.0, 0.2, 0.9]]\n\nFor whitening, compute the matrix inverse square root only for those candidates that are valid SPD/HPD covariance matrices (i.e., suitable for eigen-decomposition with strictly positive eigenvalues). Return the inverse square root for each matrix that passes this criterion, in a single pipeline run.", "answers": "[{\"name\":\"pyriemann_utils_base_invsqrtm\",\"arguments\":{\"C\":[[[2.0,0.5,0.3],[0.5,1.5,0.4],[0.3,0.4,1.2]],[[1.2,0.3,0.1],[0.3,0.9,0.2],[0.1,0.2,1.1]],[[2.0,0.4,0.0],[0.4,1.5,0.3],[0.0,0.3,1.8]],[[1.7,0.2,0.1],[0.2,1.3,0.4],[0.1,0.4,1.6]]]}}]"}
{"func_name": "pyriemann_utils_base_logm", "func_desc": "pyriemann.utils.base.logm: Compute the matrix logarithm of symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nComputes the symmetric (for real-valued SPD) or Hermitian (for complex-valued HPD) matrix logarithm of the input matrix or batch of matrices using an eigendecomposition. The computation follows the spectral formula D = V log(Λ) V^H where Λ is the diagonal matrix of eigenvalues of C and V the corresponding eigenvectors. In the pyRiemann library this operation is used to map covariance matrices (SPD) or Hermitian positive-definite matrices (HPD) to their matrix-log domain as part of Riemannian-geometry-based processing pipelines (for example, TangentSpace mapping, distance computations, or preprocessing of EEG/MEG covariance matrices for BCI and remote sensing applications).", "tools": [{"function": {"description": "pyriemann.utils.base.logm: Compute the matrix logarithm of symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nComputes the symmetric (for real-valued SPD) or Hermitian (for complex-valued HPD) matrix logarithm of the input matrix or batch of matrices using an eigendecomposition. The computation follows the spectral formula D = V log(Λ) V^H where Λ is the diagonal matrix of eigenvalues of C and V the corresponding eigenvectors. In the pyRiemann library this operation is used to map covariance matrices (SPD) or Hermitian positive-definite matrices (HPD) to their matrix-log domain as part of Riemannian-geometry-based processing pipelines (for example, TangentSpace mapping, distance computations, or preprocessing of EEG/MEG covariance matrices for BCI and remote sensing applications).", "name": "pyriemann_utils_base_logm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices with shape (..., n, n). C must be at least a 2-D NumPy array where the last two dimensions form square matrices. Leading dimensions (if any) are treated as batch dimensions and the operator is applied independently to each matrix in the batch. Each matrix is expected to be symmetric positive-definite (real-valued SPD) or Hermitian positive-definite (complex-valued HPD). The function does not modify C in-place.", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "We’re doing QC-aware Riemannian preprocessing before TangentSpace mapping for a motor-imagery BCI study. We have a mixed batch of trial-level covariance estimates coming from two acquisition configs (3×3 EEG montages and 2×2 sensor-pairs). Only map matrices into the matrix-log domain if they are numerically well-conditioned in the strict SPD sense: accept a covariance only when all eigenvalues are strictly positive and the spectral condition number (max eigenvalue / min eigenvalue) is ≤ 4. Apply the matrix logarithm (eig-based symmetric/Hermitian log) only to the covariances that satisfy this criterion; ignore the rest as likely artifact-contaminated trials.\n\nCandidate covariances:\n- 3×3 EEG trials: C1=[[1.0,0.2,0.1],[0.2,0.9,0.0],[0.1,0.0,1.1]], C2=[[1.2,0.1,0.0],[0.1,1.0,0.3],[0.0,0.3,0.8]]\n- 2×2 sensor-pairs: S1=[[2.0,0.3],[0.3,1.5]], S2=[[1.8,0.2],[0.2,1.2]]", "answers": "[{\"name\":\"pyriemann_utils_base_logm\",\"arguments\":{\"C\":[[[1.0,0.2,0.1],[0.2,0.9,0.0],[0.1,0.0,1.1]],[[1.2,0.1,0.0],[0.1,1.0,0.3],[0.0,0.3,0.8]]]}}]"}
{"func_name": "pyriemann_utils_base_sqrtm", "func_desc": "pyriemann.utils.base.sqrtm computes the symmetric (or Hermitian) matrix square root of one or multiple symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices using an eigen-decomposition-based formula D = V sqrt(Lambda) V^H. This function is used in pyRiemann workflows that manipulate covariance or scattering matrices derived from multivariate biosignals (EEG, MEG, EMG) or remote sensing images, for example when applying Riemannian geometry operations, whitening, or tangent-space mappings on SPD/HPD matrices.", "tools": [{"function": {"description": "pyriemann.utils.base.sqrtm computes the symmetric (or Hermitian) matrix square root of one or multiple symmetric positive definite (SPD) or Hermitian positive definite (HPD) matrices using an eigen-decomposition-based formula D = V sqrt(Lambda) V^H. This function is used in pyRiemann workflows that manipulate covariance or scattering matrices derived from multivariate biosignals (EEG, MEG, EMG) or remote sensing images, for example when applying Riemannian geometry operations, whitening, or tangent-space mappings on SPD/HPD matrices.\n", "name": "pyriemann_utils_base_sqrtm", "parameters": {"properties": {"C": {"type": "array", "items": {"type": "float"}, "description": "SPD/HPD matrices to process, provided as an ndarray with shape (..., n, n). The last two dimensions must form square matrices of size n x n; leading dimensions (the \"...\" part) are interpreted as a stack/batch of matrices and the operation is applied independently to each matrix in the stack. For real-valued covariance matrices (common in EEG/BCI applications) C is expected to be symmetric positive definite (SPD); for complex-valued matrices (e.g., some SAR or signal-processing contexts) C is expected to be Hermitian positive definite (HPD). This argument is the primary input to the function and represents the matrices whose principal square roots are required in downstream Riemannian and machine-learning computations.", "default": ""}}, "required": ["C"], "type": "any"}}, "type": "function"}], "query": "In the next EEG whitening/preconditioning stage, we need square-root factors only for covariance estimates that pass a numerical stability gate. We have two cohorts of 3×3 covariance matrices:\n\nCohort A (time-window covariances):\nC1=[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,1.1]]\nC2=[[1.5,0.1,0.0],[0.1,1.3,0.2],[0.0,0.2,1.4]]\nC3=[[0.8,0.05,0.02],[0.05,0.9,0.04],[0.02,0.04,0.95]]\n\nCohort B (trial-wise covariances):\nT1=[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]]\nT2=[[1.8,0.2,0.0],[0.2,1.3,0.1],[0.0,0.1,1.1]]\n\nApply an SPD conditioning rule per cohort: within each cohort, compute the trace of every matrix and keep only matrices whose trace is at least the cohort’s median trace (to focus on higher-energy windows/trials that yield stabler whitening operators). Then compute the symmetric matrix square root (eigen-decomposition-based, preserving SPD structure) for the retained matrices, as two separate batched operations (one call per cohort, each call containing only the retained matrices in their original order).", "answers": "[{\"name\":\"pyriemann_utils_base_sqrtm\",\"arguments\":{\"C\":[[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,1.1]],[[1.5,0.1,0.0],[0.1,1.3,0.2],[0.0,0.2,1.4]]] }},{\"name\":\"pyriemann_utils_base_sqrtm\",\"arguments\":{\"C\":[[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]]] }}]"}
{"func_name": "pyriemann_utils_covariance_cospectrum", "func_desc": "Compute co-spectral matrices (the real part of cross-spectra) from a multi-channel\ntime-series. This function is intended for frequency-domain covariance estimation\nused in applications such as biosignal processing (EEG, MEG, EMG) and brain-computer\ninterfaces (BCI). It segments the input signal into FFT windows, computes the\ncross-spectral density per frequency bin via cross_spectrum, and returns the real\npart of those cross-spectra (co-spectral matrices) together with the associated\nfrequency vector. These co-spectral matrices provide frequency-specific estimates\nof linear dependencies between channels and can be used as features for\nclassification, connectivity analysis, or as inputs to Riemannian geometry-based\nalgorithms described in the project README.", "tools": [{"function": {"description": "Compute co-spectral matrices (the real part of cross-spectra) from a multi-channel\ntime-series. This function is intended for frequency-domain covariance estimation\nused in applications such as biosignal processing (EEG, MEG, EMG) and brain-computer\ninterfaces (BCI). It segments the input signal into FFT windows, computes the\ncross-spectral density per frequency bin via cross_spectrum, and returns the real\npart of those cross-spectra (co-spectral matrices) together with the associated\nfrequency vector. These co-spectral matrices provide frequency-specific estimates\nof linear dependencies between channels and can be used as features for\nclassification, connectivity analysis, or as inputs to Riemannian geometry-based\nalgorithms described in the project README.", "name": "pyriemann_utils_covariance_cospectrum", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Multi-channel time-series, real-valued, with shape\n(n_channels, n_times). Each row corresponds to one channel and each\ncolumn to a time sample. This array is the primary input from which\nfrequency-domain covariance (co-spectra) are estimated.", "default": ""}, "window": {"type": "integer", "description": "Length of the FFT window used for spectral estimation, in\nsamples. This integer controls the time-frequency resolution trade-off:\nlarger windows yield finer frequency resolution but coarser time\nlocalization. Default is 128.", "default": 128}, "overlap": {"type": "float", "description": "Fractional overlap between consecutive windows used for the\nFFT segmentation. For example, 0.75 corresponds to 75% overlap. Higher\noverlap increases the number of averaged segments and can reduce variance\nof the spectral estimates. Default is 0.75.", "default": 0.75}, "fmin": {"type": "float", "nullable": true, "description": "Minimal frequency to be returned, in the same units as\nthe frequencies produced by the function (see fs). If None, the lowest\nfrequency produced by the internal spectral estimator is returned. Use\nthis parameter to restrict the output to a band of interest (e.g., alpha\nband in EEG).", "default": null}, "fmax": {"type": "float", "nullable": true, "description": "Maximal frequency to be returned, in the same units as\nthe frequencies produced by the function (see fs). If None, the highest\nfrequency produced by the internal spectral estimator is returned. Use\nthis to limit output to an upper frequency of interest.", "default": null}, "fs": {"type": "float", "nullable": true, "description": "Sampling frequency of the time-series in Hertz. If provided,\nthe returned frequency vector is expressed in Hertz. If None, the\nfrequency vector is returned using the same units produced by the\nunderlying cross_spectrum implementation (commonly cycles per sample or\nnormalized frequency), and downstream code should interpret frequencies\naccordingly.", "default": null}}, "required": ["X", "fmin", "fmax", "window", "overlap", "fs"], "type": "any"}}, "type": "function"}], "query": "We’re validating an alpha-band (8–12 Hz) co-spectral feature extractor on a messy mini-batch of EEG snippets (all nominally fs=256 Hz, use 50% overlap). Treat each snippet as a cohort candidate and apply an adaptive QC + parameterization protocol before extracting co-spectral matrices: (1) Only analyze snippets whose channels all have the same sample count and whose per-channel variance is non-zero (flatlines indicate a disconnected electrode). (2) Use an FFT window that is the largest power-of-two not exceeding the snippet length (per channel), but never smaller than 32 samples. (3) For amplitude scaling: if the median absolute value across all samples/channels exceeds 1.0 (i.e., likely in microvolts rather than normalized units), keep the alpha band at 8–12 Hz; otherwise, widen slightly to 7–13 Hz to compensate for low-SNR synthetic test segments. Compute and return co-spectral matrices and their frequency vector for each snippet passing QC.\n\nCohort candidates (each X is channels × time):\nA) X=[[0.12,0.15,0.11,0.09,0.05,0.03,0.08,0.14,0.19,0.21,0.18,0.16,0.10,0.07,0.04,0.02,0.00,0.01,0.03,0.06],[0.05,0.07,0.09,0.10,0.08,0.06,0.04,0.03,0.02,0.01,0.00,-0.01,-0.02,-0.03,-0.04,-0.03,-0.01,0.00,0.02,0.04],[0.02,0.03,0.05,0.07,0.09,0.10,0.11,0.10,0.09,0.07,0.05,0.03,0.01,-0.01,-0.02,-0.03,-0.02,-0.01,0.00,0.01],[-0.01,0.00,0.01,0.03,0.05,0.07,0.09,0.10,0.11,0.10,0.09,0.07,0.05,0.03,0.02,0.01,0.00,-0.01,-0.02,-0.03]]\nB) X=[[12.3,11.8,12.1,11.9,12.0,11.7,12.2,11.8],[8.5,8.7,8.6,8.4,8.5,8.3,8.6,8.4],[5.1,5.0,5.2,5.1,5.0,5.1,5.2,5.1],[3.2,3.1,3.3,3.2,3.1,3.2,3.3,3.2]]\nC) X=[[0.12,0.15,0.11,0.09,0.05,0.02,-0.01,-0.04,-0.06,-0.03,0.01,0.06,0.10,0.13,0.16,0.14],[0.08,0.10,0.12,0.11,0.07,0.03,0.00,-0.03,-0.05,-0.04,-0.01,0.03,0.07,0.11,0.13,0.12],[0.05,0.07,0.09,0.10,0.08,0.05,0.02,-0.01,-0.03,-0.02,0.01,0.04,0.06,0.08,0.09,0.08]]", "answers": "[{\"name\":\"pyriemann_utils_covariance_cospectrum\",\"arguments\":{\"X\":[[12.3,11.8,12.1,11.9,12.0,11.7,12.2,11.8],[8.5,8.7,8.6,8.4,8.5,8.3,8.6,8.4],[5.1,5.0,5.2,5.1,5.0,5.1,5.2,5.1],[3.2,3.1,3.3,3.2,3.1,3.2,3.3,3.2]],\"window\":8,\"overlap\":0.5,\"fmin\":8.0,\"fmax\":12.0,\"fs\":256.0}},{\"name\":\"pyriemann_utils_covariance_cospectrum\",\"arguments\":{\"X\":[[0.12,0.15,0.11,0.09,0.05,0.02,-0.01,-0.04,-0.06,-0.03,0.01,0.06,0.1,0.13,0.16,0.14],[0.08,0.1,0.12,0.11,0.07,0.03,0.0,-0.03,-0.05,-0.04,-0.01,0.03,0.07,0.11,0.13,0.12],[0.05,0.07,0.09,0.1,0.08,0.05,0.02,-0.01,-0.03,-0.02,0.01,0.04,0.06,0.08,0.09,0.08]],\"window\":16,\"overlap\":0.5,\"fmin\":7.0,\"fmax\":13.0,\"fs\":256.0}}]"}
{"func_name": "pyriemann_utils_covariance_get_nondiag_weight", "func_desc": "Compute non-diagonality weights of a set of square matrices.\n\nThis function computes a scalar weight for each square matrix provided in X that quantifies how non-diagonal the matrix is, following Eq(B.1) in [1]. In the context of pyRiemann and covariance-based multivariate analysis (for example EEG/MEG covariance matrices used in brain-computer interface pipelines and approximate joint diagonalization algorithms), these weights measure the relative energy of the off-diagonal elements compared to the diagonal elements and can be used to down-weight matrices that are nearly diagonal during joint-diagonalization or other aggregation steps.", "tools": [{"function": {"description": "Compute non-diagonality weights of a set of square matrices.\n\nThis function computes a scalar weight for each square matrix provided in X that quantifies how non-diagonal the matrix is, following Eq(B.1) in [1]. In the context of pyRiemann and covariance-based multivariate analysis (for example EEG/MEG covariance matrices used in brain-computer interface pipelines and approximate joint diagonalization algorithms), these weights measure the relative energy of the off-diagonal elements compared to the diagonal elements and can be used to down-weight matrices that are nearly diagonal during joint-diagonalization or other aggregation steps.", "name": "pyriemann_utils_covariance_get_nondiag_weight", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Set of square matrices. The last two axes of X must form square matrices of size n x n; any leading axes are treated as batch dimensions and are preserved in the output. Each matrix is treated elementwise (X**2 in the implementation), so X should be a numeric ndarray containing real or complex values as appropriate for the application (e.g., covariance/second-order statistics estimated from multichannel time series). The function expects at least a 2D ndarray; if X has additional leading dimensions they are interpreted as independent matrices.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re running a covariance QC gate ahead of approximate joint diagonalization for two EEG cohorts (sessions vs trials). The raw covariance batches below may include matrices that are effectively already diagonal (e.g., due to aggressive spatial whitening / channel decoupling), which should be down-weighted rather than carried forward into AJD.\n\nFor each cohort independently, compute non-diagonality weights (Eq. B.1: off-diagonal energy relative to diagonal energy) only for matrices whose off-diagonal structure is non-negligible under this screening rule: keep a matrix if the largest absolute off-diagonal entry is at least 0.05×(mean of the diagonal entries of that matrix). Treat each cohort as its own batch X after applying the rule, and compute weights for the retained matrices.\n\nCohort A (recording sessions), raw X = [\n  [[1.0, 0.2, 0.0, 0.1], [0.2, 0.9, 0.3, 0.0], [0.0, 0.3, 1.1, 0.2], [0.1, 0.0, 0.2, 1.0]],\n  [[1.2, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.95, 0.0], [0.0, 0.0, 0.0, 1.05]],\n  [[0.8, 0.4, 0.1, 0.0], [0.4, 1.1, 0.2, 0.3], [0.1, 0.2, 0.9, 0.1], [0.0, 0.3, 0.1, 1.2]]\n].\n\nCohort B (EEG trials), raw X = [\n  [[1.0, 0.2, 0.0, -0.1], [0.2, 0.9, 0.3, 0.0], [0.0, 0.3, 1.1, 0.4], [-0.1, 0.0, 0.4, 0.95]],\n  [[1.2, 0.01, -0.02, 0.0], [0.01, 1.1, 0.0, -0.01], [-0.02, 0.0, 0.9, 0.02], [0.0, -0.01, 0.02, 1.05]],\n  [[0.8, 0.4, -0.3, 0.1], [0.4, 1.3, 0.2, -0.2], [-0.3, 0.2, 1.0, 0.5], [0.1, -0.2, 0.5, 1.1]]\n].", "answers": "[{\"name\":\"pyriemann_utils_covariance_get_nondiag_weight\",\"arguments\":{\"X\":[[[1.0,0.2,0.0,0.1],[0.2,0.9,0.3,0.0],[0.0,0.3,1.1,0.2],[0.1,0.0,0.2,1.0]],[[0.8,0.4,0.1,0.0],[0.4,1.1,0.2,0.3],[0.1,0.2,0.9,0.1],[0.0,0.3,0.1,1.2]]]}},{\"name\":\"pyriemann_utils_covariance_get_nondiag_weight\",\"arguments\":{\"X\":[[[1.0,0.2,0.0,-0.1],[0.2,0.9,0.3,0.0],[0.0,0.3,1.1,0.4],[-0.1,0.0,0.4,0.95]],[[0.8,0.4,-0.3,0.1],[0.4,1.3,0.2,-0.2],[-0.3,0.2,1.0,0.5],[0.1,-0.2,0.5,1.1]]]}}]"}
{"func_name": "pyriemann_utils_covariance_normalize", "func_desc": "pyriemann.utils.covariance.normalize normalizes a batch of square matrices (covariance or similar) using one of three normalization schemes (\"corr\", \"trace\", \"determinant\"). This function is typically used in pyRiemann preprocessing pipelines to make covariance or Hermitian positive-definite matrices comparable across epochs, channels, sensors, or acquisitions before applying Riemannian geometry-based methods (for example, MDM classification, TangentSpace mapping, or other covariance-based BCI and remote sensing workflows described in the README). The normalization choices produce correlation matrices (\"corr\"), unit-trace matrices (\"trace\"), or matrices with determinant equal to +/-1 (\"determinant\"), which can help stabilize numerical processing and remove scale differences between matrices coming from different trials, sessions, or sensors.", "tools": [{"function": {"description": "pyriemann.utils.covariance.normalize normalizes a batch of square matrices (covariance or similar) using one of three normalization schemes (\"corr\", \"trace\", \"determinant\"). This function is typically used in pyRiemann preprocessing pipelines to make covariance or Hermitian positive-definite matrices comparable across epochs, channels, sensors, or acquisitions before applying Riemannian geometry-based methods (for example, MDM classification, TangentSpace mapping, or other covariance-based BCI and remote sensing workflows described in the README). The normalization choices produce correlation matrices (\"corr\"), unit-trace matrices (\"trace\"), or matrices with determinant equal to +/-1 (\"determinant\"), which can help stabilize numerical processing and remove scale differences between matrices coming from different trials, sessions, or sensors.\n", "name": "pyriemann_utils_covariance_normalize", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Set of square matrices with shape (..., n, n). This must be at least a 2-D NumPy array where the last two dimensions index square matrices. Each slice X[..., i, j] represents a matrix element. In the pyRiemann context, X typically contains estimated covariance or HPD/SPD matrices computed from multichannel biosignals (EEG/MEG/EMG) or spatial covariance estimates from remote sensing. The function does not modify the input array X in place; it returns a new array containing normalized matrices. Note that zero diagonal entries (for \"corr\") or zero traces (for \"trace\") or zero determinants (for \"determinant\") will lead to division by zero and produce inf or nan values in the output.", "default": ""}, "norm": {"type": "string", "description": "Normalization mode, one of {\"corr\", \"trace\", \"determinant\"}. The practical meaning of each mode in the pyRiemann workflow is:\n\"corr\": convert each matrix to a correlation matrix by dividing by the outer product of standard deviations computed from the diagonal. Resulting matrices have diagonal values equal to 1, off-diagonal values nominally in [-1, 1], and are suitable when only linear relationships (correlations) between channels are of interest.\n\"trace\": scale each matrix so its trace equals 1. This preserves relative variances across matrix entries while removing overall scale, useful when comparing matrices whose total power (trace) differs across trials or sensors.\n\"determinant\": scale each matrix so that its determinant equals +/-1. Implementation divides by |det(X)|^(1/n) (where n is matrix size), so the normalized determinant equals det(X)/|det(X)| (i.e., the sign of the original determinant). This is useful when relative volume (generalized variance) should be normalized while preserving determinant sign. Determinant normalization requires matrices to be invertible to avoid zero denominators; singular matrices will produce zeros in the denominator and lead to inf/nan in the result.", "default": ""}}, "required": ["X", "norm"], "type": "any"}}, "type": "function"}], "query": "We’re running a cross-session Riemannian-BCI QC+preprocessing pass where covariance magnitudes drift across days and a few epochs show sign inversions in overall scaling. You’re given two raw cohorts of symmetric covariance estimates (same matrices as below). Apply an adaptive normalization policy per-epoch based only on intrinsic matrix properties:\n\nCohort A (10 epochs, 3×3): treat these as channel-level EEG covariances. For epochs whose diagonal entries are all strictly positive, convert them to correlation matrices (corr normalization) to remove per-channel scaling. If an epoch has any non-positive diagonal entry (indicating a broken variance estimate), normalize it by determinant instead to stabilize scale without relying on per-channel standard deviations. Return the normalized Cohort A batch in the original epoch order.\n\nCohort B (3 epochs, 4×4): treat these as trial covariances across a sensor array where we want power-invariant features unless cross-sensor coupling dominates. For an epoch where the sum of absolute off-diagonal elements exceeds the sum of diagonal elements (strong coupling regime), use correlation normalization; otherwise use unit-trace normalization. Return the normalized Cohort B batch in the original epoch order.\n\nCohort A batch: [[[1.2, 0.3, -0.1], [0.3, 0.9, 0.25], [-0.1, 0.25, 1.5]], [[2.0, 0.5, 0.0], [0.5, 1.8, 0.4], [0.0, 0.4, 2.2]], [[0.8, -0.2, 0.15], [-0.2, 1.1, -0.3], [0.15, -0.3, 0.9]], [[1.5, 0.1, 0.2], [0.1, 1.3, -0.25], [0.2, -0.25, 1.7]], [[1.0, 0.2, 0.05], [0.2, 0.95, 0.3], [0.05, 0.3, 1.4]], [[1.3, -0.1, 0.25], [-0.1, 1.0, 0.15], [0.25, 0.15, 1.6]], [[0.9, 0.35, -0.05], [0.35, 1.2, 0.2], [-0.05, 0.2, 1.3]], [[1.7, -0.15, 0.1], [-0.15, 1.4, -0.2], [0.1, -0.2, 1.9]], [[1.1, 0.05, 0.3], [0.05, 1.05, -0.1], [0.3, -0.1, 1.2]], [[1.4, -0.25, 0.15], [-0.25, 1.6, 0.05], [0.15, 0.05, 1.8]]]\n\nCohort B batch: [[[2.0, 0.3, 0.1, 0.0], [0.3, 1.5, 0.2, 0.1], [0.1, 0.2, 1.2, 0.4], [0.0, 0.1, 0.4, 1.0]], [[1.8, 0.2, 0.0, 0.1], [0.2, 1.1, 0.3, 0.0], [0.0, 0.3, 0.9, 0.2], [0.1, 0.0, 0.2, 0.7]], [[2.5, 0.4, 0.2, 0.1], [0.4, 2.0, 0.5, 0.2], [0.2, 0.5, 1.6, 0.3], [0.1, 0.2, 0.3, 1.3]]]", "answers": "[{\"name\":\"pyriemann_utils_covariance_normalize\",\"arguments\":{\"X\":[[[1.2,0.3,-0.1],[0.3,0.9,0.25],[-0.1,0.25,1.5]],[[2.0,0.5,0.0],[0.5,1.8,0.4],[0.0,0.4,2.2]],[[0.8,-0.2,0.15],[-0.2,1.1,-0.3],[0.15,-0.3,0.9]],[[1.5,0.1,0.2],[0.1,1.3,-0.25],[0.2,-0.25,1.7]],[[1.0,0.2,0.05],[0.2,0.95,0.3],[0.05,0.3,1.4]],[[1.3,-0.1,0.25],[-0.1,1.0,0.15],[0.25,0.15,1.6]],[[0.9,0.35,-0.05],[0.35,1.2,0.2],[-0.05,0.2,1.3]],[[1.7,-0.15,0.1],[-0.15,1.4,-0.2],[0.1,-0.2,1.9]],[[1.1,0.05,0.3],[0.05,1.05,-0.1],[0.3,-0.1,1.2]],[[1.4,-0.25,0.15],[-0.25,1.6,0.05],[0.15,0.05,1.8]]],\"norm\":\"corr\"}},{\"name\":\"pyriemann_utils_covariance_normalize\",\"arguments\":{\"X\":[[[2.0,0.3,0.1,0.0],[0.3,1.5,0.2,0.1],[0.1,0.2,1.2,0.4],[0.0,0.1,0.4,1.0]],[[1.8,0.2,0.0,0.1],[0.2,1.1,0.3,0.0],[0.0,0.3,0.9,0.2],[0.1,0.0,0.2,0.7]],[[2.5,0.4,0.2,0.1],[0.4,2.0,0.5,0.2],[0.2,0.5,1.6,0.3],[0.1,0.2,0.3,1.3]]],\"norm\":\"trace\"}}]"}
{"func_name": "pyriemann_utils_distance_distance", "func_desc": "Distance between matrices according to a chosen metric, used for computing\npairwise dissimilarities between symmetric/Hermitian positive definite (SPD/HPD)\nmatrices such as covariance matrices estimated from multichannel biosignals\n(EEG/MEG/EMG) in brain-computer interface (BCI) workflows or covariance\ndescriptors in remote sensing and hyperspectral imaging. This function accepts\neither a single pair of matrices A and B (both shape (n, n)) or a stack of\nmatrices A with shape (n_matrices, n, n) and a single matrix B (n, n), and\nreturns the scalar distance or a column vector of distances computed by the\nspecified metric. The metric can be one of the predefined string identifiers\nimplemented in the pyriemann distance module (for example \"riemann\", \"euclid\",\n\"kullback\", \"wasserstein\", etc.) or a user-provided callable implementing the\nsame distance signature.", "tools": [{"function": {"description": "Distance between matrices according to a chosen metric, used for computing\npairwise dissimilarities between symmetric/Hermitian positive definite (SPD/HPD)\nmatrices such as covariance matrices estimated from multichannel biosignals\n(EEG/MEG/EMG) in brain-computer interface (BCI) workflows or covariance\ndescriptors in remote sensing and hyperspectral imaging. This function accepts\neither a single pair of matrices A and B (both shape (n, n)) or a stack of\nmatrices A with shape (n_matrices, n, n) and a single matrix B (n, n), and\nreturns the scalar distance or a column vector of distances computed by the\nspecified metric. The metric can be one of the predefined string identifiers\nimplemented in the pyriemann distance module (for example \"riemann\", \"euclid\",\n\"kullback\", \"wasserstein\", etc.) or a user-provided callable implementing the\nsame distance signature.", "name": "pyriemann_utils_distance_distance", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First matrix or set of matrices. Must be either a 2-D\narray of shape (n, n) representing a single matrix, or a 3-D array of\nshape (n_matrices, n, n) representing a stack of n_matrices matrices.\nIn typical pyRiemann use (BCI covariance-based pipelines), A contains\nsymmetric (or Hermitian) positive definite covariance matrices estimated\nfrom multichannel time series. When A is a stack, the function computes\nthe distance between each A[i] and B and returns an array of distances.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second matrix. Must be a 2-D array of shape (n, n)\ncompatible with A (same n). In standard applications B is typically a\ncovariance or reference SPD matrix such as a class mean or a template.", "default": ""}, "metric": {"type": "string", "description": "Metric for distance, default \"riemann\". Can be\none of the implemented string identifiers: \"chol\", \"euclid\", \"harmonic\",\n\"kullback\", \"kullback_right\", \"kullback_sym\", \"logchol\", \"logdet\",\n\"logeuclid\", \"riemann\", \"wasserstein\", or a callable. If a callable is\nprovided, it must accept arguments (A_sub, B, squared=bool) and return a\nscalar distance or a numpy array of shape (1,) consistent with the\nfunction's outputs. The default \"riemann\" corresponds to the Riemannian\ngeodesic distance on the manifold of SPD matrices commonly used in BCI\nclassification and covariance-based analysis.", "default": "riemann"}, "squared": {"type": "boolean", "description": "Return squared distance when True, default False. This\nbehavior was added in version 0.5. When True, the function returns the\nsquared value of the chosen metric; when False, it returns the usual\n(non-squared) distance. Use squared distances if downstream algorithms\nexpect squared dissimilarities (for example some kernel or variance\ncomputations).", "default": false}}, "required": ["A", "B", "squared", "metric"], "type": "any"}}, "type": "function"}], "query": "We’re running a QC-driven dissimilarity audit on covariance descriptors from two EEG acquisition cohorts where some trials are contaminated by numerical artifacts. For each cohort, treat the trial set as a stack A and compare against its cohort-specific class-mean reference covariance B.\n\nCohort A (4×4): From the 4 trial covariance estimates below, only retain trials that are symmetric (A≈Aᵀ within numerical rounding) and strictly positive definite (i.e., admissible as SPD for Riemannian geometry). For the retained trials, compute distances to the provided 4×4 reference using:\n1) the Riemannian metric with non-squared distances (for interpretability in our report), and\n2) the Wasserstein metric, but only for those retained trials whose diagonal dynamic range (max(diag)/min(diag)) exceeds 1.5 (these are the trials we suspect are affected by gain drift); return squared distances for that baseline.\n\nCohort B (3×3): From the 4 trial covariance estimates below, retain only trials that are SPD and have strictly positive diagonal entries. For retained trials, compute Riemannian distances to the provided 3×3 reference, returning squared distances (for downstream kernel input).\n\nData:\n\nCohort A trials (4×4):\nA1 = [[1.0, 0.2, 0.1, 0.0], [0.2, 1.5, 0.3, 0.1], [0.1, 0.3, 2.0, 0.4], [0.0, 0.1, 0.4, 1.2]]\nA2 = [[1.2, 0.1, 0.0, 0.2], [0.1, 1.3, 0.2, 0.0], [0.0, 0.2, 1.8, 0.3], [0.2, 0.0, 0.3, 1.1]]\nA3 = [[0.9, 0.0, 0.2, 0.1], [0.0, 1.4, 0.1, 0.2], [0.25, 0.1, 1.9, 0.0], [0.1, 0.2, 0.0, 1.3]]\nA4 = [[1.6, 0.0, 0.0, 0.0], [0.0, 0.02, 0.0, 0.0], [0.0, 0.0, 1.1, 0.0], [0.0, 0.0, 0.0, 0.9]]\nCohort A reference B4 = [[1.1, 0.15, 0.05, 0.1], [0.15, 1.45, 0.25, 0.05], [0.05, 0.25, 1.95, 0.35], [0.1, 0.05, 0.35, 1.25]]\n\nCohort B trials (3×3):\nB1 = [[1.2, 0.3, 0.1], [0.3, 0.9, 0.2], [0.1, 0.2, 1.1]]\nB2 = [[1.5, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 0.8]]\nB3 = [[0.9, 0.2, 0.1], [0.2, 1.3, 0.4], [0.1, 0.4, 1.0]]\nB4_bad = [[1.0, 0.0, 0.0], [0.0, -0.2, 0.0], [0.0, 0.0, 0.7]]\nCohort B reference R3 = [[1.1, 0.2, 0.0], [0.2, 1.0, 0.3], [0.0, 0.3, 0.9]]\n\nReturn the per-trial distance vectors for each requested metric/squared setting after applying the above retention/branching rules.", "answers": "[{\"name\":\"pyriemann_utils_distance_distance\",\"arguments\":{\"A\":[[[1.0,0.2,0.1,0.0],[0.2,1.5,0.3,0.1],[0.1,0.3,2.0,0.4],[0.0,0.1,0.4,1.2]],[[1.2,0.1,0.0,0.2],[0.1,1.3,0.2,0.0],[0.0,0.2,1.8,0.3],[0.2,0.0,0.3,1.1]],[[1.6,0.0,0.0,0.0],[0.0,0.02,0.0,0.0],[0.0,0.0,1.1,0.0],[0.0,0.0,0.0,0.9]]],\"B\":[[1.1,0.15,0.05,0.1],[0.15,1.45,0.25,0.05],[0.05,0.25,1.95,0.35],[0.1,0.05,0.35,1.25]],\"metric\":\"riemann\",\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance\",\"arguments\":{\"A\":[[[1.6,0.0,0.0,0.0],[0.0,0.02,0.0,0.0],[0.0,0.0,1.1,0.0],[0.0,0.0,0.0,0.9]]],\"B\":[[1.1,0.15,0.05,0.1],[0.15,1.45,0.25,0.05],[0.05,0.25,1.95,0.35],[0.1,0.05,0.35,1.25]],\"metric\":\"wasserstein\",\"squared\":true}},{\"name\":\"pyriemann_utils_distance_distance\",\"arguments\":{\"A\":[[[1.2,0.3,0.1],[0.3,0.9,0.2],[0.1,0.2,1.1]],[[1.5,0.1,0.0],[0.1,1.0,0.2],[0.0,0.2,0.8]],[[0.9,0.2,0.1],[0.2,1.3,0.4],[0.1,0.4,1.0]]],\"B\":[[1.1,0.2,0.0],[0.2,1.0,0.3],[0.0,0.3,0.9]],\"metric\":\"riemann\",\"squared\":true}}]"}
{"func_name": "pyriemann_utils_distance_distance_chol", "func_desc": "Cholesky distance between two symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nThis function computes the Cholesky distance used in applications of Riemannian geometry to covariance matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, MEG or EMG in brain–computer interfaces, or local covariance estimates in remote sensing). The Cholesky distance between two SPD/HPD matrices A and B is defined as the Frobenius norm of the difference between their Cholesky factors: the function computes chol(A) and chol(B) using NumPy's Cholesky factorization and then returns the Frobenius norm (or its square) of their difference. Internally this function delegates to the Euclidean distance on the Cholesky factors (distance_euclid on np.linalg.cholesky(A) and np.linalg.cholesky(B)).", "tools": [{"function": {"description": "Cholesky distance between two symmetric/Hermitian positive-definite (SPD/HPD) matrices.\n\nThis function computes the Cholesky distance used in applications of Riemannian geometry to covariance matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, MEG or EMG in brain–computer interfaces, or local covariance estimates in remote sensing). The Cholesky distance between two SPD/HPD matrices A and B is defined as the Frobenius norm of the difference between their Cholesky factors: the function computes chol(A) and chol(B) using NumPy's Cholesky factorization and then returns the Frobenius norm (or its square) of their difference. Internally this function delegates to the Euclidean distance on the Cholesky factors (distance_euclid on np.linalg.cholesky(A) and np.linalg.cholesky(B)).", "name": "pyriemann_utils_distance_distance_chol", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First SPD/HPD matrix or batch of matrices. Must be at least 2-D with shape (..., n, n) where the last two dimensions form square, symmetric (real) or Hermitian (complex) positive-definite matrices. In the typical BCI/EEG workflow, A represents one estimated covariance matrix (or an array of covariance matrices across epochs or trials).", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second SPD/HPD matrix or batch of matrices, with the same shape as A (identical leading/batch dimensions and identical n for the last two dimensions). B represents the covariance matrix (or matrices) to compare with A in practical signal processing or classification pipelines.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default), return the Frobenius norm distance d = ||chol(A) - chol(B)||_F. If True, return the squared Frobenius norm distance d^2. Use squared=True when squared distances are required directly (for example, in some loss formulations) to avoid an unnecessary square-root.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "We’re validating a preprocessing step in an EEG/MEG covariance pipeline where some covariance estimates are borderline SPD due to short windows and numerical drift. For each candidate covariance pair (A, B) below, run the Cholesky-factor distance only for pairs that are consistent with an SPD sensor-noise floor: both matrices must be symmetric with all diagonal entries strictly positive and with maximum absolute off-diagonal entry not exceeding the smaller of the two matrices’ smallest diagonal entry. For accepted pairs, return the **squared** Cholesky distance.\n\nCandidate pairs:\n1) Resting-state (alpha-band) window: A = [[1.0, 0.3, 0.1], [0.3, 0.8, 0.2], [0.1, 0.2, 0.9]], B = [[0.9, 0.25, 0.05], [0.25, 0.85, 0.15], [0.05, 0.15, 1.0]]\n2) Motor-imagery trial: A = [[1.0, 0.2, 0.1], [0.2, 1.5, 0.3], [0.1, 0.3, 2.0]], B = [[1.2, 0.1, 0.0], [0.1, 1.3, 0.4], [0.0, 0.4, 1.8]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_chol\",\"arguments\":{\"A\":[[1.0,0.3,0.1],[0.3,0.8,0.2],[0.1,0.2,0.9]],\"B\":[[0.9,0.25,0.05],[0.25,0.85,0.15],[0.05,0.15,1.0]],\"squared\":true}},{\"name\":\"pyriemann_utils_distance_distance_chol\",\"arguments\":{\"A\":[[1.0,0.2,0.1],[0.2,1.5,0.3],[0.1,0.3,2.0]],\"B\":[[1.2,0.1,0.0],[0.1,1.3,0.4],[0.0,0.4,1.8]],\"squared\":true}}]"}
{"func_name": "pyriemann_utils_distance_distance_harmonic", "func_desc": "Harmonic distance between invertible matrices.\n\nCompute the harmonic distance d(A, B) = ||A^{-1} - B^{-1}||_F between two invertible matrices A and B. In the pyRiemann library this distance quantifies dissimilarity between precision matrices (inverses of covariance matrices) that are commonly estimated from multichannel biosignals (for example EEG/MEG in brain–computer interface applications) or from spatial patches in remote sensing. The function inverts each input matrix using numpy.linalg.inv and delegates the Frobenius-norm computation to the Euclidean-distance implementation (distance_euclid), so it is effectively the Euclidean distance applied to the inverses of the inputs.", "tools": [{"function": {"description": "Harmonic distance between invertible matrices.\n\nCompute the harmonic distance d(A, B) = ||A^{-1} - B^{-1}||_F between two invertible matrices A and B. In the pyRiemann library this distance quantifies dissimilarity between precision matrices (inverses of covariance matrices) that are commonly estimated from multichannel biosignals (for example EEG/MEG in brain–computer interface applications) or from spatial patches in remote sensing. The function inverts each input matrix using numpy.linalg.inv and delegates the Frobenius-norm computation to the Euclidean-distance implementation (distance_euclid), so it is effectively the Euclidean distance applied to the inverses of the inputs.", "name": "pyriemann_utils_distance_distance_harmonic", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First invertible matrices. Must be at least a 2D square ndarray with shape (..., n, n). Each n-by-n matrix represents an invertible linear operator such as a covariance matrix estimated from multichannel time series; the function computes A^{-1} for every matrix in the leading batch dimensions. Passing non-square or singular matrices will cause numpy.linalg.inv to raise an error.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second invertible matrices. Must have the same shape as A, i.e., (..., n, n), and each matrix must be invertible. B represents the matrices to compare against A (for example a template covariance or another batch of covariance estimates). Mismatched shapes between A and B will cause an error when the pairwise difference of inverses is computed.", "default": ""}, "squared": {"type": "boolean", "description": "Whether to return the squared harmonic distance. Default False. If False, the function returns the Frobenius norm ||A^{-1} - B^{-1}||_F. If True, it returns the squared Frobenius norm (||A^{-1} - B^{-1}||_F)^2. Using the squared option avoids an extra square-root when algorithms require squared distances for efficiency or numerical reasons.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "In our EEG/BCI precision-matrix QC notebook we receive a mixed batch of 3×3 precision estimates from multiple preprocessing branches (template-matching, task contrasts, and a few numerically unstable fits). For each candidate pair (A,B), compute the harmonic distance d(A,B)=||A^{-1}−B^{-1}||_F (always report the non-squared Frobenius norm). Only evaluate pairs that are physically plausible as precision matrices for a 3-channel montage: both A and B must be symmetric (within exact entry equality as given), have strictly positive diagonal entries, and must not be near-singular under a simple determinant check (|det(M)| > 0.05 for each matrix M in the pair). Use the following raw batch (pairs are independent replicates/conditions):\n\n1) template_vs_segment:\nA=[[2.0,0.1,0.0],[0.1,1.5,0.2],[0.0,0.2,1.2]]\nB=[[1.8,0.05,0.1],[0.05,1.7,0.0],[0.1,0.0,1.1]]\n\n2) left_vs_right_task:\nA=[[2.5,0.3,-0.1],[0.3,1.8,0.2],[-0.1,0.2,1.2]]\nB=[[2.2,0.1,-0.05],[0.1,1.9,0.15],[-0.05,0.15,1.3]]\n\n3) unstable_fit_control:\nA=[[1.0,0.99,0.99],[0.99,1.0,0.99],[0.99,0.99,1.0]]\nB=[[1.1,0.98,0.97],[0.98,1.1,0.96],[0.97,0.96,1.05]]\n\n4) asymmetric_artifact:\nA=[[2.0,0.2,0.0],[0.1,1.6,0.2],[0.0,0.2,1.1]]\nB=[[2.1,0.2,0.0],[0.2,1.6,0.2],[0.0,0.2,1.1]]\n\nReturn harmonic distances only for the pairs that pass these plausibility gates.", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_harmonic\",\"arguments\":{\"A\":[[2.0,0.1,0.0],[0.1,1.5,0.2],[0.0,0.2,1.2]],\"B\":[[1.8,0.05,0.1],[0.05,1.7,0.0],[0.1,0.0,1.1]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_harmonic\",\"arguments\":{\"A\":[[2.5,0.3,-0.1],[0.3,1.8,0.2],[-0.1,0.2,1.2]],\"B\":[[2.2,0.1,-0.05],[0.1,1.9,0.15],[-0.05,0.15,1.3]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_harmonic\",\"arguments\":{\"A\":[[1.0,0.99,0.99],[0.99,1.0,0.99],[0.99,0.99,1.0]],\"B\":[[1.1,0.98,0.97],[0.98,1.1,0.96],[0.97,0.96,1.05]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_kullback_right", "func_desc": "Summary:\nCompute the right Kullback-Leibler divergence between two covariance matrices. This function is a thin wrapper that implements the \"right\" version of the Kullback-Leibler divergence used in pyRiemann for comparing symmetric (or Hermitian) positive definite (SPD/HPD) covariance matrices commonly estimated from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data. Concretely, it calls the underlying distance_kullback function with its arguments swapped so that the divergence is evaluated as D_right(A, B) = distance_kullback(B, A, squared=squared). The returned value quantifies how much the distribution characterized by A diverges from that characterized by B in the context of Riemannian geometry on SPD/HPD matrices and can be used as a dissimilarity measure in classification, clustering, or transfer-learning pipelines described in the project README.", "tools": [{"function": {"description": "Summary:\nCompute the right Kullback-Leibler divergence between two covariance matrices. This function is a thin wrapper that implements the \"right\" version of the Kullback-Leibler divergence used in pyRiemann for comparing symmetric (or Hermitian) positive definite (SPD/HPD) covariance matrices commonly estimated from multichannel biosignals (e.g., EEG, MEG, EMG) or remote sensing data. Concretely, it calls the underlying distance_kullback function with its arguments swapped so that the divergence is evaluated as D_right(A, B) = distance_kullback(B, A, squared=squared). The returned value quantifies how much the distribution characterized by A diverges from that characterized by B in the context of Riemannian geometry on SPD/HPD matrices and can be used as a dissimilarity measure in classification, clustering, or transfer-learning pipelines described in the project README.", "name": "pyriemann_utils_distance_distance_kullback_right", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "Left-hand matrix argument of the right Kullback-Leibler divergence. Expected to be a square numpy.ndarray representing a covariance matrix (symmetric positive definite for real-valued data or Hermitian positive definite for complex-valued data). In typical pyRiemann usage this is an estimated covariance matrix of shape (n_channels, n_channels). The function treats A as the first argument of the right divergence D_right(A, B) which is implemented by calling distance_kullback(B, A, ...).", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Right-hand matrix argument of the right Kullback-Leibler divergence. Expected to be a square numpy.ndarray with the same shape and type constraints as A (covariance / SPD / HPD matrix). In practical pipelines B often represents a reference or template covariance matrix (for example a class mean in Riemannian classification) against which A is compared.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default), return the standard Kullback-Leibler divergence value (a non-negative scalar). If True, return the squared form of the divergence if the underlying distance_kullback implementation supports a squared option. This flag is forwarded unchanged to the underlying distance_kullback call. Default behavior is squared=False.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "We’re running an EEG covariance QC + transfer check where some trial-wise 3×3 covariance estimates are known to be numerically fragile (near-singular) due to short epochs. Using the right Kullback–Leibler divergence against a fixed template/reference covariance B_template = [[1.00, 0.08, 0.03],[0.08, 1.05, 0.04],[0.03, 0.04, 0.95]], evaluate only those candidate covariances that are symmetric positive definite (SPD) and sufficiently well-conditioned for stable divergence computation: treat a candidate as admissible if it is symmetric and its determinant is strictly greater than 1e-3. For admissible candidates, return the *unsquared* right-KL when the determinant is at least 0.5 (stable, high-SNR epochs), otherwise return the *squared* right-KL (borderline conditioning). Use the following raw candidates (some may be rejected by the admissibility rule):\nC1 = [[1.20, 0.10, 0.05],[0.10, 0.90, 0.02],[0.05, 0.02, 1.10]]\nC2 = [[1.20, 0.30, 0.10],[0.30, 0.90, 0.20],[0.10, 0.20, 1.10]]\nC3 = [[1.00, 0.40, 0.00],[0.10, 1.00, 0.20],[0.00, 0.20, 0.90]]\nC4 = [[1.00, 0.20, 0.00],[0.20, 1.30, 0.10],[0.00, 0.10, 0.80]]\nCompute the right-KL divergences for all admissible candidates versus B_template under these rules.", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_kullback_right\",\"arguments\":{\"A\":[[1.2,0.1,0.05],[0.1,0.9,0.02],[0.05,0.02,1.1]],\"B\":[[1.0,0.08,0.03],[0.08,1.05,0.04],[0.03,0.04,0.95]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_kullback_right\",\"arguments\":{\"A\":[[1.2,0.3,0.1],[0.3,0.9,0.2],[0.1,0.2,1.1]],\"B\":[[1.0,0.08,0.03],[0.08,1.05,0.04],[0.03,0.04,0.95]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_kullback_right\",\"arguments\":{\"A\":[[1.0,0.2,0.0],[0.2,1.3,0.1],[0.0,0.1,0.8]],\"B\":[[1.0,0.08,0.03],[0.08,1.05,0.04],[0.03,0.04,0.95]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_logchol", "func_desc": "Log-Cholesky distance between two symmetric/Hermitian positive definite (SPD/HPD)\nmatrices.\n\nThis function computes the Log-Cholesky distance used in pyRiemann to compare SPD/HPD\nmatrices such as covariance matrices estimated from multichannel biosignals (EEG,\nMEG, EMG) or from remote sensing data. The distance is computed by taking the\nCholesky decomposition of each input matrix, extracting the strictly lower\ntriangular entries and the diagonal entries, applying a natural logarithm to the\ndiagonal entries, and then computing the Euclidean norm of the concatenated\ndifferences. Concretely, for matrices A and B, if L_A and L_B are their (lower)\nCholesky factors, the squared distance equals the squared Frobenius norm of the\ndifference of the strictly lower triangular parts plus the squared Frobenius\nnorm of the difference of the elementwise logarithms of the diagonals. Returning\nthe non-squared distance takes the square root of this sum. This representation\nis useful in Riemannian processing pipelines (e.g., covariance estimation and\nclassification in BCI) because it provides a vectorized representation of SPD/HPD\nmatrices that can be used with standard Euclidean methods.", "tools": [{"function": {"description": "Log-Cholesky distance between two symmetric/Hermitian positive definite (SPD/HPD)\nmatrices.\n\nThis function computes the Log-Cholesky distance used in pyRiemann to compare SPD/HPD\nmatrices such as covariance matrices estimated from multichannel biosignals (EEG,\nMEG, EMG) or from remote sensing data. The distance is computed by taking the\nCholesky decomposition of each input matrix, extracting the strictly lower\ntriangular entries and the diagonal entries, applying a natural logarithm to the\ndiagonal entries, and then computing the Euclidean norm of the concatenated\ndifferences. Concretely, for matrices A and B, if L_A and L_B are their (lower)\nCholesky factors, the squared distance equals the squared Frobenius norm of the\ndifference of the strictly lower triangular parts plus the squared Frobenius\nnorm of the difference of the elementwise logarithms of the diagonals. Returning\nthe non-squared distance takes the square root of this sum. This representation\nis useful in Riemannian processing pipelines (e.g., covariance estimation and\nclassification in BCI) because it provides a vectorized representation of SPD/HPD\nmatrices that can be used with standard Euclidean methods.", "name": "pyriemann_utils_distance_distance_logchol", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First input SPD/HPD matrix or batch of matrices with\nshape (..., n, n). Must be at least 2D. In pyRiemann typical usage is to\npass covariance matrices estimated from multichannel time series (for\nexample, shape n_epochs x n_channels x n_channels). The function treats\nthe last two dimensions as the square matrix dimensions and computes the\nCholesky decomposition along those dimensions.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second input SPD/HPD matrix or batch of matrices with\nthe same shape as A: (..., n, n). The function computes the pairwise\nLog-Cholesky distance between corresponding matrices in A and B along\nthe leading dimensions. A and B must be compatible in shape; broadcasting\nis not performed by this function.", "default": ""}, "squared": {"type": "boolean", "description": "Default False. If True, return the squared Log-Cholesky\ndistance (the sum of squared differences of the strictly lower triangular\nparts and the log-diagonals) to avoid the square-root operation when a\nsquared metric is desired for numerical or performance reasons. If False,\nreturn the non-squared distance (the square root of that sum).", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a Riemannian BCI covariance QA stage where the upstream estimator sometimes emits near-degenerate (non-SPD) covariances due to blink/muscle artifacts and too-short epochs. For each candidate 3×3 covariance-matrix pair (A,B) below, attempt a Cholesky-based Log-Cholesky distance (pyRiemann definition, non-squared). Only evaluate pairs where both A and B are strictly SPD (i.e., Cholesky factorization succeeds without regularization). Candidate pairs:\n\nPair 1: A = [[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 1.1]]; B = [[1.1, 0.25, 0.05], [0.25, 0.95, 0.28], [0.05, 0.28, 1.05]]\nPair 2: A = [[2.0, 0.3, 0.1], [0.3, 1.5, 0.2], [0.1, 0.2, 1.2]]; B = [[1.8, 0.25, 0.05], [0.25, 1.7, 0.15], [0.05, 0.15, 1.1]]\nPair 3 (suspected artifact): A = [[1.0, 0.9, 0.9], [0.9, 0.81, 0.81], [0.9, 0.81, 0.81]]; B = [[1.0, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 1.0]]\n\nReturn the computed Log-Cholesky distances for the pairs that pass the SPD gate.", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_logchol\",\"arguments\":{\"A\":[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,1.1]],\"B\":[[1.1,0.25,0.05],[0.25,0.95,0.28],[0.05,0.28,1.05]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_logchol\",\"arguments\":{\"A\":[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]],\"B\":[[1.8,0.25,0.05],[0.25,1.7,0.15],[0.05,0.15,1.1]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_riemann", "func_desc": "Affine-invariant Riemannian distance between two symmetric/Hermitian positive definite (SPD/HPD) matrices.\n\nThis function computes the affine-invariant Riemannian metric used throughout pyRiemann for comparing covariance or similar SPD/HPD matrices arising in multivariate signal processing (for example EEG/MEG/EMG covariance matrices in brain–computer interface workflows, and covariance descriptors in remote sensing). Concretely, for two input matrices A and B the distance is computed as sqrt(sum_i (log(lambda_i))^2) where lambda_i are the joint (generalized) eigenvalues of the matrix pair (A, B). The value quantifies the geometric difference between A and B on the manifold of SPD/HPD matrices and is commonly used as a metric in classification (e.g., MDM), clustering, or embedding procedures in pyRiemann pipelines.", "tools": [{"function": {"description": "Affine-invariant Riemannian distance between two symmetric/Hermitian positive definite (SPD/HPD) matrices.\n\nThis function computes the affine-invariant Riemannian metric used throughout pyRiemann for comparing covariance or similar SPD/HPD matrices arising in multivariate signal processing (for example EEG/MEG/EMG covariance matrices in brain–computer interface workflows, and covariance descriptors in remote sensing). Concretely, for two input matrices A and B the distance is computed as sqrt(sum_i (log(lambda_i))^2) where lambda_i are the joint (generalized) eigenvalues of the matrix pair (A, B). The value quantifies the geometric difference between A and B on the manifold of SPD/HPD matrices and is commonly used as a metric in classification (e.g., MDM), clustering, or embedding procedures in pyRiemann pipelines.", "name": "pyriemann_utils_distance_distance_riemann", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First SPD/HPD matrices. Must be at least 2-D with shape (..., n, n). In typical use this is one or a batch of covariance matrices estimated from multichannel time series (e.g., EEG epochs). The function expects each trailing (n, n) block to be symmetric (or Hermitian) and positive definite. If these conditions are not met a validation error is raised by the internal input checker.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second SPD/HPD matrices, same dimensions as A (shape (..., n, n)). B plays the role of the comparison target: the function measures the affine-invariant Riemannian distance from A to B elementwise across any leading batch dimensions. A and B must be aligned in shape; a mismatch in dimensions triggers a ValueError.", "default": ""}, "squared": {"type": "boolean", "description": "Return squared distance when True (i.e., sum_i (log(lambda_i))^2) instead of the square root. Default is False (returns the standard Riemannian distance). Version added: 0.5. Using the squared distance can avoid an extra square-root operation when only squared distances are needed (for example when computing pairwise squared distances or optimizing least-squares objectives).", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "We’re doing a Riemannian-geometry QC screen before training an MDM decoder on EEG covariance features. Below is a mixed batch of candidate 3×3 covariance pairs coming from different acquisition conditions (rest/task within-subject, cross-run, and a few questionable segments). Treat each pair as usable only if both matrices are symmetric and strictly positive definite. For every usable pair, compute the affine-invariant Riemannian distance with an adaptive output rule: if the two matrices have the same trace (within numerical precision, i.e., exactly equal given the numbers below), return the squared distance (for a least-squares objective); otherwise return the standard (non-squared) distance (for reporting drift). Dataset:\n\nP1 (within-subject rest vs task):\nA=[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]]\nB=[[1.8,0.25,0.05],[0.25,1.7,0.15],[0.05,0.15,1.1]]\n\nP2 (cross-run 1s epochs):\nA=[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.1]]\nB=[[1.0,0.05,0.0],[0.05,1.1,0.1],[0.0,0.1,0.95]]\n\nP3 (suspected preprocessing bug; slight asymmetry):\nA=[[1.1,0.2,0.0],[0.1,1.0,0.1],[0.0,0.1,0.9]]\nB=[[1.05,0.0,0.0],[0.0,0.98,0.0],[0.0,0.0,1.02]]\n\nP4 (likely rank-deficient segment):\nA=[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]\nB=[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_riemann\",\"arguments\":{\"A\":[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]],\"B\":[[1.8,0.25,0.05],[0.25,1.7,0.15],[0.05,0.15,1.1]],\"squared\":false}},{\"name\":\"pyriemann_utils_distance_distance_riemann\",\"arguments\":{\"A\":[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.1]],\"B\":[[1.0,0.05,0.0],[0.05,1.1,0.1],[0.0,0.1,0.95]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_distance_distance_wasserstein", "func_desc": "pyriemann.utils.distance.distance_wasserstein computes the Wasserstein (Bures) distance between two symmetric/Hermitian positive semidefinite (SPSD/HPSD) matrices. In the pyRiemann context this function is used to measure dissimilarity between covariance or covariance-like matrices (for example, covariance matrices estimated from EEG/MEG/EMG epochs in brain–computer interface pipelines or covariance estimates in remote sensing applications) and can be employed in classification, clustering, or pipeline evaluation where Riemannian-aware distances are required.", "tools": [{"function": {"description": "pyriemann.utils.distance.distance_wasserstein computes the Wasserstein (Bures) distance between two symmetric/Hermitian positive semidefinite (SPSD/HPSD) matrices. In the pyRiemann context this function is used to measure dissimilarity between covariance or covariance-like matrices (for example, covariance matrices estimated from EEG/MEG/EMG epochs in brain–computer interface pipelines or covariance estimates in remote sensing applications) and can be employed in classification, clustering, or pipeline evaluation where Riemannian-aware distances are required.\n", "name": "pyriemann_utils_distance_distance_wasserstein", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "First SPSD/HPSD matrices, at least 2-D ndarray with shape (..., n, n). Each trailing pair of dimensions represents one n-by-n SPSD (real symmetric) or HPSD (complex Hermitian) matrix. In practical pyRiemann use this typically contains covariance matrices computed per epoch or spatial window; leading dimensions allow broadcasting and batch computation of distances across multiple matrices.", "default": ""}, "B": {"type": "array", "items": {"type": "float"}, "description": "Second SPSD/HPSD matrices, same dtype and shape constraints as A and with identical trailing matrix dimensions (..., n, n). B plays the role of the second element in each pairwise comparison; when computing distances between corresponding matrices in A and B they must align on the leading shapes or be broadcastable according to numpy broadcasting rules that are accepted by the internal input checker.", "default": ""}, "squared": {"type": "boolean", "description": "If False (default) the function returns the Wasserstein distance d(A,B) = sqrt(tr(A + B - 2 (B^{1/2} A B^{1/2})^{1/2})). If True the function returns the squared distance d^2 = tr(A + B - 2 (B^{1/2} A B^{1/2})^{1/2}) without taking the square root. The squared option was added in pyRiemann version 0.5 and can be useful to avoid an extra square-root operation when squared distances are needed for optimization or variance calculations.", "default": false}}, "required": ["A", "B", "squared"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an EEG covariance QC stage where epoch-wise 3×3 covariance(-like) estimates can be slightly nonstationary, so we want a distance readout that adapts to the pair’s overall power level. For each epoch pair (Ai, Bi) below, compute the Wasserstein/Bures distance with a rule-driven objective: if the combined trace ratio max(trace(Ai), trace(Bi)) / min(trace(Ai), trace(Bi)) exceeds 1.15 (suggesting a noticeable power mismatch), output the squared Wasserstein/Bures distance (for a penalty term). Otherwise output the non-squared Wasserstein/Bures distance (for reporting). Use the same rule for both cohorts without hard-coding exceptions.\n\nEpoch pairs:\n1) A1=[[1.0,0.3,0.1],[0.3,0.8,0.2],[0.1,0.2,0.5]], B1=[[0.9,0.25,0.05],[0.25,0.7,0.15],[0.05,0.15,0.4]]\n2) A2=[[1.20,0.10,0.00],[0.10,0.80,0.05],[0.00,0.05,0.50]], B2=[[1.00,0.05,0.02],[0.05,0.90,0.00],[0.02,0.00,0.60]]", "answers": "[{\"name\":\"pyriemann_utils_distance_distance_wasserstein\",\"arguments\":{\"A\":[[1.0,0.3,0.1],[0.3,0.8,0.2],[0.1,0.2,0.5]],\"B\":[[0.9,0.25,0.05],[0.25,0.7,0.15],[0.05,0.15,0.4]],\"squared\":true}},{\"name\":\"pyriemann_utils_distance_distance_wasserstein\",\"arguments\":{\"A\":[[1.2,0.1,0.0],[0.1,0.8,0.05],[0.0,0.05,0.5]],\"B\":[[1.0,0.05,0.02],[0.05,0.9,0.0],[0.02,0.0,0.6]],\"squared\":false}}]"}
{"func_name": "pyriemann_utils_mean_mean_kullback_sym", "func_desc": "Mean of SPD/HPD matrices according to the symmetrized Kullback-Leibler divergence.\n\nThis function computes the symmetrized Kullback-Leibler mean (also called the Kullback symmetrized mean) of a set of symmetric positive-definite (SPD) or Hermitian positive-definite (HPD) matrices. The symmetrized Kullback-Leibler mean is implemented as the geometric midpoint (Riemannian geodesic at t=0.5) between the Euclidean mean and the harmonic mean: it first calls mean_euclid(X, sample_weight) and mean_harmonic(X, sample_weight) and then computes their Riemannian geodesic midpoint via geodesic_riemann(..., 0.5). In pyRiemann this is used to aggregate covariance or scatter matrices (for example EEG/MEG covariance matrices in brain–computer interface pipelines or windowed covariance matrices in remote sensing) into a single representative SPD/HPD matrix that is meaningful under information-geometric criteria. The returned matrix is suitable as a central estimator on the SPD/HPD manifold and can be used downstream in Riemannian algorithms such as MDM classification or tangent-space projection.", "tools": [{"function": {"description": "Mean of SPD/HPD matrices according to the symmetrized Kullback-Leibler divergence.\n\nThis function computes the symmetrized Kullback-Leibler mean (also called the Kullback symmetrized mean) of a set of symmetric positive-definite (SPD) or Hermitian positive-definite (HPD) matrices. The symmetrized Kullback-Leibler mean is implemented as the geometric midpoint (Riemannian geodesic at t=0.5) between the Euclidean mean and the harmonic mean: it first calls mean_euclid(X, sample_weight) and mean_harmonic(X, sample_weight) and then computes their Riemannian geodesic midpoint via geodesic_riemann(..., 0.5). In pyRiemann this is used to aggregate covariance or scatter matrices (for example EEG/MEG covariance matrices in brain–computer interface pipelines or windowed covariance matrices in remote sensing) into a single representative SPD/HPD matrix that is meaningful under information-geometric criteria. The returned matrix is suitable as a central estimator on the SPD/HPD manifold and can be used downstream in Riemannian algorithms such as MDM classification or tangent-space projection.", "name": "pyriemann_utils_mean_mean_kullback_sym", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Set of input SPD/HPD matrices to average. Each entry X[i] is an n-by-n symmetric (or Hermitian) positive-definite matrix representing, for instance, a covariance matrix estimated from one epoch/trial (in BCI applications) or from a local spatial window (in remote sensing). The first dimension n_matrices is the number of matrices to aggregate. All matrices must be square and of identical dimension; mismatch in shapes will lead to an error in the underlying mean computations. The matrices are expected to be positive-definite; providing matrices that are not SPD/HPD (for example singular matrices or matrices with non-positive eigenvalues) may cause numerical errors or exceptions in the underlying linear algebra operations.", "default": ""}, "sample_weight": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional nonnegative weights for each input matrix. If None, equal weights are used so that each matrix contributes uniformly to both the Euclidean and harmonic means before forming the geodesic midpoint. When provided, sample_weight must have the same length as the first dimension of X; mismatched lengths will raise an error in the underlying mean functions. Negative weights or NaNs are not supported and can produce invalid results or exceptions. The weights affect the influence of each input matrix on both intermediate means (euclidean and harmonic) and therefore on the final symmetrized Kullback-Leibler mean.", "default": null}}, "required": ["X", "sample_weight"], "type": "any"}}, "type": "function"}], "query": "We’re aggregating EEG covariance estimates with the symmetrized Kullback–Leibler mean, but the batch contains mixed-quality windows/trials and the weighting must be derived from intrinsic matrix properties.\n\nProcess two cohorts in one run:\n\n(A) Motor-cortex windowed segments (3×3). Raw window covariances:\nX = [\n  [[1.2, 0.3, 0.1],[0.3, 0.9, 0.2],[0.1, 0.2, 1.1]],\n  [[1.5, 0.4, 0.2],[0.4, 1.0, 0.3],[0.2, 0.3, 1.3]],\n  [[1.1, 0.2, 0.0],[0.2, 0.8, 0.1],[0.0, 0.1, 1.0]]\n]\nAssign sample weights by a stability rule: compute the determinant of each 3×3 covariance; any window whose determinant is strictly below the median determinant of this cohort is treated as low-stability and given half the weight of the other windows. Then compute the symmetrized KL mean using these derived weights.\n\n(B) Trial-level covariances (2×2). Raw trial covariances:\nX = [\n  [[1.2, 0.3],[0.3, 0.9]],\n  [[0.8, 0.1],[0.1, 1.1]],\n  [[1.5, 0.4],[0.4, 1.3]]\n]\nAssign sample weights by an anisotropy rule: compute the eigenvalues of each 2×2 covariance and take the condition number (max/min). Trials with condition number strictly above the cohort median are treated as anisotropy-dominated and given double weight relative to the others. Then compute the symmetrized KL mean using these derived weights.\n\nReturn one representative covariance per cohort.", "answers": "[{\"name\":\"pyriemann_utils_mean_mean_kullback_sym\",\"arguments\":{\"X\":[[[1.2,0.3,0.1],[0.3,0.9,0.2],[0.1,0.2,1.1]],[[1.5,0.4,0.2],[0.4,1.0,0.3],[0.2,0.3,1.3]],[[1.1,0.2,0.0],[0.2,0.8,0.1],[0.0,0.1,1.0]]],\"sample_weight\":[1.0,1.0,0.5]}},{\"name\":\"pyriemann_utils_mean_mean_kullback_sym\",\"arguments\":{\"X\":[[[1.2,0.3],[0.3,0.9]],[[0.8,0.1],[0.1,1.1]],[[1.5,0.4],[0.4,1.3]]],\"sample_weight\":[1.0,1.0,2.0]}}]"}
{"func_name": "pyriemann_utils_tangentspace_exp_map_logchol", "func_desc": "Project matrices back to SPD/HPD manifold using the log-Cholesky exponential map.\n\nThis function implements the log-Cholesky exponential map described in Table 2 of Lin (2019) to move a batch of matrices X from the tangent space at a reference SPD/HPD matrix Cref back onto the SPD/HPD manifold. In the context of pyRiemann, this operation is used when working with covariance matrices (real symmetric positive definite, SPD) or Hermitian positive definite (HPD) matrices arising from multivariate biosignal (EEG/MEG/EMG) processing and BCI pipelines: for example, after projecting covariance matrices to a tangent space for classification or transfer learning, exp_map_logchol reconstructs manifold-valued covariance matrices from their tangent representations. The implementation performs a Cholesky-based reconstruction: it computes the Cholesky factor of Cref, forms a normalized tangent increment, enforces the upper-triangular/log-Cholesky structure, exponentiates diagonal log-parameters, builds a modified Cholesky factor, and returns the reconstructed SPD/HPD matrices as L @ L^H.", "tools": [{"function": {"description": "Project matrices back to SPD/HPD manifold using the log-Cholesky exponential map.\n\nThis function implements the log-Cholesky exponential map described in Table 2 of Lin (2019) to move a batch of matrices X from the tangent space at a reference SPD/HPD matrix Cref back onto the SPD/HPD manifold. In the context of pyRiemann, this operation is used when working with covariance matrices (real symmetric positive definite, SPD) or Hermitian positive definite (HPD) matrices arising from multivariate biosignal (EEG/MEG/EMG) processing and BCI pipelines: for example, after projecting covariance matrices to a tangent space for classification or transfer learning, exp_map_logchol reconstructs manifold-valued covariance matrices from their tangent representations. The implementation performs a Cholesky-based reconstruction: it computes the Cholesky factor of Cref, forms a normalized tangent increment, enforces the upper-triangular/log-Cholesky structure, exponentiates diagonal log-parameters, builds a modified Cholesky factor, and returns the reconstructed SPD/HPD matrices as L @ L^H.", "name": "pyriemann_utils_tangentspace_exp_map_logchol", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Matrices expressed in the tangent space at Cref. X may contain a batch of matrices in its leading dimensions (denoted by ...). Each trailing (n, n) block represents the tangent-space coordinates that will be mapped back to the manifold using the log-Cholesky exponential map. In practical pyRiemann usage, X typically contains tangent vectors obtained from a TangentSpace transform applied to covariance matrices for machine-learning pipelines (e.g., feature extraction before a classifier).", "default": ""}, "Cref": {"type": "array", "items": {"type": "float"}, "description": "Reference SPD (real) or HPD (complex) matrix that defines the base point of the tangent space. Cref is the matrix at which the exponential map is centered (for example, a Riemannian mean covariance estimated from training data). Cref must be positive definite so that a Cholesky decomposition exists; its Cholesky factor is used to normalize tangent increments and to reconstruct manifold-valued matrices.", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "We’re running a transfer-learning QC pass on trial-level tangent-space updates before feeding reconstructed covariances into a Riemannian alignment step. Each cohort has a shared Cref (3×3). However, the upstream tangent-estimation stage sometimes emits matrices that are not valid tangent representatives for the log-Cholesky exp map. For each cohort, take the provided list of candidate 3×3 increments and reconstruct only those increments that are symmetric to numerical tolerance (treat entries as symmetric if |X_ij−X_ji| ≤ 1e-8 for all i,j) and whose diagonal drift is within a “small-update” regime (max(|diag(X)|) ≤ 0.08) to avoid exploding the diagonal exponentials. Apply the log-Cholesky exponential map to the remaining increments using that cohort’s Cref. \n\nCohort A (Cref_A = [[2.0, 0.3, 0.1],[0.3, 1.5, 0.2],[0.1, 0.2, 1.2]]), candidate increments:\nA1 = [[0.02,-0.01,0.00],[-0.01,0.03,0.01],[0.00,0.01,-0.02]]\nA2 = [[-0.01,0.02,-0.01],[0.02,0.00,0.02],[-0.01,0.02,0.01]]\nA3 = [[0.00,0.03,0.00],[0.00,0.01,0.00],[0.00,0.00,0.00]]\n\nCohort B (Cref_B = [[1.2, 0.1, 0.0],[0.1, 0.9, 0.05],[0.0, 0.05, 1.1]]), candidate increments:\nB1 = [[0.1,0.02,-0.01],[0.02,0.05,0.03],[-0.01,0.03,0.08]]\nB2 = [[-0.04,0.01,0.0],[0.01,0.06,-0.02],[0.0,-0.02,0.03]]\nB3 = [[0.00,0.02,0.00],[0.01,0.00,0.00],[0.00,0.00,0.00]]\n\nReturn the reconstructed SPD matrices for whichever candidate increments pass the above QC gates, preserving their within-cohort order.", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_exp_map_logchol\",\"arguments\":{\"X\":[[[0.02,-0.01,0.0],[-0.01,0.03,0.01],[0.0,0.01,-0.02]],[[-0.01,0.02,-0.01],[0.02,0.0,0.02],[-0.01,0.02,0.01]]],\"Cref\":[[2.0,0.3,0.1],[0.3,1.5,0.2],[0.1,0.2,1.2]]}},{\"name\":\"pyriemann_utils_tangentspace_exp_map_logchol\",\"arguments\":{\"X\":[[[-0.04,0.01,0.0],[0.01,0.06,-0.02],[0.0,-0.02,0.03]]],\"Cref\":[[1.2,0.1,0.0],[0.1,0.9,0.05],[0.0,0.05,1.1]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_exp_map_wasserstein", "func_desc": "Project matrices back to SPD/HPD manifold using the Wasserstein exponential map.\n\nThis function implements the Wasserstein Riemannian exponential map that takes a perturbation in the tangent space at a reference symmetric (or Hermitian) positive definite (SPD/HPD) matrix and returns the corresponding matrix on the SPD/HPD manifold. In pyRiemann pipelines for biosignal processing and brain–computer interfaces (BCI), this is used to map tangent-space representations (for example, results of a TangentSpace transform or a gradient update) back to covariance matrices (SPD) or Hermitian positive definite matrices (HPD) so they can be interpreted or further processed as covariance estimates for multichannel time series. The implementation follows Eq.(36) in the referenced Wasserstein geometry paper and uses the eigen-decomposition of the reference matrix Cref to compute the map.", "tools": [{"function": {"description": "Project matrices back to SPD/HPD manifold using the Wasserstein exponential map.\n\nThis function implements the Wasserstein Riemannian exponential map that takes a perturbation in the tangent space at a reference symmetric (or Hermitian) positive definite (SPD/HPD) matrix and returns the corresponding matrix on the SPD/HPD manifold. In pyRiemann pipelines for biosignal processing and brain–computer interfaces (BCI), this is used to map tangent-space representations (for example, results of a TangentSpace transform or a gradient update) back to covariance matrices (SPD) or Hermitian positive definite matrices (HPD) so they can be interpreted or further processed as covariance estimates for multichannel time series. The implementation follows Eq.(36) in the referenced Wasserstein geometry paper and uses the eigen-decomposition of the reference matrix Cref to compute the map.", "name": "pyriemann_utils_tangentspace_exp_map_wasserstein", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Matrices in tangent space with shape (..., n, n). Each leading index corresponds to a tangent-space perturbation associated with the same reference matrix Cref. In practice X contains symmetric (or Hermitian) perturbation matrices computed from covariance estimators or operations performed in the tangent space. The last two dimensions must form square matrices of size n and must be consistent with Cref.", "default": ""}, "Cref": {"type": "array", "items": {"type": "float"}, "description": "Reference SPD/HPD matrix with shape (n, n). This matrix defines the base point on the manifold where the exponential map is applied. Cref must be symmetric (real) positive definite or Hermitian positive definite (complex) with strictly positive eigenvalues; it is typically a covariance matrix estimated from multichannel biosignal data (EEG/MEG/EMG) or spatial covariance in remote sensing applications.", "default": ""}}, "required": ["X", "Cref"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the Wasserstein exponential back-projection step in an EEG-BCI covariance update pipeline where incoming tangent-space gradients can be variably well-formed. You’re given a mixed batch of candidate tangent perturbations (some are clean symmetric updates; others exhibit symmetry-breaking numerical residue). For each candidate update, compute its symmetry score s = max(|X−Xᵀ|) (elementwise max absolute). Only back-project updates that satisfy s ≤ 0.001; treat them as valid tangent vectors at their provided 3×3 SPD reference covariance Cref and apply the Wasserstein Riemannian exponential map (Eq.(36) implementation via eigendecomposition of Cref). For any valid candidate that comes as a cohort (stacked replicates), apply the same validity rule independently per replicate and back-project only the replicates that pass. Raw candidates:\n\n1) Single-step candidate at Cref = [[1.2, 0.3, 0.1], [0.3, 0.9, 0.2], [0.1, 0.2, 1.5]] with X = [[0.05, 0.01, -0.02], [0.0105, -0.03, 0.004], [-0.02, 0.004, 0.02]].\n\n2) Two-replicate cohort at Cref = [[1.2, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 0.7]] with X = [\n  [[0.02, -0.01, 0.0], [-0.01, 0.03, 0.005], [0.0, 0.005, -0.015]],\n  [[-0.005, 0.002, 0.001], [0.001, -0.01, 0.0], [0.001, 0.0, 0.008]]\n].\n\nReturn the mapped SPD/HPD matrices for every update/replicate that passes the symmetry criterion (in the same nested structure as the inputs you actually back-project).", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_exp_map_wasserstein\",\"arguments\":{\"X\":[[0.02,-0.01,0.0],[-0.01,0.03,0.005],[0.0,0.005,-0.015]],\"Cref\":[[1.2,0.1,0.0],[0.1,0.9,0.05],[0.0,0.05,0.7]]}}]"}
{"func_name": "pyriemann_utils_tangentspace_upper", "func_desc": "pyriemann.utils.tangentspace.upper returns the weighted, vectorized upper-triangular part of square symmetric/Hermitian matrices to produce the minimal tangent-space representation used throughout pyRiemann for processing covariance/HPD matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG/MEG/EMG in BCI pipelines).\n\nThis function extracts the upper triangular entries of each input matrix (including the diagonal), applies a weight of 1.0 to diagonal elements and sqrt(2) to off-diagonal elements, and flattens those weighted entries into a 1-D vector per matrix. The weighting (unity on the diagonal, sqrt(2) on off-diagonals) yields the minimal representation commonly used in tangent-space/vectorization routines so that inner products and norms are preserved when mapping symmetric/Hermitian matrices to Euclidean vectors. The function supports broadcasting over any leading dimensions, so it accepts batches of matrices with shape (..., n, n) and returns a batch of vectors with shape (..., n * (n + 1) / 2). The routine does not validate that the numerical values of X are symmetric/Hermitian; it only requires square matrices and will operate on the provided upper-triangular values. It performs no in-place modification of the input and returns a new ndarray.", "tools": [{"function": {"description": "pyriemann.utils.tangentspace.upper returns the weighted, vectorized upper-triangular part of square symmetric/Hermitian matrices to produce the minimal tangent-space representation used throughout pyRiemann for processing covariance/HPD matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG/MEG/EMG in BCI pipelines).\n\nThis function extracts the upper triangular entries of each input matrix (including the diagonal), applies a weight of 1.0 to diagonal elements and sqrt(2) to off-diagonal elements, and flattens those weighted entries into a 1-D vector per matrix. The weighting (unity on the diagonal, sqrt(2) on off-diagonals) yields the minimal representation commonly used in tangent-space/vectorization routines so that inner products and norms are preserved when mapping symmetric/Hermitian matrices to Euclidean vectors. The function supports broadcasting over any leading dimensions, so it accepts batches of matrices with shape (..., n, n) and returns a batch of vectors with shape (..., n * (n + 1) / 2). The routine does not validate that the numerical values of X are symmetric/Hermitian; it only requires square matrices and will operate on the provided upper-triangular values. It performs no in-place modification of the input and returns a new ndarray.", "name": "pyriemann_utils_tangentspace_upper", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Symmetric/Hermitian matrices with shape (..., n, n). In the pyRiemann workflow these are typically covariance (SPD) or Hermitian positive-definite (HPD) matrices estimated from multichannel time-series (e.g., EEG epochs). The trailing two dimensions must be square (n x n). Leading dimensions, if any, are treated as batch dimensions and are preserved in the output. The function assumes the matrix entries are arranged so that taking the upper-triangular part is meaningful; it does not check numerical symmetry beyond the shape requirement.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our tangent-space feature extraction stage with messy EEG covariance outputs coming from three acquisition modes. Each mode yields a mini-batch of two 3×3 covariance estimates, but only matrices that look physically plausible should be vectorized (pyRiemann-style upper-triangular weighting). Apply this QC gate per matrix: accept a covariance estimate only if (i) all diagonal entries are strictly positive (sensor variances can’t be ≤ 0), and (ii) every off-diagonal magnitude is ≤ the smaller of the two corresponding diagonal entries (a conservative cross-covariance sanity bound). For each acquisition mode, run `pyriemann.utils.tangentspace.upper` on the accepted matrices only (keep their original order within that mode) and return the tangent-space vectors.\n\nAcquisition mode A (online calibration): X = [ [[1.0, 0.2, -0.1],[0.2, 0.9, 0.05],[-0.1, 0.05, 1.1]], [[0.8, -0.3, 0.0],[-0.3, 1.2, 0.4],[0.0, 0.4, 0.7]] ]\n\nAcquisition mode B (two-subject motor imagery cohort): X = [ [[1.0, 0.2, -0.1],[0.2, 0.9, 0.05],[-0.1, 0.05, 1.1]], [[0.8, -0.15, 0.0],[-0.15, 1.2, 0.3],[0.0, 0.3, 0.95]] ]\n\nAcquisition mode C (two-epoch sensor covariances): X = [ [[1.0, 0.2, -0.1],[0.2, 0.9, 0.3],[-0.1, 0.3, 1.1]], [[0.8, -0.05, 0.15],[-0.05, 1.2, -0.2],[0.15, -0.2, 0.95]] ]", "answers": "[{\"name\":\"pyriemann_utils_tangentspace_upper\",\"arguments\":{\"X\":[[[1.0,0.2,-0.1],[0.2,0.9,0.05],[-0.1,0.05,1.1]],[[0.8,-0.3,0.0],[-0.3,1.2,0.4],[0.0,0.4,0.7]]]}},{\"name\":\"pyriemann_utils_tangentspace_upper\",\"arguments\":{\"X\":[[[1.0,0.2,-0.1],[0.2,0.9,0.05],[-0.1,0.05,1.1]],[[0.8,-0.15,0.0],[-0.15,1.2,0.3],[0.0,0.3,0.95]]]}},{\"name\":\"pyriemann_utils_tangentspace_upper\",\"arguments\":{\"X\":[[[1.0,0.2,-0.1],[0.2,0.9,0.3],[-0.1,0.3,1.1]],[[0.8,-0.05,0.15],[-0.05,1.2,-0.2],[0.15,-0.2,0.95]]]}}]"}
{"func_name": "pyriemann_utils_test_is_hankel", "func_desc": "pyriemann.utils.test.is_hankel checks whether a given square numpy.ndarray is a Hankel matrix.\n\nA Hankel matrix is a square matrix that is constant along its anti-diagonals (elements with equal i+j indices are equal). In the context of pyRiemann, which processes multichannel time-series and covariance-like matrices for biosignals (EEG/MEG/EMG) and remote sensing, detecting a Hankel structure can be used to validate time-delay embeddings, structured covariance estimates, or to gate algorithms that assume anti-diagonal constancy. This function performs an element-wise exact comparison to verify that property without modifying the input.", "tools": [{"function": {"description": "pyriemann.utils.test.is_hankel checks whether a given square numpy.ndarray is a Hankel matrix.\n\nA Hankel matrix is a square matrix that is constant along its anti-diagonals (elements with equal i+j indices are equal). In the context of pyRiemann, which processes multichannel time-series and covariance-like matrices for biosignals (EEG/MEG/EMG) and remote sensing, detecting a Hankel structure can be used to validate time-delay embeddings, structured covariance estimates, or to gate algorithms that assume anti-diagonal constancy. This function performs an element-wise exact comparison to verify that property without modifying the input.", "name": "pyriemann_utils_test_is_hankel", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "2-D square array with shape (n, n). This argument is the matrix to test for the Hankel property. The matrix typically represents a covariance-like or time-delay embedded matrix in pyRiemann workflows; the function expects a two-dimensional, square numpy.ndarray and will return False for inputs that are not square or not two-dimensional.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re running a QC gate on candidate 4×4 structured matrices coming from an EEG time-delay embedding/covariance pipeline where some replicates are expected to be Hankel-structured and others are known to be Toeplitz-like (symmetric along the main diagonal) depending on the estimator used.\n\nGiven the following batch of matrices (each is an independent replicate/condition), do the following:\n1) First, screen the batch for matrices that are compatible with the Hankel-structure assumption by a simple pre-check: only matrices whose (0,0) entry equals the (last,last) entry should be forwarded to the strict Hankel verifier (this screens out matrices that cannot satisfy anti-diagonal constancy across the full i+j range).\n2) For every matrix that passes that pre-check, verify **exactly** (no tolerance/rounding) whether it is Hankel (constant along each anti-diagonal) using the same criterion as pyRiemann.\n\nBatch (keep this order for screening):\nA = [[0.5, 0.6, 0.7, 0.8],\n     [0.6, 0.7, 0.8, 0.9],\n     [0.7, 0.8, 0.9, 1.0],\n     [0.8, 0.9, 1.0, 1.1]]\n\nB = [[1.0, 2.0, 3.0, 4.0],\n     [2.0, 3.0, 4.0, 5.0],\n     [3.0, 4.0, 5.0, 6.0],\n     [4.0, 5.0, 6.0, 7.0]]\n\nC = [[0.5, 0.6, 0.7, 0.8],\n     [0.6, 0.7, 0.8, 0.9],\n     [0.7, 0.8, 0.9, 1.0],\n     [0.8, 0.9, 1.0, 0.5]]\n\nD = [[0.2, 0.4, 0.6, 0.8],\n     [0.1, 0.2, 0.4, 0.6],\n     [0.0, 0.1, 0.2, 0.4],\n     [-0.1, 0.0, 0.1, 0.2]]", "answers": "[{\"name\":\"pyriemann_utils_test_is_hankel\",\"arguments\":{\"X\":[[0.5,0.6,0.7,0.8],[0.6,0.7,0.8,0.9],[0.7,0.8,0.9,1.0],[0.8,0.9,1.0,0.5]]}},{\"name\":\"pyriemann_utils_test_is_hankel\",\"arguments\":{\"X\":[[0.2,0.4,0.6,0.8],[0.1,0.2,0.4,0.6],[0.0,0.1,0.2,0.4],[-0.1,0.0,0.1,0.2]]}}]"}
{"func_name": "pyriemann_utils_test_is_herm_pos_semi_def", "func_desc": "Check whether every matrix in a collection is Hermitian positive semi-definite (HPSD).", "tools": [{"function": {"description": "Check whether every matrix in a collection is Hermitian positive semi-definite (HPSD).\n", "name": "pyriemann_utils_test_is_herm_pos_semi_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of square matrices to test. This must be at least a 2-D numpy.ndarray with shape (..., n, n) where the last two dimensions index each n-by-n matrix. In the pyRiemann context, X typically contains covariance or cross-spectral matrices estimated from multichannel biosignals (EEG/MEG/EMG) or from spatial windows in remote sensing; these matrices may be real-valued symmetric (for SPD/real-valued covariance) or complex-valued Hermitian (for HPD/complex-valued covariance). The function verifies properties across the whole collection provided in X.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re running a pre-Riemannian QC stage on mixed EEG covariance and cross-spectral estimates coming from multiple cohorts and sliding windows. Treat each matrix as a Hermitian candidate and apply a data-sieve before HPSD testing: only include matrices that (a) are square and finite-valued, (b) have a real-valued diagonal with all diagonal entries strictly positive (to reject obvious sensor-dropout/line-noise artifacts), and (c) are numerically Hermitian within tolerance (use the matrix as-is if it already satisfies this condition). From the following raw batches, build the retained collections per cohort/window and validate Hermitian positive semi-definiteness for each retained collection:\n\n- Cohort A (three 2×2 covariances across time windows):\n  [[[1.0, 0.5], [0.5, 2.0]], [[2.0, -0.3], [-0.3, 1.5]], [[1.2, 0.0], [0.0, 0.8]]]\n\n- Cohort B (two 3×3 sensor-space covariances):\n  [[[1.0, 0.2, 0.1], [0.2, 1.5, 0.3], [0.1, 0.3, 2.0]], [[1.0, 0.5, -0.2], [0.5, 1.2, 0.0], [-0.2, 0.0, 0.8]]]\n\n- Cohort C (three 2×2 cross-spectral estimates from another window):\n  [[[2.0, 1.0], [1.0, 3.0]], [[1.0, 0.0], [0.0, -0.2]], [[1.0, 0.5], [0.5, 1.0]]]", "answers": "[{\"name\":\"pyriemann_utils_test_is_herm_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.5],[0.5,2.0]],[[2.0,-0.3],[-0.3,1.5]],[[1.2,0.0],[0.0,0.8]]]}},{\"name\":\"pyriemann_utils_test_is_herm_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,0.1],[0.2,1.5,0.3],[0.1,0.3,2.0]],[[1.0,0.5,-0.2],[0.5,1.2,0.0],[-0.2,0.0,0.8]]]}},{\"name\":\"pyriemann_utils_test_is_herm_pos_semi_def\",\"arguments\":{\"X\":[[[2.0,1.0],[1.0,3.0]],[[1.0,0.5],[0.5,1.0]]]}}]"}
{"func_name": "pyriemann_utils_test_is_hermitian", "func_desc": "Check whether every square matrix in X is Hermitian.\n\nIn the pyRiemann library this function is used to validate complex-valued square matrices (for example complex covariance estimates or kernel matrices encountered in biosignal processing, BCI, or remote sensing workflows) before treating them as Hermitian positive-definite (HPD) objects for Riemannian geometry operations. A matrix is considered Hermitian here when its real part is symmetric and its imaginary part is skew-symmetric. The implementation performs this check by testing symmetry of X.real and skew-symmetry of X.imag using the library helpers is_sym and is_skew_sym.", "tools": [{"function": {"description": "Check whether every square matrix in X is Hermitian.\n\nIn the pyRiemann library this function is used to validate complex-valued square matrices (for example complex covariance estimates or kernel matrices encountered in biosignal processing, BCI, or remote sensing workflows) before treating them as Hermitian positive-definite (HPD) objects for Riemannian geometry operations. A matrix is considered Hermitian here when its real part is symmetric and its imaginary part is skew-symmetric. The implementation performs this check by testing symmetry of X.real and skew-symmetry of X.imag using the library helpers is_sym and is_skew_sym.", "name": "pyriemann_utils_test_is_hermitian", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of square matrices to test, with shape (..., n, n). Must be at least 2D: the trailing two dimensions correspond to n x n matrices. Typically this array contains real or complex-valued matrices arising from multichannel time-series analysis (e.g., covariance matrices estimated from EEG/MEG/EMG epochs) where Hermitian structure is required for downstream HPD/HPD-based algorithms.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed batch of complex 2×2 cross-spectral covariance estimates coming from two acquisition modes (EEG and radar). Before Riemannian HPD processing, apply a Hermitian QC gate **only** to matrices whose diagonal is purely real (i.e., the imaginary part on the diagonal is numerically zero), since nonzero imaginary self-power indicates a phase-referencing failure.\n\nDataset (each matrix provided as (real, imag) blocks; interpret as real + 1j*imag):\n1) EEG/eyes-open: real=[[1.0, 0.2],[0.2, 2.0]], imag=[[0.0, 0.5],[-0.5, 0.0]]\n2) EEG/eyes-closed: real=[[0.9, 0.1],[0.3, 1.5]], imag=[[0.0, 0.4],[-0.6, 0.0]]\n3) Radar/track-01: real=[[2.0, 0.0],[0.0, 3.0]], imag=[[0.0, 0.0],[0.0, 0.0]]\n4) Radar/track-02: real=[[1.0, 2.0],[-2.0, 1.0]], imag=[[0.0, 0.0],[0.0, 0.2]]\n5) EEG/artifact-suspect: real=[[0.0, 1.0],[1.0, 0.0]], imag=[[0.0, 0.0],[0.0, 0.0]]\n\nFor the subset that passes the ‘purely-real diagonal’ prefilter, check whether **every** matrix in that subset is Hermitian under the library criterion: real part symmetric AND imaginary part skew-symmetric.", "answers": "[{\"name\":\"pyriemann_utils_test_is_hermitian\",\"arguments\":{\"X\":[[[[1.0,0.0],[0.2,0.5]],[[0.2,-0.5],[2.0,0.0]]],[[[0.9,0.0],[0.1,0.4]],[[0.3,-0.6],[1.5,0.0]]],[[[2.0,0.0],[0.0,0.0]],[[0.0,0.0],[3.0,0.0]]],[[[0.0,0.0],[1.0,0.0]],[[1.0,0.0],[0.0,0.0]]]]}}]"}
{"func_name": "pyriemann_utils_test_is_pos_def", "func_desc": "Check whether all matrices contained in X are positive definite (PD).\n\nThis utility is used within pyRiemann to validate symmetric/Hermitian positive definite (SPD/HPD) matrices such as covariance matrices estimated from multichannel biosignals (EEG/MEG/EMG) or spatial covariance blocks in remote sensing. A return value of True indicates that every matrix in the input satisfies the positive definiteness criterion required by downstream Riemannian-geometry-based algorithms (e.g., covariance-based classification, MDM, TangentSpace). The function accepts a single square matrix of shape (n, n) or a batch of matrices with shape (..., n, n).", "tools": [{"function": {"description": "Check whether all matrices contained in X are positive definite (PD).\n\nThis utility is used within pyRiemann to validate symmetric/Hermitian positive definite (SPD/HPD) matrices such as covariance matrices estimated from multichannel biosignals (EEG/MEG/EMG) or spatial covariance blocks in remote sensing. A return value of True indicates that every matrix in the input satisfies the positive definiteness criterion required by downstream Riemannian-geometry-based algorithms (e.g., covariance-based classification, MDM, TangentSpace). The function accepts a single square matrix of shape (n, n) or a batch of matrices with shape (..., n, n).", "name": "pyriemann_utils_test_is_pos_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of square matrices to test. Expected shape is (..., n, n) (at least a 2-D ndarray). Elements may be real (SPDs) or complex (HPDs) as used across pyRiemann workflows. The last two dimensions must form square matrices; otherwise the checks will fail and the function will return False (fast_mode) or perform an explicit square check (non-fast mode).", "default": ""}, "tol": {"type": "float", "description": "Threshold below which eigenvalues are considered non-positive. Default is 0.0. In the full (non-fast) mode, each matrix is declared positive definite only if all of its eigenvalues are strictly greater than tol (i.e., eigenvalue > tol). This parameter controls numerical tolerance for near-singular matrices when using eigenvalue-based verification. Note: tol is ignored when fast_mode is True.", "default": 0.0}, "fast_mode": {"type": "boolean", "description": "Use a Cholesky decomposition-based check when True (default False). In fast mode the function attempts np.linalg.cholesky(X) on the input arrays: if the decomposition succeeds for all matrices, they are considered positive definite and the function returns True; if np.linalg.cholesky raises numpy.linalg.LinAlgError (for non-PD or non-square last two dims) the function catches it and returns False. Fast mode avoids the more expensive eigen decomposition but may misclassify matrices that are theoretically PD but numerically unstable under Cholesky due to floating-point errors.", "default": false}}, "required": ["X", "fast_mode", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re running an EEG covariance QC gate before any Riemannian step (TangentSpace/MDM). The raw export below mixes true trial-level 3×3 covariance estimates with occasional scale/collapse artifacts. Apply an SPD/HPD validation policy that mimics our pipeline rules:\n\n- Treat each 3×3 block as symmetric.\n- Only submit a cohort to the strict PD validator if its matrices look like plausible covariances: every matrix must have strictly positive diagonal entries (interpreted as channel variances), and the matrix trace must fall within the cohort’s typical range, defined as within ±20% of the cohort’s median trace.\n- For every cohort that passes the plausibility sieve, confirm whether *all* matrices are strictly positive definite using eigenvalue threshold tol = 1e-6, with fast_mode disabled.\n\nCohort A (TangentSpace pre-check; 2 matrices):\nX = [\n  [[2.0, 0.2, 0.1], [0.2, 1.5, 0.0], [0.1, 0.0, 1.2]],\n  [[1.0, 0.3, 0.2], [0.3, 0.9, 0.1], [0.2, 0.1, 0.8]]\n]\n\nCohort B (preprocessing QC; 2 matrices):\nX = [\n  [[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 1.1]],\n  [[0.5, 0.4, 0.0], [0.4, 0.5, 0.0], [0.0, 0.0, 0.0]]\n]\n\nCohort C (sensor-cluster blocks for MDM; 3 matrices: frontal, temporal, occipital):\nX = [\n  [[1.2, 0.3, 0.1], [0.3, 0.9, 0.2], [0.1, 0.2, 0.8]],\n  [[0.7, -0.1, 0.0], [-0.1, 0.6, 0.05], [0.0, 0.05, 0.5]],\n  [[1.0, 0.4, 0.2], [0.4, 1.1, 0.3], [0.2, 0.3, 0.9]]\n]\n\nReturn a boolean pass/fail per cohort: cohorts failing the plausibility sieve are marked fail without running PD validation; otherwise, report the PD validation result (all matrices PD under tol=1e-6, fast_mode=False).", "answers": "[{\"name\":\"pyriemann_utils_test_is_pos_def\",\"arguments\":{\"X\":[[[2.0,0.2,0.1],[0.2,1.5,0.0],[0.1,0.0,1.2]],[[1.0,0.3,0.2],[0.3,0.9,0.1],[0.2,0.1,0.8]]],\"tol\":1e-06,\"fast_mode\":false}},{\"name\":\"pyriemann_utils_test_is_pos_def\",\"arguments\":{\"X\":[[[1.2,0.3,0.1],[0.3,0.9,0.2],[0.1,0.2,0.8]],[[0.7,-0.1,0.0],[-0.1,0.6,0.05],[0.0,0.05,0.5]],[[1.0,0.4,0.2],[0.4,1.1,0.3],[0.2,0.3,0.9]]],\"tol\":1e-06,\"fast_mode\":false}}]"}
{"func_name": "pyriemann_utils_test_is_pos_semi_def", "func_desc": "pyriemann.utils.test.is_pos_semi_def checks whether every matrix contained in the input array X is positive semi-definite (PSD). This is a lightweight validator used in pyRiemann to verify covariance matrices or other square matrix collections before algorithms that assume non-negative eigenvalues (for example, Riemannian geometry routines that require SPD/PSD inputs).", "tools": [{"function": {"description": "pyriemann.utils.test.is_pos_semi_def checks whether every matrix contained in the input array X is positive semi-definite (PSD). This is a lightweight validator used in pyRiemann to verify covariance matrices or other square matrix collections before algorithms that assume non-negative eigenvalues (for example, Riemannian geometry routines that require SPD/PSD inputs).\n", "name": "pyriemann_utils_test_is_pos_semi_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The collection of square matrices to test. X must be a NumPy ndarray with shape (..., n, n), i.e., the last two axes represent n-by-n matrices and any leading axes index multiple matrices. In the pyRiemann context, X commonly holds estimated covariance matrices from multichannel biosignals (e.g., EEG epochs n_epochs x n_channels x n_channels) or spatial covariance blocks for remote sensing. The function first checks squareness via is_square(X); if X is not at least 2-D with square trailing dimensions, the function will return False. Elements are expected to be numeric (typically real-valued floats) because the check relies on eigenvalue computation. Note that floating-point round-off can produce small negative eigenvalues for matrices that are theoretically PSD; such numerical artifacts will cause this function to return False because the implementation tests eigenvalues with the comparison >= 0.0. The implementation delegates eigenvalue extraction to _get_eigenvals and does not modify X.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "In our EEG covariance QA step (prior to Riemannian feature extraction), we receive three cohorts of 3×3 epoch-level covariance estimates, but some epochs are contaminated by referencing/rounding artifacts that break covariance symmetry. For each cohort below (shape (3, 3, 3)), construct a per-epoch QC subset by retaining only those 3×3 matrices that are symmetric within a strict elementwise tolerance of 1e-12 (i.e., |C_ij − C_ji| ≤ 1e-12 for all i,j). Run `pyriemann.utils.test.is_pos_semi_def` on the retained subset for each cohort to confirm that all *retained* epochs are PSD before downstream processing.\n\nCohort A (3 epochs):\n[[[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 0.8]],\n [[0.5, -0.1, 0.0], [-0.1, 0.6, 0.2], [0.0, 0.2, 0.7]],\n [[2.0, 1.0, 0.5], [1.0, 1.5, 0.3], [0.5, 0.3, 1.2]]]\n\nCohort B (3 epochs):\n[[[1.0, 0.2, 0.1], [0.2, 0.9, 0.3], [0.1, 0.3, 0.8]],\n [[0.5, -0.1, 0.0], [-0.1, 0.4, 0.2], [0.0, 0.2, 0.6]],\n [[2.0, 0.5, -0.3], [0.5, 1.5, 0.4], [-0.300000000001, 0.4, 1.2]]]\n\nCohort C (3 trials):\n[[[1.0, 0.2, 0.1], [0.2, 0.9, 0.0], [0.1, 0.0, 0.8]],\n [[0.5, -0.1, 0.0], [-0.1, 0.4, 0.05], [0.0, 0.05, 0.3]],\n [[1.0, 0.6, -0.2], [0.600000000002, 0.36, -0.12], [-0.2, -0.12, 0.1]]]\n\nReport the PSD pass/fail per cohort under this symmetry-gated policy (i.e., whether every retained matrix in the cohort is PSD).", "answers": "[{\"name\":\"pyriemann_utils_test_is_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,0.8]],[[0.5,-0.1,0.0],[-0.1,0.6,0.2],[0.0,0.2,0.7]],[[2.0,1.0,0.5],[1.0,1.5,0.3],[0.5,0.3,1.2]]]}},{\"name\":\"pyriemann_utils_test_is_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,0.8]],[[0.5,-0.1,0.0],[-0.1,0.4,0.2],[0.0,0.2,0.6]]]}},{\"name\":\"pyriemann_utils_test_is_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,0.1],[0.2,0.9,0.0],[0.1,0.0,0.8]],[[0.5,-0.1,0.0],[-0.1,0.4,0.05],[0.0,0.05,0.3]]]}}]"}
{"func_name": "pyriemann_utils_test_is_real", "func_desc": "Check whether every matrix in a collection has no significant imaginary component.", "tools": [{"function": {"description": "Check whether every matrix in a collection has no significant imaginary component.\n", "name": "pyriemann_utils_test_is_real", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Array of matrices to test, with shape (..., n, m). In the pyRiemann context this is typically a collection of multichannel covariance or cross-spectral matrices estimated from biosignals (e.g., EEG, MEG, EMG) or image patches (remote sensing). The function expects a NumPy ndarray as provided by upstream estimation routines; the dtype may be real or complex. The check is performed on the imaginary part of X (X.imag), so passing a non-ndarray or an object without an imaginary attribute may raise an exception.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re doing pre-whitening QC on an EEG connectivity export that mixes truly real-valued cross-spectral estimates with complex-valued covariance estimates. Treat the input as a raw dump that may contain heterogeneous numeric types (real floats vs complex) and run a two-stage sieve before downstream covariance-based processing:\n\n1) Identify matrices that are numerically compatible with being real-valued under their acquisition regime:\n- For 2×2 cross-spectral matrices, they should be strictly real as provided.\n- For 4×4 covariance matrices, accept them as effectively real only when the largest absolute imaginary entry in that matrix is within the numerical-noise floor typical of double-precision EEG pipelines (~1e-10 order).\n\n2) For each regime, run the “effectively real-valued” check on the matrices that pass the regime-specific expectation above.\n\nUse exactly these cohorts as the raw dump:\n(A) [[[1.0, 0.2], [0.2, 0.9]], [[0.8, 0.1], [0.1, 1.1]], [[1.2, 0.05], [0.05, 0.7]]]\n(B) [[ [1.0+0.0j, 0.2+1e-10j, 0.1-2e-10j, 0.0+0.0j], [0.2-1e-10j, 1.1+0.0j, 0.05+0.0j, -0.1+3e-10j], [0.1+2e-10j, 0.05+0.0j, 0.9+0.0j, 0.04-1e-10j], [0.0+0.0j, -0.1-3e-10j, 0.04+1e-10j, 1.2+0.0j] ],\n     [ [0.9+0.0j, -0.3+5e-11j, 0.0+0.0j, 0.2-5e-11j], [-0.3-5e-11j, 1.0+0.0j, 0.15+0.0j, 0.0+0.0j], [0.0+0.0j, 0.15+0.0j, 1.05+0.0j, -0.05+0.0j], [0.2+5e-11j, 0.0+0.0j, -0.05+0.0j, 0.95+0.0j] ],\n     [ [1.1+0.0j, 0.0+0.0j, -0.2+0.0j, 0.05+0.0j], [0.0+0.0j, 0.8+0.0j, 0.1+0.0j, 0.0+0.0j], [-0.2+0.0j, 0.1+0.0j, 0.9+0.0j, 0.0+0.0j], [0.05+0.0j, 0.0+0.0j, 0.0+0.0j, 1.0+0.0j] ] ]", "answers": "[{\"name\":\"pyriemann_utils_test_is_real\",\"arguments\":{\"X\":[[[1.0,0.2],[0.2,0.9]],[[0.8,0.1],[0.1,1.1]],[[1.2,0.05],[0.05,0.7]]]}},{\"name\":\"pyriemann_utils_test_is_real\",\"arguments\":{\"X\":[[[1.0,0.2,0.1,0.0],[0.2,1.1,0.05,-0.1],[0.1,0.05,0.9,0.04],[0.0,-0.1,0.04,1.2]],[[0.9,-0.3,0.0,0.2],[-0.3,1.0,0.15,0.0],[0.0,0.15,1.05,-0.05],[0.2,0.0,-0.05,0.95]],[[1.1,0.0,-0.2,0.05],[0.0,0.8,0.1,0.0],[-0.2,0.1,0.9,0.0],[0.05,0.0,0.0,1.0]]]}}]"}
{"func_name": "pyriemann_utils_test_is_real_type", "func_desc": "Check if a collection of matrices is of a real numeric type.\n\nThis function is used in pyRiemann preprocessing and validation steps where algorithms\nassume real-valued matrices (for example, covariance estimation and Riemannian\noperations on symmetric positive definite (SPD) matrices). It inspects the numeric\ndtype of the input matrix array to determine whether the data are stored as real\nvalued numbers (no imaginary component). This is a lightweight, dtype-level test:\nit does not check matrix properties such as symmetry, positive-definiteness, or\nshape validity beyond the array's dtype.", "tools": [{"function": {"description": "Check if a collection of matrices is of a real numeric type.\n\nThis function is used in pyRiemann preprocessing and validation steps where algorithms\nassume real-valued matrices (for example, covariance estimation and Riemannian\noperations on symmetric positive definite (SPD) matrices). It inspects the numeric\ndtype of the input matrix array to determine whether the data are stored as real\nvalued numbers (no imaginary component). This is a lightweight, dtype-level test:\nit does not check matrix properties such as symmetry, positive-definiteness, or\nshape validity beyond the array's dtype.", "name": "pyriemann_utils_test_is_real_type", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of matrices to test, expected to be a NumPy ndarray\nwith shape (..., n, m). The leading dimensions (represented by ...)\nallow batching (e.g., multiple covariance matrices stacked as\nn_matrices x n_channels x n_channels). The function expects numeric\nentries and uses the array's dtype to decide whether values are real.\nBehavior is defined for ndarray inputs; passing non-ndarray objects is\nnot guaranteed by this docstring and may produce results consistent\nwith NumPy's isrealobj but is otherwise outside the stated contract.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "In our EEG Riemannian pipeline we’re aggregating covariance estimates from multiple acquisition modes (time-domain sensor covariance vs frequency-domain cross-spectral covariance). We need a dtype-only preflight gate before any pyRiemann operations: run `pyriemann_utils_test_is_real_type` on each cohort whose entries are consistent with a real-valued covariance stream (i.e., every scalar has no imaginary component). Treat cohorts as-is (no symmetry/PD/shape checks); cohorts that contain any complex-valued scalar should be routed to the complex branch and therefore won’t be screened by this real-type gate.\n\nRaw cohorts:\n- Cohort A (2× 3×3):\n  [[[1.2, 0.1, 0.0],\n    [0.1, 0.9, 0.2],\n    [0.0, 0.2, 1.1]],\n   [[0.8, 0.05, 0.03],\n    [0.05, 1.3, 0.12],\n    [0.03, 0.12, 0.7]]]\n\n- Cohort B (3× 4×4):\n  [[[1.0, 0.2, 0.1, 0.0],\n    [0.2, 1.5, 0.3, 0.1],\n    [0.1, 0.3, 0.9, 0.2],\n    [0.0, 0.1, 0.2, 1.2]],\n   [[0.8, 0.1, 0.0, 0.2],\n    [0.1, 1.1, 0.4, 0.0],\n    [0.0, 0.4, 0.7, 0.3],\n    [0.2, 0.0, 0.3, 1.0]],\n   [[1.3, 0.0, 0.2, 0.1],\n    [0.0, 0.9, 0.1, 0.0],\n    [0.2, 0.1, 1.4, 0.3],\n    [0.1, 0.0, 0.3, 0.8]]]\n\n- Cohort C (2× 3×3, cross-spectral; contains imaginary leakage):\n  [[[1.0, 0.1+0.0j, 0.0],\n    [0.1-0.0j, 0.95, 0.05],\n    [0.0, 0.05, 1.05]],\n   [[0.9, 0.02, 0.01+0.2j],\n    [0.02, 1.1, 0.0],\n    [0.01-0.2j, 0.0, 0.85]]]\n\nApply the real-type gate only to cohorts that qualify under the rule above.", "answers": "[{\"name\":\"pyriemann_utils_test_is_real_type\",\"arguments\":{\"X\":[[[1.2,0.1,0.0],[0.1,0.9,0.2],[0.0,0.2,1.1]],[[0.8,0.05,0.03],[0.05,1.3,0.12],[0.03,0.12,0.7]]]}},{\"name\":\"pyriemann_utils_test_is_real_type\",\"arguments\":{\"X\":[[[1.0,0.2,0.1,0.0],[0.2,1.5,0.3,0.1],[0.1,0.3,0.9,0.2],[0.0,0.1,0.2,1.2]],[[0.8,0.1,0.0,0.2],[0.1,1.1,0.4,0.0],[0.0,0.4,0.7,0.3],[0.2,0.0,0.3,1.0]],[[1.3,0.0,0.2,0.1],[0.0,0.9,0.1,0.0],[0.2,0.1,1.4,0.3],[0.1,0.0,0.3,0.8]]]}}]"}
{"func_name": "pyriemann_utils_test_is_skew_sym", "func_desc": "Check if all matrices in X are skew-symmetric (X^T = -X) across the last two axes.\n\nThis utility is part of pyriemann.utils.test and is intended for use in unit tests, validation checks, and preprocessing/diagnostics inside the pyRiemann library. In the pyRiemann context (multivariate biosignal processing and Riemannian geometry of matrices), it helps verify that a candidate array of square matrices represents antisymmetric operators (for example, elements of the Lie algebra of SO(n) or antisymmetric residuals produced during intermediate computations). The check is performed element-wise with a numerical tolerance (see behavior below) and does not modify the input array.", "tools": [{"function": {"description": "Check if all matrices in X are skew-symmetric (X^T = -X) across the last two axes.\n\nThis utility is part of pyriemann.utils.test and is intended for use in unit tests, validation checks, and preprocessing/diagnostics inside the pyRiemann library. In the pyRiemann context (multivariate biosignal processing and Riemannian geometry of matrices), it helps verify that a candidate array of square matrices represents antisymmetric operators (for example, elements of the Lie algebra of SO(n) or antisymmetric residuals produced during intermediate computations). The check is performed element-wise with a numerical tolerance (see behavior below) and does not modify the input array.", "name": "pyriemann_utils_test_is_skew_sym", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The input array containing one or more square matrices to test. Expected shape is (..., n, n), i.e. at least a 2-D array where the last two dimensions form n-by-n matrices. Each n-by-n matrix is tested independently for skew-symmetry. The array must be a NumPy ndarray; if another array-like object is supplied, NumPy operations inside the function may raise an exception.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "During QC for a multivariate biosignal Riemannian pipeline, we receive a mixed bundle of candidate infinitesimal rotation operators that may include scaling contamination. For each cohort (each provided as a stacked array of 3×3 matrices), first compute a per-matrix isotropy score defined as the Frobenius norm ratio r = ||(X + X^T)||_F / ||X||_F (treat ||X||_F=0 as invalid). Only matrices with r ≤ 1e-2 should be treated as plausible so(3) elements and passed to the skew-symmetry validator (X^T = −X across the last two axes, within numerical tolerance). Report whether each cohort’s *plausible subset* passes the skew-symmetry check. Cohort (1) preprocessing has A1=[[0.0,1.2,-0.5],[-1.2,0.0,2.3],[0.5,-2.3,0.0]], A2=[[0.0,-0.9,0.4],[0.9,0.0,-1.1],[-0.4,1.1,0.0]], A3=[[0.0,0.7,0.0],[-0.7,0.0,-0.2],[0.01,0.2,0.0]]. Cohort (2) validation has B1=[[0.0,2.0,-1.5],[-2.0,0.0,0.3],[1.5,-0.3,0.0]], B2=[[0.0,1.0,0.0],[-1.0,0.0,4.2],[0.0,-4.1,0.0]]. Cohort (3) experiment has C1=[[0.0,-1.2,0.5],[1.2,0.0,-0.3],[-0.5,0.3,0.0]], C2=[[0.0,-2.0,1.0],[1.9,0.0,-0.8],[-1.0,0.8,0.0]].", "answers": "[{\"name\":\"pyriemann_utils_test_is_skew_sym\",\"arguments\":{\"X\":[[[0.0,1.2,-0.5],[-1.2,0.0,2.3],[0.5,-2.3,0.0]],[[0.0,-0.9,0.4],[0.9,0.0,-1.1],[-0.4,1.1,0.0]]]}},{\"name\":\"pyriemann_utils_test_is_skew_sym\",\"arguments\":{\"X\":[[[0.0,2.0,-1.5],[-2.0,0.0,0.3],[1.5,-0.3,0.0]]]}},{\"name\":\"pyriemann_utils_test_is_skew_sym\",\"arguments\":{\"X\":[[[0.0,-1.2,0.5],[1.2,0.0,-0.3],[-0.5,0.3,0.0]]]}}]"}
{"func_name": "pyriemann_utils_test_is_square", "func_desc": "pyriemann.utils.test.is_square checks whether the last two dimensions of a numpy array represent square matrices.\n\nThis function is typically used in pyRiemann preprocessing and validation code to ensure that inputs that are expected to be batches of square matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, which are symmetric positive definite and therefore square) have the correct shape before further Riemannian operations. It performs a lightweight shape check only and does not inspect matrix values or properties such as symmetry or positive definiteness.", "tools": [{"function": {"description": "pyriemann.utils.test.is_square checks whether the last two dimensions of a numpy array represent square matrices.\n\nThis function is typically used in pyRiemann preprocessing and validation code to ensure that inputs that are expected to be batches of square matrices (for example, covariance matrices estimated from multichannel biosignals such as EEG, which are symmetric positive definite and therefore square) have the correct shape before further Riemannian operations. It performs a lightweight shape check only and does not inspect matrix values or properties such as symmetry or positive definiteness.", "name": "pyriemann_utils_test_is_square", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Array expected to contain one or more matrices arranged in its last two axes, with shape (..., n, n). The function requires X to be at least 2-dimensional (ndim >= 2). In typical pyRiemann usage, X would be an array of covariance matrices with shape (n_epochs, n_channels, n_channels) or a single matrix with shape (n_channels, n_channels).", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re integrating covariance batches from three EEG acquisition sites into a single Riemannian pipeline, but the upstream exporters are inconsistent about whether they emit full 2D covariance matrices per epoch or accidentally flatten/pad them into non-matrix tensors. Before feature extraction, run a shape-only gate using `pyriemann.utils.test.is_square` on any cohort whose epochs are represented as rank-2 matrices (i.e., every epoch is a list of rows, not a single flat vector). Apply the check to the full batch as provided for each eligible cohort; do not assess symmetry/PD—only whether the last two dimensions form square matrices.\n\nRaw cohorts:\n- Cohort A (5 epochs, each epoch provided as a 4×4 array-of-rows):\n[[[1.0, 0.2, 0.1, 0.0], [0.2, 1.1, -0.1, 0.3], [0.1, -0.1, 0.9, 0.4], [0.0, 0.3, 0.4, 1.2]], [[0.9, 0.0, 0.2, -0.1], [0.0, 1.0, 0.1, 0.0], [0.2, 0.1, 1.1, 0.3], [-0.1, 0.0, 0.3, 0.8]], [[1.2, -0.2, 0.0, 0.1], [-0.2, 0.8, 0.2, 0.0], [0.0, 0.2, 1.0, -0.3], [0.1, 0.0, -0.3, 0.9]], [[1.1, 0.1, -0.2, 0.0], [0.1, 0.9, 0.0, 0.2], [-0.2, 0.0, 1.3, 0.1], [0.0, 0.2, 0.1, 0.7]], [[0.8, 0.0, 0.3, 0.2], [0.0, 1.2, -0.1, 0.0], [0.3, -0.1, 0.9, 0.4], [0.2, 0.0, 0.4, 1.0]]]\n- Cohort B (2 epochs, but the site’s logger sometimes serializes covariances as flat length-16 vectors; this cohort arrived as per-epoch 4×4 array-of-rows):\n[[[1.0, 0.2, 0.1, 0.0], [0.2, 1.3, 0.0, 0.1], [0.1, 0.0, 0.9, 0.05], [0.0, 0.1, 0.05, 1.1]], [[1.1, 0.0, 0.2, -0.1], [0.0, 0.8, 0.05, 0.0], [0.2, 0.05, 1.4, 0.3], [-0.1, 0.0, 0.3, 1.0]]]\n- Cohort C (3 epochs, each epoch provided as a 4×4 array-of-rows):\n[[[1.0, 0.2, 0.1, 0.0], [0.2, 1.0, 0.3, 0.1], [0.1, 0.3, 1.0, 0.4], [0.0, 0.1, 0.4, 1.0]], [[1.1, 0.0, 0.2, 0.1], [0.0, 0.9, 0.1, 0.0], [0.2, 0.1, 1.2, 0.3], [0.1, 0.0, 0.3, 1.0]], [[0.8, 0.1, 0.0, 0.2], [0.1, 1.0, 0.2, 0.0], [0.0, 0.2, 0.9, 0.1], [0.2, 0.0, 0.1, 1.1]]]\n\nReturn the pass/fail outcome for each cohort that meets the rank-2 per-epoch representation criterion.", "answers": "[{\"name\":\"pyriemann_utils_test_is_square\",\"arguments\":{\"X\":[[[1.0,0.2,0.1,0.0],[0.2,1.1,-0.1,0.3],[0.1,-0.1,0.9,0.4],[0.0,0.3,0.4,1.2]],[[0.9,0.0,0.2,-0.1],[0.0,1.0,0.1,0.0],[0.2,0.1,1.1,0.3],[-0.1,0.0,0.3,0.8]],[[1.2,-0.2,0.0,0.1],[-0.2,0.8,0.2,0.0],[0.0,0.2,1.0,-0.3],[0.1,0.0,-0.3,0.9]],[[1.1,0.1,-0.2,0.0],[0.1,0.9,0.0,0.2],[-0.2,0.0,1.3,0.1],[0.0,0.2,0.1,0.7]],[[0.8,0.0,0.3,0.2],[0.0,1.2,-0.1,0.0],[0.3,-0.1,0.9,0.4],[0.2,0.0,0.4,1.0]]]}},{\"name\":\"pyriemann_utils_test_is_square\",\"arguments\":{\"X\":[[[1.0,0.2,0.1,0.0],[0.2,1.3,0.0,0.1],[0.1,0.0,0.9,0.05],[0.0,0.1,0.05,1.1]],[[1.1,0.0,0.2,-0.1],[0.0,0.8,0.05,0.0],[0.2,0.05,1.4,0.3],[-0.1,0.0,0.3,1.0]]]}},{\"name\":\"pyriemann_utils_test_is_square\",\"arguments\":{\"X\":[[[1.0,0.2,0.1,0.0],[0.2,1.0,0.3,0.1],[0.1,0.3,1.0,0.4],[0.0,0.1,0.4,1.0]],[[1.1,0.0,0.2,0.1],[0.0,0.9,0.1,0.0],[0.2,0.1,1.2,0.3],[0.1,0.0,0.3,1.0]],[[0.8,0.1,0.0,0.2],[0.1,1.0,0.2,0.0],[0.0,0.2,0.9,0.1],[0.2,0.0,0.1,1.1]]]}}]"}
{"func_name": "pyriemann_utils_test_is_sym_pos_semi_def", "func_desc": "Check whether every square matrix in X is symmetric and positive semi-definite (SPSD).\n\nThis utility is used in pyRiemann to validate matrices that are expected to represent covariance or kernel matrices (for example, covariance matrices estimated from multichannel EEG/MEG/EMG trials in brain–computer interface pipelines). The function returns True only when every matrix in the input is both symmetric and positive semi-definite according to the underlying checks is_sym and is_pos_semi_def; otherwise it returns False. The function does not modify the input array.", "tools": [{"function": {"description": "Check whether every square matrix in X is symmetric and positive semi-definite (SPSD).\n\nThis utility is used in pyRiemann to validate matrices that are expected to represent covariance or kernel matrices (for example, covariance matrices estimated from multichannel EEG/MEG/EMG trials in brain–computer interface pipelines). The function returns True only when every matrix in the input is both symmetric and positive semi-definite according to the underlying checks is_sym and is_pos_semi_def; otherwise it returns False. The function does not modify the input array.", "name": "pyriemann_utils_test_is_sym_pos_semi_def", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "The set of square matrices to test, with shape (..., n, n). The array must be at least 2-D and the last two dimensions are interpreted as the row and column indices of n-by-n matrices. In the pyRiemann domain, X typically contains covariance matrices estimated from multivariate time-series; the check ensures these matrices meet the mathematical prerequisites for Riemannian-geometry-based processing (e.g., MDM, tangent-space mapping). The function delegates to the helper functions is_sym (which verifies symmetry numerically) and is_pos_semi_def (which verifies positive semi-definiteness) and returns the logical AND of their results. The function does not change X in place. If X contains NaNs or infinities, or if it is not shaped as (..., n, n), the helper checks will typically fail and the function will return False (or may raise an error raised by those helper functions if they do not accept the input type).", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re running a QC gate on trial-wise covariance estimates from two acquisition setups, but the raw export is messy: each cohort includes a mix of proper covariance matrices and non-covariance artifacts. For each cohort, construct the batch to be validated by keeping only matrices that look like physically plausible covariance estimates: they must be square, have strictly positive diagonal entries (no zero/negative channel variances), and be numerically symmetric up to measurement rounding (i.e., treat entries as symmetric if the absolute mismatch between mirrored off-diagonal elements is ≤ 0.05). For any matrix that meets those plausibility rules, validate it with the SPSD checker and report whether the entire plausibility-filtered batch passes (True only if every retained matrix is SPSD).\n\nCohort A (3×3 candidates; in this order):\nA1 = [[1.0, 0.2, -0.1],[0.2, 0.5, 0.0],[-0.1, 0.0, 0.3]]\nA2 = [[2.0, -0.3, 0.4],[-0.3, 1.0, -0.2],[0.4, -0.2, 1.5]]\nA3 = [[1.0, 2.0, 0.0],[2.0, 1.0, 0.0],[0.0, 0.0, 0.1]]\nA4 = [[0.0, 0.1, 0.0],[0.1, 0.4, 0.0],[0.0, 0.0, 0.2]]\nA5 = [[1.0, 0.4, -0.1],[0.46, 0.9, 0.02],[-0.08, 0.01, 0.7]]\n\nCohort B (4×4 candidates; in this order):\nB1 = [[1.0, 0.2, -0.1, 0.0],[0.2, 0.9, 0.05, -0.02],[-0.1, 0.05, 1.1, 0.3],[0.0, -0.02, 0.3, 0.8]]\nB2 = [[0.7, -0.15, 0.0, 0.05],[-0.15, 0.8, 0.1, 0.0],[0.0, 0.1, 0.6, -0.08],[0.05, 0.0, -0.08, 0.9]]\nB3 = [[1.2, 0.0, 0.2, -0.1],[0.0, 0.95, -0.05, 0.0],[0.2, -0.05, 1.05, 0.15],[-0.1, 0.0, 0.15, 0.85]]\nB4 = [[1.0, 0.3, 0.0, 0.0],[0.3, 0.0, 0.0, 0.0],[0.0, 0.0, 0.9, 0.1],[0.0, 0.0, 0.1, 0.8]]\nB5 = [[0.9, 0.12, -0.05, 0.02],[0.08, 1.0, 0.07, 0.01],[-0.04, 0.09, 0.85, -0.03],[0.02, 0.0, -0.01, 0.95]]", "answers": "[{\"name\":\"pyriemann_utils_test_is_sym_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,-0.1],[0.2,0.5,0.0],[-0.1,0.0,0.3]],[[2.0,-0.3,0.4],[-0.3,1.0,-0.2],[0.4,-0.2,1.5]],[[1.0,0.4,-0.1],[0.46,0.9,0.02],[-0.08,0.01,0.7]]] }},{\"name\":\"pyriemann_utils_test_is_sym_pos_semi_def\",\"arguments\":{\"X\":[[[1.0,0.2,-0.1,0.0],[0.2,0.9,0.05,-0.02],[-0.1,0.05,1.1,0.3],[0.0,-0.02,0.3,0.8]],[[0.7,-0.15,0.0,0.05],[-0.15,0.8,0.1,0.0],[0.0,0.1,0.6,-0.08],[0.05,0.0,-0.08,0.9]],[[1.2,0.0,0.2,-0.1],[0.0,0.95,-0.05,0.0],[0.2,-0.05,1.05,0.15],[-0.1,0.0,0.15,0.85]],[[0.9,0.12,-0.05,0.02],[0.08,1.0,0.07,0.01],[-0.04,0.09,0.85,-0.03],[0.02,0.0,-0.01,0.95]]] }}]"}
{"func_name": "pyriemann_utils_utils_check_init", "func_desc": "Check the initial matrix used to initialize algorithms in pyriemann.utils.utils.check_init. This function verifies that the provided initial matrix has the exact square shape (n, n) required by downstream algorithms that operate on n-by-n matrices (for example, covariance matrices and other SPD/HPD matrices used in pyRiemann for biosignal processing and remote sensing). The function also converts the input to a numpy.ndarray using np.asarray but does not perform checks on symmetry or positive-definiteness.", "tools": [{"function": {"description": "Check the initial matrix used to initialize algorithms in pyriemann.utils.utils.check_init. This function verifies that the provided initial matrix has the exact square shape (n, n) required by downstream algorithms that operate on n-by-n matrices (for example, covariance matrices and other SPD/HPD matrices used in pyRiemann for biosignal processing and remote sensing). The function also converts the input to a numpy.ndarray using np.asarray but does not perform checks on symmetry or positive-definiteness.\n", "name": "pyriemann_utils_utils_check_init", "parameters": {"properties": {"init": {"type": "array", "items": {"type": "float"}, "description": "A square matrix intended to initialize an algorithm. This parameter is converted to a numpy.ndarray via np.asarray so inputs provided as lists or array-like objects become numpy arrays. The matrix is expected to represent an n-by-n quantity such as a covariance matrix used by Riemannian geometry based methods in pyRiemann; however, this function only enforces the shape and does not validate matrix properties like symmetry or positive-definiteness.", "default": ""}, "n": {"type": "integer", "description": "The expected dimension of the matrix. This integer specifies the required number of rows and columns for init. Typical callers provide the number of channels or features (for example, the number of EEG sensors or spatial bands used to build an n-by-n covariance matrix).", "default": ""}}, "required": ["init", "n"], "type": "any"}}, "type": "function"}], "query": "We’re staging an EEG + inertial remote-sensing fusion pilot where the effective channel count varies by subject after QC. For each subject below, determine the required matrix dimension n from the provided channel list (n = number of channels that are not marked as 'BAD'). Then run the pyRiemann initialization sanity-check only for candidates whose serialized init payload is a strict, fully rectangular square matrix with row/column count exactly equal to that subject’s n (treat ragged rows, missing rows, or extra columns as acquisition/serialization artifacts that should fail the shape gate). Use the init payload exactly as given (no symmetry/PD fixes), and rely on the checker’s array coercion.\n\nSubjects / candidates:\n1) subj_01 channels: ['Fz','Cz','Pz','Oz'] ; init payload: [[1.0, 0.2, 0.0, 0.1], [0.2, 1.5, -0.1, 0.0], [0.0, -0.1, 0.9, 0.3], [0.1, 0.0, 0.3, 1.2]]\n2) subj_02 channels: ['F3','F4','C3','C4','P3','P4','O1','O2'] ; init payload: [[1.0, 0.1, 0.0, 0.0], [0.1, 1.2, 0.2, 0.0], [0.0, 0.2, 0.9, 0.3], [0.0, 0.0, 0.3, 1.1]]\n3) subj_03 channels: ['Fp1','Fp2','BAD','Oz'] ; init payload: [[1.0, 0.05, 0.0], [0.05, 1.1, 0.2], [0.0, 0.2, 0.95]]\n4) subj_04 channels: ['Cz','Pz','Oz','T8'] ; init payload (telemetry JSON decode): [[1.0, 0.2, 0.0, 0.1], [0.2, 1.5, -0.1], [0.0, -0.1, 0.9, 0.3], [0.1, 0.0, 0.3, 1.2]]", "answers": "[{\"name\":\"pyriemann_utils_utils_check_init\",\"arguments\":{\"init\":[[1.0,0.2,0.0,0.1],[0.2,1.5,-0.1,0.0],[0.0,-0.1,0.9,0.3],[0.1,0.0,0.3,1.2]],\"n\":4}},{\"name\":\"pyriemann_utils_utils_check_init\",\"arguments\":{\"init\":[[1.0,0.05,0.0],[0.05,1.1,0.2],[0.0,0.2,0.95]],\"n\":3}}]"}
{"func_name": "pyriemann_utils_utils_check_metric", "func_desc": "Check and normalize a metric argument for pyriemann algorithms.\n\nThis function validates and normalizes the \"metric\" argument used throughout pyRiemann algorithms that operate on symmetric/Hermitian positive definite (SPD/HPD) covariance matrices (for example in EEG/MEG/BCI pipelines and remote sensing applications described in the project README). The metric argument can be provided as a single metric name string to be applied to all algorithm steps, or as a dictionary mapping specific algorithm steps (for example \"mean\" and \"distance\") to metric name strings. A common practical use is to pass a faster metric (e.g., \"logeuclid\") for the \"mean\" computation to speed up covariance averaging, while using a more sensitive metric (e.g., \"riemann\") for the \"distance\" computation to preserve classification performance in BCI pipelines.", "tools": [{"function": {"description": "Check and normalize a metric argument for pyriemann algorithms.\n\nThis function validates and normalizes the \"metric\" argument used throughout pyRiemann algorithms that operate on symmetric/Hermitian positive definite (SPD/HPD) covariance matrices (for example in EEG/MEG/BCI pipelines and remote sensing applications described in the project README). The metric argument can be provided as a single metric name string to be applied to all algorithm steps, or as a dictionary mapping specific algorithm steps (for example \"mean\" and \"distance\") to metric name strings. A common practical use is to pass a faster metric (e.g., \"logeuclid\") for the \"mean\" computation to speed up covariance averaging, while using a more sensitive metric (e.g., \"riemann\") for the \"distance\" computation to preserve classification performance in BCI pipelines.", "name": "pyriemann_utils_utils_check_metric", "parameters": {"properties": {"metric": {"type": "any", "description": "Metric specification for algorithm steps. If a string is provided, that single metric name is duplicated and returned for every entry in expected_keys; this is convenient when the same metric should be used for all steps. If a dict is provided, it must map each name in expected_keys to a metric name string (for example {\"mean\": \"logeuclid\", \"distance\": \"riemann\"}). The metric names returned are intended to be consumed by downstream pyRiemann components that compute means or distances on covariance matrices. Supplying a dict allows fine-grained control of computational trade-offs (speed vs. sensitivity) per algorithm step.", "default": ""}, "expected_keys": {"type": "array", "items": {"type": "float"}, "description": "Ordered list of step names for which a metric is required. The function returns a list of metrics whose ordering matches this list. By default the typical steps are \"mean\" (covariance averaging step used in estimators/classifiers) and \"distance\" (pairwise or classifier distance used for comparisons). Pass a different list to adapt to algorithms that use other named steps; the function will enforce that a provided dict contains all these keys.", "default": ["mean", "distance"]}}, "required": ["metric", "expected_keys"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating metric-specification hygiene checks across a mixed EEG/MEG SPD-covariance benchmark where configuration files come from multiple labs and have inconsistent conventions. For each candidate pipeline configuration below, validate/normalize the per-step metric specification **only when** the declared step list contains both a covariance-averaging stage (\"mean\") and a dissimilarity stage (\"distance\"). When a configuration is eligible, enforce the lab policy: use a fast metric for averaging (\"logeuclid\") and a sensitive metric for dissimilarity (\"riemann\"). If an eligible configuration includes any additional steps beyond {\"mean\",\"distance\"}, those extra steps must reuse the averaging metric. Candidate configs:\n\n- cfg_A: steps=[\"mean\",\"distance\",\"projection\"], metric_spec=\"logeuclid\" (lab provided a single global string)\n- cfg_B: steps=[\"mean\",\"distance\"], metric_spec={\"mean\":\"logeuclid\",\"distance\":\"riemann\"}\n- cfg_C: steps=[\"mean\",\"projection\"], metric_spec={\"mean\":\"logeuclid\"}\n- cfg_D: steps=[\"mean\",\"distance\",\"projection\"], metric_spec={\"mean\":\"logeuclid\",\"distance\":\"riemann\"}  (projection omitted in the dict)\n- cfg_E: steps=[\"distance\"], metric_spec=\"riemann\"", "answers": "[{\"name\":\"pyriemann_utils_utils_check_metric\",\"arguments\":{\"metric\":\"logeuclid\",\"expected_keys\":[\"mean\",\"distance\",\"projection\"]}},{\"name\":\"pyriemann_utils_utils_check_metric\",\"arguments\":{\"metric\":{\"mean\":\"logeuclid\",\"distance\":\"riemann\"},\"expected_keys\":[\"mean\",\"distance\"]}},{\"name\":\"pyriemann_utils_utils_check_metric\",\"arguments\":{\"metric\":{\"mean\":\"logeuclid\",\"distance\":\"riemann\"},\"expected_keys\":[\"mean\",\"distance\",\"projection\"]}}]"}
{"func_name": "pyriemann_utils_utils_check_version", "func_desc": "Check minimum library version required for pyRiemann runtime or optional dependency checks.\n\nThis function is used in pyRiemann (a package for processing multivariate biosignal data such as EEG/MEG for BCI applications) to verify that an external Python library dependency is present and meets a minimum semantic version requirement before enabling features that rely on that library. It attempts to import the named library, reads its __version__ attribute, optionally strips PEP440 development markers (for backward compatibility with LooseVersion-like behavior), and compares the resulting version string against the supplied minimum. The function returns a boolean indicating whether the dependency is available and sufficiently recent. Behavior and edge cases follow the implementation in pyriemann.utils.utils and are adapted from MNE-Python.", "tools": [{"function": {"description": "Check minimum library version required for pyRiemann runtime or optional dependency checks.\n\nThis function is used in pyRiemann (a package for processing multivariate biosignal data such as EEG/MEG for BCI applications) to verify that an external Python library dependency is present and meets a minimum semantic version requirement before enabling features that rely on that library. It attempts to import the named library, reads its __version__ attribute, optionally strips PEP440 development markers (for backward compatibility with LooseVersion-like behavior), and compares the resulting version string against the supplied minimum. The function returns a boolean indicating whether the dependency is available and sufficiently recent. Behavior and edge cases follow the implementation in pyriemann.utils.utils and are adapted from MNE-Python.", "name": "pyriemann_utils_utils_check_version", "parameters": {"properties": {"library": {"type": "string", "description": "The import name of the external Python library to check (for example, \"mne\"). This string is passed to the built-in import mechanism (i.e., __import__), so it must be a valid importable module name. The imported module is expected to expose a __version__ attribute; if it does not, attribute access will raise an exception that is propagated to the caller. Importing the module is a side effect: module-level code may run and the module will be added to sys.modules.", "default": ""}, "min_version": {"type": "string", "description": "The minimum acceptable version string for the library (for example, \"1.1\"). The implementation expects a conventional version pattern composed of digits, lowercase letters, and dots (the original code refers to components matching '(\\d+ | [a-z]+ | \\.)'). If min_version is falsy or equal to the literal string \"0.0\", the function will skip the version comparison step and consider the import alone sufficient. This parameter is used to decide whether the installed library version is new enough for pyRiemann features that depend on it.", "default": ""}, "strip": {"type": "boolean", "description": "If True (default), strip PEP440 development markers such as \".devN\" from the library's __version__ before comparison. This makes prerelease/dev versions like \"1.1.dev0\" compare as if they were \"1.1\", preserving backward-compatible behavior similar to LooseVersion. If False, development markers are left intact and the comparison follows the module's internal parsing semantics, which may diverge from the stripped behavior. Changing this flag only affects how the version string is normalized prior to comparison.", "default": true}}, "required": ["library", "min_version", "strip"], "type": "any"}}, "type": "function"}], "query": "We’re running a preflight dependency audit for a heterogeneous EEG/MEG cohort where analysis branches depend on which optional toolchains are *actually* usable. Treat the candidate backends as two labeled pipelines:\n\n- **Pipeline A (MNE I/O + preprocessing)**: enable only if the `mne` dependency meets a minimum version computed from the target feature set: use **1.4.0** when the feature set is tagged as `io_v2`, otherwise **1.3** for legacy-compatible runs. For the `io_v2` branch, treat any PEP440 development suffix (e.g., `.devN`) as equivalent to the corresponding release when comparing.\n\n- **Pipeline B (Riemannian covariance + estimators)**: enable only if the scikit-learn backend is sufficiently recent for the estimator API we need; require **sklearn >= 1.2.0**, and for this check also treat any `.devN` suffix as equivalent to the corresponding release.\n\nReturn pass/fail booleans for the dependency gates corresponding to these branches so the workflow engine can decide which stages to activate.", "answers": "[{\"name\":\"pyriemann_utils_utils_check_version\",\"arguments\":{\"library\":\"mne\",\"min_version\":\"1.4.0\",\"strip\":true}},{\"name\":\"pyriemann_utils_utils_check_version\",\"arguments\":{\"library\":\"mne\",\"min_version\":\"1.3\"}},{\"name\":\"pyriemann_utils_utils_check_version\",\"arguments\":{\"library\":\"sklearn\",\"min_version\":\"1.2.0\",\"strip\":true}}]"}
{"func_name": "pyscf_agf2__agf2_cholesky_build", "func_desc": "Constructs truncated auxiliary tensors (auxiliaries) from the metric-like\n    matrices vv and vev used in the AGF2 self-energy construction.\n    \n    This function is part of the AGF2 implementation in PySCF and is used by\n    build_se_part to produce a compact representation of the auxiliary basis that\n    parameterizes the frequency-dependent self-energy for occupied or virtual\n    spaces. In practice, vv is expected to be a Hermitian, positive-definite or\n    positive-semidefinite metric (for example when gf_occ.naux < gf_occ.nphys or\n    gf_vir.naux < gf_vir.nphys the matrix can be positive-semidefinite). The\n    function performs a Cholesky decomposition of vv to obtain a factor b, falls\n    back to an eigenvalue regularization when vv is not strictly positive-definite,\n    and then diagonalizes a transformed vev to produce eigenvalues and transformed\n    eigenvectors that define the truncated auxiliaries. This numerical procedure\n    stabilizes the auxiliary representation used in correlated Green's function\n    (Self-Energy) computations within the PySCF AGF2 module.", "tools": [{"function": {"description": "Constructs truncated auxiliary tensors (auxiliaries) from the metric-like\nmatrices vv and vev used in the AGF2 self-energy construction.\n\nThis function is part of the AGF2 implementation in PySCF and is used by\nbuild_se_part to produce a compact representation of the auxiliary basis that\nparameterizes the frequency-dependent self-energy for occupied or virtual\nspaces. In practice, vv is expected to be a Hermitian, positive-definite or\npositive-semidefinite metric (for example when gf_occ.naux < gf_occ.nphys or\ngf_vir.naux < gf_vir.nphys the matrix can be positive-semidefinite). The\nfunction performs a Cholesky decomposition of vv to obtain a factor b, falls\nback to an eigenvalue regularization when vv is not strictly positive-definite,\nand then diagonalizes a transformed vev to produce eigenvalues and transformed\neigenvectors that define the truncated auxiliaries. This numerical procedure\nstabilizes the auxiliary representation used in correlated Green's function\n(Self-Energy) computations within the PySCF AGF2 module.", "name": "pyscf_agf2__agf2_cholesky_build", "parameters": {"properties": {"vv": {"type": "array", "items": {"type": "float"}, "description": "Square Hermitian matrix defining the metric for the\nauxiliary space. In AGF2 this matrix arises in build_se_part and\nrepresents overlaps or coupling weights of auxiliary functions; its\nshape and leading dimension determine nmo (the number of molecular\norbitals or the physical dimension for which auxiliaries are formed).\nThe function attempts a Cholesky decomposition of vv. If numpy.linalg.cholesky\nraises a LinAlgError (vv not strictly positive-definite), the code\nperforms an eigenvalue decomposition, replaces eigenvalues smaller than\neps with eps to enforce positive-definiteness (numerical regularization),\nreconstructs a positive-definite vv, and proceeds with the Cholesky\ndecomposition. This regularization removes exact null-space (zero\neigenvalues) by elevating them to eps and thereby enables stable\nfactorization.", "default": ""}, "vev": {"type": "array", "items": {"type": "float"}, "description": "Hermitian matrix in the same auxiliary/physical basis\nas vv that is transformed into the Cholesky-normalized space and\ndiagonalized to obtain auxiliary energies and coefficients. In AGF2\nusage vev encodes energy-weighted couplings of auxiliaries (for\noccupied/virtual self-energy parts) produced by build_se_part. The\nfunction forms m = b_inv.T @ vev @ b_inv (with b from the Cholesky\nfactor of vv) and diagonalizes m to obtain eigenvalues and eigenvectors.", "default": ""}, "eps": {"type": "float", "description": "Threshold for numerical regularization of vv eigenvalues;\neigenvalues smaller than eps are replaced by eps when vv is\nnot strictly positive-definite. Default is 1e-16. This parameter\ncontrols how small or near-zero eigenvalues are treated: too-large\neps may artificially inflate a null space, while too-small eps may\nfail to stabilize the Cholesky decomposition in the presence of\nnumerical noise. A LinAlgError during the initial Cholesky triggers\nthe eigen-regularization using eps. No other side effects or global\nstate are modified by changing eps.", "default": 1e-16}}, "required": ["vv", "vev", "eps"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing AGF2 auxiliary-basis stabilization on a mixed-quality batch of candidate (vv, vev) pairs coming from different orbital subspaces and numerical regimes. Treat each candidate as a metric-like vv with an energy-weighted coupling vev. For each pair, first verify it is structurally admissible for AGF2 truncation: vv and vev must be square, same dimension, symmetric/Hermitian within numerical noise, and vv must be positive-semidefinite after applying a near-zero eigenvalue cutoff. Only admissible pairs should be passed into the standard Cholesky-first auxiliary construction (with eigenvalue regularization fallback handled internally). Use an adaptive eps per pair: set eps to 1e-12 if the spectrum of vv indicates a null-space (minimum eigenvalue is exactly zero within floating tolerance or the condition number exceeds 1e12); otherwise use eps=1e-14. Process the following raw candidates in this order (some contain artifacts such as asymmetry, dimension mismatch, or indefiniteness):\n\n1) vv=[[1.0,0.2,0.0],[0.2,1.0,0.0],[0.0,0.0,0.0]], vev=[[0.5,0.1,0.0],[0.1,0.4,0.0],[0.0,0.0,0.0]]\n2) vv=[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,0.8]], vev=[[0.5,0.1,0.0],[0.1,0.6,0.2],[0.0,0.2,0.7]]\n3) vv=[[1.0,0.3],[0.3,0.2]], vev=[[0.2,0.0,0.0],[0.0,0.2,0.0],[0.0,0.0,0.2]]\n4) vv=[[1.0,0.4,0.0],[0.1,1.0,0.0],[0.0,0.0,1.0]], vev=[[0.3,0.0,0.0],[0.0,0.3,0.0],[0.0,0.0,0.3]]\n5) vv=[[1.0,2.0],[2.0,1.0]], vev=[[0.1,0.0],[0.0,0.1]]\n\nReturn the list of tool calls needed for the admissible candidates under this protocol, with eps chosen by the rule above.", "answers": "[{\"name\":\"pyscf_agf2__agf2_cholesky_build\",\"arguments\":{\"vv\":[[1.0,0.2,0.0],[0.2,1.0,0.0],[0.0,0.0,0.0]],\"vev\":[[0.5,0.1,0.0],[0.1,0.4,0.0],[0.0,0.0,0.0]],\"eps\":1e-12}},{\"name\":\"pyscf_agf2__agf2_cholesky_build\",\"arguments\":{\"vv\":[[1.0,0.2,0.1],[0.2,0.9,0.3],[0.1,0.3,0.8]],\"vev\":[[0.5,0.1,0.0],[0.1,0.6,0.2],[0.0,0.2,0.7]],\"eps\":1e-14}}]"}
{"func_name": "pyscf_agf2_chempot_binsearch_chempot", "func_desc": "Finds a chemical potential that best matches a target number of physical electrons by inspecting the eigenvalues and eigenvectors of a Fock matrix and enforcing the Aufbau principle.\n    \n    This routine is used in the AGF2 chemical-potential determination within PySCF: given a Fock matrix describing either a physical fragment or an extended (physical + auxiliary) system, it identifies the highest-occupied and lowest-unoccupied molecular orbitals (HOMO/LUMO) with respect to the physical subspace and returns the midpoint eigenvalue as the chemical potential. If the caller supplies a pre-diagonalised Fock (eigenvalues and eigenvectors), the function uses them directly; otherwise it diagonalises the supplied Fock matrix with numpy.linalg.eigh. The function computes the occupation contributed by each molecular orbital to the first nphys rows (the physical degrees of freedom) via the projection n = occupancy * v[:nphys,i].conj().T @ v[:nphys,i] and accumulates this until the accumulated physical electron count brackets the requested nelec. The returned error is the signed difference between the requested nelec and the chosen accumulated electron count (positive means the chosen state has fewer electrons than requested).", "tools": [{"function": {"description": "Finds a chemical potential that best matches a target number of physical electrons by inspecting the eigenvalues and eigenvectors of a Fock matrix and enforcing the Aufbau principle.\n\nThis routine is used in the AGF2 chemical-potential determination within PySCF: given a Fock matrix describing either a physical fragment or an extended (physical + auxiliary) system, it identifies the highest-occupied and lowest-unoccupied molecular orbitals (HOMO/LUMO) with respect to the physical subspace and returns the midpoint eigenvalue as the chemical potential. If the caller supplies a pre-diagonalised Fock (eigenvalues and eigenvectors), the function uses them directly; otherwise it diagonalises the supplied Fock matrix with numpy.linalg.eigh. The function computes the occupation contributed by each molecular orbital to the first nphys rows (the physical degrees of freedom) via the projection n = occupancy * v[:nphys,i].conj().T @ v[:nphys,i] and accumulates this until the accumulated physical electron count brackets the requested nelec. The returned error is the signed difference between the requested nelec and the chosen accumulated electron count (positive means the chosen state has fewer electrons than requested).", "name": "pyscf_agf2_chempot_binsearch_chempot", "parameters": {"properties": {"fock": {"type": "any", "description": "Fock matrix to diagonalise, or a precomputed eigen-decomposition. In practical AGF2 usage this is either the physical Fock matrix or an extended Fock matrix that includes auxiliary degrees of freedom. If a tuple is provided it must follow the output convention of numpy.linalg.eigh: (w, v) where w is a 1D array of eigenvalues and v is a 2D array whose columns are the corresponding eigenvectors. When a 2D array is provided the function will call numpy.linalg.eigh(fock) internally. The eigenvectors v are expected to have at least nphys rows so that v[:nphys, i] projects the i-th molecular orbital onto the physical subspace.", "default": ""}, "nphys": {"type": "integer", "description": "Number of physical degrees of freedom. In AGF2 this is the dimension of the physical fragment (for example, the number of physical atomic orbitals or orbitals retained as \"physical\" in the extended problem). The function uses nphys to restrict population counting to the physical subspace: only the first nphys components of each eigenvector are included when computing how much each molecular orbital contributes to the physical electron count.", "default": ""}, "nelec": {"type": "integer", "description": "Number of physical electrons to match. This is the target electron count in the physical subspace for which the chemical potential is being determined (for example, the number of electrons in the molecule or fragment). The routine attempts to choose a chemical potential so that, when orbitals are filled according to increasing eigenvalue and weighted by their projection into the physical subspace, the resulting physical electron count is as close as possible to nelec.", "default": ""}, "occupancy": {"type": "integer", "description": "Occupancy of each spatial orbital when fully occupied: 2 for restricted (RHF) spin-paired occupancy and 1 for unrestricted (UHF) spin-resolved occupancy. Default 2. This factor multiplies the projected MO population and therefore scales the maximum possible physical electron count (maximum = occupancy * nphys). Provide the occupancy that matches how the rest of the AGF2 calculation counts electrons.", "default": 2}}, "required": ["fock", "nphys", "nelec", "occupancy"], "type": "any"}}, "type": "function"}], "query": "We have a small bank of AO-basis fragment Fock matrices from an AGF2 pre-screen where some candidates are not physically usable as-is (e.g., wrong symmetry/conditioning for a Hermitian diagonalisation step). Treat each candidate as a restricted (spin-paired) case with occupancy = 2, and let the routine diagonalise internally (do not provide pre-diagonalised eigenpairs). \n\nPipeline rules:\n1) Only run chemical-potential determination on candidates whose supplied Fock is numerically Hermitian (F ≈ F^T within tight tolerance) so that the Aufbau-based bracketing is meaningful.\n2) Determine the physical subspace size nphys from the matrix itself: if the matrix is block-diagonal into two disconnected 2×2 blocks (no coupling between orbitals 1–2 and 3–4), treat it as an embedded/fragmented calculation with nphys = 2; otherwise treat it as fully physical with nphys = dimension.\n3) Set the target physical electron count nelec from nphys assuming a closed-shell reference: nelec = 2*nphys − 2 (i.e., one doubly-occupied hole relative to full filling of the physical subspace).\n\nFor each candidate that passes rule (1), compute μ as the HOMO/LUMO midpoint obtained by bracketing the requested physical electron count via projection onto the first nphys rows, and report both μ and the signed occupation error.\n\nCandidates (Hartree):\nA:\n[[-0.8, 0.1, 0.0, 0.0],\n [ 0.1,-0.5, 0.05,0.0],\n [ 0.0,0.05, 0.2, 0.02],\n [ 0.0,0.0, 0.02,0.4]]\n\nB:\n[[-0.8,-0.1,0.0,0.0],\n [-0.1,-0.5,0.0,0.0],\n [ 0.0,0.0,0.2,0.05],\n [ 0.0,0.0,0.05,0.4]]\n\nC:\n[[-0.8,0.1,0.0,0.0],\n [ 0.1,-0.6,0.05,0.0],\n [ 0.0,0.05,-0.3,0.02],\n [ 0.0,0.0,0.02,0.1]]\n\nD (instrumentation artifact: slight non-Hermiticity injected during AO transformation):\n[[-0.8,0.12,0.0,0.0],\n [ 0.1,-0.5,0.05,0.0],\n [ 0.0,0.05,0.2,0.02],\n [ 0.0,0.0,0.02,0.4]]", "answers": "[{\"name\":\"pyscf_agf2_chempot_binsearch_chempot\",\"arguments\":{\"fock\":[[-0.8,0.1,0.0,0.0],[0.1,-0.5,0.05,0.0],[0.0,0.05,0.2,0.02],[0.0,0.0,0.02,0.4]],\"nphys\":4,\"nelec\":6,\"occupancy\":2}},{\"name\":\"pyscf_agf2_chempot_binsearch_chempot\",\"arguments\":{\"fock\":[[-0.8,-0.1,0.0,0.0],[-0.1,-0.5,0.0,0.0],[0.0,0.0,0.2,0.05],[0.0,0.0,0.05,0.4]],\"nphys\":2,\"nelec\":2,\"occupancy\":2}},{\"name\":\"pyscf_agf2_chempot_binsearch_chempot\",\"arguments\":{\"fock\":[[-0.8,0.1,0.0,0.0],[0.1,-0.6,0.05,0.0],[0.0,0.05,-0.3,0.02],[0.0,0.0,0.02,0.1]],\"nphys\":4,\"nelec\":6,\"occupancy\":2}}]"}
{"func_name": "pyscf_cc_addons_spin2spatial", "func_desc": "Convert T1/T2 amplitudes from spin-orbital basis to spatial-orbital spin-separated basis used in PySCF coupled-cluster (CC) workflows.\n    \n    This function is used in the PySCF CC addons to transform CC single (T1) and double (T2) excitation amplitude tensors that are expressed in a spin-orbital indexing (interleaved alpha/beta spin-orbitals) into spatial-orbital amplitude tensors separated by spin (alpha and beta). This conversion is required when downstream CC code or post-processing routines expect amplitudes grouped by spatial orbitals and spin blocks (alpha-alpha, alpha-beta, beta-beta) rather than a single spin-orbital tensor. The implementation assumes the convention used elsewhere in PySCF: the first nocc entries of the spin-orbital index correspond to occupied spin-orbitals and the remaining entries correspond to virtual spin-orbitals; orbspin labels each spin-orbital as 0 (alpha) or 1 (beta).", "tools": [{"function": {"description": "Convert T1/T2 amplitudes from spin-orbital basis to spatial-orbital spin-separated basis used in PySCF coupled-cluster (CC) workflows.\n\nThis function is used in the PySCF CC addons to transform CC single (T1) and double (T2) excitation amplitude tensors that are expressed in a spin-orbital indexing (interleaved alpha/beta spin-orbitals) into spatial-orbital amplitude tensors separated by spin (alpha and beta). This conversion is required when downstream CC code or post-processing routines expect amplitudes grouped by spatial orbitals and spin blocks (alpha-alpha, alpha-beta, beta-beta) rather than a single spin-orbital tensor. The implementation assumes the convention used elsewhere in PySCF: the first nocc entries of the spin-orbital index correspond to occupied spin-orbitals and the remaining entries correspond to virtual spin-orbitals; orbspin labels each spin-orbital as 0 (alpha) or 1 (beta).", "name": "pyscf_cc_addons_spin2spatial", "parameters": {"properties": {"tx": {"type": "array", "items": {"type": "float"}, "description": "Input T amplitudes in the spin-orbital basis. For T1 amplitudes this must be a 2-D array with shape (nocc, nvir) where nocc and nvir are numbers of occupied and virtual spin-orbitals respectively. For T2 amplitudes this must be a 4-D array with shape (nocc, nocc, nvir, nvir) where the first two indices are occupied spin-orbitals and the last two are virtual spin-orbitals. The function inspects tx.ndim to determine whether tx encodes T1 (ndim == 2) or T2 (ndim == 4) amplitudes. If tx has any other dimensionality, the function raises RuntimeError('Unknown T amplitudes').", "default": ""}, "orbspin": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array that assigns a spin label to each spin-orbital index used by tx. Its length is expected to be at least nocc + nvir (the total number of spin-orbitals referenced by tx). Entries must be 0 for alpha spin and 1 for beta spin. The function uses orbspin[:nocc] to classify occupied spin-orbitals and orbspin[nocc:] to classify virtual spin-orbitals and then partitions indices into alpha and beta subsets for both occupied and virtual spaces. If orbspin contains values other than 0 or 1, some spin blocks may be empty; if orbspin is shorter than required, standard indexing errors (e.g., IndexError) may occur.", "default": ""}}, "required": ["tx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed CC amplitude payload from three electronic-structure micro-benchmarks (some were logged with inconsistent metadata). For each payload entry, infer whether `tx` is a CCSD T1 (rank-2) or T2 (rank-4) spin-orbital tensor from its dimensionality, and then convert it to PySCF’s spatial-orbital, spin-separated block representation using the provided `orbspin` labels (0=alpha, 1=beta). Only run the conversion for entries whose `orbspin` is a physically consistent spin labeling for an interleaved spin-orbital basis (i.e., it contains only {0,1} and has exactly as many labels as the total number of spin-orbitals implied by `tx`, with occupied spin-orbitals first and virtuals following). Payload entries:\n\nA) Open-shell diagnostic T2 snapshot: nocc=4, nvir=2 (spin-orbital T2 shape (4,4,2,2)), orbspin=[0,1,0,1,0,1], tx=\n[[[[0.01,0.02],[0.03,0.04]],[[0.05,0.06],[0.07,0.08]],[[0.09,0.10],[0.11,0.12]],[[0.13,0.14],[0.15,0.16]]],\n [[[0.17,0.18],[0.19,0.20]],[[0.21,0.22],[0.23,0.24]],[[0.25,0.26],[0.27,0.28]],[[0.29,0.30],[0.31,0.32]]],\n [[[0.33,0.34],[0.35,0.36]],[[0.37,0.38],[0.39,0.40]],[[0.41,0.42],[0.43,0.44]],[[0.45,0.46],[0.47,0.48]]],\n [[[0.49,0.50],[0.51,0.52]],[[0.53,0.54],[0.55,0.56]],[[0.57,0.58],[0.59,0.60]],[[0.61,0.62],[0.63,0.64]]]].\n\nB) Minimal RHF H2 T1 probe: nocc=2, nvir=2 (spin-orbital T1 shape (2,2)), orbspin=[0,1,0,1], tx=[[0.01,-0.02],[0.03,0.0]].\n\nC) Closed-shell RHF T2 sparse regression: nocc=4, nvir=4 (spin-orbital T2 shape (4,4,4,4)), orbspin=[0,1,0,1,0,1,0,1], tx=\n[[[[0.0,0.01,0.0,-0.02],[0.0,0.0,0.03,0.0],[0.0,-0.01,0.0,0.02],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.02,0.0,-0.01,0.0],[0.0,0.03,0.0,-0.02],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.01,0.0],[0.0,0.0,0.0,0.02],[-0.01,0.0,0.0,0.0],[0.0,-0.02,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],\n [[[0.0,0.0,0.0,0.0],[0.0,0.01,0.0,-0.01],[0.0,0.0,0.02,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[-0.02,0.0,0.0,0.01],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],\n [[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.02,0.0,-0.01],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],\n [[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],\n  [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]]].\n\nReturn the converted spatial-orbital amplitudes (spin blocks) for each payload entry that passes the consistency check.", "answers": "[{\"name\":\"pyscf_cc_addons_spin2spatial\",\"arguments\":{\"tx\":[[[[0.01,0.02],[0.03,0.04]],[[0.05,0.06],[0.07,0.08]],[[0.09,0.1],[0.11,0.12]],[[0.13,0.14],[0.15,0.16]]],[[[0.17,0.18],[0.19,0.2]],[[0.21,0.22],[0.23,0.24]],[[0.25,0.26],[0.27,0.28]],[[0.29,0.3],[0.31,0.32]]],[[[0.33,0.34],[0.35,0.36]],[[0.37,0.38],[0.39,0.4]],[[0.41,0.42],[0.43,0.44]],[[0.45,0.46],[0.47,0.48]]],[[[0.49,0.5],[0.51,0.52]],[[0.53,0.54],[0.55,0.56]],[[0.57,0.58],[0.59,0.6]],[[0.61,0.62],[0.63,0.64]]]],\"orbspin\":[0,1,0,1,0,1]}},{\"name\":\"pyscf_cc_addons_spin2spatial\",\"arguments\":{\"tx\":[[0.01,-0.02],[0.03,0.0]],\"orbspin\":[0,1,0,1]}},{\"name\":\"pyscf_cc_addons_spin2spatial\",\"arguments\":{\"tx\":[[[[0.0,0.01,0.0,-0.02],[0.0,0.0,0.03,0.0],[0.0,-0.01,0.0,0.02],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.02,0.0,-0.01,0.0],[0.0,0.03,0.0,-0.02],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.01,0.0],[0.0,0.0,0.0,0.02],[-0.01,0.0,0.0,0.0],[0.0,-0.02,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],[[[0.0,0.0,0.0,0.0],[0.0,0.01,0.0,-0.01],[0.0,0.0,0.02,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[-0.02,0.0,0.0,0.01],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],[[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.02,0.0,-0.01],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]],[[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]],[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]]],\"orbspin\":[0,1,0,1,0,1,0,1]}}]"}
{"func_name": "pyscf_cc_bccd_transform_l1_to_bo", "func_desc": "pyscf.cc.bccd.transform_l1_to_bo transforms single-excitation amplitudes (t1) into the Brueckner orbital (BO) basis used in Brueckner coupled-cluster doubles (BCCD) procedures. This function is used in the BCCD orbital-rotation and amplitude-transformation steps to express t1 amplitudes in the rotated (Brueckner) molecular-orbital basis defined by the orbital rotation matrix umat, which is required when checking or enforcing the Brueckner condition (vanishing singles) or when converting amplitudes between orbital representations.", "tools": [{"function": {"description": "pyscf.cc.bccd.transform_l1_to_bo transforms single-excitation amplitudes (t1) into the Brueckner orbital (BO) basis used in Brueckner coupled-cluster doubles (BCCD) procedures. This function is used in the BCCD orbital-rotation and amplitude-transformation steps to express t1 amplitudes in the rotated (Brueckner) molecular-orbital basis defined by the orbital rotation matrix umat, which is required when checking or enforcing the Brueckner condition (vanishing singles) or when converting amplitudes between orbital representations.\n", "name": "pyscf_cc_bccd_transform_l1_to_bo", "parameters": {"properties": {"t1": {"type": "array", "items": {"type": "float"}, "description": "The single-excitation amplitude(s) in the original molecular-orbital basis. For a restricted/HF-like case supply a 2-D numpy.ndarray with shape (nocc, nvir), where nocc is the number of occupied orbitals and nvir is the number of virtual orbitals; the function treats this as a single block of amplitudes and returns a single transformed 2-D array. For an unrestricted (UHF) calculation supply a sequence (e.g., list or tuple) of per-spin 2-D numpy.ndarray objects (one entry per spin), and the function will return a list of transformed arrays, applying the per-spin umat matrices. The t1 amplitudes represent the practical quantum-chemistry quantity of single excitation cluster amplitudes used by the coupled-cluster solver and by BCCD orbital-update logic.", "default": ""}, "umat": {"type": "array", "items": {"type": "float"}, "description": "The orbital rotation (unitary) matrix that carries molecular orbitals from the original basis into the Brueckner orbital basis. In the restricted/2-D-t1 case, umat must be a square numpy.ndarray whose first nocc rows/columns correspond to the occupied space and whose remaining nvir rows/columns correspond to the virtual space; the code slices umat as umat[:nocc,:nocc] for the occupied block and umat[nocc:,nocc:] for the virtual block. In the unrestricted/UHF case, umat is expected to be a sequence (e.g., list or tuple) of per-spin square numpy.ndarray matrices with the same occupied/virtual block layout for each spin. The umat matrix encodes the orbital rotations computed by BCCD orbital optimization and is practically required to change the representation of amplitudes between orbital bases.", "default": ""}}, "required": ["t1", "umat"], "type": "any"}}, "type": "function"}], "query": "We’re triaging Brueckner-CCD iteration snapshots for a 2-occ/3-vir closed-shell model, but only snapshots consistent with a physically meaningful orbital-rotation update should be fed into the singles-to-Brueckner-orbital transformation. For each snapshot below, compute the Frobenius norm of the occ–vir coupling block of the provided 5×5 rotation matrix umat (i.e., the 2×3 block with occ rows and vir columns). Transform t1 → BO only for snapshots whose occ–vir coupling block norm is exactly zero (numerically as given). Use the corresponding canonical-basis t1 (2×3) and full umat (5×5) exactly as listed.\n\nSnapshots:\n1) t1 = [[0.01, -0.02, 0.005], [0.003, 0.015, -0.008]]; umat = [[0.999, -0.01, 0.0, 0.0, 0.0], [0.01, 0.999, 0.0, 0.0, 0.0], [0.0, 0.0, 0.998, -0.03, 0.0], [0.0, 0.0, 0.03, 0.998, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0]]\n\n2) t1 = [[0.012, -0.034, 0.005], [0.02, 0.015, -0.01]]; umat = [[0.998, 0.02, 0.0, 0.0, 0.0], [-0.02, 0.998, 0.0, 0.0, 0.0], [0.0, 0.0, 0.999, 0.01, 0.0], [0.0, 0.0, -0.01, 0.999, 0.015], [0.0, 0.0, 0.0, -0.015, 0.999]]\n\n3) t1 = [[0.02, -0.01, 0.0], [0.03, 0.01, -0.02]]; umat = [[0.99, 0.05, 0.0, 0.0, 0.0], [-0.05, 0.99, 0.0, 0.0, 0.0], [0.0, 0.0, 0.98, 0.1, 0.0], [0.0, 0.0, -0.1, 0.98, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0]]", "answers": "[{\"name\":\"pyscf_cc_bccd_transform_l1_to_bo\",\"arguments\":{\"t1\":[[0.01,-0.02,0.005],[0.003,0.015,-0.008]],\"umat\":[[0.999,-0.01,0.0,0.0,0.0],[0.01,0.999,0.0,0.0,0.0],[0.0,0.0,0.998,-0.03,0.0],[0.0,0.0,0.03,0.998,0.0],[0.0,0.0,0.0,0.0,1.0]]}},{\"name\":\"pyscf_cc_bccd_transform_l1_to_bo\",\"arguments\":{\"t1\":[[0.012,-0.034,0.005],[0.02,0.015,-0.01]],\"umat\":[[0.998,0.02,0.0,0.0,0.0],[-0.02,0.998,0.0,0.0,0.0],[0.0,0.0,0.999,0.01,0.0],[0.0,0.0,-0.01,0.999,0.015],[0.0,0.0,0.0,-0.015,0.999]]}},{\"name\":\"pyscf_cc_bccd_transform_l1_to_bo\",\"arguments\":{\"t1\":[[0.02,-0.01,0.0],[0.03,0.01,-0.02]],\"umat\":[[0.99,0.05,0.0,0.0,0.0],[-0.05,0.99,0.0,0.0,0.0],[0.0,0.0,0.98,0.1,0.0],[0.0,0.0,-0.1,0.98,0.0],[0.0,0.0,0.0,0.0,1.0]]}}]"}
{"func_name": "pyscf_cc_ccsd_get_d1_diagnostic", "func_desc": "Compute the D1 diagnostic for CCSD single-excitation amplitudes as defined in Janssen et al., Chem. Phys. Lett. 290 (1998) 423. This diagnostic extracts a single non-negative scalar that quantifies the magnitude of single-excitation amplitudes and is used within the PySCF coupled-cluster (CCSD) context to assess the importance of single excitations and, indirectly, the potential multireference character of a wavefunction.", "tools": [{"function": {"description": "Compute the D1 diagnostic for CCSD single-excitation amplitudes as defined in Janssen et al., Chem. Phys. Lett. 290 (1998) 423. This diagnostic extracts a single non-negative scalar that quantifies the magnitude of single-excitation amplitudes and is used within the PySCF coupled-cluster (CCSD) context to assess the importance of single excitations and, indirectly, the potential multireference character of a wavefunction.\n", "name": "pyscf_cc_ccsd_get_d1_diagnostic", "parameters": {"properties": {"t1": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array containing CCSD single-excitation amplitudes. In the typical CCSD convention used in PySCF, t1 is a 2-D array t1[i,a] with occupied-index i and virtual-index a (shape commonly (nocc, nvir)). The function treats the array as numeric real (or complex) amplitude data and computes two symmetric matrices by contracting t1 with itself: one in the occupied index space (einsum 'ia,ja->ij') and one in the virtual index space (einsum 'ia,ib->ab'). These matrices are then diagonalized and their largest eigenvalue magnitudes are converted to the diagnostic value. The parameter has no default; the function will raise NumPy errors if t1 is not a compatible ndarray (for example, wrong number of dimensions or non-numeric entries will propagate exceptions from numpy.einsum or numpy.linalg.eigh).", "default": ""}}, "required": ["t1"], "type": "any"}}, "type": "function"}], "query": "We’re doing CCSD QC on a small water/minimal-basis benchmark where the singles-amplitude dumps are coming from mixed logging backends. Each dump is supposed to represent a single CCSD replicate with 2 occupied × 4 virtual t1 amplitudes, but some records are malformed (wrong shape) and should be treated as ingestion artifacts.\n\nGiven the following raw t1 payloads, compute the Janssen et al. D1 diagnostic (PySCF CCSD definition) only for those entries that can be interpreted as a complete 2×4 t1 tensor (either as a nested list with 2 rows of length 4, or as a dict keyed by row index containing exactly two length-4 rows). Ignore anything that fails this structural validity check.\n\nRaw payloads:\n1) Replicate A: {\"0\": [0.01, -0.02, 0.00, 0.03], \"1\": [0.05, 0.01, -0.01, 0.02]}\n2) Replicate B: [[0.015, -0.032, 0.005, 0.020], [0.010, 0.025, -0.018, 0.007]]\n3) Replicate C (suspicious export): [[0.002, -0.001, 0.000, 0.004]]\n4) Replicate D (truncated row): {\"0\": [0.01, -0.02, 0.00, 0.03], \"1\": [0.05, 0.01, -0.01]}\n\nReturn the D1 scalar for each structurally valid replicate.", "answers": "[{\"name\":\"pyscf_cc_ccsd_get_d1_diagnostic\",\"arguments\":{\"t1\":{\"0\":[0.01,-0.02,0.0,0.03],\"1\":[0.05,0.01,-0.01,0.02]}}},{\"name\":\"pyscf_cc_ccsd_get_d1_diagnostic\",\"arguments\":{\"t1\":[[0.015,-0.032,0.005,0.02],[0.01,0.025,-0.018,0.007]]}}]"}
{"func_name": "pyscf_cc_ccsd_get_d2_diagnostic", "func_desc": "Compute the D2 diagnostic for CCSD T2 amplitudes.\n    \n    This function implements the D2 diagnostic defined in Nielsen et al., Chem. Phys. Lett. 310 (1999) 568. It is intended for use in the PySCF coupled-cluster (ccsd) workflow to quantify the magnitude of double-excitation amplitudes (T2) coming from a restricted closed-shell CCSD calculation. The diagnostic is computed as the maximum of two norms derived from the largest eigenvalues of Hermitian matrices formed by contracting the supplied T2 tensor with itself over occupied or virtual index pairs. Larger values of the D2 diagnostic indicate stronger double-excitation character and can signal increasing nondynamic (multireference) correlation where single-reference CCSD may become unreliable. Note: this diagnostic is currently only defined in the literature and in this implementation for restricted closed-shell systems; it should not be used for unrestricted or general open-shell T2 tensors.", "tools": [{"function": {"description": "Compute the D2 diagnostic for CCSD T2 amplitudes.\n\nThis function implements the D2 diagnostic defined in Nielsen et al., Chem. Phys. Lett. 310 (1999) 568. It is intended for use in the PySCF coupled-cluster (ccsd) workflow to quantify the magnitude of double-excitation amplitudes (T2) coming from a restricted closed-shell CCSD calculation. The diagnostic is computed as the maximum of two norms derived from the largest eigenvalues of Hermitian matrices formed by contracting the supplied T2 tensor with itself over occupied or virtual index pairs. Larger values of the D2 diagnostic indicate stronger double-excitation character and can signal increasing nondynamic (multireference) correlation where single-reference CCSD may become unreliable. Note: this diagnostic is currently only defined in the literature and in this implementation for restricted closed-shell systems; it should not be used for unrestricted or general open-shell T2 tensors.", "name": "pyscf_cc_ccsd_get_d2_diagnostic", "parameters": {"properties": {"t2": {"type": "array", "items": {"type": "float"}, "description": "T2 amplitude tensor from a restricted closed-shell CCSD calculation. The array is expected to have the standard occupied-occupied-virtual-virtual ordering with shape (nocc, nocc, nvir, nvir), corresponding to indices (i, j, a, b) where i and j label occupied orbitals and a and b label virtual orbitals. The function uses numpy.einsum contractions \"ikab,jkab->ij\" and \"ijac,ijbc->ab\" to build two Hermitian matrices from t2 and then computes their eigenvalues with numpy.linalg.eigh. If t2 is not a numpy.ndarray, has incompatible shape, contains non-finite values (NaN or infinite), or does not follow the expected index ordering, the underlying einsum or eigh calls will raise a ValueError or produce unreliable results.", "default": ""}}, "required": ["t2"], "type": "any"}}, "type": "function"}], "query": "We’re curating T2 amplitude tensors coming out of a restricted closed-shell CCSD regression suite before computing the Nielsen et al. (1999) D2 diagnostic. The raw batch below may include runs with inconsistent tensor shapes (e.g., broken symmetry / mis-indexed checkpoints) that should not be advanced to D2 scoring. From the provided candidate T2 tensors, compute the D2 diagnostic only for those replicates whose T2 is a well-formed rank-4 tensor with i and j dimensions equal to 2 and whose a and b dimensions are equal (square virtual blocks), consistent with a closed-shell 2-occupied testbed. Candidate replicates: (A) T2(i,j,a,b)=[[[[0.0,0.02],[0.02,0.0]],[[0.01,0.0],[0.0,0.015]]],[[[0.01,0.0],[0.0,0.015]],[[0.0,0.03],[0.03,0.0]]]]; (B) T2(i,j,a,b)=[[[[0.0,0.01,-0.02],[0.015,0.0,0.005],[-0.01,0.02,0.0]],[[0.005,-0.01,0.0],[0.0,0.012,-0.004],[0.008,0.0,-0.006]]],[[[0.005,-0.01,0.0],[0.0,0.012,-0.004],[0.008,0.0,-0.006]],[[0.0,0.02,-0.01],[-0.015,0.0,0.007],[0.003,-0.005,0.0]]]]; (C) another replicate specified by blocks t2[0,0,:,:]=[[0.00,0.02],[0.02,0.00]], t2[0,1,:,:]=[[0.03,0.01],[-0.01,0.04]], t2[1,0,:,:]=[[0.03,-0.01],[0.01,0.04]], t2[1,1,:,:]=[[0.00,0.05],[0.05,0.00]]. Return the D2 values for the retained replicates for downstream multireference-risk flagging.", "answers": "[{\"name\":\"pyscf_cc_ccsd_get_d2_diagnostic\",\"arguments\":{\"t2\":[[[[0.0,0.02],[0.02,0.0]],[[0.01,0.0],[0.0,0.015]]],[[[0.01,0.0],[0.0,0.015]],[[0.0,0.03],[0.03,0.0]]]]}},{\"name\":\"pyscf_cc_ccsd_get_d2_diagnostic\",\"arguments\":{\"t2\":[[[[0.0,0.02],[0.02,0.0]],[[0.03,0.01],[-0.01,0.04]]],[[[0.03,-0.01],[0.01,0.04]],[[0.0,0.05],[0.05,0.0]]]]}}]"}
{"func_name": "pyscf_cc_ccsd_get_t1_diagnostic", "func_desc": "Returns the t1 amplitude norm normalized by the number of correlated electrons, commonly called the T1 diagnostic in coupled-cluster singles and doubles (CCSD) calculations. In the PySCF quantum-chemistry framework this function is used to quantify the overall magnitude of single-excitation amplitudes produced by a CCSD calculation and to provide a compact, dimensionless measure that helps assess the degree to which a system departs from a single-reference description.", "tools": [{"function": {"description": "Returns the t1 amplitude norm normalized by the number of correlated electrons, commonly called the T1 diagnostic in coupled-cluster singles and doubles (CCSD) calculations. In the PySCF quantum-chemistry framework this function is used to quantify the overall magnitude of single-excitation amplitudes produced by a CCSD calculation and to provide a compact, dimensionless measure that helps assess the degree to which a system departs from a single-reference description.\n", "name": "pyscf_cc_ccsd_get_t1_diagnostic", "parameters": {"properties": {"t1": {"type": "array", "items": {"type": "float"}, "description": "Array of CCSD single-excitation amplitudes (t_i^a) as produced/used by PySCF CCSD routines. The function infers the number of correlated occupied orbitals from t1.shape[0] and therefore treats the first array dimension as the occupied-orbital index count. Practically, t1 contains complex or real numerical amplitude values; entries are squared and summed by numpy.linalg.norm to compute the amplitude magnitude. The implementation assumes closed-shell pairing when estimating total correlated electrons via nelectron = 2 * t1.shape[0]; if t1 has an unexpected shape or zero size this will lead to errors (IndexError or ZeroDivisionError) or meaningless results.", "default": ""}}, "required": ["t1"], "type": "any"}}, "type": "function"}], "query": "We’re triaging CCSD single-excitation amplitude blocks coming from a mixed QC run log where some jobs are not usable as closed-shell single-reference benchmarks. Each candidate provides a t1 matrix (occupied×virtual). Treat a block as eligible for the closed-shell QC panel only if it is strictly 2D, has no missing values, and its occupied and virtual dimensions are both even (to be consistent with a closed-shell electron-pair picture). For every eligible block, compute the standard PySCF CCSD T1 diagnostic using the provided t1.\n\nRaw t1 blocks:\n- run_id=pilot_A: t1 = [[0.012, -0.004, 0.0], [0.006, 0.009, -0.003]]\n- run_id=prod_H2O: t1 = [[0.0123, -0.0045, 0.0008, 0.0031, -0.0022, 0.0015], [-0.0067, 0.0099, -0.0011, 0.0024, 0.0005, -0.0033], [0.0042, -0.0028, 0.0075, -0.0009, 0.0037, -0.0018], [-0.0036, 0.0019, -0.0025, 0.0061, -0.0012, 0.0029]]", "answers": "[{\"name\":\"pyscf_cc_ccsd_get_t1_diagnostic\",\"arguments\":{\"t1\":[[0.0123,-0.0045,0.0008,0.0031,-0.0022,0.0015],[-0.0067,0.0099,-0.0011,0.0024,0.0005,-0.0033],[0.0042,-0.0028,0.0075,-0.0009,0.0037,-0.0018],[-0.0036,0.0019,-0.0025,0.0061,-0.0012,0.0029]]}}]"}
{"func_name": "pyscf_cc_eom_uccsd_spin2spatial_eomsf", "func_desc": "Convert EOMEE spin-orbital R1/R2 amplitudes into spin-flip EOMEE spatial-orbital R1/R2 blocks.\n    \n    This function is used in the EOM-UCCSD code path of the PySCF quantum chemistry package to transform excitation (R1) or double-excitation (R2) amplitudes that are expressed in a spin-orbital basis into spatial-orbital blocks arranged for spin-flip EOM calculations. The conversion groups amplitude elements according to the spin (alpha/beta) of the occupied and virtual orbitals defined by orbspin, returning only the spin-flip relevant blocks. It is a pure transformation with no side effects on inputs.", "tools": [{"function": {"description": "Convert EOMEE spin-orbital R1/R2 amplitudes into spin-flip EOMEE spatial-orbital R1/R2 blocks.\n\nThis function is used in the EOM-UCCSD code path of the PySCF quantum chemistry package to transform excitation (R1) or double-excitation (R2) amplitudes that are expressed in a spin-orbital basis into spatial-orbital blocks arranged for spin-flip EOM calculations. The conversion groups amplitude elements according to the spin (alpha/beta) of the occupied and virtual orbitals defined by orbspin, returning only the spin-flip relevant blocks. It is a pure transformation with no side effects on inputs.", "name": "pyscf_cc_eom_uccsd_spin2spatial_eomsf", "parameters": {"properties": {"rx": {"type": "array", "items": {"type": "float"}, "description": "The input excitation amplitudes in spin-orbital representation. For single-excitation amplitudes (R1) rx must be a 2-dimensional array with shape (nocc, nvir) where nocc is the number of occupied spin-orbitals and nvir the number of virtual spin-orbitals. For double-excitation amplitudes (R2) rx must be an array whose middle two dimensions are (nocc, nvir) so that the code reshapes it to a 2D array of shape (nocc**2, nvir**2); in typical use R2 is provided as a 4-dimensional array with shape (nocc, nocc, nvir, nvir). The function slices rx and reshapes blocks; if rx has unexpected dimensionality or incompatible shape a ValueError or IndexError may be raised during reshaping or indexing.", "default": ""}, "orbspin": {"type": "array", "items": {"type": "float"}, "description": "1-dimensional array labeling the spin of each spatial orbital. Elements are integer codes where 0 denotes an alpha (↑) spin orbital and 1 denotes a beta (↓) spin orbital. The array must have at least nocc + nvir entries, where nocc and nvir are derived from rx as described above. The first nocc entries are interpreted as the spins of occupied orbitals and the remaining entries as the spins of virtual orbitals. If orbspin does not follow this convention (wrong length or values other than 0/1), the returned blocks will be incorrect or indexing will fail.", "default": ""}}, "required": ["rx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "We’re QA-ing a spin-flip EOM-UCCSD transformation stage against messy open-shell inputs coming from two different upstream generators. Each payload contains a candidate amplitude tensor `rx` (either R2 with 4 indices or R1 with 2 indices) and an `orbspin` vector (0=alpha, 1=beta) for all spin-orbitals. In this validation pass, treat any payload as admissible only if its `orbspin` contains at least one alpha and one beta label (otherwise it can’t produce spin-flip blocks). For each admissible payload, convert the provided spin-orbital amplitudes into the spin-flip EOMEE spatial-orbital R1/R2 blocks (keeping only the spin-flip-relevant blocks).\n\nRaw payloads:\n1) Candidate doubles (R2) amplitude, nominally with 2 occupied and 2 virtual spin-orbitals: rx shape (2,2,2,2) with rx = [[[[0.10, -0.05],[0.02, 0.00]],[[-0.03, 0.01],[0.04, -0.02]]],[[[0.06, -0.01],[0.00, 0.03]],[[-0.02, 0.05],[0.01, -0.04]]]] and orbspin = [0, 1, 0, 1].\n2) Candidate singles (R1) amplitude, nominally with 2 occupied and 3 virtual spin-orbitals: rx shape (2,3) with rx = [[0.10, -0.05, 0.02],[0.03, 0.07, -0.01]] and orbspin = [0, 1, 0, 1, 1].\n\nReturn the transformed spin-flip spatial-orbital blocks for each admissible payload for downstream cross-checking.", "answers": "[{\"name\":\"pyscf_cc_eom_uccsd_spin2spatial_eomsf\",\"arguments\":{\"rx\":[[[[0.1,-0.05],[0.02,0.0]],[[-0.03,0.01],[0.04,-0.02]]],[[[0.06,-0.01],[0.0,0.03]],[[-0.02,0.05],[0.01,-0.04]]]],\"orbspin\":[0,1,0,1]}},{\"name\":\"pyscf_cc_eom_uccsd_spin2spatial_eomsf\",\"arguments\":{\"rx\":[[0.1,-0.05,0.02],[0.03,0.07,-0.01]],\"orbspin\":[0,1,0,1,1]}}]"}
{"func_name": "pyscf_cc_eom_uccsd_spin2spatial_ip", "func_desc": "Convert EOM-IP spin-orbital R1/R2 amplitudes to spatial-orbital R1/R2 blocks used in EOM-UCCSD.\n    \n    This function is used within the pyscf.cc.eom_uccsd module of the PySCF quantum chemistry framework to translate amplitude tensors expressed in a spin-orbital basis (where alpha/beta spin are interleaved) into spatial-orbital blocks separated by spin (alpha/beta). The conversion is required when working with unrestricted coupled-cluster EOM for ionization potentials (EOM-IP) where intermediate code or algorithms expect spatial-orbital block structures: for a 1-D input it extracts spatial occupied single-hole (R1) amplitudes for alpha and beta spins; for a higher-dimensional input it reshapes and partitions two-hole/one-particle (R2) spin-orbital amplitudes into the four spin blocks (alpha-alpha, beta-alpha, alpha-beta, beta-beta) for occupied-occupied -> virtual mapping. The function does not modify its inputs and returns newly allocated numpy arrays.", "tools": [{"function": {"description": "Convert EOM-IP spin-orbital R1/R2 amplitudes to spatial-orbital R1/R2 blocks used in EOM-UCCSD.\n\nThis function is used within the pyscf.cc.eom_uccsd module of the PySCF quantum chemistry framework to translate amplitude tensors expressed in a spin-orbital basis (where alpha/beta spin are interleaved) into spatial-orbital blocks separated by spin (alpha/beta). The conversion is required when working with unrestricted coupled-cluster EOM for ionization potentials (EOM-IP) where intermediate code or algorithms expect spatial-orbital block structures: for a 1-D input it extracts spatial occupied single-hole (R1) amplitudes for alpha and beta spins; for a higher-dimensional input it reshapes and partitions two-hole/one-particle (R2) spin-orbital amplitudes into the four spin blocks (alpha-alpha, beta-alpha, alpha-beta, beta-beta) for occupied-occupied -> virtual mapping. The function does not modify its inputs and returns newly allocated numpy arrays.", "name": "pyscf_cc_eom_uccsd_spin2spatial_ip", "parameters": {"properties": {"rx": {"type": "array", "items": {"type": "float"}, "description": "Spin-orbital amplitudes to be converted. If rx.ndim == 1, rx is interpreted as a 1-D R1 vector of length equal to the total number of occupied spin-orbitals (nocc_total), and the function returns the spatial R1 blocks for alpha and beta occupied orbitals. If rx.ndim != 1, the function expects rx to be an array whose second and third dimensions are (nocc_total, nvir_total) so that nocc, nvir = rx.shape[1:] are used; in this case rx is treated as spin-orbital R2 amplitudes packed as a 2-D (nocc_total**2, nvir_total) block after a reshape. The caller is responsible for providing rx in the spin-orbital ordering consistent with orbspin (occupied indices first, then virtual indices). The function returns newly created numpy arrays and does not alter rx in place.", "default": ""}, "orbspin": {"type": "array", "items": {"type": "float"}, "description": "1-D integer array that encodes the spin label for each spatial orbital in the ordering expected by rx. The array must contain entries 0 for alpha and 1 for beta and must be ordered so that the first nocc entries correspond to the nocc occupied spin-orbitals and the remaining entries correspond to the nvir virtual spin-orbitals (i.e., orbspin[:nocc] are occupied spins and orbspin[nocc:] are virtual spins, where nocc and nvir are inferred from rx when rx.ndim != 1). orbspin is used to partition indices into alpha and beta sets for occupied and virtual spaces. If orbspin length or contents do not match the shapes and conventions expected from rx, indexing or reshape operations will raise an error (e.g., IndexError or ValueError).", "default": ""}}, "required": ["rx", "orbspin"], "type": "any"}}, "type": "function"}], "query": "In an EOM-IP(UCCSD) workflow, I’m aggregating raw single-hole (R1) amplitude vectors coming from multiple geometry points, but the spin annotation quality is inconsistent. Use the following rule-driven sieve before converting to spatial-orbital R1 blocks: treat an entry as convertible only if (i) the amplitude tensor is 1D (R1-like), (ii) its length matches the number of occupied spin labels available (ignore any extra spin labels that correspond to virtual space), and (iii) the occupied spin-label subsequence is physically consistent with an interleaved unrestricted ordering (alpha/beta alternating), i.e., it contains equal counts of 0 and 1 and strictly alternates. For each convertible dataset, run the spin-orbital to spatial-orbital conversion using the occupied orbspin labels (plus any trailing virtual orbspin labels if present).\n\nRaw datasets:\n- Dataset A (debug snapshot): rx = [0.12, -0.08, 0.05, 0.01, -0.03, 0.09] with orbspin = [0,1,0,1,0,1,0,1,0,1] (occupied labels first, then virtual labels).\n- Dataset B (production open-shell): rx = [0.12, -0.05, 0.30, 0.10, -0.22, 0.08] with orbspin = [0,1,0,1,0,1] (occupied only).\n- Dataset C (messy export): rx = [0.07, -0.02, 0.11, -0.04, 0.09, -0.01] with orbspin = [0,0,1,1,0,1] (spin labels not strictly alternating).\nReturn the function calls needed for only the datasets that pass the criteria above.", "answers": "[{\"name\":\"pyscf_cc_eom_uccsd_spin2spatial_ip\",\"arguments\":{\"rx\":[0.12,-0.08,0.05,0.01,-0.03,0.09],\"orbspin\":[0,1,0,1,0,1,0,1,0,1]}},{\"name\":\"pyscf_cc_eom_uccsd_spin2spatial_ip\",\"arguments\":{\"rx\":[0.12,-0.05,0.3,0.1,-0.22,0.08],\"orbspin\":[0,1,0,1,0,1]}}]"}
{"func_name": "pyscf_ci_cisd_t1strs", "func_desc": "pyscf.ci.cisd.t1strs: Compute the FCI string addresses and fermionic sign factors needed for CIS single-excitation amplitudes and for converting CI coefficients between the physics vacuum and the Hartree–Fock (HF) vacuum.", "tools": [{"function": {"description": "pyscf.ci.cisd.t1strs: Compute the FCI string addresses and fermionic sign factors needed for CIS single-excitation amplitudes and for converting CI coefficients between the physics vacuum and the Hartree–Fock (HF) vacuum.\n", "name": "pyscf_ci_cisd_t1strs", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of orbitals used to construct the full configuration-interaction (FCI) string space. In the context of PySCF CI/CIS routines, norb defines the size of the single-particle basis from which occupation bit-strings (determinants) are formed; it therefore controls the combinatorial size of the FCI string list that this routine enumerates. The argument must be an integer; non-integer types will propagate a TypeError from internal routines.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the reference determinant used to define occupied and virtual orbitals. In CIS and related CI modules, nelec determines which determinants are considered as single excitations relative to the HF reference. This argument must be an integer; invalid or inconsistent electron counts (for example, values that cannot be accommodated by the underlying tn_addrs_signs implementation) will result in an exception being raised.", "default": ""}}, "required": ["norb", "nelec"], "type": "any"}}, "type": "function"}], "query": "We’re extending our closed-shell HF CIS/CISD benchmarking to a heterogeneous set of active spaces collected from different upstream integral-build steps, where some entries are in a spin-orbital basis already and others are specified in spatial orbitals. Each cohort below reports `basis` (either `spin` or `spatial`), an orbital count `n` in that basis, and an electron count `nelec`.\n\nRaw cohorts:\n1) basis=spin, n=12, nelec=6\n2) basis=spin, n=6, nelec=6\n3) basis=spatial, n=10, nelec=6\n\nFor each cohort that is compatible with a closed-shell HF reference in the *spin-orbital* representation, generate the FCI string address mappings and fermionic sign factors needed for CIS single-excitation amplitudes and for converting CI coefficients between the physics vacuum and the HF vacuum. Interpret `n` as `norb` directly when `basis=spin`; when `basis=spatial`, convert to a spin-orbital count before calling the routine.", "answers": "[{\"name\":\"pyscf_ci_cisd_t1strs\",\"arguments\":{\"norb\":12,\"nelec\":6}},{\"name\":\"pyscf_ci_cisd_t1strs\",\"arguments\":{\"norb\":6,\"nelec\":6}},{\"name\":\"pyscf_ci_cisd_t1strs\",\"arguments\":{\"norb\":20,\"nelec\":6}}]"}
{"func_name": "pyscf_dft_gen_grid_stratmann", "func_desc": "pyscf.dft.gen_grid.stratmann computes the Stratmann–Scuseria–Frisch switching function (CPL, 257, 213 (1996); DOI:10.1016/0009-2614(96)00600-8) used in PySCF density-functional-theory (DFT) grid construction and atomic partitioning. The implementation follows Eq. 14 of the reference with the internal parameter a = 0.64 and evaluates a degree-7 odd polynomial inside the interval [-a, a], while clamping values to ±1 outside that interval. In the context of pyscf.dft.gen_grid, this function provides the elementwise scalar mapping used to produce smooth, compact switch values that appear in weighted numerical integration over molecular DFT grids.", "tools": [{"function": {"description": "pyscf.dft.gen_grid.stratmann computes the Stratmann–Scuseria–Frisch switching function (CPL, 257, 213 (1996); DOI:10.1016/0009-2614(96)00600-8) used in PySCF density-functional-theory (DFT) grid construction and atomic partitioning. The implementation follows Eq. 14 of the reference with the internal parameter a = 0.64 and evaluates a degree-7 odd polynomial inside the interval [-a, a], while clamping values to ±1 outside that interval. In the context of pyscf.dft.gen_grid, this function provides the elementwise scalar mapping used to produce smooth, compact switch values that appear in weighted numerical integration over molecular DFT grids.\n", "name": "pyscf_dft_gen_grid_stratmann", "parameters": {"properties": {"g": {"type": "array", "items": {"type": "float"}, "description": "Input array of scalar arguments to the Stratmann switching function. In DFT grid generation this typically represents a normalized coordinate or difference used to partition space between atoms for numerical integration. The function treats g as a numeric array (numpy.ndarray) and computes the switching value elementwise. The input array's shape is preserved in the output. The implementation calls numpy.asarray(g) internally, so invalid types that cannot be converted to a numpy.ndarray will raise the same exceptions numpy.asarray raises.", "default": ""}}, "required": ["g"], "type": "any"}}, "type": "function"}], "query": "In our DFT grid/partitioning QA, we need a realism-driven validation of the Stratmann–Scuseria–Frisch switch on a mixed-quality stream of normalized partition coordinates g collected from multiple atom-pair partitions. The stream contains both physically plausible reduced coordinates and numerical artifacts from normalization/overflow. Apply a data-sieve rule before evaluation: only evaluate the switching function for coordinates that are finite and fall within the physically meaningful reduced-coordinate envelope |g| ≤ 1. From the raw set g_raw = [-1.2, -1.0, -0.8000000000000002, -0.64, -0.30000000000000004, -0.0, 0.25, 0.6, 0.64, 0.8, 1.0, 1.1, float('inf'), float('nan')], run two protocol branches: (1) an interior-resolution cohort that focuses on the polynomial region by evaluating only the retained points that also satisfy |g| < a, and (2) a boundary/clamp cohort that evaluates the retained points that satisfy |g| ≥ a (including exactly ±a and the saturation at ±1). Return the elementwise switching values for each cohort in the original numeric order within that cohort.", "answers": "[{\"name\":\"pyscf_dft_gen_grid_stratmann\",\"arguments\":{\"g\":[-0.30000000000000004,-0.0,0.25,0.6]}},{\"name\":\"pyscf_dft_gen_grid_stratmann\",\"arguments\":{\"g\":[-1.0,-0.8000000000000002,-0.64,0.64,0.8,1.0]}}]"}
{"func_name": "pyscf_dft_gen_grid_treutler_prune", "func_desc": "Treutler-Ahlrichs pruning rule to assign angular-quadrature sizes to radial grid points for a given nucleus.\n    \n    This function implements the Treutler–Ahlrichs pruning heuristic used in PySCF's grid generation for density-functional calculations (see PySCF README and gen_grid usage). Given the radial grid coordinates along a radial axis for one nucleus, it partitions the radial points into three contiguous groups (first third, middle third, remaining third) and assigns a Lebedev angular-grid size to each radial point: the inner group is assigned 14 angular points (corresponding to Lebedev order l=5), the middle group 50 angular points (l=11), and the outer group is assigned the provided n_ang value (maximum angular resolution). The returned integer array is intended to be consumed by higher-level grid construction routines to determine how many angular quadrature points are used at each radial shell for this nucleus.", "tools": [{"function": {"description": "Treutler-Ahlrichs pruning rule to assign angular-quadrature sizes to radial grid points for a given nucleus.\n\nThis function implements the Treutler–Ahlrichs pruning heuristic used in PySCF's grid generation for density-functional calculations (see PySCF README and gen_grid usage). Given the radial grid coordinates along a radial axis for one nucleus, it partitions the radial points into three contiguous groups (first third, middle third, remaining third) and assigns a Lebedev angular-grid size to each radial point: the inner group is assigned 14 angular points (corresponding to Lebedev order l=5), the middle group 50 angular points (l=11), and the outer group is assigned the provided n_ang value (maximum angular resolution). The returned integer array is intended to be consumed by higher-level grid construction routines to determine how many angular quadrature points are used at each radial shell for this nucleus.", "name": "pyscf_dft_gen_grid_treutler_prune", "parameters": {"properties": {"nuc": {"type": "integer", "description": "Nuclear charge of the nucleus for which the pruning is computed. In practice this identifies the atomic center in multi-center grid generation and is part of the gen_grid API; this integer is accepted by the function but is not used in the current implementation of the Treutler–Ahlrichs mapping (it is kept for interface consistency with other pruning schemes).", "default": ""}, "rads": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of radial grid coordinates along the radial axis for the nucleus. The length of rads (nr) determines the number of radial shells; the function returns an integer array of length nr giving the angular grid size for each radial shell. rads must be a sequence or numpy array for which len(rads) is defined; the numerical values of rads are not inspected by this implementation, only their ordering/length matters.", "default": ""}, "n_ang": {"type": "integer", "description": "Maximum number of angular grid points to assign to the outer radial shells (the third of rads with largest radii). This integer is used verbatim for the last partition of radial points and represents the highest angular resolution requested by the caller.", "default": ""}, "radii": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional array of atomic radii or related per-nucleus radius data accepted for API compatibility with other pruning routines. Default is None. In this function, radii is ignored (no side effects) and provided only to maintain a stable gen_grid function signature; supplying radii will not change the returned result.", "default": null}}, "required": ["nuc", "rads", "n_ang"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our atom-centered DFT grid builder on messy radial-shell logs from two independent nuclei. Each nucleus comes with a raw list of candidate radial coordinates (bohr) that may include non-physical entries (≤0), exact duplicates (from merge artifacts), and unsorted ordering (from concatenated checkpoint segments). For each nucleus: (i) retain only physically valid radii (>0), (ii) de-duplicate exact repeats, (iii) sort ascending, then (iv) apply the Treutler–Ahlrichs angular pruning on the cleaned radial list (first third shells → 14, middle third → 50, final third → nucleus-specific maximum n_ang). Use the following raw inputs:\n\nA) chlorine nucleus Z=17, raw rads = [0.02, 0.06, 0.12, 0.25, 0.5, 0.9, 1.6, 2.8, 4.5, 0.0, -0.1, 0.9, 2.8] with outer-shell maximum n_ang = 302.\n\nB) carbon nucleus Z=6, raw rads = [4.00, 0.05, 0.10, 0.20, 0.35, 0.50, 0.75, 1.00, 1.50, 2.00, 2.50, 3.00, 0.20, -3.0] with outer-shell maximum n_ang = 194.\n\nReturn the assigned angular grid size for every retained radial shell for each nucleus.", "answers": "[{\"name\":\"pyscf_dft_gen_grid_treutler_prune\",\"arguments\":{\"nuc\":17,\"rads\":[0.02,0.06,0.12,0.25,0.5,0.9,1.6,2.8,4.5],\"n_ang\":302}},{\"name\":\"pyscf_dft_gen_grid_treutler_prune\",\"arguments\":{\"nuc\":6,\"rads\":[0.05,0.1,0.2,0.35,0.5,0.75,1.0,1.5,2.0,2.5,3.0,4.0],\"n_ang\":194}}]"}
{"func_name": "pyscf_dft_libxc_is_gga", "func_desc": "pyscf.dft.libxc.is_gga: Determine whether a given exchange-correlation functional identifier corresponds to a Generalized Gradient Approximation (GGA) functional.\n    \n    This function is used in the PySCF density-functional subsystem to classify Libxc/XCFun-style exchange-correlation functionals by their type. In practical DFT workflows within PySCF, knowing that a functional is a GGA signals that the functional depends on the electron density and its gradient (∇ρ) but not higher-order derivatives; this information is used to choose integration grids, to enable gradient-dependent terms in numerical evaluation, and to select appropriate functional evaluation code paths. The implementation delegates the classification to the helper function xc_type(xc_code) and performs an exact string comparison against the category name 'GGA' as returned by xc_type. The comparison is therefore sensitive to the exact output of xc_type and to its case and spelling.", "tools": [{"function": {"description": "pyscf.dft.libxc.is_gga: Determine whether a given exchange-correlation functional identifier corresponds to a Generalized Gradient Approximation (GGA) functional.\n\nThis function is used in the PySCF density-functional subsystem to classify Libxc/XCFun-style exchange-correlation functionals by their type. In practical DFT workflows within PySCF, knowing that a functional is a GGA signals that the functional depends on the electron density and its gradient (∇ρ) but not higher-order derivatives; this information is used to choose integration grids, to enable gradient-dependent terms in numerical evaluation, and to select appropriate functional evaluation code paths. The implementation delegates the classification to the helper function xc_type(xc_code) and performs an exact string comparison against the category name 'GGA' as returned by xc_type. The comparison is therefore sensitive to the exact output of xc_type and to its case and spelling.", "name": "pyscf_dft_libxc_is_gga", "parameters": {"properties": {"xc_code": {"type": "string", "description": "Exchange-correlation functional identifier string as accepted by PySCF/Libxc (for example a Libxc short name or numeric code represented as a string). This argument specifies which functional to classify. It must be a Python str because the function passes it to xc_type(xc_code) for classification. There are no defaults; the caller must supply a valid identifier string.", "default": ""}}, "required": ["xc_code"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed set of PySCF geometry-optimization templates coming from multiple labs. Each template specifies an XC label, but only those that resolve to a gradient-dependent functional should be routed to the “GGA grid/eval” code path in our workflow manager. Apply this rule: for each template, first normalize the XC label by stripping leading/trailing whitespace and uppercasing it; then classify it with `pyscf.dft.libxc.is_gga`. Only run the classifier on templates whose normalized XC label either (a) contains an explicit Libxc-style exchange/correlation decomposition using a '+' (composite XC), or (b) is a short alias of length ≤ 3 (these are our legacy shorthands in the template system). Use the following raw template XC fields exactly as recorded: ['  GGA_X_PBE+GGA_C_PBE  ', 'PBE', 'pbe', '  LDA  ', 'MGGA_X_SCAN+MGGA_C_SCAN'].", "answers": "[{\"name\":\"pyscf_dft_libxc_is_gga\",\"arguments\":{\"xc_code\":\"GGA_X_PBE+GGA_C_PBE\"}},{\"name\":\"pyscf_dft_libxc_is_gga\",\"arguments\":{\"xc_code\":\"PBE\"}},{\"name\":\"pyscf_dft_libxc_is_gga\",\"arguments\":{\"xc_code\":\"PBE\"}}]"}
{"func_name": "pyscf_dft_libxc_parse_xc", "func_desc": "pyscf.dft.libxc.parse_xc parses and decodes an XC (exchange–correlation) functional description string used by PySCF to interface with external functional libraries such as LibXC. The function translates a compact, human-editable description (for example \".8*LDA+.2*B86,VWN\" or \"B3LYP\") into a machine-readable representation used by PySCF to assemble exchange and correlation contributions, to determine exact-exchange (HF) fractions, and to configure range-separated hybrids (RSH/SR_HF/LR_HF). This parser is used in density functional calculations within PySCF to map user-specified functional formulas to libxc numeric identifiers and fractional coefficients and to extract hybrid parameters required by the SCF and DFT routines.", "tools": [{"function": {"description": "pyscf.dft.libxc.parse_xc parses and decodes an XC (exchange–correlation) functional description string used by PySCF to interface with external functional libraries such as LibXC. The function translates a compact, human-editable description (for example \".8*LDA+.2*B86,VWN\" or \"B3LYP\") into a machine-readable representation used by PySCF to assemble exchange and correlation contributions, to determine exact-exchange (HF) fractions, and to configure range-separated hybrids (RSH/SR_HF/LR_HF). This parser is used in density functional calculations within PySCF to map user-specified functional formulas to libxc numeric identifiers and fractional coefficients and to extract hybrid parameters required by the SCF and DFT routines.\n", "name": "pyscf_dft_libxc_parse_xc", "parameters": {"properties": {"description": {"type": "string", "description": "A one-line, case-insensitive string that describes a linear combination of exchange and correlation functionals and optional scaling factors. The expected form is \"Xpart,Cpart\" where the substring before the comma defines the exchange contribution and the substring after the comma defines the correlation contribution; if the comma is absent, the entire string is interpreted as a compound XC functional (both X and C parts). Valid operators are \"+\", \"-\", and \"*\" (multiplicative scaling); blanks are ignored. Functional names may appear in arbitrary order and can be scaled by a single numeric factor (default factor = 1 when omitted). Special tokens are interpreted: \"HF\" denotes exact (Hartree–Fock) exchange; \"RSH(alpha;beta;omega)\" or \"SR_HF(omega)\" and \"LR_HF(omega)\" denote range-separated-hybrid components and set the omega parameter; numeric libxc IDs (decimal digits) are accepted directly. The parser accepts common shorthand and aliases via XC_ALIAS/XC_CODES and will query libxc through _itrf.xc_functional_get_number for names not found in the local tables. Although the argument is documented as str, the implementation also handles None (returns the default empty hybrid and no functionals), integer numeric IDs (interpreted as a single libxc ID with unit weight), and non-string iterables by joining the iterable into a \"X,C\" description; these alternate inputs are supported because callers in PySCF may pass None, integer, or tuple forms. The parser also recognizes and strips dispersion suffixes \"-D3\" and \"-D4\" by delegating to pyscf.scf.dispersion.parse_dft. Practical significance: callers (users or higher-level PySCF code) supply this string to select and scale libxc functionals for DFT calculations; correct string syntax is required to obtain the intended exchange/correlation mix and hybrid parameters.", "default": ""}}, "required": ["description"], "type": "any"}}, "type": "function"}], "query": "We’re sanity-checking a mixed-quality XC manifest coming from several collaborating groups before running a LibXC-backed PySCF DFT benchmark. Each entry is a human-typed hybrid specification and may contain whitespace/noise or be physically inconsistent. Use `pyscf.dft.libxc.parse_xc` to decode only those candidate XC strings that (a) explicitly define an exchange part and a correlation part separated by a single comma, and (b) yield a non-negative total exact-exchange (HF) fraction that does not exceed 0.50 when the exchange expression is interpreted. From the following raw manifest, parse the entries that pass the above criteria so we can compare HF fraction and the exchange/correlation decomposition:\n\nraw_xc_manifest = [\n  \"0.7*PBE+0.3*HF,PBE+0.25*VWN\",\n  \"0.25*HF + 0.75*PBE, PBE\",\n  \"PBE0\",\n  \"HF,PBE\",\n  \"0.60*HF+0.40*PBE,PBE\"\n]", "answers": "[{\"name\":\"pyscf_dft_libxc_parse_xc\",\"arguments\":{\"description\":\"0.25*HF + 0.75*PBE, PBE\"}},{\"name\":\"pyscf_dft_libxc_parse_xc\",\"arguments\":{\"description\":\"PBE0\"}},{\"name\":\"pyscf_dft_libxc_parse_xc\",\"arguments\":{\"description\":\"HF,PBE\"}}]"}
{"func_name": "pyscf_dft_libxc_parse_xc_name", "func_desc": "Convert the XC (exchange–correlation) functional name to the libxc library internal IDs for the primary exchange and correlation components used by PySCF density-functional calculations.", "tools": [{"function": {"description": "Convert the XC (exchange–correlation) functional name to the libxc library internal IDs for the primary exchange and correlation components used by PySCF density-functional calculations.\n", "name": "pyscf_dft_libxc_parse_xc_name", "parameters": {"properties": {"xc_name": {"type": "string", "description": "Textual specification of the exchange–correlation functional, e.g. the default \"LDA,VWN\". In the PySCF project and its DFT workflows (which delegate functional evaluation to external libraries such as Libxc), this string identifies the functional to be used for electronic structure calculations. The string is expected to follow the conventions accepted by pyscf.dft.libxc.parse_xc (for example comma-separated exchange and correlation names, hybrid/mixed functional syntax, or any name/alias supported by the underlying parser). This function calls parse_xc(xc_name) and extracts the first entry from the parsed exchange list and the first entry from the parsed correlation list. If the input is malformed or names are unrecognized, the underlying parser will raise an exception (for example ValueError or other errors propagated from parse_xc); callers should catch or validate input before invoking this helper. The default \"LDA,VWN\" corresponds to the standard local-density approximation exchange with the VWN correlation functional.", "default": "LDA,VWN"}}, "required": ["xc_name"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our PySCF DFT input generator against a messy set of XC labels collected from multiple sources (hand-edited input decks and autogenerated templates). Treat the following as raw candidates: ['LDA,VWN', 'PBE,PBE', 'PBE,PBE', 'PBE0,PBE', 'LDA,', 'HF,PBE', 'PBE', '  PBE,PBE  ', 'LDA,VWN5', 'B3LYP']. For this benchmark, only accept XC specifications that are (a) strictly two-component 'X,C' forms with exactly one comma, (b) use only alphabetic tokens with no digits, (c) are already canonicalized with no leading/trailing whitespace, and (d) correspond to either an LDA baseline (LDA,VWN) or a GGA reference (PBE,PBE). Parse every qualifying entry as its own record (including duplicates) and report the primary Libxc exchange and correlation IDs that PySCF resolves for each accepted XC string.", "answers": "[{\"name\":\"pyscf_dft_libxc_parse_xc_name\",\"arguments\":{\"xc_name\":\"LDA,VWN\"}},{\"name\":\"pyscf_dft_libxc_parse_xc_name\",\"arguments\":{\"xc_name\":\"PBE,PBE\"}},{\"name\":\"pyscf_dft_libxc_parse_xc_name\",\"arguments\":{\"xc_name\":\"PBE,PBE\"}}]"}
{"func_name": "pyscf_dft_libxc_xc_reference", "func_desc": "pyscf.dft.libxc.xc_reference returns the bibliographic reference(s) associated with a Libxc exchange-correlation (XC) functional identifier. In the PySCF DFT workflow, PySCF delegates evaluation of density functionals to external libraries such as Libxc; this function provides the canonical citation information for the exact XC functional requested, which is required for properly crediting the source of the functional in publications and reports (see README section on citing Libxc).", "tools": [{"function": {"description": "pyscf.dft.libxc.xc_reference returns the bibliographic reference(s) associated with a Libxc exchange-correlation (XC) functional identifier. In the PySCF DFT workflow, PySCF delegates evaluation of density functionals to external libraries such as Libxc; this function provides the canonical citation information for the exact XC functional requested, which is required for properly crediting the source of the functional in publications and reports (see README section on citing Libxc).\n", "name": "pyscf_dft_libxc_xc_reference", "parameters": {"properties": {"xc_code": {"type": "string", "description": "String identifier of the XC functional to query. This is the same identifier accepted by the internal _get_xc lookup (for example symbolic names or Libxc codes passed to PySCF DFT routines). The argument specifies which exchange-correlation functional the caller used (or plans to use) in a calculation so that the corresponding bibliographic reference can be retrieved for citation and reproducibility.", "default": ""}}, "required": ["xc_code"], "type": "any"}}, "type": "function"}], "query": "We’re curating the Methods/Citations section for a PySCF DFT benchmarking dataset where the requested XC identifiers come from heterogeneous provenance (user input, autogenerated templates, and archived run logs). The raw manifest below may include aliases, case/whitespace inconsistencies, composite expressions, and placeholders that should not generate citations.\n\nRaw XC manifest (strings exactly as recorded):\n1) \" B3LYP \"\n2) \"HYB_GGA_XC_PBEH\"\n3) \"b3lyp\"\n4) \"PBE0\"\n5) \"HYB_GGA_XC_PBEH + D3\"\n6) \"\"\n7) \"UNKNOWN\"\n\nFor citation harvesting, generate canonical Libxc bibliographic reference(s) only for entries that correspond to a single, explicit Libxc XC functional identifier after normalizing case and trimming whitespace. If multiple manifest entries normalize to the same canonical identifier, retrieve the reference(s) once per unique canonical identifier. Use Libxc canonical identifiers (e.g., map common alias PBE0 to its Libxc hybrid identifier) when normalization reveals an alias rather than a direct Libxc identifier.", "answers": "[{\"name\":\"pyscf_dft_libxc_xc_reference\",\"arguments\":{\"xc_code\":\"B3LYP\"}},{\"name\":\"pyscf_dft_libxc_xc_reference\",\"arguments\":{\"xc_code\":\"HYB_GGA_XC_PBEH\"}}]"}
{"func_name": "pyscf_dft_xc_deriv_count_combinations", "func_desc": "pyscf.dft.xc_deriv.count_combinations counts combinatorial multiplicities used when enumerating derivative components for exchange–correlation (XC) functionals in density-functional-theory (DFT) code paths. In the PySCF DFT/xc_deriv context this function provides the integer number of distinct multisets (combinations with replacement) that arise when selecting \"order\" items from \"nvar\" independent variables; this count is used to size and index arrays that store unique derivative terms of XC quantities (for example when building higher-order functional derivatives or allocating storage for symmetrized derivative tensors). The original source-level hint for the intent was the expression sum(len(combinations_with_replacement(range(nvar), o) for o in range(order)), and the implementation returns the closed-form combinatorial value via lib.comb(nvar+order, order).", "tools": [{"function": {"description": "pyscf.dft.xc_deriv.count_combinations counts combinatorial multiplicities used when enumerating derivative components for exchange–correlation (XC) functionals in density-functional-theory (DFT) code paths. In the PySCF DFT/xc_deriv context this function provides the integer number of distinct multisets (combinations with replacement) that arise when selecting \"order\" items from \"nvar\" independent variables; this count is used to size and index arrays that store unique derivative terms of XC quantities (for example when building higher-order functional derivatives or allocating storage for symmetrized derivative tensors). The original source-level hint for the intent was the expression sum(len(combinations_with_replacement(range(nvar), o) for o in range(order)), and the implementation returns the closed-form combinatorial value via lib.comb(nvar+order, order).\n", "name": "pyscf_dft_xc_deriv_count_combinations", "parameters": {"properties": {"nvar": {"type": "integer", "description": "The number of independent variables from which items are drawn in the combinatorial enumeration. In PySCF DFT/xc_deriv usage this represents the number of distinct variable components (for example independent density/gradient components or basis-indexed degrees of freedom) that contribute to XC derivative terms. This value is expected to be an integer; negative or non-integer inputs are not valid for the underlying combinatorics and will cause the underlying combinatorial routine (lib.comb) to raise an error.", "default": ""}, "order": {"type": "integer", "description": "The order (non-negative integer) of the multiset selection, i.e., the number of items selected with replacement when forming each combination. In practical PySCF usage this corresponds to the derivative order or the size of the index multiset whose distinct permutations are being counted. Passing larger order increases the returned count combinatorially; non-integer or negative values are invalid and will result in an error from the underlying combinatorial function.", "default": ""}}, "required": ["nvar", "order"], "type": "any"}}, "type": "function"}], "query": "We’re profiling memory footprints for a heterogeneous set of XC derivative builds coming out of a mixed DFT workflow (spin-restricted and spin-polarized jobs, with occasional meta-GGA inputs). Each build record reports an estimated number of independent density-like variables (nvar) and the intended derivative expansion order (order), but some records are malformed due to upstream logging glitches. Use pyscf.dft.xc_deriv.count_combinations to compute the unique multiset multiplicity only for records that are physically meaningful for allocating symmetrized derivative tensors: keep records where nvar is an even positive integer and order is an even integer with 2 ≤ order ≤ 6, and where the resulting allocation would not exceed the largest allocation among the original two benchmark anchors (nvar=4, order=2) and (nvar=6, order=4). Raw build records: [(\"RKS_LDA\", 4, 2), (\"UKS_GGA\", 6, 4), (\"metaGGA_aux\", 5, 4), (\"debug_neg\", -2, 2), (\"overshoot_order\", 6, 8), (\"bad_float\", 4.0, 2), (\"tiny\", 2, 2), (\"hi_nvar_low_order\", 8, 2)]. Return the multiplicities for the records that pass the rules, in the same order as the input records that pass.", "answers": "[{\"name\":\"pyscf_dft_xc_deriv_count_combinations\",\"arguments\":{\"nvar\":4,\"order\":2}},{\"name\":\"pyscf_dft_xc_deriv_count_combinations\",\"arguments\":{\"nvar\":6,\"order\":4}},{\"name\":\"pyscf_dft_xc_deriv_count_combinations\",\"arguments\":{\"nvar\":2,\"order\":2}},{\"name\":\"pyscf_dft_xc_deriv_count_combinations\",\"arguments\":{\"nvar\":8,\"order\":2}}]"}
{"func_name": "pyscf_dft_xc_deriv_ud2ts", "func_desc": "pyscf.dft.xc_deriv.ud2ts converts exchange–correlation (XC) derivative arrays from the spin-up/spin-down (\"u\",\"d\" or \"a\",\"b\") representation to the total-density/spin-density (\"rho\",\"s\") representation used in PySCF's DFT grid-based integrals and response calculations. This conversion is commonly needed when evaluating XC potentials and kernels coming from spin-resolved functional evaluations (external XC libraries) and when downstream routines expect total- and spin-density derivative components. The routine delegates the low-level memory transformation to the compiled helper libdft.VXCud2ts for performance.\n    \n    This function accepts a NumPy array containing XC derivatives in a spin-channel representation and returns a NumPy array of the same shape and dtype containing the corresponding derivatives in the total-density / spin-density representation. The conversion implemented corresponds to the linear map vrho = (va + vb)/2 and vs = (va - vb)/2 applied to the spin channels, applied across the array layout used by PySCF's grid-based XC derivative machinery.", "tools": [{"function": {"description": "pyscf.dft.xc_deriv.ud2ts converts exchange–correlation (XC) derivative arrays from the spin-up/spin-down (\"u\",\"d\" or \"a\",\"b\") representation to the total-density/spin-density (\"rho\",\"s\") representation used in PySCF's DFT grid-based integrals and response calculations. This conversion is commonly needed when evaluating XC potentials and kernels coming from spin-resolved functional evaluations (external XC libraries) and when downstream routines expect total- and spin-density derivative components. The routine delegates the low-level memory transformation to the compiled helper libdft.VXCud2ts for performance.\n\nThis function accepts a NumPy array containing XC derivatives in a spin-channel representation and returns a NumPy array of the same shape and dtype containing the corresponding derivatives in the total-density / spin-density representation. The conversion implemented corresponds to the linear map vrho = (va + vb)/2 and vs = (va - vb)/2 applied to the spin channels, applied across the array layout used by PySCF's grid-based XC derivative machinery.", "name": "pyscf_dft_xc_deriv_ud2ts", "parameters": {"properties": {"v_ud": {"type": "array", "items": {"type": "float"}, "description": "Input array of XC derivatives in the spin-up / spin-down representation. The array must be convertible to a C-contiguous numpy.ndarray (the function calls numpy.asarray with order='C'). The last two dimensions of v_ud are interpreted as (nvar, ngrids), where nvar is the number of variables per spin channel and ngrids is the number of grid points. Any leading dimensions are interpreted in paired groups that represent higher-order derivative structure; the function computes order = v_ud.ndim // 2 and therefore expects v_ud.ndim to be an even integer (for example ndims >= 2 and ndims % 2 == 0). Practically, v_ud is the array produced by PySCF/XC evaluation code that stores, for each grid point and variable, the contributions for the two spin channels; this function re-expresses those contributions as total-density and spin-density components. The dtype and shape of the returned array will match v_ud.", "default": ""}}, "required": ["v_ud"], "type": "any"}}, "type": "function"}], "query": "In our DFT response regression suite, we ingest spin-resolved XC derivative blocks from an external evaluator where the two spin channels may arrive in mixed memory layouts and occasionally contain sentinel contamination from failed grid microbatches.\n\nRun the u/d→(rho,s) harmonization using `pyscf.dft.xc_deriv.ud2ts` only on derivative arrays that are physically admissible (finite-valued everywhere and not containing the external-evaluator sentinel magnitude ≥1e100). Treat each admissible array as an independent replicate.\n\nReplicate A (kernel-derivative smoke test, spin-first layout): construct a C-contiguous `v_ud` with shape (2,2,2) such that `v_ud[0,:,:]=va` and `v_ud[1,:,:]=vb`. Populate it so that at grid point 0, `va=[0.8,0.2]` and `vb=[0.4,0.1]`, and at grid point 1, `va=[1.0,0.3]` and `vb=[0.6,0.2]`. Convert the full tensor.\n\nReplicate B (potential-derivative batch, 2×5): use the provided spin-resolved array (row0=va,row1=vb) `[[0.12, 0.15, 0.1, 0.08, 0.11], [-0.03, -0.02, -0.04, -0.05, -0.01]]` and convert it.\n\nReplicate C (corrupted microbatch, 2×4): the external evaluator returned `[[1e101, 0.2, 0.1, 0.05], [0.0, 0.1, 0.05, 0.02]]` in the same (2,ngrids) layout. Apply the admissibility rule above to decide whether it should be harmonized or skipped.\n\nReturn the list of conversions actually performed (as function calls).", "answers": "[{\"name\":\"pyscf_dft_xc_deriv_ud2ts\",\"arguments\":{\"v_ud\":[[[0.8,1.0],[0.2,0.3]],[[0.4,0.6],[0.1,0.2]]]}},{\"name\":\"pyscf_dft_xc_deriv_ud2ts\",\"arguments\":{\"v_ud\":[[0.12,0.15,0.1,0.08,0.11],[-0.03,-0.02,-0.04,-0.05,-0.01]]}}]"}
{"func_name": "pyscf_dft_xc_utils_format_xc_code", "func_desc": "pyscf.dft.xc.utils.format_xc_code formats and normalizes a user-supplied density-functional exchange–correlation (XC) description string for use inside PySCF. In the PySCF DFT workflow (which delegates functional evaluation to external libraries such as Libxc or XCFun), this function is used to produce a compact, upper-cased, whitespace-free representation of an XC description and to translate range-separated-hybrid (RSH) parameter notation from the common RSH(omega, alpha, beta) form into the internal RSH(alpha; beta; omega) form expected by downstream PySCF parsers and code.\n    \n    This function performs the following concrete operations observed in the source code:\n    1. Removes all ASCII space characters and newline characters from description, then converts the entire string to upper case. This normalization makes functional names and tokens consistent with PySCF’s downstream parsing and with conventions used when citing or delegating to external XC libraries.\n    2. If the token 'RSH' is absent after normalization, returns the normalized string unchanged.\n    3. If one or more 'RSH' fragments are present, splits the string on the substring 'RSH' and for each fragment of the form RSH(omega,alpha,beta) rewrites it as RSH(alpha;beta;omega) by moving the first numeric parameter (omega) to the final position and replacing commas between the three parameters with semicolons in the internal notation. Non-RSH parts of the description are preserved in their normalized form and rejoined with the literal substring 'RSH'.", "tools": [{"function": {"description": "pyscf.dft.xc.utils.format_xc_code formats and normalizes a user-supplied density-functional exchange–correlation (XC) description string for use inside PySCF. In the PySCF DFT workflow (which delegates functional evaluation to external libraries such as Libxc or XCFun), this function is used to produce a compact, upper-cased, whitespace-free representation of an XC description and to translate range-separated-hybrid (RSH) parameter notation from the common RSH(omega, alpha, beta) form into the internal RSH(alpha; beta; omega) form expected by downstream PySCF parsers and code.\n\nThis function performs the following concrete operations observed in the source code:\n1. Removes all ASCII space characters and newline characters from description, then converts the entire string to upper case. This normalization makes functional names and tokens consistent with PySCF’s downstream parsing and with conventions used when citing or delegating to external XC libraries.\n2. If the token 'RSH' is absent after normalization, returns the normalized string unchanged.\n3. If one or more 'RSH' fragments are present, splits the string on the substring 'RSH' and for each fragment of the form RSH(omega,alpha,beta) rewrites it as RSH(alpha;beta;omega) by moving the first numeric parameter (omega) to the final position and replacing commas between the three parameters with semicolons in the internal notation. Non-RSH parts of the description are preserved in their normalized form and rejoined with the literal substring 'RSH'.", "name": "pyscf_dft_xc_utils_format_xc_code", "parameters": {"properties": {"description": {"type": "string", "description": "The original XC description string provided by the caller. In PySCF this typically identifies a density-functional or a compound specification that may include tokens such as 'RSH(...)' for range-separated-hybrid functionals. The function expects a Python str; the function will remove spaces and newlines and convert to upper case before performing any RSH-specific reordering. Supplying strings that are already normalized is allowed and will be returned (possibly with only the RSH reordering applied).", "default": ""}}, "required": ["description"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed provenance XC-spec manifest for a PySCF DFT benchmark (hand-edited input decks + templated generators). Before job submission, canonicalize only those XC descriptions that appear syntactically complete after whitespace/newline cleanup, defined here as: after removing ASCII spaces and newlines, the string must contain at least one top-level operator token ('+' or '*') and must have balanced parentheses. For every retained description, produce PySCF’s internal normalized XC code by uppercasing and, wherever a range-separated-hybrid fragment appears in the common form RSH(omega,alpha,beta), rewriting it into PySCF’s internal parameter order RSH(alpha;beta;omega) with semicolons (leave any non-RSH parts untouched aside from normalization). Apply this to the following raw XC descriptions: (a) \"B3LYP + 0.25 * RSH(0.33, 0.20, 0.75) + VV10\"; (b) \"b3lyp + RSH(0.35, 0.20, 0.80) + pbe_x\"; (c) \"  rsh(0.33, 0.25, 0.75) +  b3lyp \\n\"; (d) \"PBE0 + RSH(0.40,0.25,0.75\"; (e) \"HF   \\n\".", "answers": "[{\"name\":\"pyscf_dft_xc_utils_format_xc_code\",\"arguments\":{\"description\":\"B3LYP + 0.25 * RSH(0.33, 0.20, 0.75) + VV10\"}},{\"name\":\"pyscf_dft_xc_utils_format_xc_code\",\"arguments\":{\"description\":\"b3lyp + RSH(0.35, 0.20, 0.80) + pbe_x\"}},{\"name\":\"pyscf_dft_xc_utils_format_xc_code\",\"arguments\":{\"description\":\"  rsh(0.33, 0.25, 0.75) +  b3lyp \\n\"}}]"}
{"func_name": "pyscf_dft_xcfun_eval_xc1", "func_desc": "pyscf.dft.xcfun.eval_xc1 evaluates an exchange-correlation (XC) functional and its derivatives by calling the XCFun backend via PySCF's xcfun wrapper. This function is used in PySCF density-functional calculations to obtain the XC energy density and higher-order derivatives on a set of real-space integration grid points; the output ordering of derivative components follows the XCFun convention used by the underlying C library. The implementation accepts local, semi-local, and hybrid functionals (types 'HF', 'LDA', 'GGA', 'MGGA') and supports up to MAX_DERIV_ORDER derivatives as asserted at runtime.", "tools": [{"function": {"description": "pyscf.dft.xcfun.eval_xc1 evaluates an exchange-correlation (XC) functional and its derivatives by calling the XCFun backend via PySCF's xcfun wrapper. This function is used in PySCF density-functional calculations to obtain the XC energy density and higher-order derivatives on a set of real-space integration grid points; the output ordering of derivative components follows the XCFun convention used by the underlying C library. The implementation accepts local, semi-local, and hybrid functionals (types 'HF', 'LDA', 'GGA', 'MGGA') and supports up to MAX_DERIV_ORDER derivatives as asserted at runtime.\n", "name": "pyscf_dft_xcfun_eval_xc1", "parameters": {"properties": {"xc_code": {"type": "string", "description": "Functional specification string understood by PySCF/ XCFun (for example, names parsed by parse_xc). This string determines which XC functional(s) are evaluated, how hybrid mixing factors and range-separation parameters are interpreted, and which XCFun internal function identifiers (fn_ids) and factors (facs) are passed to the XCFun C routine. The function will call parse_xc(xc_code) internally to extract hybrid parameters and component function identifiers.", "default": ""}, "rho": {"type": "array", "items": {"type": "float"}, "description": "Input density array provided on integration grid points. This array is coerced to numpy.asarray(..., order='C', dtype=numpy.double) before use. For non-spin-polarized functionals (spin=0) rho is expected to contain the density-related variables arranged so that the final axis indexes grid points; for spin-polarized cases (spin=1) rho must contain spin-resolved variables accordingly. For meta-GGA (xctype == 'MGGA') if rho.shape[-2] == 6 the code will internally select components rho[..., [0,1,2,3,5], :] to match the MGGA variable ordering expected by XCFun. The last axis of rho is interpreted as the number of grid points (ngrids).", "default": ""}, "spin": {"type": "integer", "description": "Spin flag used to select variable count and ordering from xc_deriv._XC_NVAR. Typical values are 0 for spin-unpolarized (restricted) and 1 for spin-polarized (unrestricted) evaluations. The chosen spin value affects nvar and xlen (internal variable counts) and therefore the reshape/mapping of rho into shape (spin+1, nvar, ngrids) before calling XCFun.", "default": 0}, "deriv": {"type": "integer", "description": "Highest derivative order to evaluate (an integer). The function asserts deriv <= MAX_DERIV_ORDER and will raise an AssertionError if the requested derivative order exceeds the compiled/allowed maximum. The number of derivative components returned per grid point is computed as comb(xlen + deriv, deriv) where xlen depends on xctype and spin via xc_deriv._XC_NVAR.", "default": 1}, "omega": {"type": "float", "nullable": true, "description": "Optional global range-separation parameter. If provided (not None), the single float value overrides the per-functional omega returned by parse_xc and is used for all component functionals. If hyb[2] (the parsed omega) is non-zero but multiple RSH components exist, the implementation currently duplicates the same omega value for each component (i.e., different omegas per component are not supported). If omega is None, the per-component omegas determined by parse_xc are used (or zeroed when no range separation applies).", "default": null}}, "required": ["xc_code", "rho", "deriv", "spin", "omega"], "type": "any"}}, "type": "function"}], "query": "I’m validating an XC evaluation stage for a grid-based DFT workflow where raw grid channels may be partially missing or physically inconsistent. Use the same 3-point integration grid and the following two raw density payloads:\n\nPayload 1 (nominal RKS candidate): a 4×3 array with rows (rho, |∇rho|^2, laplacian, tau) = [[0.12, 0.08, 0.20], [0.001, 0.002, 0.0005], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]].\nPayload 2 (nominal UKS candidate): a 2×4×3 spin-resolved tensor rho[sigma,var,grid] with alpha = [[0.12, 0.15, 0.11],[0.01, 0.02, 0.015],[0.00, 0.01, 0.005],[0.02, 0.03, 0.025]] and beta = [[0.10, 0.09, 0.095],[0.015, 0.018, 0.017],[0.005, 0.006, 0.005],[0.025, 0.027, 0.026]].\n\nProtocol:\n1) Treat a payload as UKS only if it is spin-resolved (has an explicit alpha/beta split); otherwise treat it as RKS.\n2) Choose the XC functional based on whether any grid point carries a nonzero tau channel in the provided payload: if tau is nonzero anywhere, use the hybrid functional PBE0; otherwise use the semi-local PBE.\n3) For range separation, set omega to 0.11 only when the chosen functional is hybrid; otherwise leave omega unset.\n4) Run eval_xc1 requesting the energy density and first derivatives (deriv=1) for each payload that satisfies the above criteria, without reshaping the provided arrays.\n\nProduce the set of eval_xc1 calls implied by this protocol for these two payloads.", "answers": "[{\"name\":\"pyscf_dft_xcfun_eval_xc1\",\"arguments\":{\"xc_code\":\"PBE\",\"rho\":[[0.12,0.08,0.2],[0.001,0.002,0.0005],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"spin\":0,\"deriv\":1,\"omega\":null}},{\"name\":\"pyscf_dft_xcfun_eval_xc1\",\"arguments\":{\"xc_code\":\"PBE0\",\"rho\":[[[0.12,0.15,0.11],[0.01,0.02,0.015],[0.0,0.01,0.005],[0.02,0.03,0.025]],[[0.1,0.09,0.095],[0.015,0.018,0.017],[0.005,0.006,0.005],[0.025,0.027,0.026]]],\"spin\":1,\"deriv\":1,\"omega\":0.11}}]"}
{"func_name": "pyscf_fci_addons_cre_a", "func_desc": "Construct (N+1)-electron CI wavefunction by applying an alpha-electron creation operator to an N-electron CI wavefunction in PySCF's FCI addon.\n    \n    This function is used in the pyscf.fci.addons module to build the second-quantized state |N+1> = a^+_p |N>, where an alpha electron is created in orbital ap_id. It is intended for Full Configuration Interaction (FCI) workflows in PySCF where one needs to add an alpha electron to an existing configuration interaction (CI) coefficient array (for example, when constructing states with one extra alpha electron or building excitation/de-excitation operators). The input CI is organized with rows indexing alpha occupation bit-strings and columns indexing beta occupation bit-strings; the output has the same column dimension (beta strings) but a larger number of rows corresponding to one more alpha electron.", "tools": [{"function": {"description": "Construct (N+1)-electron CI wavefunction by applying an alpha-electron creation operator to an N-electron CI wavefunction in PySCF's FCI addon.\n\nThis function is used in the pyscf.fci.addons module to build the second-quantized state |N+1> = a^+_p |N>, where an alpha electron is created in orbital ap_id. It is intended for Full Configuration Interaction (FCI) workflows in PySCF where one needs to add an alpha electron to an existing configuration interaction (CI) coefficient array (for example, when constructing states with one extra alpha electron or building excitation/de-excitation operators). The input CI is organized with rows indexing alpha occupation bit-strings and columns indexing beta occupation bit-strings; the output has the same column dimension (beta strings) but a larger number of rows corresponding to one more alpha electron.", "name": "pyscf_fci_addons_cre_a", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "float"}, "description": "CI coefficients of the N-electron wavefunction. Expected as a NumPy array whose rows correspond to alpha occupation strings and whose columns correspond to beta occupation strings. If a 1D array (flattened vector) is provided, the function will reshape it internally to (num_alpha_strings, num_beta_strings) where num_alpha_strings = cistring.num_strings(norb, neleca) and num_beta_strings = cistring.num_strings(norb, nelecb). The function does not modify the input array in-place; it returns a new array.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the orbital basis used by the CI expansion. This determines the dimensionality of occupation bit-strings and is passed to cistring helper routines that enumerate string indices and counts.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "Pair of integers (neleca, nelecb) giving the number of alpha and beta electrons in the input CI function ci0. These integers determine which set of alpha strings in the input are valid and are used to compute the target number of alpha strings after creation: cistring.num_strings(norb, neleca+1).", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) specifying which orbital the alpha-electron creation operator a^+_p acts on. This is interpreted as an index into the list of orbitals {0, ..., norb-1}. If ap_id does not correspond to any legal creation from the enumerated alpha strings (for example, if the orbital is already occupied in all contributing strings or if ap_id is outside the range of created entries), the result for those entries will be zero.", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an FCI state-expansion routine on a noisy toy dataset (norb=4) where each replicate CI matrix is supposed to represent a normalized (neleca, nelecb)=(1,1) state, but some replicates are contaminated by an unphysical “zeroed-out orbital-2 alpha channel” artifact. For each replicate ci0, decide the alpha creation target orbital dynamically: compute the L1-norm of the row corresponding to alpha occupation of spatial orbital 2 (row index 2 in these (1,1) CI matrices). If that row’s L1-norm is below 0.11, treat it as corrupted and create the alpha electron in ap_id=1 instead; otherwise create in ap_id=2. Then, for each replicate, build the (N+1)-electron CI coefficients via a† to obtain the (2,1) CI while keeping the beta-string column dimension consistent. Process these two replicates independently and return the resulting (N+1)-electron CI arrays in input order: (1) [[0.12, -0.03], [0.05, 0.2], [0.0, -0.1], [0.07, 0.01]]; (2) [[0.80, -0.10, 0.00, 0.05], [0.20, 0.30, -0.40, 0.00], [0.00, 0.10, 0.25, -0.15], [0.05, 0.00, 0.10, 0.60]].", "answers": "[{\"name\":\"pyscf_fci_addons_cre_a\",\"arguments\":{\"ci0\":[[0.12,-0.03],[0.05,0.2],[0.0,-0.1],[0.07,0.01]],\"norb\":4,\"neleca_nelecb\":[1,1],\"ap_id\":1}},{\"name\":\"pyscf_fci_addons_cre_a\",\"arguments\":{\"ci0\":[[0.8,-0.1,0.0,0.05],[0.2,0.3,-0.4,0.0],[0.0,0.1,0.25,-0.15],[0.05,0.0,0.1,0.6]],\"norb\":4,\"neleca_nelecb\":[1,1],\"ap_id\":2}}]"}
{"func_name": "pyscf_fci_addons_cre_b", "func_desc": "pyscf.fci.addons.cre_b constructs an (N+1)-electron configuration-interaction (CI) wavefunction by applying a beta-spin electron creation operator to an input N-electron CI wavefunction. This routine is part of the PySCF FCI addons used to manipulate CI coefficient arrays expressed in the occupation-string basis (rows index alpha strings, columns index beta strings). It maps each input beta string to the newly created beta string indices using cistring.gen_cre_str_index and applies the fermionic sign factors; an additional sign flip is applied when the number of alpha electrons is odd to account for operator interchange between alpha and beta spins.", "tools": [{"function": {"description": "pyscf.fci.addons.cre_b constructs an (N+1)-electron configuration-interaction (CI) wavefunction by applying a beta-spin electron creation operator to an input N-electron CI wavefunction. This routine is part of the PySCF FCI addons used to manipulate CI coefficient arrays expressed in the occupation-string basis (rows index alpha strings, columns index beta strings). It maps each input beta string to the newly created beta string indices using cistring.gen_cre_str_index and applies the fermionic sign factors; an additional sign flip is applied when the number of alpha electrons is odd to account for operator interchange between alpha and beta spins.\n", "name": "pyscf_fci_addons_cre_b", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "float"}, "description": "CI coefficients of the input N-electron wavefunction. Expected to be a 2D array with rows for alpha occupation strings and columns for beta occupation strings. If a 1D array is provided, the function will attempt to reshape it to shape (num_strings(norb, neleca), num_strings(norb, nelecb)) where num_strings is cistring.num_strings. The function does not modify the logical contents of the input array (it binds a local reshaped view/copy when needed), but an incompatible 1D shape will raise the same exceptions as numpy.reshape.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (norb). This is the size of the single-particle orbital basis used to enumerate occupation strings. It determines the domain of valid orbital indices for the creation operator and is forwarded to cistring helper routines.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "Tuple (neleca, nelecb) of two integers specifying the number of alpha (neleca) and beta (nelecb) electrons in the input CI function. These values are used to (1) determine expected string counts via cistring.num_strings and (2) select the appropriate creation-index table for adding a beta electron (nelecb -> nelecb+1).", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) at which the beta creation operator acts. This is an integer in the range of orbital labels [0, norb-1]. If ap_id does not match any created-orbital entry in the generated creation-index table (for example, if ap_id is outside 0..norb-1), the function produces a zero wavefunction for the attempted creation (see Returns). ap_id is interpreted exactly as an orbital label; no implicit wrapping or modulo is applied.", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing beta-string creation in a 4-spatial-orbital toy FCI code path using deliberately messy CI tensors. You’re given two candidate CI replicates (each is an N-electron CI coefficient matrix in the occupation-string basis, rows=alpha strings, cols=beta strings). Treat a replicate as *actionable* only if its CI tensor is consistent with a 4-orbital determinant basis, i.e., its shape matches (C(4,neleca), C(4,nelecb)) for the stated (neleca,nelecb). For each actionable replicate, choose the beta creation orbital index ap_id using this protocol: attempt creation on the highest-index orbital first (ap_id=3); if that would necessarily annihilate all contributions because the target orbital is already occupied in every beta string of that sector, fall back to ap_id=2. Then, for each actionable replicate, construct the (N+1)-electron CI by applying the beta-spin creation operator with the correct fermionic sign from the beta-string creation mapping and the additional global sign flip when neleca is odd.\n\nCandidate replicates:\n(A) neleca/nelecb=(2,1), ci0 = [[0.10, -0.05, 0.00, 0.02], [0.00, 0.03, -0.01, 0.00], [0.04, 0.00, 0.00, -0.02], [0.00, 0.01, 0.00, 0.00], [-0.03, 0.00, 0.02, 0.00], [0.00, 0.00, -0.02, 0.05]]\n(B) neleca/nelecb=(1,1), ci0 = [[0.12, 0.00, -0.03, 0.05], [0.01, 0.20, 0.00, -0.04], [-0.06, 0.07, 0.15, 0.00], [0.00, -0.02, 0.09, 0.11]]", "answers": "[{\"name\":\"pyscf_fci_addons_cre_b\",\"arguments\":{\"ci0\":[[0.1,-0.05,0.0,0.02],[0.0,0.03,-0.01,0.0],[0.04,0.0,0.0,-0.02],[0.0,0.01,0.0,0.0],[-0.03,0.0,0.02,0.0],[0.0,0.0,-0.02,0.05]],\"norb\":4,\"neleca_nelecb\":[2,1],\"ap_id\":3}},{\"name\":\"pyscf_fci_addons_cre_b\",\"arguments\":{\"ci0\":[[0.12,0.0,-0.03,0.05],[0.01,0.2,0.0,-0.04],[-0.06,0.07,0.15,0.0],[0.0,-0.02,0.09,0.11]],\"norb\":4,\"neleca_nelecb\":[1,1],\"ap_id\":3}}]"}
{"func_name": "pyscf_fci_addons_des_b", "func_desc": "pyscf.fci.addons.des_b\n    Construct an (N-1)-electron configuration interaction (CI) wavefunction by applying a beta-spin electron annihilation operator to an N-electron CI wavefunction. This function is used in the PySCF FCI addons to obtain the CI vector that results from removing one beta electron from a specified spatial orbital; it is commonly used when building reduced-density matrices, computing transition amplitudes that involve electron removal, or when constructing intermediate states in FCI-based post-processing.", "tools": [{"function": {"description": "pyscf.fci.addons.des_b\nConstruct an (N-1)-electron configuration interaction (CI) wavefunction by applying a beta-spin electron annihilation operator to an N-electron CI wavefunction. This function is used in the PySCF FCI addons to obtain the CI vector that results from removing one beta electron from a specified spatial orbital; it is commonly used when building reduced-density matrices, computing transition amplitudes that involve electron removal, or when constructing intermediate states in FCI-based post-processing.", "name": "pyscf_fci_addons_des_b", "parameters": {"properties": {"ci0": {"type": "array", "items": {"type": "float"}, "description": "CI coefficients of the input N-electron wavefunction. This is a 2D array with rows indexed by alpha-spin occupation strings and columns indexed by beta-spin occupation strings. Each element is the CI amplitude for the corresponding (alpha, beta) occupation pair. If a 1D array is passed, the function reshapes it internally to (num_alpha_strings, num_beta_strings) using cistring.num_strings(norb, neleca) and cistring.num_strings(norb, nelecb). The array is treated as read-only by this function; the input ci0 is not modified.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals in the system. This determines the length of occupation bitstrings used by underlying cistring utilities (cistring.gen_des_str_index and cistring.num_strings) and bounds valid orbital indices for the annihilation operator. Values must match the orbital indexing used to construct ci0.", "default": ""}, "neleca_nelecb": {"type": "any", "description": "A two-element tuple (neleca, nelecb) giving the number of alpha and beta electrons in the input CI wavefunction ci0. These integers determine the dimensions of the input CI array (number of alpha strings = cistring.num_strings(norb, neleca); number of beta strings = cistring.num_strings(norb, nelecb)) and the parity correction applied to the sign prefactor when removing a beta electron.", "default": ""}, "ap_id": {"type": "integer", "description": "Orbital index (0-based) specifying which spatial orbital the beta annihilation operator acts on. ap_id must be in the range [0, norb-1] for a physically meaningful annihilation; if ap_id does not match any occupied beta orbital patterns in the input ci0, the returned (N-1)-electron CI vector will be zero in the corresponding columns (see failure modes below).", "default": ""}}, "required": ["ci0", "norb", "neleca_nelecb", "ap_id"], "type": "any"}}, "type": "function"}], "query": "We’re building a spin-resolved electron-removal manifold for downstream transition-amplitude/RDM checks from a small set of trial FCI CI vectors that may include mixed electron counts. For each candidate, apply a beta-spin annihilation (des_b) only if the beta sector is non-empty and the target spatial orbital index is inside the active space. Use an orbital-selection rule tied to the active space size: choose ap_id equal to the middle spatial orbital (i.e., floor(norb/2), 0-based) for each system. Inputs (treat each as independent):\n\n1) norb=4, (neleca,nelecb)=(2,2), ci0=[[0.12,-0.05,0.03,0.0,0.01,-0.02],[-0.04,0.08,0.0,0.02,-0.01,0.05],[0.06,0.0,-0.07,0.04,0.02,0.0],[0.0,0.03,0.01,-0.06,0.09,-0.04],[-0.02,0.05,0.0,0.07,-0.03,0.01],[0.01,-0.04,0.02,0.0,0.06,-0.08]]\n\n2) norb=3, (neleca,nelecb)=(1,1), ci0=[[0.10,-0.05,0.00],[0.20,0.30,-0.10],[0.00,0.15,0.25]]\n\n3) norb=5, (neleca,nelecb)=(3,0), ci0=[[0.02]]\n\nReturn the resulting (N−1) CI vectors for every candidate that yields a physically meaningful beta-electron removal under these rules.", "answers": "[{\"name\":\"pyscf_fci_addons_des_b\",\"arguments\":{\"ci0\":[[0.12,-0.05,0.03,0.0,0.01,-0.02],[-0.04,0.08,0.0,0.02,-0.01,0.05],[0.06,0.0,-0.07,0.04,0.02,0.0],[0.0,0.03,0.01,-0.06,0.09,-0.04],[-0.02,0.05,0.0,0.07,-0.03,0.01],[0.01,-0.04,0.02,0.0,0.06,-0.08]],\"norb\":4,\"neleca_nelecb\":[2,2],\"ap_id\":2}},{\"name\":\"pyscf_fci_addons_des_b\",\"arguments\":{\"ci0\":[[0.1,-0.05,0.0],[0.2,0.3,-0.1],[0.0,0.15,0.25]],\"norb\":3,\"neleca_nelecb\":[1,1],\"ap_id\":1}}]"}
{"func_name": "pyscf_fci_addons_initguess_triplet", "func_desc": "pyscf.fci.addons.initguess_triplet: Generate a normalized triplet initial guess CI vector/matrix for the PySCF FCI solver.\n    \n    Constructs a simple, normalized initial configuration interaction (CI) guess intended to represent a spin triplet (M_S = 0) combination for use as a starting vector in Full Configuration Interaction (FCI) diagonalization or iterative solvers within the PySCF quantum chemistry framework. The routine uses the number of spatial orbitals, the alpha/beta electron counts, and a binary occupation string that selects a particular Slater determinant (address) in one spin sector. Internally it uses cistring.num_strings to determine the number of alpha and beta string basis functions and cistring.str2addr to map the provided binary occupation pattern to the integer address of that determinant. The returned CI array has only two nonzero entries, +sqrt(1/2) and -sqrt(1/2), producing a normalized antisymmetric combination between the two determinants needed to form the triplet M_S = 0 state. This initial guess is useful in practical FCI workflows to bias the solver toward a triplet state and to accelerate convergence when a triplet solution is expected.", "tools": [{"function": {"description": "pyscf.fci.addons.initguess_triplet: Generate a normalized triplet initial guess CI vector/matrix for the PySCF FCI solver.\n\nConstructs a simple, normalized initial configuration interaction (CI) guess intended to represent a spin triplet (M_S = 0) combination for use as a starting vector in Full Configuration Interaction (FCI) diagonalization or iterative solvers within the PySCF quantum chemistry framework. The routine uses the number of spatial orbitals, the alpha/beta electron counts, and a binary occupation string that selects a particular Slater determinant (address) in one spin sector. Internally it uses cistring.num_strings to determine the number of alpha and beta string basis functions and cistring.str2addr to map the provided binary occupation pattern to the integer address of that determinant. The returned CI array has only two nonzero entries, +sqrt(1/2) and -sqrt(1/2), producing a normalized antisymmetric combination between the two determinants needed to form the triplet M_S = 0 state. This initial guess is useful in practical FCI workflows to bias the solver toward a triplet state and to accelerate convergence when a triplet solution is expected.", "name": "pyscf_fci_addons_initguess_triplet", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of spatial orbitals in the active space. This integer determines the length of the binary occupation strings and is passed to cistring.num_strings and cistring.str2addr to enumerate and index determinant bitstrings. In the quantum chemistry domain, norb corresponds to the number of spatial molecular orbitals used to build Slater determinants for the FCI problem.", "default": ""}, "nelec": {"type": "any", "description": "A two-element tuple (neleca, nelecb) giving the number of alpha (spin-up) and beta (spin-down) electrons, respectively. These counts define the FCI sector (number of electrons per spin) and are used to compute the numbers of alpha and beta string basis functions (na and nb). The function expects a tuple of two integers; mismatched counts relative to the provided binstring will lead to errors from cistring.str2addr.", "default": ""}, "binstring": {"type": "string", "description": "A base-2 string (sequence of '0' and '1' characters) encoding the occupation pattern of a determinant in one spin sector. The string is converted to an integer via int(binstring, 2) and then mapped to an address with cistring.str2addr(norb, neleca, int(binstring,2)). The number of '1' bits in binstring must equal neleca (the first element of nelec); otherwise cistring.str2addr or the integer conversion will raise an exception. The length of binstring should be consistent with norb (leading zeros are allowed to reach norb bits).", "default": ""}}, "required": ["norb", "nelec", "binstring"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing triplet-biased seeding in an automated FCI workflow where upstream active-space selection sometimes emits questionable determinants. For each cohort below, generate an M_S = 0 triplet initial CI guess **only if** the provided alpha-sector occupation pattern is consistent with the requested n_alpha (i.e., the bitstring contains exactly n_alpha ones) and its length matches norb. Use the cohort’s alpha-sector bitstring when it passes validation; otherwise, fall back to a deterministic repair rule: construct the alpha reference as the left-packed occupation with n_alpha electrons (ones in the lowest-index orbitals). Cohort A: norb=4, nelec=(2,2), proposed alpha bitstring='0101'. Cohort B: norb=6, nelec=(3,3), proposed alpha bitstring='001011'. Return the triplet antisymmetric two-determinant initial guess suitable for initializing PySCF FCI.", "answers": "[{\"name\":\"pyscf_fci_addons_initguess_triplet\",\"arguments\":{\"norb\":4,\"nelec\":[2,2],\"binstring\":\"0101\"}},{\"name\":\"pyscf_fci_addons_initguess_triplet\",\"arguments\":{\"norb\":6,\"nelec\":[3,3],\"binstring\":\"001011\"}}]"}
{"func_name": "pyscf_fci_addons_large_ci", "func_desc": "pyscf.fci.addons.large_ci searches a Full Configuration Interaction (FCI) coefficient array for large-amplitude configuration state contributions and returns the corresponding determinant identifiers in either binary-string form or as occupied-orbital lists. In the PySCF quantum chemistry context (see README), this helper is used after an FCI calculation to locate and inspect the most important Slater determinants (alpha and beta occupation strings) that contribute to a correlated wavefunction. The function reshapes a flattened CI vector into the rectangular array of alpha- and beta-string combinations, thresholds by absolute amplitude, and returns matched (coefficient, alpha-id, beta-id) tuples for downstream analysis, debugging, or human inspection.", "tools": [{"function": {"description": "pyscf.fci.addons.large_ci searches a Full Configuration Interaction (FCI) coefficient array for large-amplitude configuration state contributions and returns the corresponding determinant identifiers in either binary-string form or as occupied-orbital lists. In the PySCF quantum chemistry context (see README), this helper is used after an FCI calculation to locate and inspect the most important Slater determinants (alpha and beta occupation strings) that contribute to a correlated wavefunction. The function reshapes a flattened CI vector into the rectangular array of alpha- and beta-string combinations, thresholds by absolute amplitude, and returns matched (coefficient, alpha-id, beta-id) tuples for downstream analysis, debugging, or human inspection.\n", "name": "pyscf_fci_addons_large_ci", "parameters": {"properties": {"ci": {"type": "array", "items": {"type": "float"}, "description": "1D array containing the flattened CI coefficient vector for all alpha-beta string pairs. Its length must equal cistring.num_strings(norb, neleca) * cistring.num_strings(norb, nelecb) where (neleca, nelecb) are unpacked from nelec; the function asserts this size and will raise AssertionError if the size does not match. The array elements may be real or complex; thresholding uses absolute values but returned coefficients preserve their original signed/complex values and phases.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals used to build alpha and beta occupation strings. This integer is passed to cistring routines to compute the number of possible strings and to convert string addresses into binary string encodings or occupation lists. Practical significance: norb determines the bit-length of returned binary strings and the range of valid occupied-orbital indices.", "default": ""}, "nelec": {"type": "any", "description": "A pair (neleca, nelecb) of integers giving the number of alpha and beta electrons, respectively. This tuple is unpacked by _unpack_nelec and used to compute the number of alpha and beta strings via cistring.num_strings(norb, neleca) and cistring.num_strings(norb, nelecb). Invalid nelec tuples (e.g., non-integer entries) will cause the unpacking or cistring routines to raise an exception.", "default": ""}, "tol": {"type": "float", "description": "Threshold on the absolute value of CI coefficients used to select \"large\" contributions. Default 0.1 (as in the function signature) selects all coefficients with abs(ci) > tol. If no coefficient exceeds tol, the function falls back to returning the single largest-amplitude coefficient (by absolute value). tol is used only for selection and does not alter the input array.", "default": 0.1}, "return_strs": {"type": "boolean", "description": "If True (default), the function returns alpha and beta identifiers as Python binary strings produced with bin(), each including the '0b' prefix; these strings represent the occupation bitmask for the determinant. If False, the function returns alpha and beta identifiers as occupied-orbital lists produced by cistring._strs2occslst, i.e., lists (per determinant) of integer orbital indices that are occupied. Choose return_strs=True for compact, human-readable bitstrings and return_strs=False for numerical occupied-orbital lists suitable for programmatic manipulation.", "default": true}}, "required": ["ci", "norb", "nelec", "return_strs", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an FCI interpretability post-processor under realistically messy replicate data for a toy CAS(4e,4o) problem (norb=4, nelec=[2,2]). We have two flattened CI coefficient vectors (length 36 each; 6 alpha strings × 6 beta strings), but the downstream analysis should adaptively screen “dominant” determinants per replicate rather than using a fixed global cutoff.\n\nProtocol:\n1) For each replicate, compute the screening tolerance as 0.9×(maximum absolute CI amplitude observed in that replicate).\n2) Run `pyscf.fci.addons.large_ci` on each replicate using its replicate-specific tolerance.\n3) Return determinant identifiers as occupied-orbital lists (not bitstrings) for human inspection.\n\nReplicate 1 CI (flattened): [0.02, -0.31, 0.05, 0.00, 0.12, -0.04, 0.08, 0.03, 0.27, -0.02, 0.01, 0.00, -0.06, 0.09, -0.01, 0.04, -0.28, 0.02, 0.00, 0.07, -0.03, 0.01, 0.05, -0.02, 0.11, 0.00, -0.02, 0.06, 0.03, 0.26, -0.01, 0.04, 0.00, -0.05, 0.02, -0.08]\n\nReplicate 2 CI (flattened): [0.02, -0.31, 0.05, 0.10, -0.03, 0.00, 0.12, 0.04, -0.27, 0.08, 0.01, -0.02, 0.00, 0.06, 0.09, -0.01, 0.03, 0.02, -0.07, 0.11, 0.00, 0.26, -0.05, 0.01, 0.03, -0.02, 0.00, 0.04, 0.07, -0.29, 0.01, 0.00, 0.02, -0.03, 0.05, 0.00]", "answers": "[{\"name\":\"pyscf_fci_addons_large_ci\",\"arguments\":{\"ci\":[0.02,-0.31,0.05,0.0,0.12,-0.04,0.08,0.03,0.27,-0.02,0.01,0.0,-0.06,0.09,-0.01,0.04,-0.28,0.02,0.0,0.07,-0.03,0.01,0.05,-0.02,0.11,0.0,-0.02,0.06,0.03,0.26,-0.01,0.04,0.0,-0.05,0.02,-0.08],\"norb\":4,\"nelec\":[2,2],\"tol\":0.279,\"return_strs\":false}},{\"name\":\"pyscf_fci_addons_large_ci\",\"arguments\":{\"ci\":[0.02,-0.31,0.05,0.1,-0.03,0.0,0.12,0.04,-0.27,0.08,0.01,-0.02,0.0,0.06,0.09,-0.01,0.03,0.02,-0.07,0.11,0.0,0.26,-0.05,0.01,0.03,-0.02,0.0,0.04,0.07,-0.29,0.01,0.0,0.02,-0.03,0.05,0.0],\"norb\":4,\"nelec\":[2,2],\"tol\":0.279,\"return_strs\":false}}]"}
{"func_name": "pyscf_fci_cistring_addrs2str", "func_desc": "Convert a list of CI determinant addresses to occupation \"strings\" used by PySCF's FCI routines.", "tools": [{"function": {"description": "Convert a list of CI determinant addresses to occupation \"strings\" used by PySCF's FCI routines.\n", "name": "pyscf_fci_cistring_addrs2str", "parameters": {"properties": {"norb": {"type": "integer", "description": "Number of spatial orbitals in the FCI problem. This determines the width\nof each occupation string (number of orbitals whose occupation is encoded).\nIn the PySCF FCI domain, norb controls the bit-width of the integer-encoded\noccupation patterns. The implementation requires norb < 64 and will raise\nNotImplementedError if norb >= 64 because the routine uses 64-bit integer\nencodings and a low-level C routine that does not support wider encodings.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons (total) for the determinants. This parameter\ndefines which sector of the combinatorial list of determinants (strings)\nis addressed: num_strings(norb, nelec) is the total number of distinct\ndeterminants with norb orbitals and nelec electrons. The function relies on\nthis value to validate input addresses and to call the underlying C routine\nthat reconstructs occupation patterns for that electron count.", "default": ""}, "addrs": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of integer addresses (indices)\ninto the ordered list of CI determinant strings for the given (norb, nelec)\nsector. The signature documents this as a numpy.ndarray; the function\nconverts the supplied array to dtype numpy.int32 internally before use.\nEach element must satisfy 0 <= addrs[i] < num_strings(norb, nelec). If any\nelement is outside this range, an AssertionError is raised. The array length\ndetermines how many strings are produced and the order of returned strings\ncorresponds one-to-one to the input order.", "default": ""}}, "required": ["norb", "nelec", "addrs"], "type": "any"}}, "type": "function"}], "query": "We’re debugging an intermittent CI-address mapping issue after merging two determinant enumerators. We suspect that some logged addresses are either negative (sentinel values), exceed the sector dimension, or were produced under a different electron-number sector. Use a consistent spatial orbital basis of norb=6. You are given a mixed bag of candidate determinant addresses: [-2, 0, 3, 4, 9, 10, 12, 14, 22, 999]. For the 3-electron sector, convert only those addresses that are valid for that sector’s CI string space. For the 4-electron sector, independently convert only those addresses that are valid for that sector’s CI string space. Return the resulting PySCF occupation strings for each sector in ascending address order within that sector.", "answers": "[{\"name\":\"pyscf_fci_cistring_addrs2str\",\"arguments\":{\"norb\":6,\"nelec\":3,\"addrs\":[0,3,4,9,10,12,14]}},{\"name\":\"pyscf_fci_cistring_addrs2str\",\"arguments\":{\"norb\":6,\"nelec\":4,\"addrs\":[0,3,4,9,10,12,14]}}]"}
{"func_name": "pyscf_fci_cistring_gen_cre_str_index", "func_desc": "Generate mapping index from N-electron occupation strings to N+1-electron occupation strings produced by a single creation operator. This function is used in the FCI (full configuration interaction) string handling in PySCF to accelerate construction of matrix elements: for every N-electron Slater determinant (represented by a bitstring produced from orb_list and nelec) it records, for each possible creation into an unoccupied orbital, the orbital index used for creation, an unused placeholder value preserved for compatibility with upstream C routines, the integer index (address) of the resulting N+1-electron string in the ordering used by the FCI code, and the fermionic parity (sign) arising from antisymmetry when inserting the electron.", "tools": [{"function": {"description": "Generate mapping index from N-electron occupation strings to N+1-electron occupation strings produced by a single creation operator. This function is used in the FCI (full configuration interaction) string handling in PySCF to accelerate construction of matrix elements: for every N-electron Slater determinant (represented by a bitstring produced from orb_list and nelec) it records, for each possible creation into an unoccupied orbital, the orbital index used for creation, an unused placeholder value preserved for compatibility with upstream C routines, the integer index (address) of the resulting N+1-electron string in the ordering used by the FCI code, and the fermionic parity (sign) arising from antisymmetry when inserting the electron.\n", "name": "pyscf_fci_cistring_gen_cre_str_index", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "float"}, "description": "List of orbital labels used to build occupation bitstrings. The length of this list defines the number of spatial spin-orbitals (norb = len(orb_list)) available for occupations and therefore determines how many creation targets exist for each N-electron string. This argument is passed to the internal make_strings routine that enumerates all N-electron bitstrings; different orb_list orders change the ordering of generated strings and thus the target addresses produced by this function. The function assumes orb_list can be handled by make_strings; if make_strings returns an OIndexList (occurs for very large orbital counts), the function raises NotImplementedError (see Failure modes).", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the input strings (N). This integer must be strictly less than the number of orbitals (nelec < len(orb_list)), otherwise the function will fail the built-in assertion and raise AssertionError. nelec selects the sector of Slater determinants for which creation mappings are produced: the function enumerates all N-electron strings and, for each, computes which N+1-electron strings result from creating one electron in each available unoccupied orbital.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the PySCF FCI creation-string indexer under a realistic “messy basis” ingestion step. A downstream integral engine emits candidate orbital label manifolds that may include duplicates (repeated irreps), negative sentinel labels from failed symmetry classification, and non-monotone ordering. Treat each candidate manifold as follows: (1) discard any orbital labels that are negative, (2) deduplicate while preserving first occurrence order, then (3) continue only for manifolds that still contain at least 4 distinct nonnegative orbitals (so that the N=2 sector has nontrivial creation targets). For every retained manifold, generate the PySCF-style creation-operator lookup table for the fixed N=2 electron sector: for each 2-electron determinant in the code’s string ordering and each allowed creation into an unoccupied orbital, record (i) the created orbital label, (ii) the reserved placeholder field, (iii) the address of the resulting 3-electron determinant in the same ordering, and (iv) the fermionic parity/sign. Candidate manifolds (raw): A=[0,1,1,2,3], B=[-1,0,2,2,3,4], C=[0,3,2,1,4,5,5], D=[0,1,2]. Output mapping tables for every manifold that passes the rules above.", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_cre_str_index\",\"arguments\":{\"orb_list\":[0,1,2,3],\"nelec\":2}},{\"name\":\"pyscf_fci_cistring_gen_cre_str_index\",\"arguments\":{\"orb_list\":[0,2,3,4],\"nelec\":2}},{\"name\":\"pyscf_fci_cistring_gen_cre_str_index\",\"arguments\":{\"orb_list\":[0,3,2,1,4,5],\"nelec\":2}}]"}
{"func_name": "pyscf_fci_cistring_gen_des_str_index", "func_desc": "Generate an index mapping that relates each N-electron occupation string (bitstring) defined by orb_list to the N-1-electron occupation strings produced by applying a fermionic annihilation operator to that N-electron string. This function is used in the PySCF full configuration interaction (FCI) cistring utilities to accelerate application of annihilation operators when assembling matrix elements, computing determinants, or performing excitation/de-excitation operations in FCI routines.\n    \n    The function builds the list of N-electron strings by calling make_strings(orb_list, nelec) and then calls the compiled helper libfci.FCIdes_str_index to fill a dense index table. The returned index table lets FCI code look up, for any N-electron string and any choice of an occupied orbital to annihilate, the address of the resulting (N-1)-electron string together with the fermionic sign (parity) resulting from the annihilation.", "tools": [{"function": {"description": "Generate an index mapping that relates each N-electron occupation string (bitstring) defined by orb_list to the N-1-electron occupation strings produced by applying a fermionic annihilation operator to that N-electron string. This function is used in the PySCF full configuration interaction (FCI) cistring utilities to accelerate application of annihilation operators when assembling matrix elements, computing determinants, or performing excitation/de-excitation operations in FCI routines.\n\nThe function builds the list of N-electron strings by calling make_strings(orb_list, nelec) and then calls the compiled helper libfci.FCIdes_str_index to fill a dense index table. The returned index table lets FCI code look up, for any N-electron string and any choice of an occupied orbital to annihilate, the address of the resulting (N-1)-electron string together with the fermionic sign (parity) resulting from the annihilation.", "name": "pyscf_fci_cistring_gen_des_str_index", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "float"}, "description": "List of orbital identifiers used to construct the occupation bitstrings. This list is passed unchanged to make_strings(orb_list, nelec). In the domain of PySCF FCI, orb_list defines the set and order of spatial spin-orbitals (or orbital labels) from which the N-electron occupation strings are drawn; the mapping produced by this function uses the same ordering and indexing convention as make_strings.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons in the source occupation strings (N). This function requires nelec > 0 and will assert if nelec is not positive. The function constructs mappings from each N-electron string to N-1-electron strings obtained by annihilating one electron.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "In our FCI operator-application benchmark, we want to stress-test `gen_des_str_index` under messy, mixed-quality orbital manifolds coming from two independent integral-generation steps. You are given two raw spin-orbital candidate pools:\n\n- Pool A candidates: `[-1, 0, 1, 2, 3, 4, 5, 5, 9]` with target electron sector `N=3`.\n- Pool B candidates: `[0, 1, 2, 3, 7, 7]` with target electron sector `N=2`.\n\nFor each pool, construct the *actual* `orb_list` by keeping only physically valid spin-orbital labels that fall within the contiguous active-space window starting at 0 and ending at the maximum label that still preserves contiguity with no gaps; use that contiguous window as the active `orb_list` (duplicates and any noncontiguous tail values should not affect the final window). Then, generate the dense annihilation (destruction) string index table for that `orb_list` at the given `nelec`, mapping every N-electron occupation string and annihilation choice to the corresponding (N−1)-electron string address and fermionic sign.", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_des_str_index\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nelec\":3}},{\"name\":\"pyscf_fci_cistring_gen_des_str_index\",\"arguments\":{\"orb_list\":[0,1,2,3],\"nelec\":2}}]"}
{"func_name": "pyscf_fci_cistring_gen_linkstr_index", "func_desc": "pyscf.fci.cistring.gen_linkstr_index generates a lookup table (link index) that describes how occupation-number strings (Slater determinant bit-strings used in Full Configuration Interaction (FCI) within PySCF) are transformed by a single fermionic annihilation-creation operator pair. This lookup table is used by FCI routines to efficiently form Hamiltonian and excitation matrix elements by mapping an initial string and an excitation (annihilate an occupied orbital, create in a target orbital) to the index of the resulting string and the accompanying fermionic phase factor.", "tools": [{"function": {"description": "pyscf.fci.cistring.gen_linkstr_index generates a lookup table (link index) that describes how occupation-number strings (Slater determinant bit-strings used in Full Configuration Interaction (FCI) within PySCF) are transformed by a single fermionic annihilation-creation operator pair. This lookup table is used by FCI routines to efficiently form Hamiltonian and excitation matrix elements by mapping an initial string and an excitation (annihilate an occupied orbital, create in a target orbital) to the index of the resulting string and the accompanying fermionic phase factor.\n", "name": "pyscf_fci_cistring_gen_linkstr_index", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "float"}, "description": "Ordered list of orbital indices (the spatial or spin-orbital labels) that define the basis for the bit-strings. The length of this list determines norb (the total number of orbitals) used to compute nvir = norb - nocc. In the PySCF FCI context, orb_list selects which orbitals are considered when constructing occupation strings via make_strings and when enumerating possible excitations.", "default": ""}, "nocc": {"type": "integer", "description": "Number of occupied orbitals in each string (the occupation count for the determinants). In the FCI domain this is the number of electrons (per spin sector when used in spin-specific routines) that are occupied in each occupation-number string; it sets the number of annihilation indices and therefore controls the layout and size (nocc and nocc*nvir blocks) of the returned link index.", "default": ""}, "strs": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional one-dimensional array of integer-encoded occupation strings (string indices) sorted in strictly increasing order. If None (the default), make_strings(orb_list, nocc) is called to generate the full ordered list of strings for the given orb_list and nocc. The array is converted to numpy.int64 internally. If strs is an instance of OIndexList, the routine delegates to gen_linkstr_index_o1(orb_list, nocc, strs, tril) and returns that result instead. This parameter allows restricting the lookup table to a precomputed subset or custom ordering of strings used elsewhere in PySCF FCI workflows.", "default": null}, "tril": {"type": "boolean", "description": "Flag controlling whether the underlying C routine should compute the link index in a triangular/half-storage mode. The boolean value is forwarded unchanged to the low-level libfci.FCIlinkstr_index routine. In practice, tril=True is used by algorithms that only need a triangular subset of links (e.g., exploiting symmetry or half-matrix storage) while tril=False computes the full link information. Default is False.", "default": false}}, "required": ["orb_list", "nocc", "tril", "strs"], "type": "any"}}, "type": "function"}], "query": "We’re validating determinant bookkeeping for a 6-orbital active space (orbital labels 0–5), but the upstream determinant enumerator is known to intermittently emit mixed-quality cohorts. Given three raw “cohorts” of intended electron sectors: cohort A targets nocc=2, cohort B targets nocc=3, and cohort C is an independent nocc=3 replicate. For each cohort, build a full (non-triangular) link-index lookup table using pyscf.fci.cistring.gen_linkstr_index, but only accept cohorts whose electron count is physically admissible for the active space and whose determinant list, if present, is internally consistent with that electron count; treat any cohort failing these criteria as having no usable explicit string list (i.e., fall back to generating over the full determinant space for its electron count). Use the same orbital label set 0–5 for all runs, and ensure no triangular storage is used.", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_linkstr_index\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":2,\"strs\":null,\"tril\":false}},{\"name\":\"pyscf_fci_cistring_gen_linkstr_index\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":3,\"strs\":null,\"tril\":false}},{\"name\":\"pyscf_fci_cistring_gen_linkstr_index\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":3,\"strs\":null,\"tril\":false}}]"}
{"func_name": "pyscf_fci_cistring_gen_linkstr_index_trilidx", "func_desc": "Generate link-string index for two-body single-excitation operators with enforced lower-triangular ordering (p > q) used in PySCF FCI string handling.\n    \n    This function is part of the pyscf.fci.cistring utilities that build data structures for mapping excitation operators onto determinant bit-string representations in full configuration interaction (FCI) algorithms. In the domain of quantum chemistry and PySCF, gen_linkstr_index_trilidx produces the link-string index entries for operators of the form p^+ q acting on many-electron basis states, with the explicit convention that the creation orbital index p is strictly greater than the annihilation orbital index q (the \"lower-triangular\" or trilidx convention). The produced index has the structure [pq, *, str1, sign] for each linked excitation as returned by the underlying gen_linkstr_index call with the trilidx flag enabled. This function simply invokes gen_linkstr_index with trilidx=True and therefore yields exactly the same result as reform_linkstr_index(gen_linkstr_index(...)) when that reform operation enforces the trilidx ordering.", "tools": [{"function": {"description": "Generate link-string index for two-body single-excitation operators with enforced lower-triangular ordering (p > q) used in PySCF FCI string handling.\n\nThis function is part of the pyscf.fci.cistring utilities that build data structures for mapping excitation operators onto determinant bit-string representations in full configuration interaction (FCI) algorithms. In the domain of quantum chemistry and PySCF, gen_linkstr_index_trilidx produces the link-string index entries for operators of the form p^+ q acting on many-electron basis states, with the explicit convention that the creation orbital index p is strictly greater than the annihilation orbital index q (the \"lower-triangular\" or trilidx convention). The produced index has the structure [pq, *, str1, sign] for each linked excitation as returned by the underlying gen_linkstr_index call with the trilidx flag enabled. This function simply invokes gen_linkstr_index with trilidx=True and therefore yields exactly the same result as reform_linkstr_index(gen_linkstr_index(...)) when that reform operation enforces the trilidx ordering.", "name": "pyscf_fci_cistring_gen_linkstr_index_trilidx", "parameters": {"properties": {"orb_list": {"type": "array", "items": {"type": "float"}, "description": "List of orbital indices or an iterable that defines the set of single-particle orbitals considered when constructing determinant bit-strings. In practice within PySCF FCI routines, orb_list determines which orbital positions are represented in the bit-string basis and therefore which p and q values are valid for generating link strings. Providing orb_list that does not match the basis used elsewhere in the calculation will produce indices that are incompatible with those determinants.", "default": ""}, "nocc": {"type": "integer", "description": "Number of occupied electrons in each determinant (number of set bits per string) used to generate the FCI string space. This integer controls the size of the determinant space over orb_list for which link strings are generated; typical use is the number of electrons (or electrons per spin sector) in the active space. If nocc is inconsistent with orb_list (for example, nocc < 0 or nocc > len(orb_list)), the underlying generator may raise an exception.", "default": ""}, "strs": {"type": "any", "nullable": true, "description": "Optional precomputed list of determinant bit-strings (usually represented as integers or bit-vectors) to use instead of constructing the full list from orb_list and nocc. When strs is None (the default), the underlying gen_linkstr_index routine will compute the determinant strings for the given orb_list and nocc. When a list is provided, this function uses that list directly to generate link-string indices for the supplied determinants. The elements and ordering of strs must match the representation expected elsewhere in the FCI workflow; mismatches will cause incorrect mappings or propagated errors.", "default": null}}, "required": ["orb_list", "nocc", "strs"], "type": "any"}}, "type": "function"}], "query": "In our automated active-space validation harness, we receive heterogeneous “candidate orbital manifolds” for a 3-electron FCI microbenchmark. Each candidate manifold is a raw orb_list that may include duplicates, be unsorted, or contain orbitals outside the allowed label range for the run. For each candidate, first canonicalize the manifold by taking the unique set of orbital labels and sorting them. Only proceed for candidates whose canonicalized orb_list contains exactly 6 distinct orbitals and spans a contiguous block of labels (max(label) − min(label) = 5). For each qualifying candidate, build the cistring link-string indices for all single excitations p^+ q under the enforced lower-triangular (trilidx) convention (p > q) at fixed nocc=3. Use two internal-string pathways depending on the manifold’s cleanliness: if the raw candidate orb_list was already strictly increasing with no duplicates, pass an explicit null/empty placeholder for the determinant string list (strs=null); otherwise, omit the strs argument entirely and rely on internal generation. Raw candidates: A=[0,1,2,3,4,5]; B=[0,1,2,3,4,5,5]; C=[5,4,3,2,1,0]; D=[0,1,2,3,4,6].", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_linkstr_index_trilidx\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":3,\"strs\":null}},{\"name\":\"pyscf_fci_cistring_gen_linkstr_index_trilidx\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":3}},{\"name\":\"pyscf_fci_cistring_gen_linkstr_index_trilidx\",\"arguments\":{\"orb_list\":[0,1,2,3,4,5],\"nocc\":3}}]"}
{"func_name": "pyscf_fci_cistring_gen_occslst", "func_desc": "pyscf.fci.cistring.gen_occslst — Generate occupied-orbital index lists for every occupation string used in FCI string representations.", "tools": [{"function": {"description": "pyscf.fci.cistring.gen_occslst — Generate occupied-orbital index lists for every occupation string used in FCI string representations.\n", "name": "pyscf_fci_cistring_gen_occslst", "parameters": {"properties": {"orb_list": {"type": "any", "description": "A sequence (typically a tuple) of orbital indices that defines the available single-particle orbitals from which occupations are formed. In the PySCF FCI context, these integers identify orbital positions that can be occupied in a bitstring representation; the function converts this input to a Python list internally (orb_list = list(orb_list)). If orb_list is not iterable, a TypeError will be raised by the conversion.", "default": ""}, "nelec": {"type": "integer", "description": "The number of electrons (occupied orbitals) per string. This integer must be non-negative; the function asserts nelec >= 0 and will raise an AssertionError for negative values. In the FCI domain, nelec controls the length of each returned occupation list and determines the combinatorial generation of occupation patterns.", "default": ""}}, "required": ["orb_list", "nelec"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing CI string back-mapping under messy active-space bookkeeping from three independent preprocessing replicates (simulating three different orbital-localization passes). Each replicate reports a candidate working-orbital label list (with possible duplicates, missing labels, and unsorted order). For each replicate, first sanitize the label list by (i) keeping only unique integer orbital labels in the physically valid range [0, 5] and (ii) ordering them ascending to form the active-space `orb_list`. Then choose `nelec` adaptively from the cleaned active-space size: use half-filling rounded to the nearest integer (i.e., `nelec = round(norb/2)`), but only proceed if `0 < nelec < norb`. For every replicate that passes these criteria, enumerate the full-CI occupation-string basis and output the occupied-orbital index list for each string via `pyscf.fci.cistring.gen_occslst`.\n\nReplicate raw orbital label reports:\n- Replicate A: [5, 3, 3, 1, 0, 2]\n- Replicate B: [0, 2, 4, 6, 6, 5]\n- Replicate C: [1, 2, 3, 4]", "answers": "[{\"name\":\"pyscf_fci_cistring_gen_occslst\",\"arguments\":{\"orb_list\":[0,1,2,3,5],\"nelec\":2}},{\"name\":\"pyscf_fci_cistring_gen_occslst\",\"arguments\":{\"orb_list\":[0,2,4,5],\"nelec\":2}},{\"name\":\"pyscf_fci_cistring_gen_occslst\",\"arguments\":{\"orb_list\":[1,2,3,4],\"nelec\":2}}]"}
{"func_name": "pyscf_fci_cistring_tn_strs", "func_desc": "Generate integer-encoded Slater determinant strings that enumerate n-electron excitation\n    patterns (Tn amplitudes) used by the Full Configuration Interaction (FCI) routines in\n    pyscf.fci.cistring. This function constructs all determinants obtained by exciting\n    exactly n electrons from the Hartree–Fock (HF) reference (first nelec orbitals occupied)\n    into the virtual space (orbitals nelec..norb-1). The result is used to index Tn amplitude\n    tensors in post-Hartree–Fock methods where excitations are represented as bitstrings\n    encoded in Python integers (consistent with cistring.make_strings).", "tools": [{"function": {"description": "Generate integer-encoded Slater determinant strings that enumerate n-electron excitation\npatterns (Tn amplitudes) used by the Full Configuration Interaction (FCI) routines in\npyscf.fci.cistring. This function constructs all determinants obtained by exciting\nexactly n electrons from the Hartree–Fock (HF) reference (first nelec orbitals occupied)\ninto the virtual space (orbitals nelec..norb-1). The result is used to index Tn amplitude\ntensors in post-Hartree–Fock methods where excitations are represented as bitstrings\nencoded in Python integers (consistent with cistring.make_strings).", "name": "pyscf_fci_cistring_tn_strs", "parameters": {"properties": {"norb": {"type": "integer", "description": "Total number of spatial orbitals (the size of the one-particle basis).\nThis integer determines the size of the Hilbert space partitioning into occupied\nand virtual orbitals. Must be an integer value; the implementation raises\nNotImplementedError if norb >= 64 (the function is not implemented for 64 or\nmore orbitals in this routine). No other implicit conversion is performed.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons (occupied orbitals) in the Hartree–Fock reference.\nThe HF reference used by this routine is the determinant with the lowest\nnelec orbitals occupied (bits 0..nelec-1 set). nelec is expected to be a\nnon-negative integer. If nelec < n or norb - nelec < n (not enough occupied or\nvirtual orbitals to form n-fold excitations), the function returns an empty\n1D numpy.ndarray of dtype int (see Returns).", "default": ""}, "n": {"type": "integer", "description": "Excitation rank (the number of electrons to be excited). This integer\nspecifies Tn (for example, n=1 for T1 amplitudes, n=2 for T2). The routine\nenumerates combinations of n occupied orbitals to remove and n virtual orbitals\nto occupy. If combinatorial conditions are not met (see nelec and norb - nelec),\nthe routine returns an empty array.", "default": ""}}, "required": ["norb", "nelec", "n"], "type": "any"}}, "type": "function"}], "query": "We’re assembling determinant indexers for a mixed benchmark suite where each entry is a candidate HF reference described by `(norb, nelec)` and a target excitation rank is inferred from the physical feasibility of the virtual space. Use the same integer/bitstring encoding as `pyscf.fci.cistring.make_strings`, with the HF reference occupying orbitals `0..nelec-1` and excitations promoting exactly `n` electrons into the virtual space `nelec..norb-1`.\n\nRaw benchmark entries (may include inconsistent setups):\n1) `LiH_min`: norb=8, nelec=4\n2) `HF_sto3g`: norb=10, nelec=6\n3) `artifact_negvirt`: norb=6, nelec=6\n4) `artifact_overfilled`: norb=5, nelec=8\n5) `tiny_active`: norb=7, nelec=6\n\nProtocol:\n- Only generate excitation strings for entries with a non-empty virtual space and electron count not exceeding the orbital count.\n- For each remaining entry, choose the excitation rank `n` as the largest rank in {2, 3} that is possible given the number of virtual orbitals (`norb - nelec`).\n- Generate the corresponding Tn determinant strings for each qualifying entry.", "answers": "[{\"name\":\"pyscf_fci_cistring_tn_strs\",\"arguments\":{\"norb\":8,\"nelec\":4,\"n\":3}},{\"name\":\"pyscf_fci_cistring_tn_strs\",\"arguments\":{\"norb\":10,\"nelec\":6,\"n\":3}},{\"name\":\"pyscf_fci_cistring_tn_strs\",\"arguments\":{\"norb\":7,\"nelec\":6,\"n\":2}}]"}
{"func_name": "pyscf_fci_direct_spin1_absorb_h1e", "func_desc": "pyscf.fci.direct_spin1.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian for use in full configuration interaction (FCI) routines in the direct_spin1 (spin-adapted) code path of PySCF. This routine is used in electronic-structure calculations to combine one-electron (kinetic + nuclear attraction or Fock-like) terms into an effective two-electron Hamiltonian representation so that subsequent FCI matrix assembly can operate on a single 4-index tensor.", "tools": [{"function": {"description": "pyscf.fci.direct_spin1.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian for use in full configuration interaction (FCI) routines in the direct_spin1 (spin-adapted) code path of PySCF. This routine is used in electronic-structure calculations to combine one-electron (kinetic + nuclear attraction or Fock-like) terms into an effective two-electron Hamiltonian representation so that subsequent FCI matrix assembly can operate on a single 4-index tensor.\n", "name": "pyscf_fci_direct_spin1_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "float"}, "description": "The one-electron Hamiltonian matrix provided as a NumPy array. In practice this is the one-electron part of the molecular Hamiltonian (for example kinetic energy plus nuclear attraction or an effective Fock-like operator) represented in the same orbital basis as the two-electron integrals. The function uses h1e to compute an average one-electron correction that is distributed into the returned two-electron tensor; the entries of h1e must be numeric and real-valued (the implementation raises NotImplementedError if h1e.dtype == numpy.complex128). The physical significance is that h1e contributes to orbital energies and coupling terms that are absorbed into the two-electron representation for the direct_spin1 FCI algorithms.", "default": ""}, "eri": {"type": "array", "items": {"type": "float"}, "description": "The two-electron integrals provided as a NumPy array in the form accepted by ao2mo.restore. This array contains the electron-electron interaction integrals for the same orbital basis as h1e and is copied internally before modification. The function calls ao2mo.restore to produce a full 4-index tensor, so eri must be compatible with that routine for the supplied norb. Complex-valued eri arrays with dtype numpy.complex128 are not supported and will cause NotImplementedError. The returned tensor is derived from eri with the h1e contribution absorbed.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals (norb) used to interpret and restore the two-electron integrals via ao2mo.restore. This integer determines the orbital dimension of the returned 4-index tensor and must match the orbital dimension of h1e and the eri data layout. It is used when calling ao2mo.restore(1, eri.copy(), norb) and ao2mo.restore(4, ... , norb) to map between packed and full-index representations.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons used to scale the one-electron correction when absorbing it into the two-electron integrals. This argument may be an integer or a NumPy numeric scalar; if nelec is not an int or numpy.number (for example a sequence like (n_alpha, n_beta)), the code will sum its elements (nelec = sum(nelec)) to obtain the total electron count. The computed per-orbital one-electron correction is divided by nelec (with a tiny offset 1e-100 to avoid division by zero) reflecting how the one-electron contribution is distributed per electron in the effective two-electron operator. The physical meaning is the averaging of one-electron terms per occupied electron when forming the effective two-electron Hamiltonian for FCI.", "default": ""}, "fac": {"type": "float", "description": "Multiplicative scaling factor applied to the final returned 4-index tensor. Default is 1. After absorbing the one-electron contribution into the two-electron tensor, the function multiplies the entire 4-index result by fac before returning. This parameter allows scaling the overall Hamiltonian if needed by caller code (for example to include symmetry or normalization prefactors).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "We’re validating a direct_spin1 FCI preprocessing stage under messy, semi-automated integral ingestion. You’ll receive a mixed batch of candidate Hamiltonian records (some are artifacts). For each record, first check whether the one-electron Hamiltonian is a physically admissible real symmetric matrix (within the numeric precision implied by the literals). Only for admissible records, fold h1e into the provided two-electron integrals with pyscf.fci.direct_spin1.absorb_h1e using the record’s norb and nelec. Use a dynamic scaling rule for fac: if the diagonal spread of h1e (max diagonal minus min diagonal) exceeds 0.30, set fac=0.5 to damp the effective tensor; otherwise keep fac=1.0. Records:\n\n1) norb=2, nelec=2; h1e=[[-1.10, 0.05],[0.05, -0.90]]; eri (chemist 4-tensor)=[[[[0.70, 0.10],[0.10, 0.60]],[[0.10, 0.05],[0.05, 0.20]]],[[[0.10, 0.05],[0.05, 0.20]],[[0.60, 0.20],[0.20, 0.50]]]]\n2) norb=3, nelec=4; h1e=[[-1.1, -0.05, 0.0],[-0.04, -0.9, -0.02],[0.0, -0.02, -0.7]]; eri (compressed)=[0.65, 0.12, 0.08, 0.1, 0.05, 0.03, 0.6, 0.09, 0.04, 0.55, 0.02, 0.5, 0.11, 0.07, 0.06, 0.58, 0.05, 0.52, 0.09, 0.57, 0.54]\n3) norb=3, nelec=4; h1e=[[-1.1, -0.05, 0.0],[-0.05, -0.9, -0.02],[0.0, -0.02, -0.7]]; eri (compressed)=[0.65, 0.12, 0.08, 0.1, 0.05, 0.03, 0.6, 0.09, 0.04, 0.55, 0.02, 0.5, 0.11, 0.07, 0.06, 0.58, 0.05, 0.52, 0.09, 0.57, 0.54]\n\nReturn the absorbed effective two-electron representation for each admissible record (in the same order they pass the sieve).", "answers": "[{\"name\":\"pyscf_fci_direct_spin1_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.1,0.05],[0.05,-0.9]],\"eri\":[[[[0.7,0.1],[0.1,0.6]],[[0.1,0.05],[0.05,0.2]]],[[[0.1,0.05],[0.05,0.2]],[[0.6,0.2],[0.2,0.5]]]],\"norb\":2,\"nelec\":2,\"fac\":1.0}},{\"name\":\"pyscf_fci_direct_spin1_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.1,-0.05,0.0],[-0.05,-0.9,-0.02],[0.0,-0.02,-0.7]],\"eri\":[0.65,0.12,0.08,0.1,0.05,0.03,0.6,0.09,0.04,0.55,0.02,0.5,0.11,0.07,0.06,0.58,0.05,0.52,0.09,0.57,0.54],\"norb\":3,\"nelec\":4,\"fac\":0.5}}]"}
{"func_name": "pyscf_fci_direct_spin1_energy", "func_desc": "Compute the FCI electronic energy (expectation value) for a given Hamiltonian and FCI vector in the direct_spin1 FCI implementation.\n    \n    This function is used in the PySCF quantum chemistry framework to evaluate the scalar electronic energy E = <C|H|C> for a Full Configuration Interaction (FCI) wavefunction C represented by fcivec. It first folds the one-electron integrals into the two-electron integrals via absorb_h1e (hence working with the effective two-electron Hamiltonian used by the direct_spin1 routines), then forms the Hamiltonian action on the FCI vector with contract_2e, and finally computes the dot product of the input vector with that action to produce the energy. This routine is applicable to non-relativistic electronic structure problems expressed in an orthonormal molecular orbital basis and follows the FCI conventions used by the pyscf.fci.direct_spin1 module.", "tools": [{"function": {"description": "Compute the FCI electronic energy (expectation value) for a given Hamiltonian and FCI vector in the direct_spin1 FCI implementation.\n\nThis function is used in the PySCF quantum chemistry framework to evaluate the scalar electronic energy E = <C|H|C> for a Full Configuration Interaction (FCI) wavefunction C represented by fcivec. It first folds the one-electron integrals into the two-electron integrals via absorb_h1e (hence working with the effective two-electron Hamiltonian used by the direct_spin1 routines), then forms the Hamiltonian action on the FCI vector with contract_2e, and finally computes the dot product of the input vector with that action to produce the energy. This routine is applicable to non-relativistic electronic structure problems expressed in an orthonormal molecular orbital basis and follows the FCI conventions used by the pyscf.fci.direct_spin1 module.", "name": "pyscf_fci_direct_spin1_energy", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "float"}, "description": "One-electron integrals in the molecular orbital basis. Practically, this is the core Hamiltonian (kinetic + nuclear attraction) matrix with shape (norb, norb), where norb is the number of spatial orbitals. The function uses these integrals when folding them into the two-electron term via absorb_h1e to produce the effective two-electron interaction used in the energy evaluation.", "default": ""}, "eri": {"type": "array", "items": {"type": "float"}, "description": "Two-electron repulsion integrals in chemist/physicist notation appropriate for the direct_spin1 routines. Typically an array with 4 indices and shape (norb, norb, norb, norb). These integrals describe electron-electron Coulomb interactions and are combined with h1e by absorb_h1e; incorrect ordering or symmetry of eri will lead to wrong energies or exceptions.", "default": ""}, "fcivec": {"type": "array", "items": {"type": "float"}, "description": "FCI wavefunction vector (configuration interaction coefficients). This array must be reshapeable to the expected FCI vector length for the given norb and nelec (the code calls reshape(-1) internally). In practice, fcivec is a 1D array of length equal to the number of determinants/configurations for the spin sectors determined by nelec; it may also be provided in other shapes as long as the total number of elements matches the expected dimension. The dot product of fcivec with the Hamiltonian action yields the scalar energy.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals used in the FCI calculation. This integer determines the dimensionality of h1e and eri and, together with nelec, determines the size of the FCI Hilbert space. Passing a norb inconsistent with the shapes of h1e, eri, or fcivec will produce errors.", "default": ""}, "nelec": {"type": "any", "description": "A tuple specifying the number of electrons in each spin sector, conventionally (nalpha, nbeta). This tuple defines the electronic sector for which the FCI wavefunction fcivec is defined and is required by absorb_h1e and contract_2e to determine the mapping between configuration indices and determinant occupations.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Precomputed link table(s) used internally by contract_2e to enumerate excitations and accelerate Hamiltonian contraction. If provided, link_index must be compatible with norb and nelec and follow the format produced by the pyscf.fci.link_index building utilities. If None (the default), contract_2e will internally construct or infer the linking structures as needed. Providing a correct precomputed link_index can substantially reduce repeated setup cost when evaluating energies multiple times for the same orbital/nelec specification.", "default": null}}, "required": ["h1e", "eri", "fcivec", "norb", "nelec", "link_index"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our direct_spin1 FCI energy evaluator on a mixed-quality micro-benchmark set for a closed-shell 2-orbital, 2-electron system (norb=2, nelec=(1,1)), with link tables left to the default auto-construction (i.e., do not pass link_index). Each record provides (h1e, eri in chemist notation, fcivec). Because some exported CI coefficient vectors are not normalized due to upstream truncation, first compute the L2 norm of each fcivec and only evaluate E = <C|H|C> for records whose fcivec is already normalized to within 1% (i.e., ||C||2 in [0.99, 1.01]).\n\nRecords:\n1) Replicate A: h1e = [[-1.1, -0.2], [-0.2, -0.5]]; eri = [[[[0.7, 0.1], [0.1, 0.3]], [[0.1, 0.05], [0.05, 0.2]]], [[[0.1, 0.05], [0.05, 0.2]], [[0.3, 0.08], [0.08, 0.6]]]]; fcivec = [0.85, -0.25, 0.35, 0.30].\n2) Replicate B: h1e = [[-1.0, -0.1], [-0.1, -0.5]]; eri = [[[[0.7, 0.0], [0.0, 0.6]], [[0.0, 0.4], [0.4, 0.0]]], [[[0.0, 0.4], [0.4, 0.0]], [[0.6, 0.0], [0.0, 0.5]]]]; fcivec = [0.8, -0.2, 0.3, 0.5].\n\nFor the qualifying records, compute the scalar electronic energy using the direct_spin1 convention: absorb h1e into eri (absorb_h1e), apply contract_2e to the provided fcivec, then take the dot product with the original fcivec.", "answers": "[{\"name\":\"pyscf_fci_direct_spin1_energy\",\"arguments\":{\"h1e\":[[-1.0,-0.1],[-0.1,-0.5]],\"eri\":[[[[0.7,0.0],[0.0,0.6]],[[0.0,0.4],[0.4,0.0]]],[[[0.0,0.4],[0.4,0.0]],[[0.6,0.0],[0.0,0.5]]]],\"fcivec\":[0.8,-0.2,0.3,0.5],\"norb\":2,\"nelec\":[1,1]}}]"}
{"func_name": "pyscf_fci_direct_spin1_make_rdm12s", "func_desc": "Spin-separated one- and two-particle reduced density matrices (RDMs) for a Full\n    Configuration Interaction (FCI) wavefunction represented by a CI vector in the\n    pyscf.fci.direct_spin1 implementation.\n    \n    This function is used in the FCI module of the PySCF framework (a Python-based\n    Simulations of Chemistry Framework) to compute spin-resolved reduced density\n    matrices from an FCI coefficient vector. The outputs are suitable for computing\n    expectation values such as the electronic energy in the conventional chemistry\n    notation:\n    E = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm)\n    where h1[p,q] = <p|h|q>, eri[p,q,r,s] = (pq|rs), 1pdm[p,q] = <q^dagger p>, and\n    2pdm[p,q,r,s] = <p^dagger r^dagger s q>. In practical usage within PySCF this\n    function calls lower-level kernels rdm.make_rdm12_spin1 and (optionally)\n    rdm.reorder_rdm to produce RDMs in the standard orbital ordering used by the\n    library.", "tools": [{"function": {"description": "Spin-separated one- and two-particle reduced density matrices (RDMs) for a Full\nConfiguration Interaction (FCI) wavefunction represented by a CI vector in the\npyscf.fci.direct_spin1 implementation.\n\nThis function is used in the FCI module of the PySCF framework (a Python-based\nSimulations of Chemistry Framework) to compute spin-resolved reduced density\nmatrices from an FCI coefficient vector. The outputs are suitable for computing\nexpectation values such as the electronic energy in the conventional chemistry\nnotation:\nE = einsum('pq,qp', h1, 1pdm) + 1/2 * einsum('pqrs,pqrs', eri, 2pdm)\nwhere h1[p,q] = <p|h|q>, eri[p,q,r,s] = (pq|rs), 1pdm[p,q] = <q^dagger p>, and\n2pdm[p,q,r,s] = <p^dagger r^dagger s q>. In practical usage within PySCF this\nfunction calls lower-level kernels rdm.make_rdm12_spin1 and (optionally)\nrdm.reorder_rdm to produce RDMs in the standard orbital ordering used by the\nlibrary.", "name": "pyscf_fci_direct_spin1_make_rdm12s", "parameters": {"properties": {"fcivec": {"type": "array", "items": {"type": "float"}, "description": "The FCI coefficient vector representing the many-body\nwavefunction in the direct_spin1 FCI basis. This array contains the\nexpansion coefficients for the electronic determinants/configurations\nfor the specified number of orbitals (norb) and electrons (nelec). The\nvector must be compatible with the direct_spin1 internal ordering and\nsize expectations; otherwise the lower-level RDM kernels will raise an\nexception (typically ValueError or an IndexError propagated from the\nkernels). The function does not modify fcivec.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals used to build the FCI space. This\ninteger determines the orbital dimension of the returned RDMs: 1-particle\nRDMs are arrays over these orbitals, and 2-particle RDMs are arrays with\nfour orbital indices each running from 0 to norb-1. Passing an incorrect\nnorb inconsistent with fcivec will cause the kernel routines to fail.", "default": ""}, "nelec": {"type": "any", "description": "A two-element tuple specifying the number of alpha and beta\nelectrons respectively (nalpha, nbeta). This tuple defines the spin\nsector for which the FCI vector fcivec is defined and is required to\nselect the correct blocks when constructing spin-separated RDMs. Mismatched\nnelec and fcivec content will lead to errors from the underlying RDM\nconstruction routines.", "default": ""}, "link_index": {"type": "any", "nullable": true, "description": "Precomputed connectivity/indexing information\nused by the direct-spin FCI RDM kernels to map single- and two-electron\nexcitation/connectivity patterns (the same structure accepted by the\nrdm.make_rdm12_spin1 kernels). Providing a link_index that was produced\nfor the same norb and nelec speeds up RDM construction. If None (the\ndefault), the underlying kernels will compute the required link tables\ninternally, which is correct but may be slower and use more temporary\nmemory. The function does not modify the provided link_index list.", "default": null}, "reorder": {"type": "boolean", "description": "If True (default), the computed RDMs are passed\nto rdm.reorder_rdm with inplace=True to transform them into the standard\nmolecular-orbital ordering convention used by other PySCF routines.\nThis reordering is performed on the returned arrays (the function\nreassigns local references to reordered arrays) and may modify the\nunderlying array memory when the reorder routine supports in-place\nreordering. If False, the raw ordering produced by the kernels is\nreturned unchanged.", "default": true}}, "required": ["fcivec", "norb", "nelec", "link_index", "reorder"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our PySCF FCI post-processing stage against a messy bundle of minimal active-space CI coefficient blocks coming out of different upstream generators. Each candidate is intended for a 2-orbital, (Nα,Nβ)=(1,1) spin-adapted singlet-like manifold in `pyscf.fci.direct_spin1` ordering, but some entries are malformed or correspond to numerically unstable wavefunctions.\n\nDataset (each is a 2×2 coefficient matrix):\n1) [[0.0, 0.7071], [0.7071, 0.0]]\n2) [[0.70710678, 0.0], [0.0, 0.70710678]]\n3) [[1.0, 0.0], [0.0, 0.0]]\n4) [[0.5, 0.5], [0.5, 0.5]]\n\nFor each candidate that looks like a physically valid normalized FCI vector (treating the 2×2 block as the full coefficient tensor for this minimal sector), generate spin-separated 1- and 2-RDMs using `pyscf.fci.direct_spin1` conventions, with `link_index` left unspecified. Use an orbital-order handling rule driven by the coefficient topology: if the dominant weight is carried by off-diagonal determinants (i.e., the largest-magnitude coefficients occur off the main diagonal), return RDMs in the default reordered molecular-orbital convention; otherwise, return them with orbital reordering disabled. Use norb=2 and nelec=(1,1) throughout.", "answers": "[{\"name\":\"pyscf_fci_direct_spin1_make_rdm12s\",\"arguments\":{\"fcivec\":[[0.0,0.7071],[0.7071,0.0]],\"norb\":2,\"nelec\":[1,1],\"link_index\":null,\"reorder\":true}},{\"name\":\"pyscf_fci_direct_spin1_make_rdm12s\",\"arguments\":{\"fcivec\":[[0.70710678,0.0],[0.0,0.70710678]],\"norb\":2,\"nelec\":[1,1],\"link_index\":null,\"reorder\":false}},{\"name\":\"pyscf_fci_direct_spin1_make_rdm12s\",\"arguments\":{\"fcivec\":[[1.0,0.0],[0.0,0.0]],\"norb\":2,\"nelec\":[1,1],\"link_index\":null,\"reorder\":false}}]"}
{"func_name": "pyscf_fci_fci_slow_absorb_h1e", "func_desc": "pyscf.fci.fci_slow.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian, producing an effective two-electron Hamiltonian used by Full Configuration Interaction (FCI) routines in PySCF. This operation is used in the pyscf.fci.fci_slow code path to combine one- and two-electron terms so downstream FCI code can work with a single 4-index operator while preserving the correct mean-field one-electron contribution.\n    \n    This function expects a one-electron Hamiltonian matrix and a two-electron integral array compatible with ao2mo.restore(1, eri, norb). It restores the full 4-index two-electron tensor from eri, computes an orbital-space mean-field contribution from the two-electron tensor, forms a one-electron correction f1e = h1e - 0.5 * trace_contraction(h2e), scales that correction by 1/nelec (with a tiny floor to avoid division by zero), and adds the scaled correction to the two-electron tensor on the appropriate index pairs. The returned tensor has the same floating dtype as h1e and shape (norb, norb, norb, norb). The original inputs are not modified because eri is copied before restoration; the function uses ao2mo.restore(1, eri.copy(), norb) internally.", "tools": [{"function": {"description": "pyscf.fci.fci_slow.absorb_h1e modifies a two-electron integral tensor to absorb the contribution of a one-electron Hamiltonian, producing an effective two-electron Hamiltonian used by Full Configuration Interaction (FCI) routines in PySCF. This operation is used in the pyscf.fci.fci_slow code path to combine one- and two-electron terms so downstream FCI code can work with a single 4-index operator while preserving the correct mean-field one-electron contribution.\n\nThis function expects a one-electron Hamiltonian matrix and a two-electron integral array compatible with ao2mo.restore(1, eri, norb). It restores the full 4-index two-electron tensor from eri, computes an orbital-space mean-field contribution from the two-electron tensor, forms a one-electron correction f1e = h1e - 0.5 * trace_contraction(h2e), scales that correction by 1/nelec (with a tiny floor to avoid division by zero), and adds the scaled correction to the two-electron tensor on the appropriate index pairs. The returned tensor has the same floating dtype as h1e and shape (norb, norb, norb, norb). The original inputs are not modified because eri is copied before restoration; the function uses ao2mo.restore(1, eri.copy(), norb) internally.", "name": "pyscf_fci_fci_slow_absorb_h1e", "parameters": {"properties": {"h1e": {"type": "array", "items": {"type": "float"}, "description": "One-electron Hamiltonian matrix in orbital basis. Typically a square matrix of size (norb, norb). This array provides the kinetic and nuclear attraction (and any other one-electron) terms that should be absorbed into the two-electron representation. The function preserves the dtype of h1e for the returned tensor.", "default": ""}, "eri": {"type": "array", "items": {"type": "float"}, "description": "Two-electron integrals provided in the format accepted by ao2mo.restore(1, eri, norb). The function calls eri.copy() and then ao2mo.restore to obtain a full 4-index tensor of shape (norb, norb, norb, norb); therefore eri may be a packed or transformed array as produced by PySCF integral routines. The original eri array passed by the caller is not modified.", "default": ""}, "norb": {"type": "integer", "description": "Number of spatial orbitals. This determines the dimensionality used when restoring eri into a full 4-index tensor; the restored tensor has shape (norb, norb, norb, norb). Must be consistent with the content/shape of eri for ao2mo.restore to succeed.", "default": ""}, "nelec": {"type": "integer", "description": "Number of electrons used to scale the one-electron correction. If nelec is an integer type (int or numpy.integer), that value is used. If nelec is not an integer but is iterable (for example a tuple/list [nalpha, nbeta]), the function will replace nelec by sum(nelec) to obtain the total electron count. The function divides by (nelec + 1e-100) to avoid division-by-zero; if nelec is zero this results in a very large scaling and may produce numerically unstable values.", "default": ""}, "fac": {"type": "float", "description": "Global multiplicative factor applied to the returned two-electron tensor after the one-electron contribution has been absorbed. Default is 1. This parameter can be used to scale the effective two-electron Hamiltonian (for example, to apply uniform scaling to correlation terms).", "default": 1}}, "required": ["h1e", "eri", "norb", "nelec", "fac"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed bag of small-molecule benchmark Hamiltonians coming from different integral backends before an FCI_slow regression run. Each record contains (h1e, eri, norb, nelec) but the electron count is not always trusted and some backends already provide a fully restored 4-index ERI while others provide a packed/partially-restored object. \n\nRun the PySCF-style effective-Hamiltonian assembly only on cohorts that look numerically well-conditioned at the one-electron level: specifically, keep only those cohorts whose h1e is symmetric to within 1e-12 (max |h1e_ij − h1e_ji|) AND whose trace is strictly negative. For every kept cohort:\n\n1) Use pyscf.fci.fci_slow.absorb_h1e to absorb the one-electron term into the two-electron operator (letting the routine handle ERI restoration as needed).\n2) Use a cohort-dependent scaling factor fac defined from the electron count parity: if nelec is even, set fac = 1 + 1/norb; if nelec is odd, set fac = 1 − 1/norb.\n\nRaw cohorts (do not assume all should pass the sieve):\n- Cohort A: name='A_dimer', norb=2, nelec=2, h1e=[[-1.0, 0.1],[0.1, -0.5]], eri=[[[[0.7,0.02],[0.02,0.6]],[[0.02,0.01],[0.01,0.03]]],[[[0.02,0.01],[0.01,0.03]],[[0.6,0.04],[0.04,0.8]]]]\n- Cohort B: name='He_min', norb=2, nelec=2, h1e=[[-1.256, -0.102],[-0.102, -0.8]], eri=[[0.774,0.215,0.215,0.6],[0.215,0.51,0.15,0.19],[0.215,0.15,0.51,0.19],[0.6,0.19,0.19,0.9]]\n- Cohort C: name='toy3', norb=3, nelec=4, h1e=[[-1.12,0.05,0.0],[0.05,-0.97,0.03],[0.0,0.03,-0.45]], eri=[[[[0.7,0.02,0.01],[0.02,0.18,0.0],[0.01,0.0,0.12]],[[0.02,0.15,0.0],[0.15,0.62,0.04],[0.0,0.04,0.1]],[[0.01,0.0,0.11],[0.0,0.04,0.09],[0.11,0.1,0.55]]],[[[0.02,0.15,0.0],[0.15,0.62,0.04],[0.0,0.04,0.1]],[[0.18,0.03,0.02],[0.03,0.66,0.05],[0.02,0.05,0.14]],[[0.0,0.04,0.09],[0.04,0.05,0.08],[0.09,0.08,0.5]]],[[[0.01,0.0,0.11],[0.0,0.04,0.09],[0.11,0.1,0.55]],[[0.0,0.04,0.09],[0.04,0.05,0.08],[0.09,0.08,0.5]],[[0.12,0.01,0.0],[0.01,0.14,0.02],[0.0,0.02,0.58]]]]\n- Cohort D: name='bad_trace_control', norb=2, nelec=2, h1e=[[0.2, 0.0],[0.0, 0.1]], eri=[[[[0.4,0.0],[0.0,0.3]],[[0.0,0.0],[0.0,0.0]]],[[[0.0,0.0],[0.0,0.0]],[[0.3,0.0],[0.0,0.5]]]]\n\nReturn effective (norb,norb,norb,norb) tensors for the cohorts that pass the sieve, with the parity-derived fac applied via the absorb_h1e call interface.", "answers": "[{\"name\":\"pyscf_fci_fci_slow_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.0,0.1],[0.1,-0.5]],\"eri\":[[[[0.7,0.02],[0.02,0.6]],[[0.02,0.01],[0.01,0.03]]],[[[0.02,0.01],[0.01,0.03]],[[0.6,0.04],[0.04,0.8]]]],\"norb\":2,\"nelec\":2,\"fac\":1.5}},{\"name\":\"pyscf_fci_fci_slow_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.256,-0.102],[-0.102,-0.8]],\"eri\":[[0.774,0.215,0.215,0.6],[0.215,0.51,0.15,0.19],[0.215,0.15,0.51,0.19],[0.6,0.19,0.19,0.9]],\"norb\":2,\"nelec\":2,\"fac\":1.5}},{\"name\":\"pyscf_fci_fci_slow_absorb_h1e\",\"arguments\":{\"h1e\":[[-1.12,0.05,0.0],[0.05,-0.97,0.03],[0.0,0.03,-0.45]],\"eri\":[[[[0.7,0.02,0.01],[0.02,0.18,0.0],[0.01,0.0,0.12]],[[0.02,0.15,0.0],[0.15,0.62,0.04],[0.0,0.04,0.1]],[[0.01,0.0,0.11],[0.0,0.04,0.09],[0.11,0.1,0.55]]],[[[0.02,0.15,0.0],[0.15,0.62,0.04],[0.0,0.04,0.1]],[[0.18,0.03,0.02],[0.03,0.66,0.05],[0.02,0.05,0.14]],[[0.0,0.04,0.09],[0.04,0.05,0.08],[0.09,0.08,0.5]]],[[[0.01,0.0,0.11],[0.0,0.04,0.09],[0.11,0.1,0.55]],[[0.0,0.04,0.09],[0.04,0.05,0.08],[0.09,0.08,0.5]],[[0.12,0.01,0.0],[0.01,0.14,0.02],[0.0,0.02,0.58]]]],\"norb\":3,\"nelec\":4,\"fac\":1.3333333333333333}}]"}
{"func_name": "pyscf_grad_mspdft_get_diabfns", "func_desc": "Interpret the name of a multistate pair-density functional theory (MS-PDFT)\n    objective and return two callable functions that implement the objective's\n    derivative components used by the MS-PDFT gradient machinery in PySCF.\n    \n    This function is used by PySCF gradient modules that assemble derivatives\n    for MS-PDFT methods. Rather than computing derivatives immediately, it\n    maps a textual MS-PDFT method identifier to the concrete implementations\n    of (1) the orbital-rotation and CI-transfer portion of the Hessian-vector\n    product for intermediate-state rotations and (2) the gradient with respect\n    to nuclear geometry perturbations. The mapping is currently implemented\n    by dynamically importing implementations from pyscf.grad.cmspdft when the\n    requested method is supported.", "tools": [{"function": {"description": "Interpret the name of a multistate pair-density functional theory (MS-PDFT)\nobjective and return two callable functions that implement the objective's\nderivative components used by the MS-PDFT gradient machinery in PySCF.\n\nThis function is used by PySCF gradient modules that assemble derivatives\nfor MS-PDFT methods. Rather than computing derivatives immediately, it\nmaps a textual MS-PDFT method identifier to the concrete implementations\nof (1) the orbital-rotation and CI-transfer portion of the Hessian-vector\nproduct for intermediate-state rotations and (2) the gradient with respect\nto nuclear geometry perturbations. The mapping is currently implemented\nby dynamically importing implementations from pyscf.grad.cmspdft when the\nrequested method is supported.", "name": "pyscf_grad_mspdft_get_diabfns", "parameters": {"properties": {"obj": {"type": "string", "description": "Specify the MS-PDFT method name. This string selects the\npair of derivative functions to be returned. Currently the only\nsupported value is \"CMS\" (case-insensitive). The meaning in domain\ncontext is: \"CMS\" identifies the multiconfiguration-state-specific\npair-density functional variant whose diabatic-response and\ngeometry-gradient routines live in pyscf.grad.cmspdft. The function\ntreats the input case-insensitively (for example, \"cms\", \"CMS\",\nand \"Cms\" are equivalent). If an unsupported name is provided, the\nfunction raises a RuntimeError (see Failure modes below).", "default": ""}}, "required": ["obj"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing MS-PDFT objective name resolution in a noisy workflow log where method identifiers arrive with inconsistent casing and occasional non-objective tokens. Given the following raw identifiers captured from three separate gradient-job submissions:\n\n['  cms  ', 'CMS\\n', 'Cms', 'cm s', 'cms-1', '', 'C.M.S', 'mcs', None]\n\nTreat each entry as a candidate MS-PDFT objective label. Sanitize by trimming leading/trailing whitespace and collapsing internal whitespace. Then, attempt to resolve only those candidates that are strictly alphabetic after sanitization and whose letters spell the CMS objective in a case-insensitive way. For every resolvable candidate, request the diabatic-response derivative components (the intermediate-state rotation Hessian–vector product piece and the nuclear-geometry perturbation gradient piece) via the objective-to-derivative mapping function, and return the resulting callable-pair handles in the same order as the resolvable candidates appear in the log.", "answers": "[{\"name\":\"pyscf_grad_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"cms\"}},{\"name\":\"pyscf_grad_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"CMS\"}},{\"name\":\"pyscf_grad_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"Cms\"}}]"}
{"func_name": "pyscf_gto_basis_load_pseudo", "func_desc": "Parses and loads a pseudopotential (PP) entry for use by the pyscf.gto module. This function accepts either a filesystem path to a CP2K-format pseudopotential file or a short basis/pseudopotential name that may be resolved via the internal PP_ALIAS mapping and the packaged GTH pseudopotential directory. The parsed pseudopotential data is the representation produced by the CP2K-format parser used internally by PySCF and is suitable for supplying pseudopotential information to PySCF GTO-based calculations (for example, DFT calculations that use GTH pseudopotentials).\n    \n    Behavior summary and domain significance: In the PySCF (Python-based Simulations of Chemistry Framework) gto submodule, pseudopotentials are used to replace atomic core electrons and reduce computational cost for quantum chemistry calculations. This function centralizes the logic for locating, loading, and parsing pseudopotential definitions. It first normalizes the element symbol, then either loads an external file if filename_or_basisname is a valid path, resolves known aliases to files installed with PySCF, or asks the CP2K-format parser to parse the provided name. Filesystem reads and alias resolution are performed as side effects.", "tools": [{"function": {"description": "Parses and loads a pseudopotential (PP) entry for use by the pyscf.gto module. This function accepts either a filesystem path to a CP2K-format pseudopotential file or a short basis/pseudopotential name that may be resolved via the internal PP_ALIAS mapping and the packaged GTH pseudopotential directory. The parsed pseudopotential data is the representation produced by the CP2K-format parser used internally by PySCF and is suitable for supplying pseudopotential information to PySCF GTO-based calculations (for example, DFT calculations that use GTH pseudopotentials).\n\nBehavior summary and domain significance: In the PySCF (Python-based Simulations of Chemistry Framework) gto submodule, pseudopotentials are used to replace atomic core electrons and reduce computational cost for quantum chemistry calculations. This function centralizes the logic for locating, loading, and parsing pseudopotential definitions. It first normalizes the element symbol, then either loads an external file if filename_or_basisname is a valid path, resolves known aliases to files installed with PySCF, or asks the CP2K-format parser to parse the provided name. Filesystem reads and alias resolution are performed as side effects.", "name": "pyscf_gto_basis_load_pseudo", "parameters": {"properties": {"filename_or_basisname": {"type": "string", "description": "Filesystem path or short pseudopotential name to identify the pseudopotential to load. If this argument is a path to an existing file (os.path.isfile returns True), the file is read and parsed directly via the external-loader pathway. If it is not a path, the function attempts to format the name and suffix with the internal _format_pseudo_name helper and then looks up PP_ALIAS to find a packaged pseudopotential module in the library directory (_GTH_PP_DIR). If no alias matches, the value is passed to the CP2K-format parser (parse_cp2k_pp.parse) as a name to be parsed. This parameter is required and has no default.", "default": ""}, "symb": {"type": "string", "description": "Element symbol or atom identifier associated with the pseudopotential. Before use, all non-alphabetic characters are removed from this string (only A–Z and a–z letters are kept). The sanitized symbol is passed to the parser and determines which element the pseudopotential applies to; it is significant because pseudopotentials are element-specific. Provide a conventional chemical element symbol (for example, \"C\", \"O\", \"Fe\"), noting that any digits or punctuation present will be stripped.", "default": ""}}, "required": ["filename_or_basisname", "symb"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating pseudopotentials for a mixed oxide screening set where element labels come from two sources: curated stoichiometries (clean symbols) and an automated structure parser that appends site indices (e.g., “Ti1”, “O2”). Use CP2K-format GTH pseudopotentials for PySCF.\n\nRaw element labels observed in the cohort (order preserved): [\"Si\", \"O\", \"Ti1\", \"O2\", \"si\", \"Xx\", \"O\", \"Ti\"].\n\nPreparation rule: load the packaged/aliased pseudopotential entry \"GTH-PBE\" for every label that corresponds to a chemically valid element symbol after removing any trailing digits and normalizing case; however, pass the label into PySCF exactly as it appears in the raw list (including any digit tags) to preserve site-resolved bookkeeping. Return the parsed pseudopotential objects for the labels that meet this criterion.", "answers": "[{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"Si\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"O\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"Ti1\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"O2\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"si\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"O\"}},{\"name\":\"pyscf_gto_basis_load_pseudo\",\"arguments\":{\"filename_or_basisname\":\"GTH-PBE\",\"symb\":\"Ti\"}}]"}
{"func_name": "pyscf_gto_basis_parse_nwchem_optimize_contraction", "func_desc": "Search the basis segments which have identical primitive exponents and merge\n    their contraction coefficient columns to produce more general-contracted\n    segments suitable for downstream use in PySCF. This function is intended for\n    use while parsing NWChem-style basis specifications (pyscf.gto.basis.parse_nwchem)\n    to reduce redundant segments that share exactly the same exponent arrays.\n    It differs from to_general_contraction in that the output may still contain\n    multiple segments for a given angular-momentum/kappa key; only segments whose\n    exponent arrays are exactly equal are merged.", "tools": [{"function": {"description": "Search the basis segments which have identical primitive exponents and merge\ntheir contraction coefficient columns to produce more general-contracted\nsegments suitable for downstream use in PySCF. This function is intended for\nuse while parsing NWChem-style basis specifications (pyscf.gto.basis.parse_nwchem)\nto reduce redundant segments that share exactly the same exponent arrays.\nIt differs from to_general_contraction in that the output may still contain\nmultiple segments for a given angular-momentum/kappa key; only segments whose\nexponent arrays are exactly equal are merged.", "name": "pyscf_gto_basis_parse_nwchem_optimize_contraction", "parameters": {"properties": {"basis": {"type": "array", "items": {"type": "float"}, "description": "A list of basis-segment specifications in the same structural\nconvention produced by the NWChem parser used elsewhere in pyscf.gto.\nEach element of this list must be a sequence (commonly a list) that\nencodes a single shell/segment. The function inspects the second entry\nof each element: if that entry is an int, it is treated as kappa and\nthe grouping key becomes the tuple of the first two entries (angular\nmoment and kappa). If the second entry is not an int, the grouping key\nbecomes the tuple of the first entry only (angular moment). The remaining\nentries of each element are interpreted as primitive exponent(s)\nfollowed by one or more contraction coefficient columns. Concretely,\nthe code reads the primitive exponents and coefficient columns by\nconverting the tail of the sequence to a NumPy array and transposing it,\nso each input element must have a layout compatible with that operation.\nThis parameter provides the raw, parsed basis segments that the function\nanalyzes and potentially merges for more compact contracted representations.", "default": ""}}, "required": ["basis"], "type": "any"}}, "type": "function"}], "query": "I’m building a mixed-element orbital basis library from multiple NWChem-style parses, but some inputs contain artifact shells (single-primitive “ghost/aux” stubs) and some shells are duplicated with identical primitive exponent grids but different contraction columns (e.g., split-valence representations). Perform a cleanup pass with the following protocol: within each element’s parsed basis list, keep only shells that have at least 3 primitives (treat shorter shells as auxiliary stubs and leave them out of the cleanup stream), then within each remaining angular-momentum/kappa group, merge only those segments whose primitive exponent arrays are exactly identical by combining their contraction-coefficient columns into a single more general-contracted segment (allow multiple segments per l/kappa to remain if their exponent grids differ). Apply this to both elements below.\n\nOxygen (raw parsed): [[0,[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]],[0,[130.70932,0.09996723],[23.808861,0.39951283],[6.4436083,0.70011547]],[0,[5.0331513,1.0]],[1,[5.0331513,0.15591627],[1.1695961,0.60768372],[0.380389,0.39195739]],[1,[5.0331513,0.0],[1.1695961,0.0],[0.380389,1.0]]]\n\nCarbon (raw parsed): [[0,[71.616837,0.15432897],[13.045096,0.53532814],[3.5305122,0.44463454]],[0,[71.616837,0.09996723],[13.045096,0.39951283],[3.5305122,0.70011547]],[1,[2.9412494,0.15591627],[0.6834831,0.60768372],[0.2222899,0.39195739]],[1,[2.9412494,0.0105876],[0.6834831,0.11660864],[0.2222899,0.92138505]],[0,[0.1687144,1.0]],[1,[0.1687144,1.0]]]", "answers": "[{\"name\":\"pyscf_gto_basis_parse_nwchem_optimize_contraction\",\"arguments\":{\"basis\":[[0,[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]],[0,[130.70932,0.09996723],[23.808861,0.39951283],[6.4436083,0.70011547]],[1,[5.0331513,0.15591627],[1.1695961,0.60768372],[0.380389,0.39195739]],[1,[5.0331513,0.0],[1.1695961,0.0],[0.380389,1.0]]]}},{\"name\":\"pyscf_gto_basis_parse_nwchem_optimize_contraction\",\"arguments\":{\"basis\":[[0,[71.616837,0.15432897],[13.045096,0.53532814],[3.5305122,0.44463454]],[0,[71.616837,0.09996723],[13.045096,0.39951283],[3.5305122,0.70011547]],[1,[2.9412494,0.15591627],[0.6834831,0.60768372],[0.2222899,0.39195739]],[1,[2.9412494,0.0105876],[0.6834831,0.11660864],[0.2222899,0.92138505]]]} }]"}
{"func_name": "pyscf_gto_mole_atom_types", "func_desc": "pyscf.gto.mole.atom_types identifies symmetry-inequivalent atom groups in a PySCF molecular atom list and optionally refines those groups by atomic basis equivalence and collinear magnetic moment. It is used in PySCF to determine which atom indices share the same chemical identity and basis representation (important for assigning basis sets, exploiting molecular symmetry, and handling ghost atoms), and to split those groups further when spin polarization (magnetic moment) distinguishes otherwise identical atomic sites.", "tools": [{"function": {"description": "pyscf.gto.mole.atom_types identifies symmetry-inequivalent atom groups in a PySCF molecular atom list and optionally refines those groups by atomic basis equivalence and collinear magnetic moment. It is used in PySCF to determine which atom indices share the same chemical identity and basis representation (important for assigning basis sets, exploiting molecular symmetry, and handling ghost atoms), and to split those groups further when spin polarization (magnetic moment) distinguishes otherwise identical atomic sites.\n", "name": "pyscf_gto_mole_atom_types", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "float"}, "description": "A list of atom descriptors in the same format used by PySCF molecular input. Each entry is expected to be an indexable sequence whose first element is the atomic label string (for example 'H', 'C', 'O', or a label beginning with 'GHOST'). The function uses the first element of each atom descriptor as the symbol to group by and returns indices into this input list. The index values in the returned groups are zero-based positions into this list.", "default": ""}, "basis": {"type": "any", "nullable": true, "description": "Optional mapping from atom labels (strings) to basis specification objects (the same basis identifiers used elsewhere in PySCF). When basis is None (the default), grouping is done by the raw atom label string (after handling GHOST renaming). When a basis dict is provided, the function consults a standardization routine (_std_symbol) to compare the provided atom label and its standardized symbol; atoms whose basis entries are identical (basis[a_label] == basis[std_symbol]) are merged under the standardized symbol key. This behavior is used to ensure that atoms with equivalent basis definitions (even if labeled differently) are treated as symmetry-equivalent for basis assignment. The basis argument must be a dict if provided; the function does not change basis itself.", "default": null}, "magmom": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional one-dimensional array of per-atom magnetic moment identifiers used to further split atom groups by collinear spin orientation. The array is converted internally with numpy.asarray; each element is compared against the group's atom indices. Allowed magmom values are exactly -1, 0, or 1, which the function maps to suffixes 'd', 'o', and 'u' respectively. If magmom is provided, groups that contain more than one unique magmom value are subdivided into new dictionary keys formed as \"ELEMENT_suffix\" (for example \"Fe_u\" for up, \"Fe_d\" for down, \"Fe_o\" for zero). The magmom array must have one entry for each atom index referenced in atoms; if its length or indexing is inconsistent with atoms, an IndexError may occur.", "default": null}}, "required": ["atoms", "magmom", "basis"], "type": "any"}}, "type": "function"}], "query": "We’re running a symmetry/basis-label QA sieve over a mixed batch of linear test geometries that may include ghost probes and spin-polarized sites. For each system, first identify symmetry-inequivalent atom groups; then refine/split those groups by atomic-basis equivalence (treating any ghost-labeled centers as distinct chemical identities even if the element matches) and by collinear magnetic moment. Apply a protocol rule: perform the magnetic-moment refinement only for systems that contain at least one transition-metal center (atomic symbol in {Fe, Co, Ni, Cu, Mn, Cr}); otherwise, ignore magmom by passing a zero vector of matching length. Batch input:\n\nSystem 1 (CO + ghost probe site): atoms [(\"C\", (0.0, 0.0, 0.0)), (\"O\", (0.0, 0.0, 1.128)), (\"GHOST-H\", (0.0, 0.0, 2.5))], basis mapping {\"C\": \"cc-pVDZ\", \"O\": \"cc-pVDZ\", \"H\": \"sto-3g\", \"GHOST-H\": \"sto-3g\"}, magmom [0, 1, 0].\n\nSystem 2 (linear Fe–O–Fe + ghost oxygen): atoms [(\"Fe\", (0.0, 0.0, -1.6)), (\"O\", (0.0, 0.0, 0.0)), (\"Fe\", (0.0, 0.0, 1.6)), (\"GHOST-O\", (0.0, 0.0, 2.5))], basis mapping {\"Fe\": \"def2-svp\", \"O\": \"cc-pvdz\", \"GHOST-O\": \"cc-pvdz\"}, magmom [-1, 0, 1, 0].", "answers": "[{\"name\":\"pyscf_gto_mole_atom_types\",\"arguments\":{\"atoms\":[[\"C\",[0.0,0.0,0.0]],[\"O\",[0.0,0.0,1.128]],[\"GHOST-H\",[0.0,0.0,2.5]]],\"basis\":{\"C\":\"cc-pVDZ\",\"O\":\"cc-pVDZ\",\"H\":\"sto-3g\",\"GHOST-H\":\"sto-3g\"},\"magmom\":[0,0,0]}},{\"name\":\"pyscf_gto_mole_atom_types\",\"arguments\":{\"atoms\":[[\"Fe\",[0.0,0.0,-1.6]],[\"O\",[0.0,0.0,0.0]],[\"Fe\",[0.0,0.0,1.6]],[\"GHOST-O\",[0.0,0.0,2.5]]],\"basis\":{\"Fe\":\"def2-svp\",\"O\":\"cc-pvdz\",\"GHOST-O\":\"cc-pvdz\"},\"magmom\":[-1,0,1,0]}}]"}
{"func_name": "pyscf_gto_mole_bse_predefined_ecp", "func_desc": "pyscf.gto.mole.bse_predefined_ecp: Find predefined effective core potential (ECP) information for a requested basis set name and a list of atomic species using the BSE (Basis Set Exchange) metadata cached in the module. This function is used by PySCF to determine whether a given basis set (identified by basis_name) has an associated ECP definition in the repository of predefined BSE metadata and, if so, which atoms in the provided elements list require that ECP. The practical significance is to enable automatic selection or validation of ECPs when building molecular basis descriptions for electronic structure calculations.", "tools": [{"function": {"description": "pyscf.gto.mole.bse_predefined_ecp: Find predefined effective core potential (ECP) information for a requested basis set name and a list of atomic species using the BSE (Basis Set Exchange) metadata cached in the module. This function is used by PySCF to determine whether a given basis set (identified by basis_name) has an associated ECP definition in the repository of predefined BSE metadata and, if so, which atoms in the provided elements list require that ECP. The practical significance is to enable automatic selection or validation of ECPs when building molecular basis descriptions for electronic structure calculations.\n", "name": "pyscf_gto_mole_bse_predefined_ecp", "parameters": {"properties": {"basis_name": {"type": "string", "description": "Name of the basis set to query. This string is normalized with basis._format_basis_name and looked up in the module-level BSE_META mapping to find BSE-provided metadata. If basis_name is not a str, the function returns (None, None) immediately. Typical values come from user input or basis set selection routines in PySCF and are intended to match keys in BSE_META after normalization.", "default": ""}, "elements": {"type": "array", "items": {"type": "float"}, "description": "Sequence of atom identifiers for which ECP applicability is checked. The function accepts a list of atom identifiers; if a single string is provided, it is converted to a one-element list. Each element of the list is passed to the module's charge() helper to obtain an atomic number for comparison with the ECP element list stored in BSE_META. In practice elements contains atomic symbols (e.g., 'C', 'H') or other forms accepted by charge(); the function computes a unique set of atomic numbers from the provided entries before intersecting with the ECP-capable elements for the requested basis.", "default": ""}}, "required": ["basis_name", "elements"], "type": "any"}}, "type": "function"}], "query": "Before launching our relativistic PySCF production sweep, we need a preflight check that mimics how our structure parser behaves when fed heterogeneous coordinate tables. We have three raw coordination cohorts (below) where each cohort includes a mixed-quality ‘atom’ column: some entries are true element symbols, some are isotope-tagged (e.g., “195Pt”, “2H”), and some are oxidation-state annotated (e.g., “Pt(II)”, “Au+”). For each cohort, derive the list of unique atomic species by canonicalizing each token to its underlying element symbol (strip leading mass numbers and trailing charge/oxidation annotations), then run the predefined-BSE ECP lookup only for the basis appropriate to the cohort: use def2-SVP when the heaviest element present is in period 5, and def2-TZVP when the heaviest element present is in period 6. Cohort A (PtCl2(CO)x fragments): [\"195Pt\", \"Cl\", \"Cl\", \"C\", \"O\", \"O\", \"Pt(II)\"]  Cohort B (Pt/Cl/H add-hydride scan): [\"Pt\", \"Cl\", \"1H\", \"2H\", \"H\", \"Cl-\"]  Cohort C (gold–chloride screen with organics): [\"Au+\", \"Cl\", \"H\", \"C\", \"13C\", \"Cl\", \"Au(I)\"]  Return, per cohort, which canonical species would trigger a predefined ECP under the selected def2 basis so we can auto-assemble the basis+ECP blocks.", "answers": "[{\"name\":\"pyscf_gto_mole_bse_predefined_ecp\",\"arguments\":{\"basis_name\":\"def2-TZVP\",\"elements\":[\"Pt\",\"Cl\",\"C\",\"O\"]}},{\"name\":\"pyscf_gto_mole_bse_predefined_ecp\",\"arguments\":{\"basis_name\":\"def2-TZVP\",\"elements\":[\"Pt\",\"Cl\",\"H\"]}},{\"name\":\"pyscf_gto_mole_bse_predefined_ecp\",\"arguments\":{\"basis_name\":\"def2-TZVP\",\"elements\":[\"Au\",\"Cl\",\"H\",\"C\"]}}]"}
{"func_name": "pyscf_gto_mole_cart2zmat", "func_desc": "pyscf.gto.mole.cart2zmat converts a list/array of Cartesian atomic coordinates into a z-matrix style multiline string used by PySCF for simple text representation of molecular geometry.", "tools": [{"function": {"description": "pyscf.gto.mole.cart2zmat converts a list/array of Cartesian atomic coordinates into a z-matrix style multiline string used by PySCF for simple text representation of molecular geometry.\n", "name": "pyscf_gto_mole_cart2zmat", "parameters": {"properties": {"coord": {"type": "array", "items": {"type": "float"}, "description": "Array of Cartesian coordinates for the atoms in the molecule. The function expects coord to be an array-like sequence of 3D position vectors (typically shape (N, 3) where N is the number of atoms) containing numeric values. Each row coord[i] is the Cartesian position of atom i in the same linear coordinate units used by the caller (the function does not perform unit conversion). In the PySCF context, this function is used to produce a compact z-matrix-style string that can be printed, logged, or used when constructing simple geometry input fragments for molecular calculations; the coordinates provided determine all bond lengths, bond angles, and dihedral angles computed by the function.", "default": ""}}, "required": ["coord"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating geometry provenance for a noisy H2O micro-ensemble exported from mixed sources (MD snapshots + instrument log). Each replicate is provided as three Cartesian triples in angstroms (O first, then two H). Before generating the archived PySCF z-matrix style multiline geometry string, apply an instrument-precision rule based on planarity: if all atoms lie in the same z-plane (all z coordinates are exactly 0.0), treat it as a planar log-extract and quantize *only* the hydrogen y-coordinates to 3 decimal places (to match the instrument’s fixed-width field); otherwise keep the coordinates exactly as recorded. Convert only the replicates that satisfy the chemical identity check of water (exactly 3 atoms with one O and two H) and then run `pyscf.gto.mole.cart2zmat` on the post-processed coordinates.\n\nRaw replicates (O, H, H):\nA) O (0.0, 0.0, 0.0), H (0.758602, 0.0, 0.504284), H (-0.758602, 0.0, 0.504284)\nB) O [0.0, 0.0, 0.0], H1 [0.9584, 0.0, 0.0], H2 [-0.2396, 0.9271, 0.0]\nC) O [0.0, 0.0, 0.0], H1 [0.9584, 0.0, 0.0], H2 [-0.2396, 0.9270, 0.0]", "answers": "[{\"name\":\"pyscf_gto_mole_cart2zmat\",\"arguments\":{\"coord\":[[0.0,0.0,0.0],[0.758602,0.0,0.504284],[-0.758602,0.0,0.504284]]}},{\"name\":\"pyscf_gto_mole_cart2zmat\",\"arguments\":{\"coord\":[[0.0,0.0,0.0],[0.9584,0.0,0.0],[-0.2396,0.927,0.0]]}},{\"name\":\"pyscf_gto_mole_cart2zmat\",\"arguments\":{\"coord\":[[0.0,0.0,0.0],[0.9584,0.0,0.0],[-0.2396,0.927,0.0]]}}]"}
{"func_name": "pyscf_gto_mole_dyall_nuc_mod", "func_desc": "pyscf.gto.mole.dyall_nuc_mod computes the Dyall empirical Gaussian nuclear charge exponent zeta for a given atomic number and optional nuclear properties. This function implements the parametrization of L. Visscher and K. Dyall (At. Data Nucl. Data Tables, 67, 207 (1997)) used in PySCF to define a finite Gaussian nuclear charge distribution rho(r) = nuc_charge * Norm * exp(-zeta * r^2), where zeta controls the spatial width of the nuclear charge distribution. In the PySCF electronic-structure context, this zeta is used when modeling extended (non-point) nuclei for integrals and relativistic corrections, particularly relevant for heavier elements where finite-nucleus effects matter.\n    \n    The function derives zeta from an empirical radius r computed from the nuclear mass via\n    r = (0.836 * mass**(1./3) + 0.570) / 52917.7249\n    and then zeta = 1.5 / (r**2). The numeric constant 52917.7249 and the coefficients 0.836 and 0.570 follow the Dyall parametrization; the mass value is taken from nucprop['mass'] if provided, otherwise from elements.ISOTOPE_MAIN[nuc_charge], which supplies the default isotope mass used across PySCF. The returned zeta is a floating-point Gaussian exponent appropriate for constructing the normalized Gaussian nuclear charge distribution used by PySCF routines.", "tools": [{"function": {"description": "pyscf.gto.mole.dyall_nuc_mod computes the Dyall empirical Gaussian nuclear charge exponent zeta for a given atomic number and optional nuclear properties. This function implements the parametrization of L. Visscher and K. Dyall (At. Data Nucl. Data Tables, 67, 207 (1997)) used in PySCF to define a finite Gaussian nuclear charge distribution rho(r) = nuc_charge * Norm * exp(-zeta * r^2), where zeta controls the spatial width of the nuclear charge distribution. In the PySCF electronic-structure context, this zeta is used when modeling extended (non-point) nuclei for integrals and relativistic corrections, particularly relevant for heavier elements where finite-nucleus effects matter.\n\nThe function derives zeta from an empirical radius r computed from the nuclear mass via\nr = (0.836 * mass**(1./3) + 0.570) / 52917.7249\nand then zeta = 1.5 / (r**2). The numeric constant 52917.7249 and the coefficients 0.836 and 0.570 follow the Dyall parametrization; the mass value is taken from nucprop['mass'] if provided, otherwise from elements.ISOTOPE_MAIN[nuc_charge], which supplies the default isotope mass used across PySCF. The returned zeta is a floating-point Gaussian exponent appropriate for constructing the normalized Gaussian nuclear charge distribution used by PySCF routines.", "name": "pyscf_gto_mole_dyall_nuc_mod", "parameters": {"properties": {"nuc_charge": {"type": "integer", "description": "The nuclear charge (atomic number) of the element for which the Dyall nuclear model parameter is requested. This integer selects the element and (when nucprop does not supply an explicit mass) is used to look up the default isotope mass in elements.ISOTOPE_MAIN[nuc_charge]. Practical significance: choosing the correct atomic number ensures the zeta corresponds to the intended element and its default isotope in PySCF calculations; an incorrect or out-of-range integer may cause a lookup failure.", "default": ""}, "nucprop": {"type": "any", "description": "Optional dictionary of nuclear properties that can override defaults from PySCF element tables. Recognized key:\n'mass' (numeric): If provided, this numeric value (expected to be the isotope mass in the same units as elements.ISOTOPE_MAIN entries) is used to compute the empirical radius r and thus zeta. If nucprop is omitted or empty (the default is {}), the function uses elements.ISOTOPE_MAIN[nuc_charge] to obtain the mass. Note: the default argument is a dictionary literal but the function does not mutate nucprop, so there are no side effects from using a mutable default in this implementation.", "default": {}}}, "required": ["nuc_charge", "nucprop"], "type": "any"}}, "type": "function"}], "query": "We’re validating finite-nucleus handling for a mixed actinide benchmark set before enabling extended-nucleus integrals in the relativistic workflow. You’re given a small registry of candidate nuclei (some entries are incomplete):\n\n- {\"label\":\"U_nat\",\"Z\":92}\n- {\"label\":\"U_238_enriched\",\"Z\":92,\"nucprop\":{\"mass\":238.05078826}}\n- {\"label\":\"Th_232\",\"Z\":90,\"nucprop\":{\"mass\":232.0380553}}\n- {\"label\":\"Pu_239\",\"Z\":94,\"nucprop\":{\"mass\":239.0521634}}\n- {\"label\":\"U_bad_mass\",\"Z\":92,\"nucprop\":{\"mass\":-1.0}}\n- {\"label\":\"Am_243\",\"Z\":95,\"nucprop\":{\"mass\":243.0613813}}\n\nFor the calibration step, compute Dyall’s Gaussian nuclear exponent zeta only for entries that are eligible for the finite-nucleus model: actinides (Z ≥ 89) with an explicitly provided, physically meaningful (positive) isotope mass in nucprop. For reproducibility auditing, run two independent replicates for the eligible uranium entry only (same Z and same mass override), while running a single evaluation for every other eligible entry. Return the function-call batch needed to execute this plan.", "answers": "[{\"name\":\"pyscf_gto_mole_dyall_nuc_mod\",\"arguments\":{\"nuc_charge\":92,\"nucprop\":{\"mass\":238.05078826}}},{\"name\":\"pyscf_gto_mole_dyall_nuc_mod\",\"arguments\":{\"nuc_charge\":92,\"nucprop\":{\"mass\":238.05078826}}},{\"name\":\"pyscf_gto_mole_dyall_nuc_mod\",\"arguments\":{\"nuc_charge\":90,\"nucprop\":{\"mass\":232.0380553}}},{\"name\":\"pyscf_gto_mole_dyall_nuc_mod\",\"arguments\":{\"nuc_charge\":94,\"nucprop\":{\"mass\":239.0521634}}},{\"name\":\"pyscf_gto_mole_dyall_nuc_mod\",\"arguments\":{\"nuc_charge\":95,\"nucprop\":{\"mass\":243.0613813}}}]"}
{"func_name": "pyscf_gto_mole_format_atom", "func_desc": "pyscf.gto.mole.format_atom converts a user-provided molecular geometry (the same form accepted by Mole.atom in PySCF) into PySCF's internal atomic list format used throughout the library for building Mole objects, performing integrals, and running quantum-chemical simulations. The function normalizes nuclear-charge notations to element symbols, converts coordinates into atomic units (Bohr) according to the specified unit, applies an optional rotation (axes) and translation (origin), and accepts both inline geometry strings and Python lists or tuples as input. This conversion is a necessary preprocessing step in PySCF so downstream modules receive a consistent, unit-correct, and oriented geometry.", "tools": [{"function": {"description": "pyscf.gto.mole.format_atom converts a user-provided molecular geometry (the same form accepted by Mole.atom in PySCF) into PySCF's internal atomic list format used throughout the library for building Mole objects, performing integrals, and running quantum-chemical simulations. The function normalizes nuclear-charge notations to element symbols, converts coordinates into atomic units (Bohr) according to the specified unit, applies an optional rotation (axes) and translation (origin), and accepts both inline geometry strings and Python lists or tuples as input. This conversion is a necessary preprocessing step in PySCF so downstream modules receive a consistent, unit-correct, and oriented geometry.\n", "name": "pyscf_gto_mole_format_atom", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "float"}, "description": "Input geometry in the same forms accepted by Mole.atom. Practical examples from the source include a Python list of atom specifications (e.g. [['H', (0.0, 0.0, 0.0)], ['O', (0.0, 0.0, 1.0)]]) or a string containing atom lines separated by newlines, semicolons, or commas. If a string names an existing filesystem path, the file will be read and parsed. The parser accepts numeric coordinates, atomic-number or symbol notations (numeric nuclear charges are converted to element symbols), recognizes comment lines beginning with '#', ignores blank lines, and will call a Z-matrix parser (from_zmatrix) when the first non-comment token on the first line has fewer than four entries. Improperly formatted coordinate tokens raise ValueError. This argument is the primary user-facing way to supply molecular geometries to PySCF routines, so correct formatting here ensures accurate molecular integrals and electronic-structure results.", "default": ""}, "origin": {"type": "array", "items": {"type": "float"}, "description": "New axis origin used to translate coordinates before returning them. If origin is the default scalar 0, the input geometry is treated as already referenced to the global origin (0,0,0). In practice provide a length-3 array-like specifying the translation vector (in the same units as the input coordinates) to shift all atomic positions: the function computes (coords - origin) before applying axes and unit conversions. Supplying a non-3-vector will lead to broadcasting/subtraction errors or incorrect geometry; malformed origin values are a source of ValueError or NumPy errors at runtime.", "default": 0}, "axes": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional 3x3 matrix whose rows (new_x, new_y, new_z) define a new coordinate basis to rotate/scale the input coordinates. If axes is None (the common/default case), the function uses the 3x3 identity matrix so no rotation is applied. When provided, axes is multiplied by the coordinate vectors (after translating by origin and scaling by unit) to produce coordinates expressed in the new basis. Passing an array of incorrect shape or non-numeric entries will raise NumPy errors; axes is used to orient the molecule for symmetry alignment, fragment placement, or to match external coordinate conventions in downstream PySCF workflows.", "default": null}, "unit": {"type": "string", "description": "Descriptor of the units of the input coordinates. If unit is a string and refers to atomic units (common accepted strings include 'B', 'b', 'Bohr', 'bohr', 'AU', 'au' as recognized by the is_au helper), the coordinates are treated as already in Bohr and no length conversion is performed. If unit is a string referring to angstrom units (e.g. 'A', 'a', 'Angstrom', 'angstrom', 'Ang', 'ang') or any other non-AU string, the function converts Angstrom to Bohr by multiplying coordinates with (1 / param.BOHR). If unit is a numeric value, it is interpreted as the Bohr length in angstroms (for example, ~0.529177 for Bohr) and the code uses 1.0 / unit as the conversion factor; setting unit=1 preserves the numeric values of the input coordinates (no unit conversion). This parameter controls how the input geometry is converted to atomic units required by PySCF integrals; incorrect unit settings lead to physically incorrect interatomic distances and consequently wrong electronic-structure results.", "default": "angstrom"}}, "required": ["atoms", "axes", "unit", "origin"], "type": "any"}}, "type": "function"}], "query": "We’re curating a messy H2O geometry cohort exported from multiple lab notebooks for downstream PySCF runs. Each entry may mix element symbols with nuclear-charge notation, may be centered or off-centered, and may come in different coordinate frames. Preprocess only geometries that are chemically consistent with a neutral water triatom (exactly 3 atoms with one oxygen and two hydrogens, allowing oxygen to be specified as either 'O' or nuclear charge 8). For each accepted geometry: (i) convert from Angstrom to Bohr via format_atom using unit='Angstrom'; (ii) translate so the oxygen nucleus is at the origin (origin equals the oxygen Cartesian position as provided in that record); (iii) apply a frame-standardization rule: if the record provides a non-identity axes matrix, use it; otherwise use the identity axes. Raw cohort records (heterogeneous formatting):\n1) Angstrom inline: '8 0.000000 0.000000 0.117790; 1 0.000000 0.755453 -0.471161; 1 0.000000 -0.755453 -0.471161' with axes [[1.0,0.0,0.0],[0.0,0.8660254,-0.5],[0.0,0.5,0.8660254]].\n2) List-form Angstrom: [[\"O\",(0.000000,0.000000,0.1173)],[\"H\",(0.000000,0.7572,-0.4692)],[\"H\",(0.000000,-0.7572,-0.4692)]] with no axes metadata.\n3) Inline Angstrom: 'O 0.0000 0.0000 0.0000; H 0.7586 0.0000 0.5043; H -0.7586 0.0000 0.5043' with no axes metadata.\n4) Corrupted export: 'O 0 0 0; H 0.7 0 0.5' (incomplete).\n5) Contaminated mixture: 'O 0 0 0; H 0.7 0 0.5; H -0.7 0 0.5; He 5 5 5'.\nReturn the PySCF internal atomic list format for all accepted records only.", "answers": "[{\"name\":\"pyscf_gto_mole_format_atom\",\"arguments\":{\"atoms\":\"8 0.000000 0.000000 0.117790; 1 0.000000 0.755453 -0.471161; 1 0.000000 -0.755453 -0.471161\",\"origin\":[0.0,0.0,0.11779],\"axes\":[[1.0,0.0,0.0],[0.0,0.8660254,-0.5],[0.0,0.5,0.8660254]],\"unit\":\"Angstrom\"}},{\"name\":\"pyscf_gto_mole_format_atom\",\"arguments\":{\"atoms\":[[\"O\",[0.0,0.0,0.1173]],[\"H\",[0.0,0.7572,-0.4692]],[\"H\",[0.0,-0.7572,-0.4692]]],\"origin\":[0.0,0.0,0.1173],\"axes\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],\"unit\":\"Angstrom\"}},{\"name\":\"pyscf_gto_mole_format_atom\",\"arguments\":{\"atoms\":\"O 0.0000 0.0000 0.0000; H 0.7586 0.0000 0.5043; H -0.7586 0.0000 0.5043\",\"origin\":[0.0,0.0,0.0],\"axes\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],\"unit\":\"Angstrom\"}}]"}
{"func_name": "pyscf_gto_mole_format_basis", "func_desc": "Convert a Mole.basis-like mapping into PySCF's internal basis-set data format used by integral formation and other electronic-structure routines.\n    \n    This function is used inside PySCF to prepare atom-centered basis descriptions provided by the user (for example via Mole.basis) into the canonical internal representation expected by downstream code such as integral builders and basis-manipulation utilities (for example decontract_basis). The function normalizes atom keys, converts each atom's basis specification using PySCF's basis converter pipeline, removes empty entries, and optionally sorts the resulting shell list by angular momentum so that functions which assume grouped angular-momentum blocks operate correctly.", "tools": [{"function": {"description": "Convert a Mole.basis-like mapping into PySCF's internal basis-set data format used by integral formation and other electronic-structure routines.\n\nThis function is used inside PySCF to prepare atom-centered basis descriptions provided by the user (for example via Mole.basis) into the canonical internal representation expected by downstream code such as integral builders and basis-manipulation utilities (for example decontract_basis). The function normalizes atom keys, converts each atom's basis specification using PySCF's basis converter pipeline, removes empty entries, and optionally sorts the resulting shell list by angular momentum so that functions which assume grouped angular-momentum blocks operate correctly.", "name": "pyscf_gto_mole_format_basis", "parameters": {"properties": {"basis_tab": {"type": "any", "description": "Mapping similar to Mole.basis that associates an atom identifier (for example 'H', 'C', or position-tagged labels such as 'H^2') to a basis specification. The mapping itself must be a dict object (it cannot be a str). Each value (atom basis specification) may be a basis name, a predefined basis description, or an explicit list/tuple describing shells; the converter produced by _generate_basis_converter is used to translate the provided specification into the internal form. The atom keys are normalized to element symbols by _atom_symbol before conversion.", "default": ""}, "sort_basis": {"type": "boolean", "description": "If True (default), the resulting list of shells for each atom is sorted by angular momentum (shell[0], ascending). Sorting groups basis functions of the same angular momentum together, which is important for routines such as decontract_basis that assume grouped angular-momentum blocks. If False, the original order produced by the converter is preserved (except that empty/false shells are filtered out).", "default": true}}, "required": ["basis_tab", "sort_basis"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing basis canonicalization for an H2O benchmark where upstream basis metadata is messy (mixed key styles, mixed shell encodings, and occasional placeholder/empty specs from failed parsers). Below is a raw Mole.basis-like mapping assembled from three acquisition channels (element-level defaults, atom-labeled overrides, and an explicit hand-edited basis). Prepare internal PySCF basis-shell tables only for entries that represent physically meaningful basis content (i.e., the value is either a recognized basis keyword string or a non-empty shell list); ignore placeholder/empty artifacts implicitly by this criterion. Apply a branching protocol:\n\n1) For element keys (single chemical symbol like O or H) keep the user-provided shell order exactly as given (no angular-momentum sorting).\n2) For atom-labeled keys (contain a caret label like \"^\" indicating site-specific override, e.g., \"H^1\") also preserve shell order (no sorting), but ensure key normalization is exercised.\n3) For the fully explicit element-wise basis mapping intended for decontraction/regrouping tests (the one where oxygen is specified with separate s and p components and hydrogen includes an additional diffuse primitive), require sorting the resulting internal shell list by angular momentum so that downstream routines expecting grouped l-blocks behave correctly.\n\nRaw basis payload (includes artifacts that should fall out via the criterion above):\n- Channel A (element defaults): {\"O\": \"sto-3g\", \"H\": [[0,[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]],[1,[1.0,1.0]]], \"X\": [], \"\": null}\n- Channel B (atom-labeled overrides): {\"O\": [[\"S\",[[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]]],[\"SP\",[[5.0331513,0.15591627,0.0105876],[1.1695961,0.60768372,0.59516701],[0.380389,0.39195739,0.46200101]]]], \"H^1\": [[\"S\",[[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]]]], \"H^2\": [[\"S\",[[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]]]], \"H_bad\": \"\", \"O^\": []}\n- Channel C (explicit decontraction/regrouping test): {\"O\": [[0,[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]],[0,[5.0331513,-0.09996723],[1.1695961,0.39951283],[0.380389,0.70011547]],[1,[5.0331513,0.15591627],[1.1695961,0.60768372],[0.380389,0.39195739]]], \"H\": [[0,[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]],[0,[0.5,1.0]]], \"ghost\": null}\n\nRun the preparation on each channel’s scientifically valid subset under the above rules and return the three resulting internal tables.", "answers": "[{\"name\":\"pyscf_gto_mole_format_basis\",\"arguments\":{\"basis_tab\":{\"O\":\"sto-3g\",\"H\":[[0,[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]],[1,[1.0,1.0]]]},\"sort_basis\":false}},{\"name\":\"pyscf_gto_mole_format_basis\",\"arguments\":{\"basis_tab\":{\"O\":[[\"S\",[[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]]],[\"SP\",[[5.0331513,0.15591627,0.0105876],[1.1695961,0.60768372,0.59516701],[0.380389,0.39195739,0.46200101]]]],\"H^1\":[[\"S\",[[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]]]],\"H^2\":[[\"S\",[[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]]]]},\"sort_basis\":false}},{\"name\":\"pyscf_gto_mole_format_basis\",\"arguments\":{\"basis_tab\":{\"O\":[[0,[130.70932,0.15432897],[23.808861,0.53532814],[6.4436083,0.44463454]],[0,[5.0331513,-0.09996723],[1.1695961,0.39951283],[0.380389,0.70011547]],[1,[5.0331513,0.15591627],[1.1695961,0.60768372],[0.380389,0.39195739]]],\"H\":[[0,[3.42525091,0.15432897],[0.62391373,0.53532814],[0.1688554,0.44463454]],[0,[0.5,1.0]]]},\"sort_basis\":true}}]"}
{"func_name": "pyscf_gto_mole_format_pseudo", "func_desc": "Convert the input pseudopotential table (pseudo_tab) to the internal data format used by PySCF's GTO module for pseudopotentials.\n    \n    This function is used in the PySCF framework to prepare pseudopotential data so that downstream modules (basis set handling, integral evaluation, and quantum chemistry solvers) can consume a consistent, fully expanded representation. The input pseudo_tab is a mapping for atomic sites to pseudopotential specifications; values in pseudo_tab may be either:\n    - a string identifier naming a pseudopotential (in which case pyscf.pbc.gto.pseudo.load is called to obtain the internal representation for the standardized atomic symbol), or\n    - an already-formatted pseudopotential data structure (in which case the value is used verbatim).\n    \n    The function canonicalizes atom labels using the internal helper _symbol(atom) before using them as keys in the returned mapping. When a string identifier is provided for an atom, _std_symbol_without_ghost is used to compute the standardized symbol passed to pyscf.pbc.gto.pseudo.load. The returned structure matches the internal :attr:`pseudo` layout expected by PySCF's GTO routines.", "tools": [{"function": {"description": "Convert the input pseudopotential table (pseudo_tab) to the internal data format used by PySCF's GTO module for pseudopotentials.\n\nThis function is used in the PySCF framework to prepare pseudopotential data so that downstream modules (basis set handling, integral evaluation, and quantum chemistry solvers) can consume a consistent, fully expanded representation. The input pseudo_tab is a mapping for atomic sites to pseudopotential specifications; values in pseudo_tab may be either:\n- a string identifier naming a pseudopotential (in which case pyscf.pbc.gto.pseudo.load is called to obtain the internal representation for the standardized atomic symbol), or\n- an already-formatted pseudopotential data structure (in which case the value is used verbatim).\n\nThe function canonicalizes atom labels using the internal helper _symbol(atom) before using them as keys in the returned mapping. When a string identifier is provided for an atom, _std_symbol_without_ghost is used to compute the standardized symbol passed to pyscf.pbc.gto.pseudo.load. The returned structure matches the internal :attr:`pseudo` layout expected by PySCF's GTO routines.", "name": "pyscf_gto_mole_format_pseudo", "parameters": {"properties": {"pseudo_tab": {"type": "any", "description": "Mapping of atoms to pseudopotential specifications. Each key is an atom identifier accepted by the module (for example an element symbol or an atom specification used elsewhere in PySCF). Each value must be either a string pseudopotential identifier (for example a filename or a keyword recognized by pyscf.pbc.gto.pseudo.load) or an already-formatted pseudopotential data structure matching PySCF's internal format. The function does not accept a string for pseudo_tab itself; passing a string (instead of a dict-like mapping with an items() method) will raise an error because the implementation iterates pseudo_tab.items(). This parameter represents the user-provided pseudopotential assignment for a molecule or cell and is typically derived from user input or higher-level molecule construction routines.", "default": ""}}, "required": ["pseudo_tab"], "type": "any"}}, "type": "function"}], "query": "We’re preprocessing pseudopotentials for a mixed periodic dataset exported from two structure generators (so atom labels are not uniform and may include isotopic tags, oxidation states, or ghost markers). Apply a data-sieve + branching protocol to build the PySCF-GTO internal pseudo mapping for each structure as follows:\n\n- Canonicalize each site label to the internal element key (PySCF-style) before inserting into the output mapping.\n- Only sites that correspond to real atoms after canonicalization should contribute pseudopotentials (i.e., ghost-tagged entries should naturally drop out because they have no physical standardized symbol).\n- For each remaining site, if the provided value is a string identifier, treat it as a pseudopotential family name and load it using the standardized element symbol (without ghost annotations). If the provided value is already an explicit pseudopotential object, preserve it verbatim.\n\nDataset:\n1) Cohort A (strained Si supercell) pseudo_tab includes three silicon sites with nonuniform labels: {\"Si1\": \"gth-pade\", \"Si2+\": \"gth-pade\", \"Si3\": {\"Si\": [[0, 4.0, [[2.0, 10.0], [1.0, 5.0]]], [1, 3.0, [[1.5, 8.0]]]]}, \"GHOST-Si4\": \"gth-pade\"}. \n2) Cohort B (SiC unit cell from another exporter) pseudo_tab uses plain element keys plus an isotopic carbon label: {\"Si\": \"gth-pbe\", \"13C\": [4, [[2, 1, [6, -4.2]], [1, 0, [3.5, -1.1]]], [[2, [4.8, 12, -6.5]]]], \"Xx\": \"gth-pbe\"}.\n\nReturn the converted internal pseudo mappings for both cohorts for downstream basis/integral/solver modules.", "answers": "[{\"name\":\"pyscf_gto_mole_format_pseudo\",\"arguments\":{\"pseudo_tab\":{\"Si1\":\"gth-pade\",\"Si2+\":\"gth-pade\",\"Si3\":{\"Si\":[[0,4.0,[[2.0,10.0],[1.0,5.0]]],[1,3.0,[[1.5,8.0]]]]},\"GHOST-Si4\":\"gth-pade\"}}},{\"name\":\"pyscf_gto_mole_format_pseudo\",\"arguments\":{\"pseudo_tab\":{\"Si\":\"gth-pbe\",\"13C\":[4,[[2,1,[6,-4.2]],[1,0,[3.5,-1.1]]],[[2,[4.8,12,-6.5]]]],\"Xx\":\"gth-pbe\"}}}]"}
{"func_name": "pyscf_gto_mole_from_zmatrix", "func_desc": "Convert a Z-matrix string to Cartesian coordinates and atom symbols suitable for PySCF molecular input.", "tools": [{"function": {"description": "Convert a Z-matrix string to Cartesian coordinates and atom symbols suitable for PySCF molecular input.\n", "name": "pyscf_gto_mole_from_zmatrix", "parameters": {"properties": {"atomstr": {"type": "string", "description": "Multiline Z-matrix specification as a single string. This string is interpreted according to the Z-matrix parsing logic used in PySCF: semicolons (';') are treated as line separators, commas (',') are treated as token separators (they are replaced by spaces before parsing), blank lines and lines beginning with '#' are ignored as comments, and numeric fields may be Python expressions that are evaluated with eval() unless the module-level flag DISABLE_EVAL is set. Each non-comment line must begin with an atomic symbol token followed by additional tokens that specify how the atom is positioned relative to previously defined atoms:\n- A single token line (\"Symbol\") places the atom at the origin (first atom).\n- Three-token lines (\"Symbol X R\") treat the third token as a numeric expression or literal for a bond length R and place the atom on the +x axis at distance R relative to the origin or to the reference atom implied by prior entries (this corresponds to the second atom in a typical Z-matrix).\n- Five-token lines (\"Symbol ibond bond iangle angle\") supply four values (the code accepts the four values either as separate tokens or as an evaluable expression). The first value (ibond, 1-based index) selects the reference atom for the bond length; the second value (bond) is the bond length; the third value (iangle, 1-based index) selects the reference atom that defines the bond angle; and the fourth value (angle) is the bond angle in degrees. The function converts angles from degrees to radians and uses rotation matrices (pyscf.symm.rotation_mat) to place the atom in 3D.\n- Seven-or-more-token lines include dihedral information (\"Symbol ibond bond iangle angle idih dihedral\"), where idih (1-based index) selects the atom that defines the dihedral and dihedral is given in degrees. The code parses six numeric values (ibond, bond, iangle, angle, idih, dihedral) and computes the Cartesian placement using successive rotations.\nThe parser enforces increasing requirements for the minimum number of tokens per line as atoms are added and raises an error if a line does not meet the current minimum. Index arguments in the Z-matrix are 1-based in the input and converted internally to 0-based indices.", "default": ""}}, "required": ["atomstr"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a PySCF validation set from heterogeneous Z-matrix sources. Convert only the candidate geometries that are physically usable for a single-point gas-phase calculation: keep structures that (i) contain no dummy atoms (e.g., X), (ii) use only numeric internal coordinates in Å/deg (no variables like r1/a1), and (iii) are non-degenerate (at least 3 atoms with a defined bond angle, i.e., not just a 2-atom diatomic specification). From the raw batch below, convert the Z-matrices that satisfy those criteria into PySCF-ready atom symbols and Cartesian coordinates.\n\nRaw Z-matrix batch:\n1) \"C; O 1 1.16; O 1 1.16 2 180\"\n2) \"O; H 1 0.9584; H 1 0.9584 2 104.5\"\n3) \"N; N 1 r1\"", "answers": "[{\"name\":\"pyscf_gto_mole_from_zmatrix\",\"arguments\":{\"atomstr\":\"C; O 1 1.16; O 1 1.16 2 180\"}},{\"name\":\"pyscf_gto_mole_from_zmatrix\",\"arguments\":{\"atomstr\":\"O; H 1 0.9584; H 1 0.9584 2 104.5\"}}]"}
{"func_name": "pyscf_gto_mole_gto_norm", "func_desc": "Normalized factor for a Gaussian-type orbital (GTO) radial function r^l * exp(-alpha * r^2).\n    \n    This function computes the scalar normalization constant N that enforces\n    1/sqrt(integral_0^inf [g(r)]^2 r^2 dr) for the radial part g(r) = r^l exp(-alpha r^2).\n    It is used throughout the PySCF quantum-chemistry codebase to normalize primitive\n    GTO radial functions when building basis functions for molecular electronic-structure\n    calculations (see the repository README for PySCF context). The returned normalization\n    factor ensures that the associated three-dimensional GTO basis function has unit\n    norm under the usual volume element r^2 dr dOmega.", "tools": [{"function": {"description": "Normalized factor for a Gaussian-type orbital (GTO) radial function r^l * exp(-alpha * r^2).\n\nThis function computes the scalar normalization constant N that enforces\n1/sqrt(integral_0^inf [g(r)]^2 r^2 dr) for the radial part g(r) = r^l exp(-alpha r^2).\nIt is used throughout the PySCF quantum-chemistry codebase to normalize primitive\nGTO radial functions when building basis functions for molecular electronic-structure\ncalculations (see the repository README for PySCF context). The returned normalization\nfactor ensures that the associated three-dimensional GTO basis function has unit\nnorm under the usual volume element r^2 dr dOmega.", "name": "pyscf_gto_mole_gto_norm", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number for the radial factor r^l. This must\nbe an integer greater than or equal to 0; negative values are not allowed.\nIn the context of basis functions, l determines the polynomial prefactor\nof the radial part and therefore the orbital type (s, p, d, ...).", "default": ""}, "expnt": {"type": "float", "description": "Gaussian exponent alpha in the radial factor exp(-alpha * r^2).\nThis is the numerical exponent that controls the radial decay of the\nGaussian primitive. In practice, expnt is supplied from basis-set definitions\nand determines the spatial extent of the GTO primitive; larger values\nproduce more tightly localized functions.", "default": ""}}, "required": ["l", "expnt"], "type": "any"}}, "type": "function"}], "query": "We’re curating a mixed-quality primitive GTO exponent table from multiple fitting stages for a PySCF-style basis build. Each record is a candidate primitive radial factor g(r)=r^l·exp(-α r^2) with a declared angular momentum l and exponent α, but the table may contain non-physical exponents from failed fits. Use the radial normalization routine to compute N only for primitives that are physically admissible for square-integrable 3D GTOs (i.e., the normalization integral over r∈[0,∞) must converge under the r^2dr volume element). For accepted primitives, apply a protocol that reflects how our pipeline treats diffuse vs tight primitives: if α is below the dataset median α (computed over the admissible subset), treat it as diffuse and use its given l; otherwise treat it as tight and increment the requested l by 1 to mimic a tighter angular channel used in the sanity-check branch. Dataset (l, α): (0, 13.6), (1, 0.0), (2, 0.75), (3, -0.2), (4, 1.5), (2, 0.12). Return the list of normalization calls implied by this rule set, in dataset order for the admissible entries only.", "answers": "[{\"name\":\"pyscf_gto_mole_gto_norm\",\"arguments\":{\"l\":1,\"expnt\":13.6}},{\"name\":\"pyscf_gto_mole_gto_norm\",\"arguments\":{\"l\":2,\"expnt\":0.75}},{\"name\":\"pyscf_gto_mole_gto_norm\",\"arguments\":{\"l\":5,\"expnt\":1.5}},{\"name\":\"pyscf_gto_mole_gto_norm\",\"arguments\":{\"l\":2,\"expnt\":0.12}}]"}
{"func_name": "pyscf_gto_mole_is_au", "func_desc": "Return whether the given unit string is recognized as atomic units (A.U.) by PySCF.\n    \n    This function is part of pyscf.gto.mole and is used within the PySCF molecular input and geometry handling code to decide whether user-supplied unit strings should be interpreted as atomic units (a.u.). In quantum chemistry and within PySCF, atomic units are a common internal convention for expressing distances, energies, and other physical quantities; correctly detecting that a user requested atomic units affects scaling and interpretation of numerical values passed to the library.", "tools": [{"function": {"description": "Return whether the given unit string is recognized as atomic units (A.U.) by PySCF.\n\nThis function is part of pyscf.gto.mole and is used within the PySCF molecular input and geometry handling code to decide whether user-supplied unit strings should be interpreted as atomic units (a.u.). In quantum chemistry and within PySCF, atomic units are a common internal convention for expressing distances, energies, and other physical quantities; correctly detecting that a user requested atomic units affects scaling and interpretation of numerical values passed to the library.", "name": "pyscf_gto_mole_is_au", "parameters": {"properties": {"unit": {"type": "string", "description": "A unit identifier provided by the caller (for example, from molecule input). The function expects a Python str and performs a case-insensitive check of its prefix. Practically, strings that start with 'AU' (e.g., 'au', 'A.U.', 'AU') or with 'B' (commonly used as a shorthand for Bohr, e.g., 'B', 'Bohr', 'BOHR') are treated as indicating atomic units. If unit is not a str (for example, None, a numeric type, or another object), the function will not attempt conversion and will return False.", "default": ""}}, "required": ["unit"], "type": "any"}}, "type": "function"}], "query": "We’re triaging unit annotations for a PySCF geometry-ingestion audit across three upstream parsers plus a legacy template. The raw metadata sometimes includes formatting artifacts (whitespace padding, inconsistent casing, and punctuation). For this QC pass, apply an ingestion rule: only unit labels that are already in a “canonicalized” form (i.e., the string is identical to its own trimmed version and contains no internal whitespace) are eligible for immediate PySCF atomic-unit recognition checks; anything else is deferred to a later normalization stage. Using that rule, evaluate atomic-unit recognition (via PySCF) for the following incoming unit labels in their given order: (1) 'Bohr' (Parser A), (2) ' Bohr' (Parser B export with a leading pad), (3) 'A.U.' (legacy template), (4) 'ang strom' (GUI export with an injected space), and (5) 'Bohr' (final reproducibility spot-check).", "answers": "[{\"name\":\"pyscf_gto_mole_is_au\",\"arguments\":{\"unit\":\"Bohr\"}},{\"name\":\"pyscf_gto_mole_is_au\",\"arguments\":{\"unit\":\"A.U.\"}},{\"name\":\"pyscf_gto_mole_is_au\",\"arguments\":{\"unit\":\"Bohr\"}}]"}
{"func_name": "pyscf_gto_mole_len_cart", "func_desc": "pyscf.gto.mole.len_cart computes the number of Cartesian Gaussian-type basis functions associated with a given angular momentum quantum number. This function is part of the PySCF gto.mole utilities and is used when building atomic orbital basis sets, allocating integrals and arrays, and mapping angular momentum labels (s, p, d, f, ...) to the corresponding number of Cartesian components required by Cartesian Gaussian basis representations.", "tools": [{"function": {"description": "pyscf.gto.mole.len_cart computes the number of Cartesian Gaussian-type basis functions associated with a given angular momentum quantum number. This function is part of the PySCF gto.mole utilities and is used when building atomic orbital basis sets, allocating integrals and arrays, and mapping angular momentum labels (s, p, d, f, ...) to the corresponding number of Cartesian components required by Cartesian Gaussian basis representations.\n", "name": "pyscf_gto_mole_len_cart", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number for the atomic orbital. This is expected to be a non-negative integer where l = 0 corresponds to an s-type function, l = 1 to p-type, l = 2 to d-type, l = 3 to f-type, and so on. The function computes the Cartesian count using the combinatorial formula (l + 1) * (l + 2) // 2, which yields common values: l=0 -> 1, l=1 -> 3, l=2 -> 6, l=3 -> 10. The argument has no default and must be supplied by the caller. Passing negative integers is not physically meaningful for angular momentum (the formula will produce a numeric result for some negative inputs but such results should not be used in basis construction). Passing non-integer types is not supported by the function signature (annotated as int) and may produce an incorrect or non-integer numeric result; callers should ensure l is an int before calling.", "default": ""}}, "required": ["l"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting three shell descriptors from separate PySCF Cartesian-GTO benchmark cohorts. Each cohort reports an angular momentum label for a single contracted shell, but the raw metadata is messy: some cohorts provide an integer l, others provide a spectroscopic letter, and some entries may be invalid or non-orbital tags. Raw cohort descriptors: `[{\"cohort\":\"A\",\"am\":\"f\"},{\"cohort\":\"B\",\"am\":3},{\"cohort\":\"C\",\"am\":\"F\"}]`. For each cohort, standardize the angular momentum to an integer using the conventional mapping (s=0, p=1, d=2, f=3, g=4, ...), treating the label case-insensitively, and then compute the number of Cartesian GTO components for that shell to drive AO-array allocation.", "answers": "[{\"name\":\"pyscf_gto_mole_len_cart\",\"arguments\":{\"l\":3}},{\"name\":\"pyscf_gto_mole_len_cart\",\"arguments\":{\"l\":3}},{\"name\":\"pyscf_gto_mole_len_cart\",\"arguments\":{\"l\":3}}]"}
{"func_name": "pyscf_gto_mole_len_spinor", "func_desc": "pyscf.gto.mole.len_spinor returns the number of spinor components (spinor basis functions) associated with a given orbital angular momentum quantum number and kappa index. This helper is used in the PySCF molecular integrals and basis construction code to determine how many spinor functions should be allocated or iterated for a particular (l, kappa) channel when building relativistic or spinor-enabled basis sets in the pyscf.gto.mole module.", "tools": [{"function": {"description": "pyscf.gto.mole.len_spinor returns the number of spinor components (spinor basis functions) associated with a given orbital angular momentum quantum number and kappa index. This helper is used in the PySCF molecular integrals and basis construction code to determine how many spinor functions should be allocated or iterated for a particular (l, kappa) channel when building relativistic or spinor-enabled basis sets in the pyscf.gto.mole module.\n", "name": "pyscf_gto_mole_len_spinor", "parameters": {"properties": {"l": {"type": "integer", "description": "Orbital angular momentum quantum number for the shell. In the context of PySCF and atomic/molecular basis construction, l selects the spatial angular momentum (e.g., 0 for s, 1 for p). This parameter is expected to be an integer (typically non-negative in normal quantum-chemical use). The function uses l in simple linear formulas to compute the spinor count; if l is negative or not an integer the numeric result may be non-physical or a TypeError may be raised by Python arithmetic/compare operations.", "default": ""}, "kappa": {"type": "integer", "description": "Kappa index that distinguishes spinor coupling types for the given angular momentum. In PySCF this integer controls which formula is used to count spinor components: a kappa value of 0 triggers the special case for full four-component-like counting (4*l + 2), kappa < 0 selects the (2*l + 2) branch, and kappa > 0 selects the (2*l) branch. The value is compared to zero using integer comparisons; if kappa is not an int the comparison may raise a TypeError.", "default": ""}}, "required": ["l", "kappa"], "type": "any"}}, "type": "function"}], "query": "We’re debugging a relativistic basis-set import for a d-manifold (fixed l=2) where kappa labels arrive from upstream metadata that may include sentinel/overflow values. Before allocating buffers, evaluate len_spinor only for those (l, kappa) channels that correspond to physically allowed Dirac kappa values for a given l (i.e., kappa must map to j=l\\u00b11/2). The metadata stream for this l=2 block reports kappa candidates [-1, 0]. Compute the spinor component counts only for the candidates that satisfy the allowed-kappa constraint, and return the function calls needed.", "answers": "[{\"name\":\"pyscf_gto_mole_len_spinor\",\"arguments\":{\"l\":2,\"kappa\":-1}}]"}
{"func_name": "pyscf_gto_moleintor_ascint3", "func_desc": "pyscf.gto.moleintor.ascint3 converts a legacy cint2-style integral routine name into the cint3-style name used by PySCF's molecular integral layer.\n    \n    This function is used in the molecular integrals subsystem of PySCF (Python-based Simulations of Chemistry Framework) to adapt older cint2 naming conventions to the cint3 naming conventions expected by the moleintor code paths that dispatch spinor, spherical, cartesian, or spin-specific integral implementations. In practice, callers supply the textual identifier of an integral routine (for example, a string used to look up a C-accelerated integral kernel) and ascint3 returns a normalized name suitable for selecting the corresponding cint3 kernel.\n    \n    Behavior and transformation rules:\n    - If the input string begins with the prefix 'cint', the leading 'c' is removed (e.g., 'cint2e' -> 'int2e'). This mirrors the transition from cint2-prefixed identifiers to cint3 identifiers within the PySCF integrals naming scheme.\n    - If the resulting name does not already end with one of the recognized suffixes '_sph', '_cart', '_spinor', or '_ssc', the function appends the suffix '_spinor'. This ensures the returned name refers to a spinor-capable cint3 kernel when the original name did not explicitly specify spherical/cartesian/spinor/ssc variants.\n    - No other parts of the name are modified.\n    \n    Side effects:\n    - This function has no side effects on program state; it returns a new string and does not mutate external objects.\n    \n    Failure modes and notes:\n    - The function expects intor_name to be a Python str as used throughout PySCF's integral name handling. If a non-string object is passed, attribute lookup for startswith/endswith will fail (raising an AttributeError), and such misuse should be avoided by callers.\n    - An empty string input will result in '_spinor' being returned per the transformation rules (empty does not start with 'cint' and does not end with any recognized suffix).\n    - The function does not validate that the returned name corresponds to an actually implemented kernel; it only performs syntactic normalization.", "tools": [{"function": {"description": "pyscf.gto.moleintor.ascint3 converts a legacy cint2-style integral routine name into the cint3-style name used by PySCF's molecular integral layer.\n\nThis function is used in the molecular integrals subsystem of PySCF (Python-based Simulations of Chemistry Framework) to adapt older cint2 naming conventions to the cint3 naming conventions expected by the moleintor code paths that dispatch spinor, spherical, cartesian, or spin-specific integral implementations. In practice, callers supply the textual identifier of an integral routine (for example, a string used to look up a C-accelerated integral kernel) and ascint3 returns a normalized name suitable for selecting the corresponding cint3 kernel.\n\nBehavior and transformation rules:\n- If the input string begins with the prefix 'cint', the leading 'c' is removed (e.g., 'cint2e' -> 'int2e'). This mirrors the transition from cint2-prefixed identifiers to cint3 identifiers within the PySCF integrals naming scheme.\n- If the resulting name does not already end with one of the recognized suffixes '_sph', '_cart', '_spinor', or '_ssc', the function appends the suffix '_spinor'. This ensures the returned name refers to a spinor-capable cint3 kernel when the original name did not explicitly specify spherical/cartesian/spinor/ssc variants.\n- No other parts of the name are modified.\n\nSide effects:\n- This function has no side effects on program state; it returns a new string and does not mutate external objects.\n\nFailure modes and notes:\n- The function expects intor_name to be a Python str as used throughout PySCF's integral name handling. If a non-string object is passed, attribute lookup for startswith/endswith will fail (raising an AttributeError), and such misuse should be avoided by callers.\n- An empty string input will result in '_spinor' being returned per the transformation rules (empty does not start with 'cint' and does not end with any recognized suffix).\n- The function does not validate that the returned name corresponds to an actually implemented kernel; it only performs syntactic normalization.", "name": "pyscf_gto_moleintor_ascint3", "parameters": {"properties": {"intor_name": {"type": "string", "description": "The original integral routine name string to convert. In the PySCF domain this is typically a cint2-style identifier used to reference C-integral implementations (for example 'cint2e_sph' or 'cint2e'). The function uses this value to produce a cint3-compatible identifier by removing a leading 'c' if present and ensuring a spinor/suffix is present.", "default": ""}}, "required": ["intor_name"], "type": "any"}}, "type": "function"}], "query": "During a mixed-basis PySCF migration audit, we’re ingesting a raw list of legacy integral-kernel labels from two cohorts where some records already carry an explicit representation suffix and others are bare cint2-style identifiers. Normalize only the cohort entries that are one-electron overlap kernels (i.e., their label encodes `1e_ovlp`) and are missing any explicit `_sph`, `_cart`, `_spinor`, or `_ssc` variant. Use the same normalization routine used by moleintor dispatch so that bare labels default to the spinor-capable kernel. Raw labels (in acquisition order): `['cint1e_ovlp', 'cint1e_ovlp_sph', 'int1e_ovlp_cart', 'cint1e_kin', 'cint1e_ovlp']`.", "answers": "[{\"name\":\"pyscf_gto_moleintor_ascint3\",\"arguments\":{\"intor_name\":\"cint1e_ovlp\"}},{\"name\":\"pyscf_gto_moleintor_ascint3\",\"arguments\":{\"intor_name\":\"cint1e_ovlp\"}}]"}
{"func_name": "pyscf_gto_moleintor_getints_by_shell", "func_desc": "For given 2, 3 or 4 atomic-orbital shells, call the underlying libcint/libcgto\n    integral routines to compute one-electron, two-electron, two-center-two-electron\n    or three-center two-electron integrals for the specified shells. This function\n    is an internal PySCF interface that (1) converts the atom/basis/environment\n    arguments into the C-compatible numpy arrays expected by libcint, (2) determines\n    the number of contracted Gaussian-type orbitals (CGTOs) in each shell from the\n    bas array, (3) allocates a Fortran-ordered buffer, and (4) invokes the C\n    integral kernel named by intor_name via libcgto. It is used throughout PySCF\n    where low-level per-shell integrals are needed (for example, building ERI\n    blocks, 3-center Coulomb integrals for density fitting, or 1e operator blocks).", "tools": [{"function": {"description": "For given 2, 3 or 4 atomic-orbital shells, call the underlying libcint/libcgto\nintegral routines to compute one-electron, two-electron, two-center-two-electron\nor three-center two-electron integrals for the specified shells. This function\nis an internal PySCF interface that (1) converts the atom/basis/environment\narguments into the C-compatible numpy arrays expected by libcint, (2) determines\nthe number of contracted Gaussian-type orbitals (CGTOs) in each shell from the\nbas array, (3) allocates a Fortran-ordered buffer, and (4) invokes the C\nintegral kernel named by intor_name via libcgto. It is used throughout PySCF\nwhere low-level per-shell integrals are needed (for example, building ERI\nblocks, 3-center Coulomb integrals for density fitting, or 1e operator blocks).", "name": "pyscf_gto_moleintor_getints_by_shell", "parameters": {"properties": {"intor_name": {"type": "string", "description": "Name of the integral kernel to invoke in libcgto. See\ngetints for a list of supported intor_name values. The prefix of\nintor_name (e.g. 'int1e', 'int2e', 'int3c', 'int4c', 'int2c') and its\nsuffixes (e.g. '_cart', '_sph', '_ssc') determine which branch of the\nimplementation is executed and how many shells must be provided in\nshls. The function will select a real dtype (numpy.double) when the\nname ends with '_cart' or '_sph', and complex128 for spinor integrals\n(other names). If the named C function is not available on the\nlibcgto object, an AttributeError will be raised when attempting to\ncall it.", "default": ""}, "shls": {"type": "array", "items": {"type": "float"}, "description": "List of AO shell indices (integers) that specify which shells\nthe integrals are evaluated for. The required length of shls depends\non intor_name: length 2 for 1e / 2c / ECP kernels, length 3 for\n3-center (int3c) kernels, and length 4 for 2e / 4-center kernels.\nAssertions in the implementation will raise AssertionError if the\nprovided shls length does not match the expectation for the selected\nintor_name.", "default": ""}, "atm": {"type": "array", "items": {"type": "float"}, "description": "Atom table in the libcint format supplied as a C\ncontiguous numpy.ndarray of dtype numpy.int32. This array encodes the\natomic coordinates and other per-atom metadata used by libcint. The\nfunction converts the provided atm to dtype int32 and order 'C' and\npasses a pointer to the C kernel.", "default": ""}, "bas": {"type": "array", "items": {"type": "float"}, "description": "Basis table in the libcint format supplied as a C\ncontiguous numpy.ndarray of dtype numpy.int32. This array encodes per-\nshell parameters (angular momentum, number of primitives/contracted\nfunctions, kappa for spinors, etc.). The function converts bas to\ndtype int32 and order 'C', and uses bas entries (ANG_OF, NCTR_OF,\nKAPPA_OF) to compute the number of contracted GTOs per shell. The\ncomputed numbers (di, dj, dk, dl) determine the buffer shape.", "default": ""}, "env": {"type": "array", "items": {"type": "float"}, "description": "Environment array required by libcint provided as a C\ncontiguous numpy.ndarray of dtype numpy.double (float64). This array\ncontains floating-point data used by the C integral routines (exponents,\nprecomputed factors, etc.). The function converts env to dtype\nfloat64 (numpy.double) and order 'C' before passing it to the C kernel.", "default": ""}, "comp": {"type": "integer", "description": "Number of components of the integral to compute. Many integral\nkernels that represent vector or tensor quantities provide multiple\ncomponents; for example, dipole-type integrals such as int1e_ipovlp have\ncomp=3. Default is 1. When comp == 1 the returned array has no leading\ncomponent axis; when comp > 1 the result is returned with the component\nindex as the leading axis (the implementation allocates an F-ordered\nbuffer with a trailing comp axis and transposes it to put comp first).", "default": 1}}, "required": ["intor_name", "shls", "atm", "bas", "env", "comp"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed set of low-level libcint/PySCF per-shell integral jobs coming from three upstream generators (two-electron ERI micro-batches and 1e operator spot-checks). Treat each record below as “raw” and decide what to run based only on intrinsic structure checks of the record itself.\n\nRules:\n1) Use spherical kernels throughout.\n2) If a record’s `shls` has length 4, treat it as a 4-center ERI request and run `int2e_sph`.\n3) If a record’s `shls` has length 2, treat it as a 1e overlap request and run `int1e_ovlp_sph`.\n4) Run only records where every shell index in `shls` is a valid shell for that record’s `bas` table (i.e., `0 <= shls[i] < len(bas)` for all i).\n5) Preserve each record’s `comp` as provided.\n\nRaw records:\n- R1 (H2 sanity): shls=[0,0,1,1], comp=1, atm=[[1,20,1,23,0,0],[1,24,1,27,0,0]], bas=[[0,0,0,1,1,0,28,29],[0,0,0,1,1,0,30,31]], env=[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,-0.7,0.0,0.0,0.0,1.24,1.0,1.24,1.0]\n- R2 (H2 STO-3G-like): shls=[0,1,0,1], comp=1, atm=[[1,20,1,23,0,0],[1,24,1,27,0,0]], bas=[[0,1,3,1,0,0,0,0],[1,1,3,1,0,0,0,0]], env=[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.4,0.0,3.42525091,0.62391373,0.1688554,0.15432897,0.53532814,0.44463454,0.09996723,0.39951283,0.70011547]\n- R3 (water 1e baseline): shls=[0,1], comp=1, atm=[[1,0,0,0,0,0],[8,3,6,0,0,0],[1,6,9,0,0,0]], bas=[[1,0,3,1,0,0,0,0],[2,3,3,1,0,0,0,0]], env=[0.0,0.0,-0.7,0.0,0.0,0.7,0.0,0.0,0.0,1.24,1.24,1.24,0.5,0.4,0.3]\n- R4 (corrupted index artifact): shls=[0,2], comp=1, atm=[[1,0,0,0,0,0],[8,3,6,0,0,0],[1,6,9,0,0,0]], bas=[[1,0,3,1,0,0,0,0],[2,3,3,1,0,0,0,0]], env=[0.0,0.0,-0.7,0.0,0.0,0.7,0.0,0.0,0.0,1.24,1.24,1.24,0.5,0.4,0.3]\n\nReturn the integral tensor/block from each record that passes the rules (no extra post-processing).", "answers": "[{\"name\":\"pyscf_gto_moleintor_getints_by_shell\",\"arguments\":{\"intor_name\":\"int2e_sph\",\"shls\":[0,0,1,1],\"atm\":[[1,20,1,23,0,0],[1,24,1,27,0,0]],\"bas\":[[0,0,0,1,1,0,28,29],[0,0,0,1,1,0,30,31]],\"env\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,-0.7,0.0,0.0,0.0,1.24,1.0,1.24,1.0],\"comp\":1}},{\"name\":\"pyscf_gto_moleintor_getints_by_shell\",\"arguments\":{\"intor_name\":\"int2e_sph\",\"shls\":[0,1,0,1],\"atm\":[[1,20,1,23,0,0],[1,24,1,27,0,0]],\"bas\":[[0,1,3,1,0,0,0,0],[1,1,3,1,0,0,0,0]],\"env\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.4,0.0,3.42525091,0.62391373,0.1688554,0.15432897,0.53532814,0.44463454,0.09996723,0.39951283,0.70011547],\"comp\":1}},{\"name\":\"pyscf_gto_moleintor_getints_by_shell\",\"arguments\":{\"intor_name\":\"int1e_ovlp_sph\",\"shls\":[0,1],\"atm\":[[1,0,0,0,0,0],[8,3,6,0,0,0],[1,6,9,0,0,0]],\"bas\":[[1,0,3,1,0,0,0,0],[2,3,3,1,0,0,0,0]],\"env\":[0.0,0.0,-0.7,0.0,0.0,0.7,0.0,0.0,0.0,1.24,1.24,1.24,0.5,0.4,0.3],\"comp\":1}}]"}
{"func_name": "pyscf_gw_gw_ac_get_rho_response", "func_desc": "Compute the electronic density response (polarizability) in the auxiliary basis at a (typically imaginary) frequency omega for use in GW screening and self-energy calculations.\n    \n    This function is used in the pyscf.gw.gw_ac module to build the density response matrix in an auxiliary basis from molecular orbital energies and auxiliary-projected three-index integrals. The computed response Pi(P,Q) is the matrix representation of the independent-particle density response (sum over occupied->virtual transitions) evaluated at frequency omega. The implementation returns the response including both spin channels; the multiplicative factor of 4 in the code accounts for the two electronic spins and the combined contribution from the occupied->virtual transition amplitude and its complex-conjugate pair in the sum-over-states expression. Typical usage is within GW screening (W) construction and related many-body perturbation theory routines in PySCF.", "tools": [{"function": {"description": "Compute the electronic density response (polarizability) in the auxiliary basis at a (typically imaginary) frequency omega for use in GW screening and self-energy calculations.\n\nThis function is used in the pyscf.gw.gw_ac module to build the density response matrix in an auxiliary basis from molecular orbital energies and auxiliary-projected three-index integrals. The computed response Pi(P,Q) is the matrix representation of the independent-particle density response (sum over occupied->virtual transitions) evaluated at frequency omega. The implementation returns the response including both spin channels; the multiplicative factor of 4 in the code accounts for the two electronic spins and the combined contribution from the occupied->virtual transition amplitude and its complex-conjugate pair in the sum-over-states expression. Typical usage is within GW screening (W) construction and related many-body perturbation theory routines in PySCF.", "name": "pyscf_gw_gw_ac_get_rho_response", "parameters": {"properties": {"omega": {"type": "float", "description": "Frequency at which the response is evaluated. In the original code this is intended as the magnitude of an imaginary frequency (i*w) used in frequency-dependent screening; the function uses omega only via omega**2 in the denominator. A caller should pass a real scalar; passing omega == 0 is allowed but can produce infinite or NaN elements if any transition energy difference is exactly zero (see Failure modes).", "default": ""}, "mo_energy": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of molecular orbital energies. The function expects the first nocc entries to be occupied orbital energies and the remaining entries to be virtual energies (i.e., mo_energy has length nocc + nvir). This ordering is required because the code constructs transition energies eia = mo_energy[:nocc, None] - mo_energy[None, nocc:], representing occupied minus virtual orbital energy differences used in the independent-particle response denominator.", "default": ""}, "Lpq": {"type": "array", "items": {"type": "float"}, "description": "Three-dimensional array of auxiliary-basis projected three-index integrals with shape (naux, nocc, nvir). Here naux is the number of auxiliary basis functions, nocc is the number of occupied orbitals, and nvir is the number of virtual orbitals. Lpq[P,i,a] are the integrals (projected amplitudes) coupling auxiliary function P to occupied orbital i and virtual orbital a; these amplitudes enter the transition contributions to the auxiliary-basis response.", "default": ""}}, "required": ["omega", "mo_energy", "Lpq"], "type": "any"}}, "type": "function"}], "query": "We’re validating the auxiliary-basis independent-particle polarizability build under messy production-like inputs from two GW pre-screening cohorts. Each cohort provides MO energies (2 occupied + 3 virtual) and a 4×2×3 auxiliary-projected three-index tensor Lpq. Apply a frequency-selection protocol driven by the electronic gap: for each cohort, compute the HOMO–LUMO gap (lowest virtual minus highest occupied). If the gap is >= 0.60 a.u., evaluate Pi(P,Q) at omega = 0.50 a.u.; otherwise evaluate at omega = 0.75 a.u. Use the cohort’s Lpq as-is (signs included). Cohort A: mo_energy = [-0.8, -0.5, 0.2, 0.4, 0.7], Lpq = [[[0.1, 0.05, 0.02], [0.03, 0.04, 0.01]], [[0.08, 0.02, 0.01], [0.02, 0.03, 0.02]], [[0.05, 0.03, 0.04], [0.01, 0.02, 0.03]], [[0.07, 0.01, 0.02], [0.03, 0.01, 0.02]]]. Cohort B: mo_energy = [-0.62, -0.41, 0.10, 0.28, 0.55], Lpq = [[[0.12, -0.03, 0.08], [0.05, 0.11, -0.02]], [[-0.07, 0.09, 0.04], [0.02, -0.06, 0.13]], [[0.15, 0.01, -0.05], [-0.04, 0.07, 0.06]], [[0.03, -0.12, 0.1], [0.09, 0.02, -0.08]]]. Return the full Pi(P,Q) matrices (including both spin channels as in the standard GW auxiliary-basis response definition) for the selected omega per cohort.", "answers": "[{\"name\":\"pyscf_gw_gw_ac_get_rho_response\",\"arguments\":{\"omega\":0.5,\"mo_energy\":[-0.8,-0.5,0.2,0.4,0.7],\"Lpq\":[[[0.1,0.05,0.02],[0.03,0.04,0.01]],[[0.08,0.02,0.01],[0.02,0.03,0.02]],[[0.05,0.03,0.04],[0.01,0.02,0.03]],[[0.07,0.01,0.02],[0.03,0.01,0.02]]]}},{\"name\":\"pyscf_gw_gw_ac_get_rho_response\",\"arguments\":{\"omega\":0.75,\"mo_energy\":[-0.62,-0.41,0.1,0.28,0.55],\"Lpq\":[[[0.12,-0.03,0.08],[0.05,0.11,-0.02]],[[-0.07,0.09,0.04],[0.02,-0.06,0.13]],[[0.15,0.01,-0.05],[-0.04,0.07,0.06]],[[0.03,-0.12,0.1],[0.09,0.02,-0.08]]]}}]"}
{"func_name": "pyscf_gw_gw_cd_get_rho_response", "func_desc": "Compute density response function in the auxiliary basis at (imaginary) frequency omega.\n    \n    This function is used in the GW module (pyscf.gw.gw_cd) to build the density response (screening) matrix in a density-fitting / auxiliary basis representation from molecular orbital energies and three-index Cholesky/auxiliary tensors. The computed response matrix Pi encodes how the electronic density (from both spin channels) responds at frequency omega and is suitable for use in GW screening and polarizability constructions where auxiliary-basis representations are employed.", "tools": [{"function": {"description": "Compute density response function in the auxiliary basis at (imaginary) frequency omega.\n\nThis function is used in the GW module (pyscf.gw.gw_cd) to build the density response (screening) matrix in a density-fitting / auxiliary basis representation from molecular orbital energies and three-index Cholesky/auxiliary tensors. The computed response matrix Pi encodes how the electronic density (from both spin channels) responds at frequency omega and is suitable for use in GW screening and polarizability constructions where auxiliary-basis representations are employed.", "name": "pyscf_gw_gw_cd_get_rho_response", "parameters": {"properties": {"omega": {"type": "float", "description": "Frequency at which the response is evaluated. The code treats this as the imaginary-frequency argument \"iw\" (omega may represent an imaginary-frequency value); it enters the denominator as omega**2 + (epsilon_i - epsilon_a)**2. Practical significance: omega controls frequency-dependent screening; non-finite or NaN omega will lead to NaN or invalid results.", "default": ""}, "mo_energy": {"type": "array", "items": {"type": "float"}, "description": "1-D array of molecular orbital energies ordered with occupied orbitals first then virtual orbitals (length nocc + nvir). The function slices mo_energy[:nocc] for occupied energies and mo_energy[nocc:] for virtual energies and constructs energy differences e_i - e_a. If the length or ordering does not match Lpq's nocc and nvir dimensions, indexing or broadcasting errors will occur. The array dtype determines the dtype of intermediate and returned arrays.", "default": ""}, "Lpq": {"type": "array", "items": {"type": "float"}, "description": "3-D array with shape (naux, nocc, nvir) containing three-index auxiliary/Cholesky vectors that couple auxiliary basis functions (P index) to occupied (i) and virtual (a) molecular orbital indices. In practical GW/CD workflows, Lpq represents the factorized electron repulsion integrals in the auxiliary basis. This array is used to form P_{P,ia} * (e_i - e_a) / (omega^2 + (e_i - e_a)^2) and then contracted to produce the auxiliary-basis response. A wrong dimensionality or ordering will raise an error.", "default": ""}}, "required": ["omega", "mo_energy", "Lpq"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our DF-based GW screening builder against messy input from two prototype molecules where orbital partitions are not explicitly labeled. For each cohort below, interpret the MO energies as a single spin-restricted spectrum and infer the occupied/virtual split by the sign of the energies (occupied: eps < 0; virtual: eps > 0). Build the auxiliary-basis density response matrix Pi at an imaginary frequency chosen per-cohort as follows: use omega = 0.5 for spectra that contain at least one virtual orbital with eps >= 0.6 (to emulate a wider gap screening point), otherwise use omega = 0.25. Use the provided three-index DF/Cholesky tensor Lpq, where its second dimension is nocc and third is nvir; if the inferred occupied/virtual counts do not match Lpq’s (nocc, nvir) layout, treat the cohort as a malformed artifact and skip it by scientific validity criteria.\n\nCohort A (naux=4): mo_energy = [-0.62, -0.41, 0.12, 0.33, 0.58]; Lpq = [[[0.12, -0.08, 0.05], [0.03, 0.1, -0.02]], [[-0.04, 0.06, 0.09], [0.11, -0.07, 0.01]], [[0.08, 0.02, -0.03], [-0.05, 0.04, 0.07]], [[0.01, -0.09, 0.02], [0.06, 0.03, -0.08]]].\n\nCohort B (naux=3): mo_energy = [-0.72, -0.41, 0.18, 0.63]; Lpq = [[[0.12, -0.05], [0.07, 0.09]], [[-0.03, 0.11], [0.04, -0.08]], [[0.2, 0.02], [-0.06, 0.1]]].\n\nReturn Pi for each scientifically valid cohort (including both spin channels) using the cohort-specific omega rule above.", "answers": "[{\"name\":\"pyscf_gw_gw_cd_get_rho_response\",\"arguments\":{\"omega\":0.25,\"mo_energy\":[-0.62,-0.41,0.12,0.33,0.58],\"Lpq\":[[[0.12,-0.08,0.05],[0.03,0.1,-0.02]],[[-0.04,0.06,0.09],[0.11,-0.07,0.01]],[[0.08,0.02,-0.03],[-0.05,0.04,0.07]],[[0.01,-0.09,0.02],[0.06,0.03,-0.08]]]}}]"}
{"func_name": "pyscf_hessian_thermo_rotation_const", "func_desc": "Compute rotational constants for a molecular geometry to characterize rotational spectra.\n    \n    This function pyscf.hessian.thermo.rotation_const computes the three principal rotational constants of a molecule from atomic masses and Cartesian coordinates. It is used in molecular spectroscopy and thermochemical analyses (as in PySCF) to convert the principal moments of inertia of the mass-distributed nuclear framework into spectroscopically relevant rotational constants. The routine forms the center-of-mass, constructs the 3×3 moment of inertia tensor in atomic units (Bohr for coordinates, atomic mass units for masses), diagonalizes it to obtain principal moments, and converts those moments to the requested unit using PySCF’s physical constants (pyscf.nist). The returned values correspond to the three principal axes used to characterize rotational spectra.", "tools": [{"function": {"description": "Compute rotational constants for a molecular geometry to characterize rotational spectra.\n\nThis function pyscf.hessian.thermo.rotation_const computes the three principal rotational constants of a molecule from atomic masses and Cartesian coordinates. It is used in molecular spectroscopy and thermochemical analyses (as in PySCF) to convert the principal moments of inertia of the mass-distributed nuclear framework into spectroscopically relevant rotational constants. The routine forms the center-of-mass, constructs the 3×3 moment of inertia tensor in atomic units (Bohr for coordinates, atomic mass units for masses), diagonalizes it to obtain principal moments, and converts those moments to the requested unit using PySCF’s physical constants (pyscf.nist). The returned values correspond to the three principal axes used to characterize rotational spectra.", "name": "pyscf_hessian_thermo_rotation_const", "parameters": {"properties": {"mass": {"type": "array", "items": {"type": "float"}, "description": "1-D array of length N containing the masses of the N atoms. Masses are interpreted as atomic mass units (u, unified atomic mass) consistent with pyscf.nist. The mass array determines the center-of-mass position and weights the inertia tensor; mass.sum() must be nonzero or a division-by-zero will occur when computing the center of mass. Negative or non-physical masses will produce invalid inertia and rotational constants.", "default": ""}, "atom_coords": {"type": "array", "items": {"type": "float"}, "description": "2-D array with shape (N, 3) containing Cartesian coordinates of the N atoms. Coordinates are interpreted in atomic units (Bohr) consistent with the conversion factors used in the code. The function computes coordinates relative to the center of mass, constructs the inertia tensor from these shifted coordinates and the provided masses, and diagonalizes the inertia tensor to obtain principal moments of inertia.", "default": ""}, "unit": {"type": "string", "description": "Unit for the returned rotational constants. Case-insensitive accepted values are 'GHz' (default) to return rotational constants in gigahertz, and 'wavenumber' to return rotational constants in reciprocal centimeters (cm^-1). The default is 'GHz'. Any other string raises a RuntimeError('Unsupported unit ' + unit). The implementation uses pyscf.nist constants and applies the conversion formula B = ħ / (4π I) with appropriate unit conversion factors; for 'wavenumber' an additional division by the speed of light is applied and results are scaled to cm^-1.", "default": "GHz"}}, "required": ["mass", "atom_coords", "unit"], "type": "any"}}, "type": "function"}], "query": "We’re doing a rotational-spectroscopy QA pass on a mixed set of candidate H2O geometries coming out of two upstream pipelines (geometry optimizer vs. microwave fit), but the export is messy and may contain malformed snapshots. Each snapshot is intended to be a 3-atom water geometry in Bohr with the isotope/mass set (u): O=15.999, H=1.008, H=1.008.\n\nRaw snapshots (Bohr):\n1) tag=eq_ref; unit_hint=cm-1; coords: O [0.0, 0.0, 0.0], H [1.809, 0.0, 0.0], H [-0.453, 1.748, 0.0]\n2) tag=mw_fit; unit_hint=GHz; coords: O (0.000000, 0.000000, 0.000000), H (1.430000, 1.110000, 0.000000), H (-1.430000, 1.110000, 0.000000)\n3) tag=corrupt_duplicate; unit_hint=cm-1; coords: O [0.0, 0.0, 0.0], H [0.0, 0.0, 0.0], H [0.0, 0.0, 0.0]\n4) tag=incomplete; unit_hint=GHz; coords: O [0.0, 0.0, 0.0], H [1.0, 0.0, 0.0]\n\nProtocol: Only characterize snapshots that represent a physically meaningful nonlinear triatomic (i.e., three atoms present, and the geometry is neither collapsed nor collinear). For each retained snapshot, compute the full triplet of principal rotational constants using pyscf.hessian.thermo.rotation_const, honoring the unit_hint: report in wavenumbers (cm^-1) when unit_hint is cm-1, otherwise in GHz when unit_hint is GHz. Return one triplet per retained snapshot, keyed by tag in the report you generate downstream.", "answers": "[{\"name\":\"pyscf_hessian_thermo_rotation_const\",\"arguments\":{\"mass\":[15.999,1.008,1.008],\"atom_coords\":[[0.0,0.0,0.0],[1.809,0.0,0.0],[-0.453,1.748,0.0]],\"unit\":\"wavenumber\"}},{\"name\":\"pyscf_hessian_thermo_rotation_const\",\"arguments\":{\"mass\":[15.999,1.008,1.008],\"atom_coords\":[[0.0,0.0,0.0],[1.43,1.11,0.0],[-1.43,1.11,0.0]],\"unit\":\"GHz\"}}]"}
{"func_name": "pyscf_lib_chkfile_dump", "func_desc": "Save array(s) or nested Python containers into a checkpoint file (HDF5)\n    used by the PySCF (Python-based Simulations of Chemistry Framework) code base.\n    \n    This function persistently stores numerical arrays, Python sequences (list/tuple),\n    and mapping objects (dict) into an HDF5-formatted checkpoint file so that\n    PySCF computations (for example, CI coefficients, symmetry metadata, density\n    matrices, and other intermediate results) can be reloaded by later steps or\n    by other PySCF modules. The function writes datasets and groups using the\n    h5py/HDF5 storage model via pyscf.lib.H5FileWrap. When given a dict, keys are\n    translated into HDF5 groups and datasets recursively; when given a list or\n    tuple, elements are stored under a generated group named with the suffix\n    \"__from_list__\" and numeric, zero-padded member names (format '%06d'). If a\n    dataset cannot be written directly because it has an object dtype or cannot be\n    broadcast into a single HDF5 dataset, the implementation falls back to saving\n    each element individually as a group of datasets. Existing entries at the\n    provided key (or its \"__from_list__\" variant) in the target file are deleted\n    before writing the new content.", "tools": [{"function": {"description": "Save array(s) or nested Python containers into a checkpoint file (HDF5)\nused by the PySCF (Python-based Simulations of Chemistry Framework) code base.\n\nThis function persistently stores numerical arrays, Python sequences (list/tuple),\nand mapping objects (dict) into an HDF5-formatted checkpoint file so that\nPySCF computations (for example, CI coefficients, symmetry metadata, density\nmatrices, and other intermediate results) can be reloaded by later steps or\nby other PySCF modules. The function writes datasets and groups using the\nh5py/HDF5 storage model via pyscf.lib.H5FileWrap. When given a dict, keys are\ntranslated into HDF5 groups and datasets recursively; when given a list or\ntuple, elements are stored under a generated group named with the suffix\n\"__from_list__\" and numeric, zero-padded member names (format '%06d'). If a\ndataset cannot be written directly because it has an object dtype or cannot be\nbroadcast into a single HDF5 dataset, the implementation falls back to saving\neach element individually as a group of datasets. Existing entries at the\nprovided key (or its \"__from_list__\" variant) in the target file are deleted\nbefore writing the new content.", "name": "pyscf_lib_chkfile_dump", "parameters": {"properties": {"chkfile": {"type": "string", "description": "Name or path of the checkpoint file to write. This is the\nfilesystem path that will be opened by H5FileWrap/h5py. If the path\nalready contains a valid HDF5 file (detected via h5py.is_hdf5), the\nfile is opened in read-write mode ('r+') and the specified key is\nremoved before writing. If the path does not point to an HDF5 file,\na new HDF5 file is created (opened with mode 'w'), which replaces or\ncreates the file at that path. The chkfile argument therefore controls\nthe persistent storage location for PySCF checkpoint data.", "default": ""}, "key": {"type": "string", "description": "HDF5 key (name or path) to use for storing value inside the\nHDF5 hierarchy. This string may contain forward slashes '/' to\nindicate nested HDF5 group paths (for example 'symm/Ci/op'). Before\nwriting, any existing entry with the same key or with the key appended\nby '__from_list__' will be deleted to ensure the new data replaces the\nold. The key determines where in the HDF5 file the dataset/group tree\nfor value will appear and how it is later retrieved by pyscf.lib.chkfile\nread operations.", "default": ""}, "value": {"type": "any", "description": "The Python object to persist. If\nvalue is a dict, each key/value pair is saved recursively as HDF5\ngroups/datasets, preserving the mapping structure so that nested\ndictionaries become nested HDF5 groups. If value is a list or tuple,\nit is saved under a group named key + '__from_list__' and each element\nis stored under a zero-padded numeric subkey ('%06d'). For plain\narray-like objects (NumPy arrays, vectors, etc.), value is written as\nan HDF5 dataset. If the array has an object dtype or cannot be written\nas a single dataset, the function falls back to storing elements\nindividually in a generated '__from_list__' group. This parameter is\nthe primary mechanism for persisting numerical and structured data\nproduced by PySCF calculations.", "default": ""}}, "required": ["chkfile", "key", "value"], "type": "any"}}, "type": "function"}], "query": "We are stress-testing PySCF restartability for an H2O workflow where upstream parsers sometimes emit mixed-quality intermediates. For each branch, checkpoint only physically usable numeric blocks into HDF5, and quarantine anything non-finite or schema-inconsistent under a separate \"qc/\" subgroup for later inspection.\n\nCASSCF branch (checkpoint file: \"/tmp/h2o_casscf.chk\"): Under group key \"casscf/intermediate\", store the numeric arrays only if they contain finite numbers and have the expected rank: CI coefficients must be a 2D array and mo_energy must be a 1D vector. Use the following raw bundle:\n- ci_coeff candidates: [[0.1023, -0.0345], [0.5567, 0.1201], [-0.2204, 0.7899]]\n- mo_energy candidates: [-20.1345, -1.0532, 0.5128, 0.7741]\n- metadata candidates: {mol: \"H2O\", ncas: 6, nelecas: [4, 2], provenance: {git_rev: \"a1b2c3\", walltime_s: 12.4}}\nIf metadata contains any non-numeric payloads, store it under \"casscf/qc/metadata_raw\" instead of mixing it into the main intermediate group.\n\nCCSD branch (checkpoint file: \"/tmp/h2o_ccsd.chk\"): Under group key \"ccsd/intermediates\", store only dense numeric tensors that are finite and square when required. Use this raw bundle:\n- dm1 candidates: [[1.98, 0.02], [0.02, 0.02]]\n- mo_energy candidates: [-20.55, -1.12, 0.35, 0.82]\n- symmetry candidates: {irrep_name: [\"A1\", \"B2\", \"A1\", \"B1\"], orb_sym: [1, 2, 1, 3]}\nStore the symmetry block under \"ccsd/qc/symmetry_raw\" because it is categorical/mixed-type; keep numeric arrays in the main intermediates group.", "answers": "[{\"name\":\"pyscf_lib_chkfile_dump\",\"arguments\":{\"chkfile\":\"/tmp/h2o_casscf.chk\",\"key\":\"casscf/intermediate\",\"value\":{\"ci_coeff\":[[0.1023,-0.0345],[0.5567,0.1201],[-0.2204,0.7899]],\"mo_energy\":[-20.1345,-1.0532,0.5128,0.7741]}}},{\"name\":\"pyscf_lib_chkfile_dump\",\"arguments\":{\"chkfile\":\"/tmp/h2o_casscf.chk\",\"key\":\"casscf/qc/metadata_raw\",\"value\":{\"mol\":\"H2O\",\"ncas\":6,\"nelecas\":[4,2],\"provenance\":{\"git_rev\":\"a1b2c3\",\"walltime_s\":12.4}}}},{\"name\":\"pyscf_lib_chkfile_dump\",\"arguments\":{\"chkfile\":\"/tmp/h2o_ccsd.chk\",\"key\":\"ccsd/intermediates\",\"value\":{\"dm1\":[[1.98,0.02],[0.02,0.02]],\"mo_energy\":[-20.55,-1.12,0.35,0.82]}}},{\"name\":\"pyscf_lib_chkfile_dump\",\"arguments\":{\"chkfile\":\"/tmp/h2o_ccsd.chk\",\"key\":\"ccsd/qc/symmetry_raw\",\"value\":{\"irrep_name\":[\"A1\",\"B2\",\"A1\",\"B1\"],\"orb_sym\":[1,2,1,3]}}}]"}
{"func_name": "pyscf_lib_linalg_helper_eigh_by_blocks", "func_desc": "Solve an ordinary or generalized eigenvalue problem by diagonalizing diagonal blocks\n    of a Hermitian (or real symmetric) matrix. This function is used in the PySCF\n    quantum chemistry framework to perform block-wise diagonalization when basis\n    functions are partitioned (for example by symmetry irreducible representations,\n    orbital symmetries, or other grouping labels). When labels are provided, the\n    rows and columns with the same label are grouped into a block and each block is\n    diagonalized independently; when labels is None, the full eigenproblem is\n    delegated to scipy.linalg.eigh.", "tools": [{"function": {"description": "Solve an ordinary or generalized eigenvalue problem by diagonalizing diagonal blocks\nof a Hermitian (or real symmetric) matrix. This function is used in the PySCF\nquantum chemistry framework to perform block-wise diagonalization when basis\nfunctions are partitioned (for example by symmetry irreducible representations,\norbital symmetries, or other grouping labels). When labels are provided, the\nrows and columns with the same label are grouped into a block and each block is\ndiagonalized independently; when labels is None, the full eigenproblem is\ndelegated to scipy.linalg.eigh.", "name": "pyscf_lib_linalg_helper_eigh_by_blocks", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "float"}, "description": "2D complex-Hermitian or real-symmetric matrix representing\nthe operator to diagonalize (for example a Fock, Hamiltonian, or other\nmolecular integral matrix). The function expects h.shape == (N, N).\nThe dtype of h determines the dtype of the returned eigenvector array.\nThis matrix is not modified in-place.", "default": ""}, "s": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional 2D overlap (metric) matrix for solving the\ngeneralized eigenvalue problem h x = s x e. If provided, s must be a\nsquare numpy.ndarray with shape (N, N) and should correspond to the\nsame basis as h (same ordering and size). If s is None (the default),\nan ordinary eigenvalue problem is solved. When labels is provided and s\nis given, each block uses the corresponding sub-block of s (s[idx][:,idx])\nto solve the generalized eigenproblem for that block.", "default": null}, "labels": {"type": "any", "nullable": true, "description": "Optional list providing a label for each basis function (each\nrow/column of h and s). labels must have length N (the matrix dimension)\nand define the partitioning into blocks: all indices with the same\nlabel are collected into a single diagonal block that is diagonalized\nindependently. Typical use in PySCF: labels are orbital symmetry names\nor irreducible representation identifiers (strings) returned by\nsymmetry utilities (e.g., symm.label_orb_symm). If labels is None\n(default), no block partitioning is performed and the full matrix is\ndiagonalized by scipy.linalg.eigh.", "default": null}}, "required": ["h", "labels", "s"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our PySCF-style symmetry-resolved diagonalization on a small “mixed-quality” batch exported from an integral builder. Each replicate is a candidate Hermitian generalized eigenproblem (H x = S x e) with per-basis-function irrep labels. In practice, some basis functions are unassigned (label is None/\"\"), and sometimes the overlap matrix is absent (orthonormal basis) so the problem reduces to an ordinary eigenproblem. For each replicate: (i) form symmetry blocks only from basis functions that have a non-empty string label (treat None/\"\" as unassigned and ignore them for the block benchmark), (ii) diagonalize each irrep block independently (no cross-block mixing), (iii) if S is provided use the generalized eigenproblem; if S is None use the ordinary eigenproblem. Use these three replicates:\n\nReplicate 1 (ordinary eigenproblem; overlap not provided):\nH = [[1.20,0.10,0,0,0,0.05],[0.10,1.10,0,0,0,0.02],[0,0,0.80,0.03,0.01,0],[0,0,0.03,0.90,0.02,0],[0,0,0.01,0.02,1.00,0],[0.05,0.02,0,0,0,1.30]]\nlabels = [\"A1\",\"A1\",\"B2\",\"B2\",\"B2\",\"\"]\n\nReplicate 2 (generalized eigenproblem):\nH = [[-1.0,0.1,0.0,0.0],[0.1,-0.5,0.0,0.0],[0.0,0.0,0.2,0.05],[0.0,0.0,0.05,0.3]]\nS = [[1.0,0.02,0.0,0.0],[0.02,1.0,0.0,0.0],[0.0,0.0,1.0,0.01],[0.0,0.0,0.01,1.0]]\nlabels = [\"A1\",null,\"B2\",\"B2\"]\n\nReplicate 3 (ordinary eigenproblem; orthonormal basis):\nH = [[0.75,0.10,0.00,0.00],[0.10,0.80,0.00,0.00],[0.00,0.00,1.10,0.05],[0.00,0.00,0.05,1.20]]\nlabels = [\"A1\",\"A1\",\"B2\",\"B2\"]\n\nReturn eigenvalues and eigenvectors for each retained symmetry block for each replicate.", "answers": "[{\"name\":\"pyscf_lib_linalg_helper_eigh_by_blocks\",\"arguments\":{\"h\":[[1.2,0.1,0.0,0.0,0.0,0.05],[0.1,1.1,0.0,0.0,0.0,0.02],[0.0,0.0,0.8,0.03,0.01,0.0],[0.0,0.0,0.03,0.9,0.02,0.0],[0.0,0.0,0.01,0.02,1.0,0.0],[0.05,0.02,0.0,0.0,0.0,1.3]],\"s\":null,\"labels\":[\"A1\",\"A1\",\"B2\",\"B2\",\"B2\",\"\"]}},{\"name\":\"pyscf_lib_linalg_helper_eigh_by_blocks\",\"arguments\":{\"h\":[[-1.0,0.1,0.0,0.0],[0.1,-0.5,0.0,0.0],[0.0,0.0,0.2,0.05],[0.0,0.0,0.05,0.3]],\"s\":[[1.0,0.02,0.0,0.0],[0.02,1.0,0.0,0.0],[0.0,0.0,1.0,0.01],[0.0,0.0,0.01,1.0]],\"labels\":[\"A1\",null,\"B2\",\"B2\"]}},{\"name\":\"pyscf_lib_linalg_helper_eigh_by_blocks\",\"arguments\":{\"h\":[[0.75,0.1,0.0,0.0],[0.1,0.8,0.0,0.0],[0.0,0.0,1.1,0.05],[0.0,0.0,0.05,1.2]],\"s\":null,\"labels\":[\"A1\",\"A1\",\"B2\",\"B2\"]}}]"}
{"func_name": "pyscf_lib_linalg_helper_make_diag_precond", "func_desc": "Generate a diagonal preconditioner function for use in PySCF linear-algebra routines (e.g., iterative eigenvalue or subspace solvers). The returned function applies an elementwise division by a shifted diagonal to regularize the usual Jacobi-style preconditioner (Ax - x*e)/(diag(A) - e), breaking the correlation that can make basis vectors linearly dependent (see issue referenced in source). The implementation uses a fixed small cutoff (1e-8) to avoid exact division by zero after applying the level shift.", "tools": [{"function": {"description": "Generate a diagonal preconditioner function for use in PySCF linear-algebra routines (e.g., iterative eigenvalue or subspace solvers). The returned function applies an elementwise division by a shifted diagonal to regularize the usual Jacobi-style preconditioner (Ax - x*e)/(diag(A) - e), breaking the correlation that can make basis vectors linearly dependent (see issue referenced in source). The implementation uses a fixed small cutoff (1e-8) to avoid exact division by zero after applying the level shift.\n", "name": "pyscf_lib_linalg_helper_make_diag_precond", "parameters": {"properties": {"diag": {"type": "array", "items": {"type": "float"}, "description": "1-D array of the diagonal elements of the matrix A used by the preconditioner. This array supplies diag(A) in the expression diag - (e - level_shift). The array is captured by closure when make_diag_precond is called; the returned preconditioner will compute a temporary shifted-diagonal array (diagd) as diag - (e - level_shift) and will not modify the original diag object. In the PySCF context, diag typically comes from the diagonal of a Hamiltonian or other operator and is used to form a cheap, elementwise preconditioner for iterative linear-algebra routines.", "default": ""}, "level_shift": {"type": "float", "description": "Small positive scalar (default 0.001) added to the eigenvalue shift inside the denominator to break correlations between the numerator (Ax - x*e) and the denominator (diag(A) - e). Practically, this reduces the chance of very small denominators that would produce instabilities or nearly linearly dependent correction vectors. The value is applied as diag - (e - level_shift). Extremely small or zero values of level_shift may fail to prevent correlation; excessively large values will bias the preconditioning and may slow convergence.", "default": 0.001}}, "required": ["diag", "level_shift"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating diagonal Hamiltonian estimates coming from two QC workflows (a tight SCF protocol and a looser, larger active-space protocol) into a single raw pool before launching multiple Davidson/subspace solver cohorts. Raw diagonals (Hartree) are:\n- SCF-like dump: [0.12, 0.55, 1.03, 1.48, 2.10]\n- active-space dump: [0.12, 0.55, 1.03, 1.58, 2.10, 3.05]\nTo reduce correlated update directions, build Jacobi-style diagonal preconditioner callables with a data-dependent regularization rule: set the level shift to 0.002 for any cohort whose diagonal span (max(diag)−min(diag)) is < 2.5, and set it to 0.005 otherwise. Generate one preconditioner per cohort using its own diagonal array (keep the built-in 1e-8 cutoff behavior as implemented). Return the two preconditioner functions to plug into the eigensolver benchmarks.", "answers": "[{\"name\":\"pyscf_lib_linalg_helper_make_diag_precond\",\"arguments\":{\"diag\":[0.12,0.55,1.03,1.48,2.1],\"level_shift\":0.002}},{\"name\":\"pyscf_lib_linalg_helper_make_diag_precond\",\"arguments\":{\"diag\":[0.12,0.55,1.03,1.58,2.1,3.05],\"level_shift\":0.005}}]"}
{"func_name": "pyscf_lib_misc_finger", "func_desc": "pyscf.lib.misc.finger computes a deterministic scalar \"fingerprint\" for a numpy.ndarray by taking the dot product of the array flattened in row-major order with a fixed cosine-based weight vector. In the PySCF (Python-based Simulations of Chemistry Framework) codebase this routine is used to produce compact numeric signatures of numerical arrays (for example, molecular integrals, orbital coefficient arrays, density matrices, or intermediate tensors) for quick equality checks, lightweight caching keys, logging, or debugging comparisons where a full elementwise comparison would be expensive.", "tools": [{"function": {"description": "pyscf.lib.misc.finger computes a deterministic scalar \"fingerprint\" for a numpy.ndarray by taking the dot product of the array flattened in row-major order with a fixed cosine-based weight vector. In the PySCF (Python-based Simulations of Chemistry Framework) codebase this routine is used to produce compact numeric signatures of numerical arrays (for example, molecular integrals, orbital coefficient arrays, density matrices, or intermediate tensors) for quick equality checks, lightweight caching keys, logging, or debugging comparisons where a full elementwise comparison would be expensive.\n", "name": "pyscf_lib_misc_finger", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Input numeric array whose compact signature is to be computed. The array is interpreted in row-major order and flattened via ravel() before weighting. The function expects a numeric numpy.ndarray containing the raw floating-point or integer data produced by PySCF computations; non-numeric dtypes or arrays that cannot be converted to a numeric ndarray may cause numpy to raise an exception. The routine does not modify the input array in-place (it calls numpy.asarray(a) which may or may not make a copy depending on the input), and the calculation is performed on the flattened view of the array so the original shape does not affect which elements contribute, only their order.", "default": ""}}, "required": ["a"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a batch of 3×3 spin-summed AO density-matrix snapshots from a PySCF debug run before generating cache keys. Because some snapshots may be numerically non-physical due to IO corruption or loss of symmetry, only generate fingerprints for matrices that are (i) symmetric within a tight tolerance and (ii) have all eigenvalues non-negative within numerical noise. Use the exact same deterministic fingerprint routine (flatten row-major, dot with the fixed cosine-based weight vector) for every matrix that passes QC, and log the resulting scalar signatures. The candidate snapshots are:\n1) [[1.0, 0.12, -0.05], [0.12, 0.9, 0.03], [-0.05, 0.03, 0.8]]\n2) [[1.0, 0.2, -0.1], [0.2, 0.9, 0.05], [-0.1, 0.05, 0.8]]\n3) [[1.0, 0.15, 0.0], [0.1, 0.9, 0.02], [0.0, 0.02, 0.8]]\n4) [[1.0, 0.0, 0.0], [0.0, -0.01, 0.0], [0.0, 0.0, 0.8]]", "answers": "[{\"name\":\"pyscf_lib_misc_finger\",\"arguments\":{\"a\":[[1.0,0.12,-0.05],[0.12,0.9,0.03],[-0.05,0.03,0.8]]}},{\"name\":\"pyscf_lib_misc_finger\",\"arguments\":{\"a\":[[1.0,0.2,-0.1],[0.2,0.9,0.05],[-0.1,0.05,0.8]]}}]"}
{"func_name": "pyscf_lib_misc_index_tril_to_pair", "func_desc": "pyscf.lib.misc.index_tril_to_pair maps a flattened lower-triangular (tril) linear index used in PySCF to the corresponding pair of row and column indices (i, j).\n    \n    Given a linear index or array of linear indices ij that encodes positions in a packed lower-triangular layout using the convention\n    ij = i * (i + 1) / 2 + j,\n    this function computes the pair indices i and j (zero-based, consistent with NumPy/Python) such that each returned pair satisfies the above relation. In PySCF and quantum chemistry codes, this mapping is commonly used when compactly storing symmetric matrices (for example overlap, density, or two-electron integrals) in a 1D array containing only the lower triangle; index_tril_to_pair recovers the original matrix coordinates used to reconstruct or address elements.", "tools": [{"function": {"description": "pyscf.lib.misc.index_tril_to_pair maps a flattened lower-triangular (tril) linear index used in PySCF to the corresponding pair of row and column indices (i, j).\n\nGiven a linear index or array of linear indices ij that encodes positions in a packed lower-triangular layout using the convention\nij = i * (i + 1) / 2 + j,\nthis function computes the pair indices i and j (zero-based, consistent with NumPy/Python) such that each returned pair satisfies the above relation. In PySCF and quantum chemistry codes, this mapping is commonly used when compactly storing symmetric matrices (for example overlap, density, or two-electron integrals) in a 1D array containing only the lower triangle; index_tril_to_pair recovers the original matrix coordinates used to reconstruct or address elements.", "name": "pyscf_lib_misc_index_tril_to_pair", "parameters": {"properties": {"ij": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of non-negative integer tril indices encoded with the convention ij = i*(i+1)/2 + j. ij must represent valid packed lower-triangular positions: i >= 0 and 0 <= j <= i for each element. The function expects a NumPy ndarray (typically integer dtype) and returns arrays of the same shape. Passing values that are negative, non-integer, or otherwise not representing valid tril indices will lead to undefined or incorrect results: negative values typically produce NaNs from the square-root step and then cause a conversion error when casting to integers.", "default": ""}}, "required": ["ij"], "type": "any"}}, "type": "function"}], "query": "We’re validating packed lower-triangular addressing in a mixed QC pipeline where indices come from heterogeneous sources (reconstruction scans, sparse updates, and debug taps) and may include corrupted sentinel values. Using the PySCF convention ij = i*(i+1)/2 + j (zero-based), take the raw packed-index stream:\n\n[0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 15, 17, 20, -1, 999]\n\nApply these protocol rules:\n1) Treat any index as usable only if it is a non-negative integer and is strictly less than the packed length of a symmetric 6×6 matrix.\n2) Route usable indices into cohorts based on their intrinsic parity: even indices go to the “Fock reconstruction” cohort; odd indices go to the “overlap update” cohort.\n3) In addition, if the usable index equals a triangular number T_k = k*(k+1)/2 for some integer k, also include it in a “diagonal debug” cohort.\n\nFor each cohort, map its packed indices back to (i, j) coordinates via index_tril_to_pair and return all resulting (i, j) pairs grouped by cohort (Fock reconstruction, overlap update, diagonal debug).", "answers": "[{\"name\":\"pyscf_lib_misc_index_tril_to_pair\",\"arguments\":{\"ij\":[0,2,4,6,8,10,12,20]}},{\"name\":\"pyscf_lib_misc_index_tril_to_pair\",\"arguments\":{\"ij\":[1,3,5,15,17]}},{\"name\":\"pyscf_lib_misc_index_tril_to_pair\",\"arguments\":{\"ij\":[0,1,3,6,10,15]}}]"}
{"func_name": "pyscf_lib_misc_invalid_method", "func_desc": "pyscf.lib.misc.invalid_method creates a placeholder callable that indicates a method is invalid or intentionally unimplemented within the PySCF framework. It is used when a class, module, or API wants to de-register or replace an existing method with a sentinel that consistently signals \"not implemented\" at runtime, aiding clear error reporting and API maintenance.", "tools": [{"function": {"description": "pyscf.lib.misc.invalid_method creates a placeholder callable that indicates a method is invalid or intentionally unimplemented within the PySCF framework. It is used when a class, module, or API wants to de-register or replace an existing method with a sentinel that consistently signals \"not implemented\" at runtime, aiding clear error reporting and API maintenance.\n", "name": "pyscf_lib_misc_invalid_method", "parameters": {"properties": {"name": {"type": "string", "description": "The canonical name to assign to the generated placeholder callable. This string is used in two practical ways within the PySCF codebase: it becomes the returned function's __name__ attribute (so tools, tracebacks, and introspection report the given name), and it is interpolated into the NotImplementedError message raised by the placeholder. The caller must provide a descriptive name for the method being de-registered; providing a non-str value is not supported and may produce a TypeError or unexpected diagnostics when the function is created or invoked.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "During a post-refactor PySCF extension audit, we’ve found a mixed registry of legacy hook names collected from multiple plugins (some are valid Python identifiers, some contain separators from old namespacing conventions, and some are clearly malformed). To harden the API, generate invalid-method sentinels only for hook names that are syntactically valid Python identifiers and correspond to CCSD(T)-related entry points (i.e., the hook name includes the token `ccsd` and also includes a standalone `t` token separated by underscores). From the following raw registry, create placeholder callables for every name that meets those criteria so any accidental runtime invocation raises a clear NotImplementedError referencing the missing method: `['run_ccsd_t_legacy', 'DF_get_jk', 'ccsd_t_gradient', 'ccsdt', 'ccsd(t)_driver', 'ccsd_t-gradient', 'run_ccsd__t__legacy', '_ccsd_t_internal', 'ccsd_t']`.", "answers": "[{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"run_ccsd_t_legacy\"}},{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"ccsd_t_gradient\"}},{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"run_ccsd__t__legacy\"}},{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"_ccsd_t_internal\"}},{\"name\":\"pyscf_lib_misc_invalid_method\",\"arguments\":{\"name\":\"ccsd_t\"}}]"}
{"func_name": "pyscf_lib_misc_num_threads", "func_desc": "pyscf.lib.misc.num_threads controls and queries the number of OpenMP (OMP) threads used by PySCF and dependent native libraries (for example, NumPy or BLAS backends) in parallel numerical workloads. When called with an integer argument, it requests the runtime to set the process-wide OMP thread count to that value using the PySCF internal numpy_helper C interface. When called without an argument, it returns the currently available OMP thread count as observed through the same internal interface.\n    \n    This function is used in the PySCF codebase and by PySCF users to manage parallelism for CPU-bound linear algebra and other OpenMP-enabled kernels. It is recommended over directly setting the environment variable OMP_NUM_THREADS at runtime because many native libraries read that environment variable when they are imported/initialized; changing os.environ['OMP_NUM_THREADS'] after import does not reliably change the runtime thread count for those libraries. Internally this function delegates to pyscf.lib.numpy_helper._np_helper.set_omp_threads and pyscf.lib.numpy_helper._np_helper.get_omp_threads via ctypes, so the observed behavior depends on that C helper and the availability of OpenMP support in the build.", "tools": [{"function": {"description": "pyscf.lib.misc.num_threads controls and queries the number of OpenMP (OMP) threads used by PySCF and dependent native libraries (for example, NumPy or BLAS backends) in parallel numerical workloads. When called with an integer argument, it requests the runtime to set the process-wide OMP thread count to that value using the PySCF internal numpy_helper C interface. When called without an argument, it returns the currently available OMP thread count as observed through the same internal interface.\n\nThis function is used in the PySCF codebase and by PySCF users to manage parallelism for CPU-bound linear algebra and other OpenMP-enabled kernels. It is recommended over directly setting the environment variable OMP_NUM_THREADS at runtime because many native libraries read that environment variable when they are imported/initialized; changing os.environ['OMP_NUM_THREADS'] after import does not reliably change the runtime thread count for those libraries. Internally this function delegates to pyscf.lib.numpy_helper._np_helper.set_omp_threads and pyscf.lib.numpy_helper._np_helper.get_omp_threads via ctypes, so the observed behavior depends on that C helper and the availability of OpenMP support in the build.", "name": "pyscf_lib_misc_num_threads", "parameters": {"properties": {"n": {"type": "integer", "nullable": true, "description": "The desired number of OpenMP threads to set for the current process. If n is None (the default), the function does not attempt to change the thread count and instead queries and returns the current number of OMP threads. When provided, n is converted to an integer with int(n) before being passed to the underlying C helper; therefore, n should be an integer or a value safely convertible to int. Calling with a numeric n causes a side effect: the process-wide OMP thread count is updated (subject to the underlying OpenMP/BLAS library behavior and availability). If OpenMP is not available in the runtime, the underlying C call returns 0 and the function emits a warning indicating that setting threads has no effect.", "default": null}}, "required": ["n"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed PySCF benchmark queue on shared hosts and need to set process-wide OpenMP threading caps based on each job’s intrinsic CPU profile to avoid oversubscription. Here’s the run manifest (each entry has an intended host core count and whether the workload is BLAS-dominated):\n\n1) id=lap-scf-r1, host_cores=8, workload=SCF, blas_dominant=false\n2) id=ws-ccsd, host_cores=16, workload=CCSD(T), blas_dominant=true\n3) id=ws-scf-heavy, host_cores=16, workload=SCF, blas_dominant=true\n\nThreading rule: set OMP threads to half the host core count, but if the workload is BLAS-dominant then cap it to at most 8 threads. For any run assigned the 8-thread cap, also query the runtime thread setting immediately after applying it so we can record the observed value in logs. Apply this rule in the manifest order before launching each calculation.", "answers": "[{\"name\":\"pyscf_lib_misc_num_threads\",\"arguments\":{\"n\":4}},{\"name\":\"pyscf_lib_misc_num_threads\",\"arguments\":{\"n\":8}},{\"name\":\"pyscf_lib_misc_num_threads\",\"arguments\":{}},{\"name\":\"pyscf_lib_misc_num_threads\",\"arguments\":{\"n\":8}},{\"name\":\"pyscf_lib_misc_num_threads\",\"arguments\":{}}]"}
{"func_name": "pyscf_lib_misc_prange", "func_desc": "pyscf.lib.misc.prange splits an integer index interval into contiguous, non-overlapping fragments of a given maximum length and yields the boundaries for each fragment. In the PySCF (Python-based Simulations of Chemistry Framework) codebase, this helper is used to partition linear index ranges (for example, ranges of atomic orbital or molecular orbital indices, integral blocks, or other loop indices) into smaller chunks for batched processing, memory-limited loops, or parallel work distribution.\n    \n    The function is a generator: for a given integer interval from start up to end, it produces successive tuples (p0, p1) that define half-open subintervals [p0, p1) covering the original range without overlap. Each yielded tuple gives the inclusive start index and the exclusive end index of a fragment; the final fragment may be shorter than step if (end - start) is not an exact multiple of step.", "tools": [{"function": {"description": "pyscf.lib.misc.prange splits an integer index interval into contiguous, non-overlapping fragments of a given maximum length and yields the boundaries for each fragment. In the PySCF (Python-based Simulations of Chemistry Framework) codebase, this helper is used to partition linear index ranges (for example, ranges of atomic orbital or molecular orbital indices, integral blocks, or other loop indices) into smaller chunks for batched processing, memory-limited loops, or parallel work distribution.\n\nThe function is a generator: for a given integer interval from start up to end, it produces successive tuples (p0, p1) that define half-open subintervals [p0, p1) covering the original range without overlap. Each yielded tuple gives the inclusive start index and the exclusive end index of a fragment; the final fragment may be shorter than step if (end - start) is not an exact multiple of step.", "name": "pyscf_lib_misc_prange", "parameters": {"properties": {"start": {"type": "integer", "description": "The inclusive starting integer index of the full interval to split. In PySCF use this is typically the first index of a set of indices to iterate over (for example, 0 for indexing into arrays of basis functions). If start >= end, no fragments are yielded.", "default": ""}, "end": {"type": "integer", "description": "The integer end boundary of the full interval. This value is exclusive: the generated fragments will cover indices up to but not including end. In practice this corresponds to the upper limit for indexing in a loop or the length of a collection in PySCF tasks.", "default": ""}, "step": {"type": "integer", "description": "The maximum length of each fragment, expressed as a positive integer. This controls how many indices are grouped into each yielded fragment and therefore affects memory and work granularity for batched or parallel operations in PySCF workflows.", "default": ""}}, "required": ["start", "end", "step"], "type": "any"}}, "type": "function"}], "query": "We’re running a mixed-basis PySCF workflow where each index domain must be chunked with `pyscf.lib.misc.prange`, but the chunk size is chosen from the domain’s intrinsic metadata (core/active boundaries, projected memory class, and spin structure). Use the following raw domains (half-open intervals), each tagged with properties:\n\n1) AO contraction domain: core-frozen boundary at 12; total AOs = 214; stage = “AO”; memory_class = “normal”. Partition only the post-core active window and use a max block length equal to 2× the number of frozen-core shells (frozen-core shells = 20).\n\n2) MO coefficient sweep: orbitals = 10,000; stage = “MO”; memory_class = “tight”; spin = “RHF”. Use a max block length equal to 2^k where k is the largest integer such that the block length does not exceed 1/8 of the orbital count.\n\n3) MO response build: orbitals = 10,000; stage = “MO”; memory_class = “very_tight”; spin = “UHF”. Use a max block length equal to half of the block length used for the tight-memory RHF MO sweep.\n\nReturn the `(p0, p1)` boundaries for each resulting batching scheme (one scheme per domain), suitable for driving contiguous, non-overlapping batched loops.", "answers": "[{\"name\":\"pyscf_lib_misc_prange\",\"arguments\":{\"start\":12,\"end\":214,\"step\":40}},{\"name\":\"pyscf_lib_misc_prange\",\"arguments\":{\"start\":0,\"end\":10000,\"step\":1024}},{\"name\":\"pyscf_lib_misc_prange\",\"arguments\":{\"start\":0,\"end\":10000,\"step\":512}}]"}
{"func_name": "pyscf_lib_misc_repo_info", "func_desc": "pyscf.lib.misc.repo_info obtains repository metadata for a PySCF source tree: it resolves the supplied path to an absolute repository location and, when a Git repository is detected, assembles a one- or two-line Git status string containing original HEAD, branch name, and commit ID. This information is used by PySCF to report the source repository location and commit identifiers for reproducibility, debugging, logging, and citation purposes.\n    \n    The function inspects the filesystem for a .git directory either at the given path or in its parent directory. If a Git repository is found, repo_info calls the internal git_info routine and formats its output into human-readable lines such as \"GIT ORIG_HEAD <orig_head>\" and \"GIT HEAD (branch <branch>) <head>\" (or \"GIT HEAD      <head>\" when no branch name is present). If no Git repository is found, only the canonical absolute path is returned. The function does not modify files on disk; it only reads filesystem metadata and delegates to git_info for Git-specific details. The implementation currently does not collect information about external libraries (for example BLAS, libcint, libxc, libxcfun, tblis), as indicated by the in-source TODO.", "tools": [{"function": {"description": "pyscf.lib.misc.repo_info obtains repository metadata for a PySCF source tree: it resolves the supplied path to an absolute repository location and, when a Git repository is detected, assembles a one- or two-line Git status string containing original HEAD, branch name, and commit ID. This information is used by PySCF to report the source repository location and commit identifiers for reproducibility, debugging, logging, and citation purposes.\n\nThe function inspects the filesystem for a .git directory either at the given path or in its parent directory. If a Git repository is found, repo_info calls the internal git_info routine and formats its output into human-readable lines such as \"GIT ORIG_HEAD <orig_head>\" and \"GIT HEAD (branch <branch>) <head>\" (or \"GIT HEAD      <head>\" when no branch name is present). If no Git repository is found, only the canonical absolute path is returned. The function does not modify files on disk; it only reads filesystem metadata and delegates to git_info for Git-specific details. The implementation currently does not collect information about external libraries (for example BLAS, libcint, libxc, libxcfun, tblis), as indicated by the in-source TODO.", "name": "pyscf_lib_misc_repo_info", "parameters": {"properties": {"repo_path": {"type": "string", "description": "Path to a directory that is the repository root or a subdirectory of the repository.\nThe function converts this path to an absolute path (os.path.abspath) and, if a .git\ndirectory is not present at that location but is present in its parent directory,\nthe parent directory is treated as the repository root. This parameter must be a\nfilesystem path string; no validation is performed beyond os.path.isdir checks.", "default": ""}}, "required": ["repo_path"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating provenance metadata for a mixed set of PySCF source trees collected from multiple compute nodes after a week of production runs. The raw inventory includes real checkouts, symlinked stage dirs, and a few “packaged” copies that have no VCS metadata. Given the following candidate locations: \n- /home/user/src/pyscf-dev\n- ./third_party/pyscf\n- /home/alice/projects/pyscf-dev\n- /scratch/run_2026-02-01/pyscf_stage -> /home/user/src/pyscf-dev (symlink)\n- /opt/modules/pyscf/2.5.0 (vendor install)\n\nFor the audit, only record provenance for entries that plausibly represent a developer-style source tree (i.e., a checkout-like directory rather than a system module install), treating symlink targets as the repository location to be inspected. For each selected location, resolve and report the canonical absolute repository path and, when a Git repository is detected by the presence of a .git directory at that path or its parent, include the Git status lines in the exact PySCF logging format (ORIG_HEAD plus current HEAD with branch name when available, including commit IDs). If no Git repo is detected, return only the canonical absolute path.", "answers": "[{\"name\":\"pyscf_lib_misc_repo_info\",\"arguments\":{\"repo_path\":\"/home/user/src/pyscf-dev\"}},{\"name\":\"pyscf_lib_misc_repo_info\",\"arguments\":{\"repo_path\":\"./third_party/pyscf\"}},{\"name\":\"pyscf_lib_misc_repo_info\",\"arguments\":{\"repo_path\":\"/home/alice/projects/pyscf-dev\"}},{\"name\":\"pyscf_lib_misc_repo_info\",\"arguments\":{\"repo_path\":\"/scratch/run_2026-02-01/pyscf_stage\"}}]"}
{"func_name": "pyscf_lib_misc_square_mat_in_trilu_indices", "func_desc": "pyscf.lib.misc.square_mat_in_trilu_indices returns an n x n integer matrix that maps the unique elements of a flattened lower-triangular (tril) vector to positions in a full symmetric square matrix. This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to convert a compact storage of unique symmetric matrix elements (commonly used for symmetric operators, density matrices, and pairwise interaction tensors in quantum chemistry) back into a full symmetric index structure that can be used to reconstruct or address the full matrix.\n    \n    The function constructs the mapping by enumerating the lower-triangular indices in the same order as numpy.tril_indices(n), assigning each unique tril element a consecutive integer index from 0 to n*(n+1)//2 - 1, and mirroring those indices across the diagonal so that the output matrix is symmetric. The result is independent of any external state (no side effects) and uses O(n^2) time and memory to produce an (n, n) integer array.", "tools": [{"function": {"description": "pyscf.lib.misc.square_mat_in_trilu_indices returns an n x n integer matrix that maps the unique elements of a flattened lower-triangular (tril) vector to positions in a full symmetric square matrix. This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to convert a compact storage of unique symmetric matrix elements (commonly used for symmetric operators, density matrices, and pairwise interaction tensors in quantum chemistry) back into a full symmetric index structure that can be used to reconstruct or address the full matrix.\n\nThe function constructs the mapping by enumerating the lower-triangular indices in the same order as numpy.tril_indices(n), assigning each unique tril element a consecutive integer index from 0 to n*(n+1)//2 - 1, and mirroring those indices across the diagonal so that the output matrix is symmetric. The result is independent of any external state (no side effects) and uses O(n^2) time and memory to produce an (n, n) integer array.", "name": "pyscf_lib_misc_square_mat_in_trilu_indices", "parameters": {"properties": {"n": {"type": "integer", "description": "The dimension of the square matrix to construct. n must be a non-negative integer representing the number of rows and columns of the square matrix. In practical PySCF usage, n corresponds to the size of a one-particle basis or the dimension of any symmetric operator whose unique lower-triangular elements have been vectorized. Passing a non-integer type or a negative integer will result in errors from the underlying numpy calls (TypeError or ValueError) and is not supported.", "default": ""}}, "required": ["n"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a PySCF symmetric-operator compaction/reconstruction step across a mixed batch of AO-basis jobs where the effective matrix dimension depends on the basis-set contraction. For each candidate dimension n in the raw job manifest `n_candidates = [4, 6, 7, 8, 0, -3, 5]`, generate the n×n integer index mapping (full symmetric matrix -> flattened lower-triangular unique-element vector) using the exact `numpy.tril_indices(n)` enumeration mirrored across the diagonal. Treat only physically meaningful AO dimensions as valid: keep n values that are positive and correspond to an odd number of unique tril elements (i.e., n(n+1)/2 is odd), since those runs are the ones flagged by our parity-based integrity check. Return the mapping matrices for every n that satisfies the criterion.", "answers": "[{\"name\":\"pyscf_lib_misc_square_mat_in_trilu_indices\",\"arguments\":{\"n\":4}},{\"name\":\"pyscf_lib_misc_square_mat_in_trilu_indices\",\"arguments\":{\"n\":7}}]"}
{"func_name": "pyscf_lib_numpy_helper_base_repr_int", "func_desc": "pyscf.lib.numpy_helper.base_repr_int converts an integer or an array of integers into a sequence of base-N digits and returns those digits as Python lists for scalars or as a 2-D numpy.ndarray for array inputs. This helper is used in PySCF utilities that need an explicit digit-wise representation of integers (for example, encoding occupation patterns, indexing combinatorial objects, or other bit/base decompositions used in quantum-chemistry bookkeeping).", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.base_repr_int converts an integer or an array of integers into a sequence of base-N digits and returns those digits as Python lists for scalars or as a 2-D numpy.ndarray for array inputs. This helper is used in PySCF utilities that need an explicit digit-wise representation of integers (for example, encoding occupation patterns, indexing combinatorial objects, or other bit/base decompositions used in quantum-chemistry bookkeeping).\n", "name": "pyscf_lib_numpy_helper_base_repr_int", "parameters": {"properties": {"number": {"type": "integer", "description": "The value(s) to convert to the given base. If a single Python integer is supplied, the function returns a Python list of integers representing the digits from most-significant to least-significant. If a numpy.ndarray is supplied, the array is flattened and each element is converted; the caller must provide ndigits in this case (the function asserts ndigits is not None). Negative integer values are treated by their absolute value (the sign is discarded) so the output always represents the non-negative magnitude.", "default": ""}, "base": {"type": "integer", "description": "The integer base to use for the conversion. This is the radix of the target number system. Practical use in PySCF includes converting integer indices or bit-pattern encodings into digit sequences. The base must be an integer; base == 0 will raise a ZeroDivisionError and base == 1 leads to non-terminating behavior (the routine relies on integer division and modulo by base), so callers should provide base >= 2 for meaningful results.", "default": ""}, "ndigits": {"type": "integer", "nullable": true, "description": "Number of digits to produce for each converted value. If provided, the result is left-padded with zeros so that the returned digit sequence has length exactly ndigits. When converting a numpy.ndarray, ndigits is required and the function will assert its presence. When ndigits is None (the default), no padding is applied and the minimal number of digits required to represent the magnitude is returned (except that zero is represented as a single digit [0]).", "default": null}}, "required": ["number", "base", "ndigits"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mixed-radix bookkeeping layer for a CI/occupation-pattern encoder. The raw index stream contains both spin-orbital occupation candidates and determinant/configuration candidates, plus occasional sentinels. Use this protocol:\n\n- Treat an index as a *spin-orbital occupation candidate* if it fits within an 8-spin-orbital register when encoded in base-2 (i.e., it can be represented with at most 8 binary digits). For these, generate a fixed-width base-2 digit sequence left-padded with zeros to exactly 8 digits, ordered most-significant to least-significant.\n\n- Treat an index as a *determinant/configuration candidate* if it fits within a 6-site quaternary register when encoded in base-4 (i.e., it can be represented with at most 6 base-4 digits). For these, generate a fixed-width base-4 digit sequence left-padded with zeros to exactly 6 digits, ordered most-significant to least-significant.\n\nRaw indices (process according to the above rules):\n- Spin-orbital stream: 173\n- Determinant/configuration stream (dual-request cross-validation replicates): 57, 57\n\nReturn the digit-wise encodings for all indices that satisfy their register constraints under the appropriate radix/width rules.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_base_repr_int\",\"arguments\":{\"number\":173,\"base\":2,\"ndigits\":8}},{\"name\":\"pyscf_lib_numpy_helper_base_repr_int\",\"arguments\":{\"number\":57,\"base\":4,\"ndigits\":6}},{\"name\":\"pyscf_lib_numpy_helper_base_repr_int\",\"arguments\":{\"number\":57,\"base\":4,\"ndigits\":6}}]"}
{"func_name": "pyscf_lib_numpy_helper_broadcast_mul", "func_desc": "Broadcasted entrywise multiplication specialized for PySCF tensor operations.", "tools": [{"function": {"description": "Broadcasted entrywise multiplication specialized for PySCF tensor operations.\n", "name": "pyscf_lib_numpy_helper_broadcast_mul", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "3D input array in C order. In the PySCF context this is typically a stack of 2D blocks or tensor slices (for example, a collection of matrices indexed by a leading index used in electronic-structure routines). The function requires a.ndim == 3, a.shape[1:] == b.shape, and a.dtype to match b.dtype. The memory layout must satisfy the stride constraint checked in code (the last axis must be contiguous: a.strides[2] == a.itemsize). This array supplies the per-slice values that are multiplied by the 2D factor b for each leading index and participates in the elementwise multiplication out[:, :, :] += a[:, :, :] * b[None, :, :].", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "2D input array in C order. In PySCF usage this represents the 2D multiplier applied to every 2D slice of a (for example, a matrix of coefficients or scaling factors common across the leading dimension). The function requires b.ndim == 2, b.shape == a.shape[1:], and b.dtype to match a.dtype. The memory layout must satisfy b.strides[1] == b.itemsize (contiguous last axis). The first stride (b.strides[0]) is interpreted as the leading-dimension stride and is passed through to the low-level helper.", "default": ""}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional output array in C order with the same shape and dtype as a. If provided, out is modified in place and receives the accumulated result (out[:, :, :] is incremented by the broadcasted product). If out is None, the function allocates and returns a new zero-initialized ndarray with the same shape and dtype as a. When supplied, out must satisfy out.shape == a.shape, out.dtype == a.dtype, and the same last-axis contiguity constraint as a (out.strides[2] == out.itemsize).", "default": null}}, "required": ["a", "b", "out"], "type": "any"}}, "type": "function"}], "query": "We’re post-processing AO-basis 3×4×4 block tensors coming out of two SCF-derived cohorts, but the raw blocks are noisy and we only want to apply a broadcasted elementwise metric where it’s scientifically meaningful.\n\nCohort A (Fock/overlap rescaling): treat each 4×4 slice as a symmetric AO block and run the broadcasted entrywise scaling only for slices that are numerically symmetric within a tight tolerance (max |a[i,j]−a[j,i]| ≤ 1e−12). Use the cohort’s full 4×4 metric b as-is and let the helper allocate the output (out=None).\n\na = [\n  [[1.0, 0.5, 0.2, 0.0], [0.5, 1.1, 0.3, 0.1], [0.2, 0.3, 0.9, 0.4], [0.0, 0.1, 0.4, 0.8]],\n  [[1.2, 0.4, 0.1, 0.0], [0.4, 0.9, 0.2, 0.2], [0.1, 0.2, 0.7, 0.3], [0.0, 0.2, 0.3, 0.6]],\n  [[0.9, 0.3, 0.0, 0.1], [0.3, 1.0, 0.2, 0.0], [0.0, 0.2, 0.8, 0.2], [0.1, 0.0, 0.2, 0.5]]\n]\n\nb = [[1.0, 0.8, 0.6, 0.4], [0.8, 1.0, 0.7, 0.5], [0.6, 0.7, 1.0, 0.9], [0.4, 0.5, 0.9, 1.0]]\n\nCohort B (integral-block coefficient scaling): only scale those 4×4 slices that exhibit at least one sign change (i.e., contain both positive and negative entries), which indicates a mixed-sign coupling block in this dataset. Use the cohort’s full 4×4 coefficient mask b as-is and let the helper allocate the output (out=None).\n\na = [\n  [[1.0, 0.5, 0.0, -0.5], [0.2, 0.3, 0.4, 0.5], [0.0, -0.1, -0.2, -0.3], [1.5, 1.0, 0.5, 0.0]],\n  [[0.1, 0.2, 0.3, 0.4], [0.0, 0.0, 0.1, 0.2], [0.5, 0.4, 0.3, 0.2], [-0.1, -0.2, -0.3, -0.4]],\n  [[2.0, 1.0, 0.0, -1.0], [0.3, 0.6, 0.9, 1.2], [0.0, 0.2, 0.4, 0.6], [1.0, 0.0, -1.0, -2.0]]\n]\n\nb = [[1.0, 0.0, 0.5, 0.0], [0.0, 1.0, 0.0, 0.5], [0.5, 0.0, 1.0, 0.0], [0.0, 0.5, 0.0, 1.0]]", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_broadcast_mul\",\"arguments\":{\"a\":[[[1.0,0.5,0.2,0.0],[0.5,1.1,0.3,0.1],[0.2,0.3,0.9,0.4],[0.0,0.1,0.4,0.8]],[[1.2,0.4,0.1,0.0],[0.4,0.9,0.2,0.2],[0.1,0.2,0.7,0.3],[0.0,0.2,0.3,0.6]],[[0.9,0.3,0.0,0.1],[0.3,1.0,0.2,0.0],[0.0,0.2,0.8,0.2],[0.1,0.0,0.2,0.5]]],\"b\":[[1.0,0.8,0.6,0.4],[0.8,1.0,0.7,0.5],[0.6,0.7,1.0,0.9],[0.4,0.5,0.9,1.0]],\"out\":null}},{\"name\":\"pyscf_lib_numpy_helper_broadcast_mul\",\"arguments\":{\"a\":[[[1.0,0.5,0.0,-0.5],[0.2,0.3,0.4,0.5],[0.0,-0.1,-0.2,-0.3],[1.5,1.0,0.5,0.0]],[[0.1,0.2,0.3,0.4],[0.0,0.0,0.1,0.2],[0.5,0.4,0.3,0.2],[-0.1,-0.2,-0.3,-0.4]],[[2.0,1.0,0.0,-1.0],[0.3,0.6,0.9,1.2],[0.0,0.2,0.4,0.6],[1.0,0.0,-1.0,-2.0]]],\"b\":[[1.0,0.0,0.5,0.0],[0.0,1.0,0.0,0.5],[0.5,0.0,1.0,0.0],[0.0,0.5,0.0,1.0]],\"out\":null}}]"}
{"func_name": "pyscf_lib_numpy_helper_cartesian_prod", "func_desc": "pyscf.lib.numpy_helper.cartesian_prod generates the Cartesian product of multiple one-dimensional input arrays and returns all ordered combinations as a two-dimensional NumPy array. This function is part of PySCF's numpy_helper utilities and is used in the PySCF (Python-based Simulations of Chemistry Framework) codebase to enumerate combinations of 1-D parameter arrays, for example when forming grids or combinatorial parameter lists required in electronic-structure workflows.\n    \n    This function accepts a sequence of 1-D array-like objects, converts them to NumPy arrays, determines a common result dtype using numpy.result_type, allocates or uses the provided output buffer, and arranges the data so that each row of the returned 2-D array is one combination (cartesian product) of the inputs. The number of rows in the result equals the product of the lengths of the input arrays and the number of columns equals the number of input arrays. If an output buffer is provided via the out parameter, the function will use it as the backing storage via numpy.ndarray(..., buffer=out), so the buffer must be compatible in size, dtype, and writable memory layout; otherwise a new array is allocated.", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.cartesian_prod generates the Cartesian product of multiple one-dimensional input arrays and returns all ordered combinations as a two-dimensional NumPy array. This function is part of PySCF's numpy_helper utilities and is used in the PySCF (Python-based Simulations of Chemistry Framework) codebase to enumerate combinations of 1-D parameter arrays, for example when forming grids or combinatorial parameter lists required in electronic-structure workflows.\n\nThis function accepts a sequence of 1-D array-like objects, converts them to NumPy arrays, determines a common result dtype using numpy.result_type, allocates or uses the provided output buffer, and arranges the data so that each row of the returned 2-D array is one combination (cartesian product) of the inputs. The number of rows in the result equals the product of the lengths of the input arrays and the number of columns equals the number of input arrays. If an output buffer is provided via the out parameter, the function will use it as the backing storage via numpy.ndarray(..., buffer=out), so the buffer must be compatible in size, dtype, and writable memory layout; otherwise a new array is allocated.", "name": "pyscf_lib_numpy_helper_cartesian_prod", "parameters": {"properties": {"arrays": {"type": "array", "items": {"type": "float"}, "description": "1-D arrays to form the cartesian product of. Each element in this list is converted with numpy.asarray and therefore may be any array-like object accepted by numpy.asarray (for example Python lists or numpy arrays). The function assumes each converted element is one-dimensional; providing non-1-D arrays or arrays whose lengths or shapes are inconsistent with the required reshaping may raise an error from NumPy during reshape or assignment. The practical significance in PySCF is to enumerate all combinations of scalar parameters or indices (for example constructing parameter grids or enumerating combinations of component values for integrals or basis-related loops).", "default": ""}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Array to place the cartesian product in. When supplied, this array is used as the backing buffer for the intermediate storage via numpy.ndarray(..., buffer=out). The provided array must therefore be compatible with the required total size and the computed dtype (numpy.result_type of the input arrays) and must expose a writable buffer. If out is None (the default), the function allocates and returns a new ndarray. Supplying an incompatible buffer typically results in NumPy raising TypeError or ValueError.", "default": null}}, "required": ["arrays"], "type": "any"}}, "type": "function"}], "query": "We’re assembling two PySCF sweep cohorts from messy pre-screened metadata rather than clean parameter tables.\n\nCohort A (state/basis robustness): start from the raw candidate arrays\n- charge_candidates = [-2, -1, 0, 1, 2]\n- multiplicity_candidates = [0, 1, 2, 3, 5]\n- basis_scale_candidates = [0.75, 0.8, 1.0, 1.2, 1.5]\nConstruct the full ordered (charge, spin_multiplicity, basis_scale) grid only for physically admissible entries, where charge must be in {−1, 0, 1}, spin multiplicity must be an odd positive integer, and basis_scale must lie in [0.8, 1.2] inclusive. Keep column order (charge, spin_multiplicity, basis_scale).\n\nCohort B (geometry-coupled diatomic scan): start from\n- bond_length_candidates_angstrom = [0.7, 0.9, 1.0, 1.1, 1.4]\n- charge_state_candidates = [-2, -1, 0, 1, 2]\n- multiplicity_candidates = [1, 2, 3, 4]\nBuild the ordered (bond_length_angstrom, charge_state, spin_multiplicity) grid, but only for bond lengths within 0.9–1.1 Å inclusive, charge states in {−1, 0, 1}, and odd spin multiplicities. Keep column order (bond_length_angstrom, charge_state, spin_multiplicity).", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_cartesian_prod\",\"arguments\":{\"arrays\":[[-1,0,1],[1,3,5],[0.8,1.0,1.2]]}},{\"name\":\"pyscf_lib_numpy_helper_cartesian_prod\",\"arguments\":{\"arrays\":[[0.9,1.0,1.1],[-1,0,1],[1,3]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_cleanse", "func_desc": "Cleanse floating-point array values by grouping nearly identical numbers (within a specified tolerance) and setting each group to a single representative value. This reduces spurious differences caused by floating-point round-off so that numpy.round_ and numpy.unique behave as expected in numerical workflows used throughout PySCF (Python-based Simulations of Chemistry Framework), for example when stabilizing arrays of orbital energies, integrals, grid values, or other computed quantities before further comparison or grouping.", "tools": [{"function": {"description": "Cleanse floating-point array values by grouping nearly identical numbers (within a specified tolerance) and setting each group to a single representative value. This reduces spurious differences caused by floating-point round-off so that numpy.round_ and numpy.unique behave as expected in numerical workflows used throughout PySCF (Python-based Simulations of Chemistry Framework), for example when stabilizing arrays of orbital energies, integrals, grid values, or other computed quantities before further comparison or grouping.\n", "name": "pyscf_lib_numpy_helper_cleanse", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Array to be cleansed. This input is the numeric array whose small floating-point discrepancies should be removed. The function expects a numeric ndarray (e.g., float dtype) that supports numpy.argsort, numpy.diff and element-wise assignment. The shape and dtype of the returned array will match this input. In PySCF, typical uses include cleansing 1-D or multi-dimensional results (energies, matrix elements, grid values) prior to applying rounding or uniqueness checks.", "default": ""}, "axis": {"type": "integer", "description": "Axis along which values are compared and clustered. Default is 0 (the first axis), meaning values are compared along rows when a is 2-D (or along the leading axis for higher dimensions). If set to None, the array is flattened and the entire flattened 1-D sequence is processed as a single set of values. Invalid axis values will raise the same errors that numpy.moveaxis would raise (e.g., ValueError/IndexError).", "default": 0}, "tol": {"type": "float", "description": "Tolerance threshold for grouping values. Two values whose absolute difference is less than or equal to tol (i.e., diff <= tol after sorting) are treated as equal and set to the same representative value. The default tol is 0, which causes only exact-equal values to be merged. If tol is negative, no values will be merged in practice because numpy.diff on a sorted numeric array is non-negative; the code will therefore treat every value as its own cluster. Use tol to express the maximum permissible floating-point discrepancy to be ignored when consolidating values.", "default": 0}}, "required": ["a", "axis", "tol"], "type": "any"}}, "type": "function"}], "query": "In a PySCF post-SCF validation run, we need to stabilize orbital-energy-like floats, but the raw export mixes true energies with sentinel/overflow artifacts from a failed SCF micro-iteration. Apply a two-stage cleanse protocol:\n\n1) Single-geometry spectrum: take this 1D dump `[-0.50000001, -0.49999999, -0.3000002, -0.3000001, 0.10000001, 0.10000002, 999.0, -999.0]` and run cleansing only on the physically plausible energies (Hartree-scale values with magnitude < 100). Use a tight tolerance of 1e-6.\n\n2) Multi-geometry replicates: we have a (4 orbitals × 3 geometries) matrix `[[ -0.50000001, -0.49999998, -0.50000003], [0.12000001, 0.11999999, 0.12000002], [0.35, 0.34999997, 0.35000004], [0.78000002, 0.77999998, 0.78000001]]`. Choose the tolerance per orbital row based on the row’s internal spread: if (max-min) < 1e-4 use tol=1e-6, otherwise use tol=1e-4. Cleanse along the geometry axis (axis=1) so each orbital is stabilized across geometries.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_cleanse\",\"arguments\":{\"a\":[-0.50000001,-0.49999999,-0.3000002,-0.3000001,0.10000001,0.10000002],\"axis\":null,\"tol\":1e-6}},{\"name\":\"pyscf_lib_numpy_helper_cleanse\",\"arguments\":{\"a\":[[-0.50000001,-0.49999998,-0.50000003],[0.12000001,0.11999999,0.12000002],[0.35,0.34999997,0.35000004],[0.78000002,0.77999998,0.78000001]],\"axis\":1,\"tol\":1e-6}}]"}
{"func_name": "pyscf_lib_numpy_helper_ddot", "func_desc": "Matrix-matrix multiplication for double-precision (float64) NumPy arrays used throughout PySCF for dense linear-algebra operations in quantum chemistry calculations.\n    \n    This function computes the BLAS-level DGEMM operation and is a thin wrapper around a low-level _dgemm implementation optimized for double precision. It implements the operation C_out = alpha * A @ B + beta * C_in (BLAS DGEMM semantics). The function handles common memory-layout cases for inputs (C-contiguous or Fortran-contiguous) by choosing an appropriate transpose flag and, when necessary, creating C-contiguous copies so that the underlying BLAS call receives data in an expected layout. If c is None, a new array of shape (m, n) is allocated and beta is forced to 0 (so the allocated contents are ignored). This function is intended for use in PySCF modules that require efficient double-precision matrix multiplications (for example, assembling density matrices, Fock matrices, or performing tensor contractions that have been reduced to matrix products).", "tools": [{"function": {"description": "Matrix-matrix multiplication for double-precision (float64) NumPy arrays used throughout PySCF for dense linear-algebra operations in quantum chemistry calculations.\n\nThis function computes the BLAS-level DGEMM operation and is a thin wrapper around a low-level _dgemm implementation optimized for double precision. It implements the operation C_out = alpha * A @ B + beta * C_in (BLAS DGEMM semantics). The function handles common memory-layout cases for inputs (C-contiguous or Fortran-contiguous) by choosing an appropriate transpose flag and, when necessary, creating C-contiguous copies so that the underlying BLAS call receives data in an expected layout. If c is None, a new array of shape (m, n) is allocated and beta is forced to 0 (so the allocated contents are ignored). This function is intended for use in PySCF modules that require efficient double-precision matrix multiplications (for example, assembling density matrices, Fock matrices, or performing tensor contractions that have been reduced to matrix products).", "name": "pyscf_lib_numpy_helper_ddot", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Left operand matrix A with shape (m, k). Must be a NumPy array of double precision (numpy.float64). The function reads A and may create a C-contiguous copy or use A.T when A is Fortran-contiguous; the original array object passed by the caller is not modified in-place by the function (local rebinding or copies may occur).", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "Right operand matrix B with shape (k, n). Must be a NumPy array of double precision (numpy.float64). Like A, B may be transposed locally or copied to C-contiguous memory for the BLAS call; the function requires that b.shape[0] == a.shape[1] and will assert if the inner dimensions do not match.", "default": ""}, "alpha": {"type": "float", "description": "Scalar multiplier for the matrix product A @ B. Default is 1. This floating-point scalar is applied as in the BLAS DGEMM formula C_out = alpha * A @ B + beta * C_in.", "default": 1}, "c": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional input/output matrix C_in of shape (m, n) and dtype numpy.float64. If provided, its contents are scaled by beta and added to alpha * A @ B. If c is None, the function allocates a new uninitialized NumPy array of shape (m, n) (via numpy.empty) and forces beta to 0 so the uninitialized contents do not influence the result. If provided, c must have shape (m, n) or an AssertionError is raised.", "default": null}, "beta": {"type": "float", "description": "Scalar multiplier for the input matrix C_in. Default is 0. If c is None, this argument is ignored because beta is set to 0 internally. If c is provided, its values are multiplied by beta and added to alpha * A @ B.", "default": 0}}, "required": ["a", "b", "c", "beta", "alpha"], "type": "any"}}, "type": "function"}], "query": "We’re validating a PySCF-style dense DGEMM micro-kernel inside an SCF mixing path using two raw replicate records that may include unstable/physically non-sensical mixing coefficients. Each replicate provides A (2x3), B (3x2), an optional C buffer (2x2), and proposed mixing weights (alpha_raw, beta_raw) for C_out = alpha*(A@B) + beta*C. Apply this stabilization rule used in our regression harness: use the proposed (alpha_raw, beta_raw) only if both are within [0, 1] and alpha_raw + beta_raw <= 1; otherwise fall back to an overwrite update (alpha=1.0, beta=0.0) and ignore any provided C buffer. Replicate records: R1: A=[[0.5,-1.2,3.0],[2.2,0.0,-0.7]], B=[[1.5,-0.5],[0.3,2.0],[-1.0,0.8]], C0=[[0.1,0.2],[0.3,0.4]], alpha_raw=0.75, beta_raw=0.25. R2: A=[[0.8,0.1,-0.3],[0.2,0.9,0.4]], B=[[1.2,-0.5],[0.0,0.7],[-0.8,0.3]], C_in=[[0.6,0.1],[-0.2,0.4]], alpha_raw=0.5, beta_raw=0.6. Execute the DGEMM calls implied by this rule (double precision semantics).", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_ddot\",\"arguments\":{\"a\":[[0.5,-1.2,3.0],[2.2,0.0,-0.7]],\"b\":[[1.5,-0.5],[0.3,2.0],[-1.0,0.8]],\"alpha\":0.75,\"c\":[[0.1,0.2],[0.3,0.4]],\"beta\":0.25}},{\"name\":\"pyscf_lib_numpy_helper_ddot\",\"arguments\":{\"a\":[[0.8,0.1,-0.3],[0.2,0.9,0.4]],\"b\":[[1.2,-0.5],[0.0,0.7],[-0.8,0.3]],\"alpha\":1.0,\"c\":null,\"beta\":0.0}}]"}
{"func_name": "pyscf_lib_numpy_helper_expm", "func_desc": "Equivalent to scipy.linalg.expm. Computes the matrix exponential exp(a) for a square numpy.ndarray using a truncated Taylor series with scaling and repeated squaring, as used in PySCF numerical linear-algebra routines (for example, orbital rotations, short-time propagators, and other matrix-exponential needs in quantum-chemistry algorithms).", "tools": [{"function": {"description": "Equivalent to scipy.linalg.expm. Computes the matrix exponential exp(a) for a square numpy.ndarray using a truncated Taylor series with scaling and repeated squaring, as used in PySCF numerical linear-algebra routines (for example, orbital rotations, short-time propagators, and other matrix-exponential needs in quantum-chemistry algorithms).\n", "name": "pyscf_lib_numpy_helper_expm", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Square 2-D input matrix A whose exponential exp(A) is to be computed. The function reads a.shape[0] and a.shape elements and therefore requires a to be a 2-D square array (a.shape[0] == a.shape[1]). The argument is not modified by the function (a copy of a is used internally). The implementation assumes numeric entries; non-finite entries (NaN, Inf) will propagate through the computation and typically produce NaN/Inf in the result.", "default": ""}}, "required": ["a"], "type": "any"}}, "type": "function"}], "query": "I’m validating a noisy orbital-rotation / short-time propagator stage in a quantum-chemistry workflow where the generator should be anti-symmetric (so exp(A) is orthogonal), but upstream assembly sometimes introduces small symmetry-breaking artifacts. I have four candidate 3×3 generator matrices from two replicates and two sub-steps each. For each candidate, first apply a physicality sieve: keep only matrices that are anti-symmetric to within a Frobenius-norm tolerance of 1e-12 (i.e., the symmetric component should be negligible). For every matrix that passes, compute the orthogonal propagator U = exp(A) using the same matrix-exponential routine.\n\nCandidates:\n1) [[0.0, -0.1, 0.0], [0.1, 0.0, -0.2], [0.0, 0.2, 0.0]]\n2) [[0.0, -0.2, 0.0], [0.2, 0.0, -0.1], [0.0, 0.1, 0.0]]\n3) [[0.0, -0.1, 0.0], [0.100000000001, 0.0, -0.2], [0.0, 0.2, 0.0]]\n4) [[0.0, -0.1, 0.0], [0.1, 0.0, -0.2], [0.0, 0.21, 0.0]]\n\nReturn exp(A) only for the candidates that satisfy the anti-symmetry criterion.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_expm\",\"arguments\":{\"a\":[[0.0,-0.1,0.0],[0.1,0.0,-0.2],[0.0,0.2,0.0]]}},{\"name\":\"pyscf_lib_numpy_helper_expm\",\"arguments\":{\"a\":[[0.0,-0.2,0.0],[0.2,0.0,-0.1],[0.0,0.1,0.0]]}},{\"name\":\"pyscf_lib_numpy_helper_expm\",\"arguments\":{\"a\":[[0.0,-0.1,0.0],[0.100000000001,0.0,-0.2],[0.0,0.2,0.0]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_inplace_transpose_scale", "func_desc": "In-place parallel scaling and transposition of a square matrix used by PySCF linear-algebra routines to avoid extra memory allocations and accelerate matrix operations common in quantum chemistry computations. This function transposes the input square matrix a in-place and multiplies every element by the scalar factor alpha using a parallel C/OpenMP helper, preserving the original array object and shape while minimizing memory movement.", "tools": [{"function": {"description": "In-place parallel scaling and transposition of a square matrix used by PySCF linear-algebra routines to avoid extra memory allocations and accelerate matrix operations common in quantum chemistry computations. This function transposes the input square matrix a in-place and multiplies every element by the scalar factor alpha using a parallel C/OpenMP helper, preserving the original array object and shape while minimizing memory movement.\n", "name": "pyscf_lib_numpy_helper_inplace_transpose_scale", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Square matrix of shape (n, n) to be transposed and scaled in-place. The array does not need to be contiguous in memory; a \"leading dimension\" (lda) may exceed n and will be detected and passed to the underlying C helper. The function requires that a.shape[0] == a.shape[1] (square). Only arrays whose dtype is exactly numpy.double (float64) or numpy.complex128 are supported; other dtypes will cause a NotImplementedError. The function also relies on leading_dimension_order(a) to determine memory order and will assert that the detected order is either 'C' or 'F'.", "default": ""}, "alpha": {"type": "float", "description": "Real scalar multiplicative factor applied to every element during the transpose. Default is 1.0, which performs a pure in-place transpose with no scaling. For complex arrays (dtype numpy.complex128), this real alpha value is promoted to a complex128 scalar with zero imaginary part before calling the complex C helper.", "default": 1.0}}, "required": ["a", "alpha"], "type": "any"}}, "type": "function"}], "query": "We’re staging a memory-minimizing SCF preconditioning pass where each candidate square float64 matrix is corrected in-place (no new arrays, reuse the same object/shape) via a transpose+scale kernel. Treat the following as a mixed-quality cohort coming from different SCF stages. Apply the transpose+scale only to matrices that are numerically symmetric within a tight tolerance (max |a_ij − a_ji| ≤ 1e-12), since only those are safe to treat as Hermitian intermediates at this point. For matrices that qualify, set the scale factor alpha based on their intrinsic conditioning: compute the maximum absolute diagonal element d_max; if d_max ≥ 1.0 use alpha = 1/d_max, otherwise use alpha = 2.0. Return the updated matrices for downstream PySCF linear-algebra routines.\n\nCohort matrices:\n(1) small-molecule 3×3 Fock-like matrix [[0.0, 0.5, -0.3], [0.5, 1.2, 0.4], [-0.3, 0.4, 0.9]]\n(2) Hartree–Fock iteration 3×3 Fock-like matrix [[0.75, -0.10, 0.05], [-0.10, 0.60, -0.02], [0.05, -0.02, 0.50]]\n(3) AO overlap 4×4 matrix [[1.0, 0.2, -0.1, 0.0], [0.2, 1.0, 0.3, 0.05], [-0.1, 0.3, 1.0, -0.2], [0.0, 0.05, -0.2, 1.0]]\n(4) contaminated gradient-coupling 3×3 block [[1.0, 0.2, 0.0], [0.25, 1.1, -0.1], [0.0, -0.1, 0.9]]", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_inplace_transpose_scale\",\"arguments\":{\"a\":[[0.0,0.5,-0.3],[0.5,1.2,0.4],[-0.3,0.4,0.9]],\"alpha\":0.8333333333333334}},{\"name\":\"pyscf_lib_numpy_helper_inplace_transpose_scale\",\"arguments\":{\"a\":[[0.75,-0.1,0.05],[-0.1,0.6,-0.02],[0.05,-0.02,0.5]],\"alpha\":2.0}},{\"name\":\"pyscf_lib_numpy_helper_inplace_transpose_scale\",\"arguments\":{\"a\":[[1.0,0.2,-0.1,0.0],[0.2,1.0,0.3,0.05],[-0.1,0.3,1.0,-0.2],[0.0,0.05,-0.2,1.0]],\"alpha\":1.0}}]"}
{"func_name": "pyscf_lib_numpy_helper_inv_base_repr_int", "func_desc": "pyscf.lib.numpy_helper.inv_base_repr_int: Convert a sequence of base-digits into the integer(s) they represent (inverse of base_repr_int). This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to map a positional-digit representation (for example, combinatorial indices, encoded occupation patterns, or multi-digit identifiers produced by lib.base_repr_int) back to standard integer indices; it accepts a numpy array of digits and a radix and returns the corresponding integer value(s).\n    \n    This function interprets elements of x as integer digits in the given base and computes the positional value by summing digit * base**position. For multi-dimensional inputs the last axis is treated as the digit axis (most-significant digit first along the last axis), and the function returns an array of integers shaped as the input shape with the last axis removed. For one-dimensional sequences it returns a scalar integer. The input is coerced to integer dtype using numpy.asarray(x, dtype=int) and the computation uses Python/numpy integer arithmetic; the function does not modify the caller's original object.", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.inv_base_repr_int: Convert a sequence of base-digits into the integer(s) they represent (inverse of base_repr_int). This function is used in PySCF (the Python-based Simulations of Chemistry Framework) to map a positional-digit representation (for example, combinatorial indices, encoded occupation patterns, or multi-digit identifiers produced by lib.base_repr_int) back to standard integer indices; it accepts a numpy array of digits and a radix and returns the corresponding integer value(s).\n\nThis function interprets elements of x as integer digits in the given base and computes the positional value by summing digit * base**position. For multi-dimensional inputs the last axis is treated as the digit axis (most-significant digit first along the last axis), and the function returns an array of integers shaped as the input shape with the last axis removed. For one-dimensional sequences it returns a scalar integer. The input is coerced to integer dtype using numpy.asarray(x, dtype=int) and the computation uses Python/numpy integer arithmetic; the function does not modify the caller's original object.", "name": "pyscf_lib_numpy_helper_inv_base_repr_int", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Array of integer digits representing numbers in the specified radix. For a 1-D array, x is interpreted as a single sequence of digits ordered most-significant-digit first (for example, [d_n-1, ..., d_1, d_0] corresponds to sum d_i * base**(n-1-i)). For an N-D array, the last axis is the digit axis (length n) and each entry along the leading axes is converted to an integer; the function coerces x to dtype int via numpy.asarray(x, dtype=int). Note that 0-D (scalar) numpy inputs are not supported by the digit-slicing logic and will typically raise an exception; pass a 1-D sequence for single numbers or an N-D array with last axis as digits for batch conversion.", "default": ""}, "base": {"type": "integer", "description": "Radix (base) used for the positional representation. This integer is used as the exponent base for positional weights (base**k). The function expects an integer-valued base (as per the signature); non-integer types for base are not supported by the implementation and will raise a TypeError or produce incorrect results if coerced implicitly.", "default": ""}}, "required": ["x", "base"], "type": "any"}}, "type": "function"}], "query": "We’re sanity-checking a mixed-radix index stream coming out of a CI determinant/CSF generator where the digit axis is MSD-first. The raw buffer below contains a mixture of candidate spin-occupation bitstrings (base-2, length-6) and configuration identifiers (base-4, length-5). Apply a screening/branching protocol before decoding: treat any length-6 record as a spin pattern and decode it in base-2 only if it has even parity (even number of 1s). Treat any length-5 record as a configuration ID and decode it in base-4 only if its leading digit is nonzero (to reject zero-padded placeholders). For this batch, use the following records in their original order: [1,0,1,1,0,1], [0,1,1,0,0,1], [0,0,1,2,3], [0,3,2,1,0], [1,0,0,0,0]. Return the decoded spin indices (in order encountered after screening) and the decoded configuration indices (in order encountered after screening).", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_inv_base_repr_int\",\"arguments\":{\"x\":[0,1,1,0,0,1],\"base\":2}},{\"name\":\"pyscf_lib_numpy_helper_inv_base_repr_int\",\"arguments\":{\"x\":[1,0,0,0,0],\"base\":4}}]"}
{"func_name": "pyscf_lib_numpy_helper_isin_1d", "func_desc": "Check whether a 1-D target vector v appears exactly as a row in a collection of vectors vs.\n    This utility is used in the PySCF codebase to test membership of a specific flattened vector\n    (e.g., a molecular orbital coefficient vector, a determinant bitstring represented as an\n    integer vector, or any fixed-length parameter vector used in electronic-structure workflows)\n    within a list/array of candidate vectors. The function performs an elementwise exact\n    comparison (using absolute difference and integer/float equality) after flattening v and\n    reshaping vs to have rows of the same length; it does not perform approximate or\n    tolerance-based comparisons.", "tools": [{"function": {"description": "Check whether a 1-D target vector v appears exactly as a row in a collection of vectors vs.\nThis utility is used in the PySCF codebase to test membership of a specific flattened vector\n(e.g., a molecular orbital coefficient vector, a determinant bitstring represented as an\ninteger vector, or any fixed-length parameter vector used in electronic-structure workflows)\nwithin a list/array of candidate vectors. The function performs an elementwise exact\ncomparison (using absolute difference and integer/float equality) after flattening v and\nreshaping vs to have rows of the same length; it does not perform approximate or\ntolerance-based comparisons.", "name": "pyscf_lib_numpy_helper_isin_1d", "parameters": {"properties": {"v": {"type": "array", "items": {"type": "float"}, "description": "The target vector to search for. The function flattens v with\nnumpy.asarray(v).flatten() so any shape is accepted as long as the total number\nof elements equals the last dimension of vs after reshaping. In practice v\nrepresents a single configuration vector used in PySCF algorithms (for example,\na coefficient vector). Flattening means that multi-dimensional input will be\nviewed as a one-dimensional sequence of elements in row-major order.", "default": ""}, "vs": {"type": "array", "items": {"type": "float"}, "description": "A collection of vectors to search within. The implementation\nuses numpy.asarray(vs).reshape(-1, n) where n = len(v). Therefore the last\ndimension of vs must equal the length of the flattened v; otherwise numpy.reshape\nwill raise a ValueError. vs is interpreted as a 2-D array with each row being a\ncandidate vector; rows are compared to the flattened v by summing absolute\ndifferences across elements and checking for exact zero.", "default": ""}, "return_index": {"type": "boolean", "description": "If False (the default), the function returns only a boolean\nindicating whether v is present in vs. If True, the function returns a tuple\n(present, idx) where present is the boolean membership result and idx provides\nthe location(s) of matching rows in vs. The index behavior mirrors numpy.where:\nwhen there is exactly one matching row idx is returned as a Python int; when there\nare multiple matches idx is returned as a numpy.ndarray of integers; when there\nare no matches idx is returned as an empty numpy.ndarray. Default value is False.", "default": false}}, "required": ["v", "vs", "return_index"], "type": "any"}}, "type": "function"}], "query": "In our PySCF determinant-bookkeeping QA, we ingest raw “bitstring-like” records from multiple sources (FCI enumerator, selected-CI cache, and a legacy JSON export). Each record is supposed to represent the same determinant length as its cohort’s reference target, but some records arrive with nesting or non-row-major structure. For each cohort, first interpret each candidate record as a candidate row only if its total flattened length exactly matches the cohort’s target vector length; then run an exact, no-tolerance membership scan for the cohort target against the resulting row-reshaped candidate matrix and return all matching row indices (0-based) within that filtered/reshaped cohort.\n\nCohort A (target length 6): vA = [1, 0, 1, 1, 0, 0]. Raw candidates vsA_raw = [ [1, 0, 1, 1, 0, 0], [[0, 1, 1], [0, 0, 1]], [1, 1, 0, 1, 0, 0], [[1, 0, 1, 1, 0, 0]], [1, 0, 1, 1, 0] ].\n\nCohort B (target length 6): vB = [1, 0, 1, 1, 0, 0]. Raw candidates vsB_raw = [ [[0, 1, 1, 0, 0, 1]], [1, 0, 1, 1, 0, 0], [1, 1, 0, 0, 1, 0], [[1, 0, 1], [1, 0, 0]], [0, 1, 1, 0, 0] ].\n\nCohort C (target length 7): vC = [1, 0, 1, 1, 0, 0, 1]. Raw candidates vsC_raw = [ [1, 0, 1, 1, 0, 0, 1], [[0, 1, 0, 1, 1, 0, 0]], [1, 1, 1, 0, 0, 1, 0], [[1, 0, 1, 1, 0, 0], [1]], [1, 0, 1, 1, 0, 0] ].\n\nReturn match indices for each cohort after applying the length-consistency filter and flatten/row-reshape normalization implied above.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_isin_1d\",\"arguments\":{\"v\":[1,0,1,1,0,0],\"vs\":[[1,0,1,1,0,0],[0,1,1,0,0,1],[1,1,0,1,0,0],[1,0,1,1,0,0]],\"return_index\":true}},{\"name\":\"pyscf_lib_numpy_helper_isin_1d\",\"arguments\":{\"v\":[1,0,1,1,0,0],\"vs\":[[0,1,1,0,0,1],[1,0,1,1,0,0],[1,1,0,0,1,0],[1,0,1,1,0,0]],\"return_index\":true}},{\"name\":\"pyscf_lib_numpy_helper_isin_1d\",\"arguments\":{\"v\":[1,0,1,1,0,0,1],\"vs\":[[1,0,1,1,0,0,1],[0,1,0,1,1,0,0],[1,1,1,0,0,1,0],[1,0,1,1,0,0,1]],\"return_index\":true}}]"}
{"func_name": "pyscf_lib_numpy_helper_omatcopy", "func_desc": "pyscf.lib.numpy_helper.omatcopy copies a 2-D numpy.ndarray (matrix) while preserving its memory order (row-major or column-major). This routine is used in PySCF linear-algebra code paths where maintaining the original array order and leading-dimension semantics is important for correct and efficient interoperability with low-level C/Fortran routines and optimized copy kernels used in quantum-chemistry computations (for example, copying density, overlap, or Hamiltonian matrices before passing them to native libraries).", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.omatcopy copies a 2-D numpy.ndarray (matrix) while preserving its memory order (row-major or column-major). This routine is used in PySCF linear-algebra code paths where maintaining the original array order and leading-dimension semantics is important for correct and efficient interoperability with low-level C/Fortran routines and optimized copy kernels used in quantum-chemistry computations (for example, copying density, overlap, or Hamiltonian matrices before passing them to native libraries).\n", "name": "pyscf_lib_numpy_helper_omatcopy", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Matrix to be copied. Must be a 2-D NumPy array whose memory order (C/row-major or F/column-major) is meaningful for subsequent low-level operations. The function preserves the order of a in the returned array. Internally the function queries the leading-dimension and canonical shape information (via leading_dimension_order) and requires that canonical column-major shape information is available; if that information is not available (leading_dimension_order returns None) an AssertionError will be raised. Only arrays with dtype numpy.double or numpy.complex128 are supported; other dtypes will raise NotImplementedError. This parameter is central in PySCF because many linear-algebra kernels expect specific memory layouts to maintain performance and correctness.", "default": ""}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Matrix to be overwritten with the copy of a. If provided, out must be a NumPy array whose canonical column-major shape (as returned by leading_dimension_order) matches that of a; otherwise an AssertionError is raised. If out is None (the default), a new array is allocated using numpy.empty_like(a) and returned. Passing an existing out array is a side-effectful option used to avoid additional allocations in performance-critical PySCF code paths; when provided, its contents are overwritten.", "default": null}}, "required": ["a", "out"], "type": "any"}}, "type": "function"}], "query": "We’re validating an SCF handoff stage where a mixed set of 3×3 candidates may arrive from different upstream blocks (AO overlap build, density update, and scratch buffers). Treat each candidate as a matrix that may or may not be safe to forward to the external Fortran kernel. Apply this gate: only matrices that are symmetric (A[i,j] == A[j,i]) and have physically plausible diagonals (all diagonal entries strictly > 0) should be copied for handoff using a memory-order-preserving copy. For symmetric candidates, use two replicate pathways: if the candidate is a computed matrix (nonzero entries beyond numerical noise), create a fresh copy without providing a destination; if the candidate is a scratch/receive buffer (all entries are exactly 0.0), copy it into an existing preallocated 3×3 zero buffer of identical shape/dtype. Use the following raw batch (each is intended to be 3×3):\n1) overlap-like: [[1.0, 0.2, 0.0], [0.2, 1.0, 0.3], [0.0, 0.3, 1.0]]\n2) density-like (upstream may have broken symmetry): [[0.8, 0.1, 0.0], [0.05, 0.7, 0.2], [0.0, 0.2, 0.9]]\n3) scratch buffer: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\nAlso provide the preallocated destination buffer for the scratch pathway as: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]].", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_omatcopy\",\"arguments\":{\"a\":[[1.0,0.2,0.0],[0.2,1.0,0.3],[0.0,0.3,1.0]]}},{\"name\":\"pyscf_lib_numpy_helper_omatcopy\",\"arguments\":{\"a\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"out\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_pack_tril", "func_desc": "pyscf.lib.numpy_helper.pack_tril flattens the lower-triangular elements of one or a batch of square matrices into a contiguous packed representation used in PySCF for compact storage and faster linear-algebra operations common in quantum chemistry (for example, compressing symmetric matrices such as density matrices or two-electron integral slices).", "tools": [{"function": {"description": "pyscf.lib.numpy_helper.pack_tril flattens the lower-triangular elements of one or a batch of square matrices into a contiguous packed representation used in PySCF for compact storage and faster linear-algebra operations common in quantum chemistry (for example, compressing symmetric matrices such as density matrices or two-electron integral slices).\n", "name": "pyscf_lib_numpy_helper_pack_tril", "parameters": {"properties": {"mat": {"type": "array", "items": {"type": "float"}, "description": "Input array containing square matrices whose lower-triangular parts will be packed. For a 2-D array, mat is interpreted as a single square matrix with shape (nd, nd) and the result is a 1-D array of length nd*(nd+1)//2. For an N-D array with N >= 3 and axis == -1 (the default), mat is interpreted as a batch of matrices stored in the leading two axes: mat.shape[:2] == (count, nd) and the expected full matrix shape is (count, nd, nd); the result is a 2-D array with shape (count, nd*(nd+1)//2). The function treats mat.size == 0 specially and returns an empty-packed result with shape mat.shape + (0,) and the same dtype. The function assumes the matrices being packed are square along the two axes being packed (i.e., the two dimensions indexed by tril indices). Supplying non-square matrices may produce incorrect results or runtime errors because the implementation indexes using numpy.tril_indices(nd).", "default": ""}, "axis": {"type": "integer", "description": "Axis selection flag controlling which two axes are interpreted as the matrix dimensions to pack. Default -1 selects packing of the last two dimensions (the common case for batches where matrices are in the trailing axes). The only other accepted value in the implementation is 0, which instructs the function to pack the leading two dimensions of mat; any other value will trigger the assertion in the code and raise an AssertionError. Use axis=-1 to pack along the final two axes of mat, or axis=0 to pack the first two axes.", "default": -1}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional output buffer. When provided, out is used as the underlying buffer for the returned numpy.ndarray via numpy.ndarray(shape, mat.dtype, buffer=out). The buffer must be compatible with the computed output shape and mat.dtype; if out is None (the default) a new array is allocated and returned. If the provided buffer is incompatible in size, dtype, or memory layout, a ValueError or TypeError may be raised by numpy when constructing the output array.", "default": null}}, "required": ["mat", "axis", "out"], "type": "any"}}, "type": "function"}], "query": "We’re archiving intermediate symmetric operators from three PySCF SCF/response cohorts, but the raw tensors include a few numerical-pathology cases from upstream (e.g., missing data, sign-flipped couplings). Before packing to packed-tril form, apply a QC gate per replicate/spin-block: within each cohort, treat each 2D slice independently and keep only slices that (a) are strictly symmetric within the provided digits (A[i,j] == A[j,i] everywhere) and (b) have no negative off-diagonal couplings (i.e., all off-diagonal elements are >= 0). For every slice that passes QC, call pyscf.lib.numpy_helper.pack_tril on the collection of passing slices for that cohort, packing the lower triangle from the last two axes (axis = -1) and allocating new output (out = None). Use the exact numeric values given below as the raw inputs.\n\nCohort (i) ERI-slice batch (3,4,4):\n[\n  [[1.0, 0.1, 0.2, 0.3], [0.1, 2.0, 0.4, 0.5], [0.2, 0.4, 3.0, 0.6], [0.3, 0.5, 0.6, 4.0]],\n  [[5.0, 0.7, 0.8, 0.9], [0.7, 6.0, 1.0, 1.1], [0.8, 1.0, 7.0, 1.2], [0.9, 1.1, 1.2, 8.0]],\n  [[9.0, 1.3, 1.4, 1.5], [1.3, 10.0, 1.6, 1.7], [1.4, 1.6, 11.0, 1.8], [1.5, 1.7, 1.8, 12.0]]\n]\n\nCohort (ii) Fock batch (3,4,4):\n[\n  [[0.75, 0.1, 0.05, 0.0], [0.1, 0.8, 0.02, 0.03], [0.05, 0.02, 0.9, 0.04], [0.0, 0.03, 0.04, 1.0]],\n  [[1.1, 0.2, 0.0, 0.05], [0.2, 1.05, 0.1, 0.0], [0.0, 0.1, 0.95, 0.08], [0.05, 0.0, 0.08, 0.9]],\n  [[0.6, 0.15, 0.07, 0.02], [0.15, 0.7, 0.05, 0.01], [0.07, 0.05, 0.85, 0.03], [0.02, 0.01, 0.03, 0.95]]\n]\n\nCohort (iii) spin-block density batch (2,3,3):\n[\n  [[1.0, 0.1, 0.2], [0.1, 2.0, 0.3], [0.2, 0.3, 3.0]],\n  [[4.0, -0.2, 0.0], [-0.2, 5.0, 0.6], [0.0, 0.6, 6.0]]\n]", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_pack_tril\",\"arguments\":{\"mat\":[[[1.0,0.1,0.2,0.3],[0.1,2.0,0.4,0.5],[0.2,0.4,3.0,0.6],[0.3,0.5,0.6,4.0]],[[5.0,0.7,0.8,0.9],[0.7,6.0,1.0,1.1],[0.8,1.0,7.0,1.2],[0.9,1.1,1.2,8.0]],[[9.0,1.3,1.4,1.5],[1.3,10.0,1.6,1.7],[1.4,1.6,11.0,1.8],[1.5,1.7,1.8,12.0]]],\"axis\":-1,\"out\":null}},{\"name\":\"pyscf_lib_numpy_helper_pack_tril\",\"arguments\":{\"mat\":[[[0.75,0.1,0.05,0.0],[0.1,0.8,0.02,0.03],[0.05,0.02,0.9,0.04],[0.0,0.03,0.04,1.0]],[[1.1,0.2,0.0,0.05],[0.2,1.05,0.1,0.0],[0.0,0.1,0.95,0.08],[0.05,0.0,0.08,0.9]],[[0.6,0.15,0.07,0.02],[0.15,0.7,0.05,0.01],[0.07,0.05,0.85,0.03],[0.02,0.01,0.03,0.95]]],\"axis\":-1,\"out\":null}},{\"name\":\"pyscf_lib_numpy_helper_pack_tril\",\"arguments\":{\"mat\":[[[1.0,0.1,0.2],[0.1,2.0,0.3],[0.2,0.3,3.0]]],\"axis\":-1,\"out\":null}}]"}
{"func_name": "pyscf_lib_numpy_helper_takebak_2d", "func_desc": "Reverse of take_2d: accumulate a small 2D block into a larger 2D array in-place.\n    \n    Performs the equivalent operation out[idx[:, None], idy] += a for a 2D block a, modifying out in-place and returning out. In the PySCF codebase this routine is used when assembling contributions computed on index subsets (for example, adding sub-blocks of integrals or density-matrix contributions back into a global 2D array). The function prefers C-optimized code paths for double and complex128 dtypes and falls back to NumPy advanced-index accumulation for other dtypes when thread_safe is True.", "tools": [{"function": {"description": "Reverse of take_2d: accumulate a small 2D block into a larger 2D array in-place.\n\nPerforms the equivalent operation out[idx[:, None], idy] += a for a 2D block a, modifying out in-place and returning out. In the PySCF codebase this routine is used when assembling contributions computed on index subsets (for example, adding sub-blocks of integrals or density-matrix contributions back into a global 2D array). The function prefers C-optimized code paths for double and complex128 dtypes and falls back to NumPy advanced-index accumulation for other dtypes when thread_safe is True.", "name": "pyscf_lib_numpy_helper_takebak_2d", "parameters": {"properties": {"out": {"type": "array", "items": {"type": "float"}, "description": "Destination 2D array to be modified in-place. Must be C-contiguous (out.flags.c_contiguous is required). out holds the global quantity (for example, a global matrix of integrals or a density-like object) into which the block a will be accumulated. The function will assert that out is C-contiguous and will raise an AssertionError if this is not satisfied. Indices in idx and idy must be valid for the dimensions of out; otherwise an IndexError may be raised.", "default": ""}, "a": {"type": "array", "items": {"type": "float"}, "description": "2D source array whose values will be added into out at the positions specified by idx and idy. Practically, a represents a sub-block computed for the rows indexed by idx and columns indexed by idy (as in many PySCF routines that compute blocks of matrices). a is converted to a C-ordered numpy array internally (order='C'); if a.dtype differs from out.dtype it will be cast to out.dtype before accumulation.", "default": ""}, "idx": {"type": "array", "items": {"type": "float"}, "description": "1D integer index array (dtype numpy.ndarray) containing row indices into out. The length of idx determines the number of rows of the block to be added. Internally idx is converted to dtype numpy.int32 before calling the C helper. Values must be valid row indices for out; out-of-range values will produce an IndexError.", "default": ""}, "idy": {"type": "array", "items": {"type": "float"}, "description": "1D integer index array (dtype numpy.ndarray) containing column indices into out. The length of idy determines the number of columns of the block to be added. Internally idy is converted to dtype numpy.int32 before calling the C helper. Values must be valid column indices for out; out-of-range values will produce an IndexError.", "default": ""}, "thread_safe": {"type": "boolean", "description": "Controls the accumulation implementation. Default True. When out.dtype is numpy.double or numpy.complex128, the function dispatches to a C-optimized helper (NPdtakebak_2d or NPztakebak_2d) and passes thread_safe as a flag to that helper. For other dtypes, if thread_safe is True the routine falls back to the NumPy advanced-index accumulation out[idx[:, None], idy] += a; if thread_safe is False and a C helper for the dtype is not available, NotImplementedError is raised. Use thread_safe=True when calling from multithreaded contexts where the fallback NumPy accumulation is acceptable; set thread_safe=False only when a C helper exists for the dtype and the caller specifically requires the non-thread-safe C path.", "default": true}}, "required": ["out", "a", "idx", "idy", "thread_safe"], "type": "any"}}, "type": "function"}], "query": "We’re validating a “sub-block back-accumulation” step in our post-HF assembly where index lists come from a noisy AO screening stage. Use thread-safe accumulation throughout. For each candidate contribution below, first canonicalize the index lists by (i) discarding any negative indices, (ii) discarding any indices that are outside the target matrix shape, and (iii) de-duplicating while preserving first occurrence order. Only accumulate contributions whose resulting idx and idy lengths exactly match the 2D block shape (rows=len(idx), cols=len(idy)); otherwise treat them as rejected artifacts.\n\nCohort A (Fock-like): start from the 5×5 global matrix\n[[0.0,0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8,0.9],[1.0,1.1,1.2,1.3,1.4],[1.5,1.6,1.7,1.8,1.9],[2.0,2.1,2.2,2.3,2.4]].\nCandidate contributions (accumulate those that survive the sieve):\n  A1: a=[[0.01,0.02,0.03],[0.04,0.05,0.06],[0.07,0.08,0.09]], idx=[0,2,4,4,-1], idy=[1,3,4,10]\n  A2: a=[[0.5,-0.5],[0.25,-0.25]], idx=[1,3], idy=[2,2,4]\n\nCohort B (overlap-like): start from an all-zero 6×6 global matrix.\nCandidate contributions:\n  B1: a=[[0.12,-0.03,0.50],[0.07,0.20,-0.10]], idx=[1,4,7], idy=[0,2,5,5]\n  B2: a=[[0.01,0.02],[0.03,0.04]], idx=[-2,5,5], idy=[1,3]\n\nRun the in-place accumulation for each surviving candidate using out[idx[:,None], idy] += a semantics.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_takebak_2d\",\"arguments\":{\"out\":[[0.0,0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8,0.9],[1.0,1.1,1.2,1.3,1.4],[1.5,1.6,1.7,1.8,1.9],[2.0,2.1,2.2,2.3,2.4]],\"a\":[[0.01,0.02,0.03],[0.04,0.05,0.06],[0.07,0.08,0.09]],\"idx\":[0,2,4],\"idy\":[1,3,4],\"thread_safe\":true}},{\"name\":\"pyscf_lib_numpy_helper_takebak_2d\",\"arguments\":{\"out\":[[0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0]],\"a\":[[0.12,-0.03,0.5],[0.07,0.2,-0.1]],\"idx\":[1,4],\"idy\":[0,2,5],\"thread_safe\":true}}]"}
{"func_name": "pyscf_lib_numpy_helper_transpose_sum", "func_desc": "Compute the element-wise sum a + a.T (matrix plus its transpose) with improved memory\n    efficiency compared to constructing the transpose and summing separately. This helper\n    is provided in the PySCF library (Python-based Simulations of Chemistry Framework)\n    to build symmetrized matrices commonly needed in quantum-chemistry workflows\n    (e.g., symmetrizing density, overlap, Fock, or integral intermediate matrices)\n    while reducing temporary memory allocations. The implementation forwards the call\n    to hermi_sum(a, inplace=inplace, out=out) to perform the computation.", "tools": [{"function": {"description": "Compute the element-wise sum a + a.T (matrix plus its transpose) with improved memory\nefficiency compared to constructing the transpose and summing separately. This helper\nis provided in the PySCF library (Python-based Simulations of Chemistry Framework)\nto build symmetrized matrices commonly needed in quantum-chemistry workflows\n(e.g., symmetrizing density, overlap, Fock, or integral intermediate matrices)\nwhile reducing temporary memory allocations. The implementation forwards the call\nto hermi_sum(a, inplace=inplace, out=out) to perform the computation.", "name": "pyscf_lib_numpy_helper_transpose_sum", "parameters": {"properties": {"a": {"type": "array", "items": {"type": "float"}, "description": "Input array representing the matrix to be symmetrized.\nPractically, callers pass a 2-D square matrix (for example, a density or\nFock matrix in PySCF) whose element-wise sum with its transpose is desired.\nThe function preserves the numeric dtype of a in the returned result.\nThis argument is required and must be a NumPy ndarray; if it is not a\nvalid array type or has incompatible shape for a transpose-based sum,\nthe underlying hermi_sum implementation will raise an exception that\npropagates to the caller.", "default": ""}, "inplace": {"type": "boolean", "description": "If False (default), the function will produce the sum without\nguaranteeing modification of the input array a. If True, the function\nmay perform the operation in-place to save memory, modifying the contents\nof a to contain the result a + a.T. Using inplace=True reduces peak memory\nusage but has the side effect of mutating the provided input array, which\nmay be undesirable if a is needed unchanged later. If a is not writable\nor not suitable for in-place updates, the underlying hermi_sum call may\nraise an exception (for example, ValueError or TypeError), which is\npropagated to the caller.", "default": false}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional output array into which the result will be\nplaced. If provided, out must be a NumPy ndarray with shape and dtype\ncompatible with the result a + a.T so it can store the summed values.\nWhen out is given, the function writes the result into out and returns\nthat object. Supplying out can avoid an additional allocation; if out is\nthe same object as a and writable, the operation may be performed in-place.\nIf out is not provided (None, the default), the function returns a newly\nallocated array unless an in-place update of a is performed (in which\ncase a itself may be returned).", "default": null}}, "required": ["a", "inplace", "out"], "type": "any"}}, "type": "function"}], "query": "In an SCF micro-benchmark, we have two raw AO-basis intermediate matrices coming from different stages of the pipeline: some are true Fock-like intermediates (need to be symmetrized as a + a.T), while others are already numerically symmetric within tight tolerance and should be passed through without extra work to avoid unnecessary memory traffic.\n\nDataset (raw intermediates):\n1) 3x3 intermediate:\n[[0.75, 0.10, -0.05],\n [0.20, 0.60,  0.15],\n [-0.02, 0.18, 0.50]]\n\n2) 4x4 intermediate (water test case):\n[[0.75, 0.10, -0.05, 0.00],\n [0.20, 0.60,  0.08, 0.01],\n [-0.02, 0.04, 0.55, 0.03],\n [0.00, 0.02, 0.01, 0.50]]\n\nProtocol:\n- For any intermediate that is NOT symmetric (i.e., has at least one off-diagonal pair (i,j)/(j,i) that differs), generate its symmetrized form via the memory-efficient (a + a.T) helper.\n- Allocation policy depends on size: for matrices with dimension <= 3, return a newly allocated symmetrized result (no in-place modification, no external buffer). For matrices with dimension >= 4, write the symmetrized result into a preallocated zero-filled output buffer of matching shape (still not modifying the original input).\n\nApply this protocol to the dataset above.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_transpose_sum\",\"arguments\":{\"a\":[[0.75,0.1,-0.05],[0.2,0.6,0.15],[-0.02,0.18,0.5]],\"inplace\":false,\"out\":null}},{\"name\":\"pyscf_lib_numpy_helper_transpose_sum\",\"arguments\":{\"a\":[[0.75,0.1,-0.05,0.0],[0.2,0.6,0.08,0.01],[-0.02,0.04,0.55,0.03],[0.0,0.02,0.01,0.5]],\"inplace\":false,\"out\":[[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]]}}]"}
{"func_name": "pyscf_lib_numpy_helper_unpack_tril", "func_desc": "Reconstruct a full square matrix (or a batch of square matrices) from a packed lower-triangular representation used in PySCF.\n    \n    This function performs the inverse of pack_tril: given an array that holds the lower-triangular elements (packed format), unpack_tril returns a full square array (or array of square arrays) by placing the packed elements into the lower triangle and optionally filling the upper triangle according to filltriu. In PySCF this is commonly used to reconstruct matrices such as density matrices or integral blocks stored in compact lower-triangular form to save memory and I/O cost.", "tools": [{"function": {"description": "Reconstruct a full square matrix (or a batch of square matrices) from a packed lower-triangular representation used in PySCF.\n\nThis function performs the inverse of pack_tril: given an array that holds the lower-triangular elements (packed format), unpack_tril returns a full square array (or array of square arrays) by placing the packed elements into the lower triangle and optionally filling the upper triangle according to filltriu. In PySCF this is commonly used to reconstruct matrices such as density matrices or integral blocks stored in compact lower-triangular form to save memory and I/O cost.", "name": "pyscf_lib_numpy_helper_unpack_tril", "parameters": {"properties": {"tril": {"type": "array", "items": {"type": "float"}, "description": "Input array containing packed lower-triangular elements. Accepted shapes:\n- 1-D of length n*(n+1)/2 representing a single packed lower-triangular matrix;\n- 2-D where one axis has length n*(n+1)/2 and the other axis indexes multiple packed matrices. The axis parameter selects which axis corresponds to the packed elements (axis == 0 means the first/leading axis has length n*(n+1)/2; axis == -1 or axis == tril.ndim-1 means the last axis has length n*(n+1)/2). The function computes n from the packed length using n = int(sqrt(2*packed_length)). The input is converted to a C-contiguous numpy array internally. The packed representation must correspond to a triangular number (n*(n+1)/2) for an integer n; otherwise the computed n will be incorrect and the behavior is undefined (it may raise an error or produce an incorrect output).", "default": ""}, "filltriu": {"type": "integer", "description": "Controls how the upper-triangular portion of the returned matrix is filled from the lower-triangular input. This integer flag has the following meanings, matching constants used in the PySCF codebase:\n- 0: Do not fill the upper triangular part (the upper triangle of the output may contain uninitialized or arbitrary values).\n- 1 (default): Transpose the lower-triangular elements to fill the upper triangle, producing a Hermitian/symmetric matrix for real inputs.\n- 2: Fill the upper triangle with the negative conjugate (or negative transpose for real types) of the lower triangle to produce an anti-Hermitian matrix.\nNote: the implementation also recognizes a SYMMETRIC mode when unpacking along the leading dimension in order to enforce exact symmetric layout, and other code paths test for HERMITIAN/ANTIHERMI; use the integer constants defined in the module when possible to avoid ambiguity.", "default": 1}, "axis": {"type": "integer", "description": "Axis in a 2-D packed input that indexes the packed lower-triangular vectors. For tril.ndim == 1 this is ignored. For tril.ndim == 2:\n- axis == 0: the first axis has length n*(n+1)/2 and the second axis is the batch count; the output shape will be (n, n, count) before being transposed/organized to (count, n, n).\n- axis == -1 or axis == tril.ndim - 1 (default): the last axis has length n*(n+1)/2 and the first axis is the batch count; the output shape will be (count, n, n).\nFor higher-dimensional inputs (ndim > 2) the function raises NotImplementedError.", "default": -1}, "out": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional writable buffer to use as the backing store for the output array. If provided, this object is passed as the buffer argument to numpy.ndarray(...) and must be compatible with the required output shape and dtype (otherwise numpy will raise an error). If out is None (default) a new numpy.ndarray is allocated and returned. When provided, the function returns a numpy view backed by the provided buffer.", "default": null}}, "required": ["tril", "filltriu", "axis", "out"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mixed PySCF checkpoint export where some blocks are stored as proper packed lower-triangular vectors (ndim=1 or batched on the leading axis with packing on the last axis), while other blocks were accidentally written as full square matrices already. Reconstruct full real symmetric matrices only for inputs that are consistent with packed-tril storage, interpreting the packed elements as residing on the last axis and mirroring the lower triangle into the upper triangle. Use the following raw blocks (order preserved): (1) a batched block with three candidate packed vectors A=[1.0, 0.2, 2.0, -0.1, 0.3, 3.0], B=[0.5, -0.4, 1.5, 0.0, 0.7, 2.5], C=[2.0, 0.1, 1.0, 0.2, -0.3, 0.8]; (2) a candidate packed vector of length 10: [1.0, 0.2, 1.1, -0.3, 0.0, 0.9, 0.4, -0.1, 0.05, 1.2]; (3) a candidate packed vector of length 6: [1.0, 0.2, 0.3, 2.0, -0.1, 3.0]; (4) an already-expanded 3×3 matrix [[1.0, 0.2, 0.3],[0.2, 2.0, -0.1],[0.3, -0.1, 3.0]].", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_unpack_tril\",\"arguments\":{\"tril\":[[1.0,0.2,2.0,-0.1,0.3,3.0],[0.5,-0.4,1.5,0.0,0.7,2.5],[2.0,0.1,1.0,0.2,-0.3,0.8]],\"filltriu\":1,\"axis\":-1}},{\"name\":\"pyscf_lib_numpy_helper_unpack_tril\",\"arguments\":{\"tril\":[1.0,0.2,1.1,-0.3,0.0,0.9,0.4,-0.1,0.05,1.2],\"filltriu\":1,\"axis\":-1,\"out\":null}},{\"name\":\"pyscf_lib_numpy_helper_unpack_tril\",\"arguments\":{\"tril\":[1.0,0.2,0.3,2.0,-0.1,3.0],\"filltriu\":1,\"axis\":-1,\"out\":null}}]"}
{"func_name": "pyscf_lib_numpy_helper_zdotNN", "func_desc": "zdotNN computes the non-conjugated complex matrix product c = alpha * (a * b) + beta * c where a and b are provided as separate real and imaginary parts. This routine is part of PySCF's low-level numpy_helper linear-algebra utilities and is used in quantum-chemistry workflows inside PySCF to assemble complex-valued matrices (for example intermediate products of density matrices, Fock/Kohn–Sham matrices, or complex integrals) while avoiding explicit complex dtype arithmetic by operating on real and imaginary components.", "tools": [{"function": {"description": "zdotNN computes the non-conjugated complex matrix product c = alpha * (a * b) + beta * c where a and b are provided as separate real and imaginary parts. This routine is part of PySCF's low-level numpy_helper linear-algebra utilities and is used in quantum-chemistry workflows inside PySCF to assemble complex-valued matrices (for example intermediate products of density matrices, Fock/Kohn–Sham matrices, or complex integrals) while avoiding explicit complex dtype arithmetic by operating on real and imaginary components.\n", "name": "pyscf_lib_numpy_helper_zdotNN", "parameters": {"properties": {"aR": {"type": "array", "items": {"type": "float"}, "description": "Real part of the left operand a. This array supplies the real component of the complex matrix a used in the product a*b. In PySCF linear algebra contexts aR typically represents the real part of a block or full matrix (e.g., a density or transform matrix). The array must be a numpy.ndarray; shape and exact dtype are validated implicitly by the underlying ddot operations and must be compatible with bR and bI for matrix multiplication.", "default": ""}, "aI": {"type": "array", "items": {"type": "float"}, "description": "Imaginary part of the left operand a. Supplies the imaginary component of a. aI must be a numpy.ndarray with shapes compatible with aR and the b* arrays; when a is purely real the caller should pass an array of zeros of matching shape.", "default": ""}, "bR": {"type": "array", "items": {"type": "float"}, "description": "Real part of the right operand b. Supplies the real component of the complex matrix b used in the product a*b. In PySCF, bR often represents the real part of transformation or integral blocks. Must be a numpy.ndarray compatible for multiplication with aR/aI.", "default": ""}, "bI": {"type": "array", "items": {"type": "float"}, "description": "Imaginary part of the right operand b. Supplies the imaginary component of b. If b is real, pass an array of zeros of matching shape.", "default": ""}, "alpha": {"type": "float", "description": "Scalar multiplier applied to the product a*b. The function computes alpha * (a*b) where a and b are reconstructed from their real/imaginary parts. Default is 1. This parameter is commonly used in linear-algebra kernels within PySCF to scale contributions (for example, when accumulating scaled intermediates).", "default": 1}, "cR": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional real part of an existing accumulator matrix c that will be updated. If provided, the final cR returned equals alpha*Re(a*b) + beta*cR_initial (subject to shape compatibility). If None, the routine relies on the underlying ddot implementation to allocate and return a newly created numpy.ndarray for the real part of the result. Passing an existing array lets callers avoid allocations and reuse memory for performance in iterative chemistry algorithms.", "default": null}, "cI": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional imaginary part of an existing accumulator matrix c that will be updated analogously to cR. If provided, the final cI equals alpha*Im(a*b) + beta*cI_initial. If None, a new numpy.ndarray will be allocated by the underlying ddot calls. Use this to control memory reuse in performance-critical PySCF code paths.", "default": null}, "beta": {"type": "float", "description": "Scalar multiplier applied to the initial accumulator c (cR and cI). The final returned c equals alpha*(a*b) + beta*c_initial when cR and/or cI are provided. Default is 0, which ignores any initial accumulator content. This is consistent with common BLAS-style scaling semantics used across PySCF's numeric kernels.", "default": 0}}, "required": ["aR", "aI", "bR", "bI", "cR", "beta", "cI", "alpha"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a PySCF post-HF complex-assembly microkernel (non-conjugated) under messy upstream data conditions. Below is a raw batch of candidate AB updates (each A is 2×3 and B is 3×2, split into real/imag parts) intended to update an existing accumulator C via C := alpha*(A·B) + beta*C. Treat each candidate as a potential intermediate from different pipeline stages.\n\nBatch candidates (in arrival order):\n1) stage_tag='orbrot_intblk', A_real=[[0.9,-0.1,0.0],[0.2,0.5,-0.3]], A_imag=[[0.0,0.3,-0.2],[-0.4,0.0,0.1]], B_real=[[1.1,-0.2],[0.0,0.7],[-0.5,0.4]], B_imag=[[0.2,0.0],[-0.3,0.1],[0.0,-0.6]], C_real=[[1.0,-0.5],[0.3,0.8]], C_imag=[[0.0,0.2],[-0.1,0.4]]\n2) stage_tag='coeff_intblk', A_real=[[1.0,-0.5,0.0],[0.2,0.3,-0.1]], A_imag=[[0.0,0.1,-0.2],[0.3,0.0,0.4]], B_real=[[0.9,-0.3],[0.0,0.5],[-0.4,0.2]], B_imag=[[0.1,0.0],[-0.2,0.3],[0.0,-0.1]], C_real=[[0.5,-0.2],[0.1,0.4]], C_imag=[[-0.3,0.0],[0.2,-0.1]]\n\nScientific sieve: only apply updates whose B block has a strictly negative imaginary trace (i.e., sum of diagonal entries of B_imag < 0), which flags the expected sign convention for our current gauge choice.\n\nBranching protocol: use alpha fixed at 0.75 for all processed updates. Set beta by stage_tag: for tags containing 'orbrot' use beta=-0.5; otherwise use beta=+0.5. For each processed candidate, run a single-pass accumulation and report the updated C_real and C_imag.", "answers": "[{\"name\":\"pyscf_lib_numpy_helper_zdotNN\",\"arguments\":{\"aR\":[[0.9,-0.1,0.0],[0.2,0.5,-0.3]],\"aI\":[[0.0,0.3,-0.2],[-0.4,0.0,0.1]],\"bR\":[[1.1,-0.2],[0.0,0.7],[-0.5,0.4]],\"bI\":[[0.2,0.0],[-0.3,0.1],[0.0,-0.6]],\"alpha\":0.75,\"cR\":[[1.0,-0.5],[0.3,0.8]],\"cI\":[[0.0,0.2],[-0.1,0.4]],\"beta\":-0.5}},{\"name\":\"pyscf_lib_numpy_helper_zdotNN\",\"arguments\":{\"aR\":[[1.0,-0.5,0.0],[0.2,0.3,-0.1]],\"aI\":[[0.0,0.1,-0.2],[0.3,0.0,0.4]],\"bR\":[[0.9,-0.3],[0.0,0.5],[-0.4,0.2]],\"bI\":[[0.1,0.0],[-0.2,0.3],[0.0,-0.1]],\"alpha\":0.75,\"cR\":[[0.5,-0.2],[0.1,0.4]],\"cI\":[[-0.3,0.0],[0.2,-0.1]],\"beta\":0.5}}]"}
{"func_name": "pyscf_lib_scipy_helper_pivoted_cholesky_python", "func_desc": "pyscf.lib.scipy_helper.pivoted_cholesky_python performs a pedestrian (pure-Python) pivoted Cholesky factorization with full column pivoting of a positive semidefinite matrix. It is intended for use within the PySCF quantum-chemistry framework to produce a low-rank Cholesky factor (for example, when decomposing positive semidefinite matrices that appear in density fitting or integral decompositions). This implementation should be used only when a LAPACK-optimized routine is unavailable, as it is significantly slower than optimized library calls.\n    \n    This routine constructs a triangular factor L by greedily selecting pivot indices and updating the residual diagonal D. The algorithm stops when the largest remaining diagonal element falls below the provided tolerance, producing a numerical rank and a permutation vector that records the pivot ordering. The returned factor and permutation can be used to form an approximate reconstruction of A after applying the permutation (see Returns).", "tools": [{"function": {"description": "pyscf.lib.scipy_helper.pivoted_cholesky_python performs a pedestrian (pure-Python) pivoted Cholesky factorization with full column pivoting of a positive semidefinite matrix. It is intended for use within the PySCF quantum-chemistry framework to produce a low-rank Cholesky factor (for example, when decomposing positive semidefinite matrices that appear in density fitting or integral decompositions). This implementation should be used only when a LAPACK-optimized routine is unavailable, as it is significantly slower than optimized library calls.\n\nThis routine constructs a triangular factor L by greedily selecting pivot indices and updating the residual diagonal D. The algorithm stops when the largest remaining diagonal element falls below the provided tolerance, producing a numerical rank and a permutation vector that records the pivot ordering. The returned factor and permutation can be used to form an approximate reconstruction of A after applying the permutation (see Returns).", "name": "pyscf_lib_scipy_helper_pivoted_cholesky_python", "parameters": {"properties": {"A": {"type": "array", "items": {"type": "float"}, "description": "The input matrix to be factorized. Must be a square (N, N) numpy.ndarray and should represent a positive semidefinite (Hermitian) matrix arising in PySCF workflows (for example, overlap matrices, density-like matrices, or two-electron integral kernels represented on a grid or auxiliary basis). The implementation reads A but does not modify it in-place. The algorithm uses only the real part of A's diagonal to initialize the residual diagonal estimator.", "default": ""}, "tol": {"type": "float", "description": "Stopping tolerance for the pivoted Cholesky procedure. If tol < 0 (the default -1.0), the function sets tol = N * machine_epsilon * max(diag(A)) where machine_epsilon is numpy.finfo(numpy.double).eps and N is the matrix dimension. This default makes the stopping criterion scale with matrix size and the largest diagonal element, providing a relative numerical threshold appropriate for typical PySCF linear-algebra tasks. A smaller tol forces more pivots (larger numerical rank); a larger tol yields a lower-rank approximation.", "default": -1.0}, "lower": {"type": "boolean", "description": "If True, return the lower-triangular Cholesky factor L (shape (N, N) with only the first `rank` columns typically nonzero). If False (default), return the conjugate transpose of that factor (an upper-triangular matrix). Choosing lower=True is the conventional Cholesky form; lower=False can be convenient when code expects an upper-triangular factor.", "default": false}}, "required": ["A", "tol", "lower"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a PySCF density-fitting preconditioner where auxiliary-basis overlap matrices come from mixed-quality sources. Given two candidate overlap matrices, first apply a physical-consistency gate: treat a matrix as usable only if it is symmetric (within exact equality for the provided literals) and all diagonal elements are strictly positive (to rule out broken normalization). For each usable matrix, run `pyscf.lib.scipy_helper.pivoted_cholesky_python` with `lower=True` and choose the stopping tolerance adaptively from the matrix itself: use `tol = 1e-10` if the largest diagonal element is ≤ 2.0 (tighter for near-normalized overlaps), otherwise use `tol = 1e-8` (looser for scaled overlaps). Process the following matrices:\n\nM1 = [[1.0, 0.8, 0.6], [0.8, 1.0, 0.9], [0.6, 0.9, 1.0]]\nM2 = [[4.0, 1.2, 0.5, 0.3], [1.2, 3.5, 0.8, 0.4], [0.5, 0.8, 2.8, 0.6], [0.3, 0.4, 0.6, 1.9]]\n\nFor each processed case, record the Cholesky factor and the pivot permutation for downstream reconstruction checks and rank comparison.", "answers": "[{\"name\":\"pyscf_lib_scipy_helper_pivoted_cholesky_python\",\"arguments\":{\"A\":[[1.0,0.8,0.6],[0.8,1.0,0.9],[0.6,0.9,1.0]],\"tol\":1e-10,\"lower\":true}},{\"name\":\"pyscf_lib_scipy_helper_pivoted_cholesky_python\",\"arguments\":{\"A\":[[4.0,1.2,0.5,0.3],[1.2,3.5,0.8,0.4],[0.5,0.8,2.8,0.6],[0.3,0.4,0.6,1.9]],\"tol\":1e-08,\"lower\":true}}]"}
{"func_name": "pyscf_lo_nao_set_atom_conf", "func_desc": "Change the default atomic core and valence shell configuration for an element in the NAO (numerical atomic orbital) AOSHELL mapping used by PySCF.\n    \n    This function is used by PySCF's localized-orbital / NAO utilities to override the default per-element core and core+valence shell counts stored in the data/elements.py AOSHELL table. It resolves the provided element to a nuclear charge using mole.charge(element) and updates AOSHELL[charge] in-place so subsequent NAO/basis logic uses the new core and valence shell counts. The update is also reported to standard error.", "tools": [{"function": {"description": "Change the default atomic core and valence shell configuration for an element in the NAO (numerical atomic orbital) AOSHELL mapping used by PySCF.\n\nThis function is used by PySCF's localized-orbital / NAO utilities to override the default per-element core and core+valence shell counts stored in the data/elements.py AOSHELL table. It resolves the provided element to a nuclear charge using mole.charge(element) and updates AOSHELL[charge] in-place so subsequent NAO/basis logic uses the new core and valence shell counts. The update is also reported to standard error.", "name": "pyscf_lo_nao_set_atom_conf", "parameters": {"properties": {"element": {"type": "string", "description": "Element symbol (e.g. \"C\", \"Fe\") or nuclear charge (atomic number).\nThis argument determines which row of the global AOSHELL table (data/elements.py)\nwill be modified. mole.charge(element) is used to convert symbols to a charge;\nif the element cannot be resolved to a valid nuclear charge, mole.charge may\nraise an error (e.g. ValueError), and no modification will occur. Supplying the\natomic number directly bypasses symbol lookup but must be a valid index into AOSHELL.", "default": ""}, "description": {"type": "array", "items": {"type": "float"}, "description": "New configuration descriptor(s) that control\nhow many core and valence shells of each angular momentum (s, p, d, f) are kept.\nWhen a single string is passed, it is interpreted as a valence-only modification\nand the existing core configuration is preserved. When a two-element list-like\nobject is passed, the first entry specifies the core configuration and the\nsecond specifies the core+valence configuration. Recognized textual forms include:\n    \"double p\", \"double d\", \"double f\"  -> increase the corresponding shell by a factor of two (mapped internally to \"2p\", \"2d\", \"2f\").\n    \"polarize\"                         -> add one polarized shell (the function inspects the existing AOSHELL entry to determine which angular momentum to increment).\n    explicit forms such as \"1s1d\"       -> set explicit counts (keep core unchanged then set specified valence shells).\n    paired tuple examples like (\"3s2p\",\"1d\") -> first element sets core (3 s, 2 p), second sets valence (1 d).\nThe function normalizes description strings by removing spaces, hyphens, and underscores and lowercasing before interpretation. If a list-like description does not contain exactly two elements, or if the strings do not match the expected compact shell syntax, the function may raise IndexError or produce unintended results. The code expects AOSHELL entries in the format of digit-letter pairs for s, p, d, f (for example \"2s1p0d0f\"); if AOSHELL has a nonconforming format, integer parsing or indexing may fail.", "default": ""}}, "required": ["element", "description"], "type": "any"}}, "type": "function"}], "query": "We’re standardizing NAO AOSHELL partitions for a mixed-element PySCF LO/NAO benchmark where element labels arrive from multiple sources (symbols, lowercase symbols, and occasional full names). Apply in-place AOSHELL overrides only for entries that resolve via mole.charge(element) to a transition metal in the 3d series (Z=21–30) or to a third-period p-block element (Z=13–18), using experiment-specific shell partitions: for the 3d-series cohort use core=\"3s2p\" and core+valence=\"3s2p3d\"; for the third-period p-block cohort use core=\"2s2p\" and core+valence=\"3s3p1d\". As an additional light-element calibration, if any provided element resolves to oxygen (by any alias), set core=\"1s\" and core+valence=\"2s2p\". Raw cohort labels: [\"oxygen\", \"Fe\", \"s\", \"Ne\", \"Cl\", \"Si\", \"Na\", \"O\", \"Cu\", \"Ar\", \"S\"].", "answers": "[{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"oxygen\",\"description\":[\"1s\",\"2s2p\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"Fe\",\"description\":[\"3s2p\",\"3s2p3d\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"s\",\"description\":[\"2s2p\",\"3s3p1d\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"Cl\",\"description\":[\"2s2p\",\"3s3p1d\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"Si\",\"description\":[\"2s2p\",\"3s3p1d\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"Cu\",\"description\":[\"3s2p\",\"3s2p3d\"]}},{\"name\":\"pyscf_lo_nao_set_atom_conf\",\"arguments\":{\"element\":\"S\",\"description\":[\"2s2p\",\"3s3p1d\"]}}]"}
{"func_name": "pyscf_lo_orth_weight_orth", "func_desc": "pyscf.lo.orth.weight_orth: Construct weight-scaled Lowdin-orthonormalization coefficients for a given overlap matrix.\n    \n    This function constructs a new set of basis-coefficient vectors c_{mu,i} that define orthonormal basis functions under the original atomic-orbital overlap matrix s, after applying a multiplicative weight per basis function. It implements the transformation\n    c = w * [ (w s w)^{-1/2} ],\n    where w is the diagonal weighting defined by the 1D array weight, s is the square overlap matrix, and (w s w)^{-1/2} is obtained by the Lowdin symmetric orthonormalization of the weighted overlap matrix s1 = w s w. In PySCF this routine is used in localized orbital and basis-transformation utilities to produce a new basis |mu> c_{mu i} whose columns are orthonormal with respect to the original quantum-chemical overlap s (i.e., c^T s c = I), enabling downstream procedures that require an orthonormal representation (for example, population analysis, projector construction, or subsequent orthogonal transformations in electronic structure workflows).", "tools": [{"function": {"description": "pyscf.lo.orth.weight_orth: Construct weight-scaled Lowdin-orthonormalization coefficients for a given overlap matrix.\n\nThis function constructs a new set of basis-coefficient vectors c_{mu,i} that define orthonormal basis functions under the original atomic-orbital overlap matrix s, after applying a multiplicative weight per basis function. It implements the transformation\nc = w * [ (w s w)^{-1/2} ],\nwhere w is the diagonal weighting defined by the 1D array weight, s is the square overlap matrix, and (w s w)^{-1/2} is obtained by the Lowdin symmetric orthonormalization of the weighted overlap matrix s1 = w s w. In PySCF this routine is used in localized orbital and basis-transformation utilities to produce a new basis |mu> c_{mu i} whose columns are orthonormal with respect to the original quantum-chemical overlap s (i.e., c^T s c = I), enabling downstream procedures that require an orthonormal representation (for example, population analysis, projector construction, or subsequent orthogonal transformations in electronic structure workflows).", "name": "pyscf_lo_orth_weight_orth", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "float"}, "description": "Square overlap matrix of the original non-orthogonal basis. This is the atomic-orbital overlap matrix used in electronic-structure calculations; it must be a two-dimensional square array with shape (n, n) where n is the number of basis functions. The routine treats s as the metric for inner products: the output coefficients satisfy c.T @ s @ c = I when Lowdin orthogonalization succeeds. If s is not symmetric positive-definite (for example, if it has zero or negative eigenvalues), the underlying Lowdin procedure may fail or produce non-physical results.", "default": ""}, "weight": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of multiplicative weights w applied to basis functions. This must have length n and real entries; it defines a diagonal scaling matrix W with W_{mu,mu} = weight[mu]. The function forms the weighted overlap s1 = W @ s @ W by elementwise multiplication weight[:,None] * s * weight, then orthonormalizes s1 and finally multiplies the orthonormalization matrix by W to return the final coefficients. The weights are typically used in PySCF to bias, scale, or normalize basis functions prior to orthogonalization (for example, to impose atomic-centered damping or population-derived scaling).", "default": ""}}, "required": ["s", "weight"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our orthonormal-basis preparation stage for a small STO-3G localization benchmark where AO weights are derived from intrinsic coupling in the overlap matrix (to mimic geometry-dependent conditioning).\n\nGiven two overlap-matrix replicates below, construct weight-scaled Löwdin orthonormalization coefficients c = w * [(w S w)^(-1/2)] using a data-driven weighting protocol:\n\nProtocol (apply independently per replicate):\n1) For each AO i, compute its off-diagonal coupling score r_i = max_{j != i} |S_ij|.\n2) Define the weight vector w by:\n   - if r_i >= 0.25, set w_i = 0.80 (strongly coupled / diffuse-like behavior; damp to stabilize)\n   - else if 0.12 <= r_i < 0.25, set w_i = 1.20 (moderately coupled; emphasize)\n   - else set w_i = 1.00 (baseline)\n3) Only run the weighted orthonormalization for replicates whose overlap matrix is numerically self-consistent at the 1e-8 level (i.e., symmetric within tolerance) and has strictly positive diagonal entries.\n\nReplicate A (3×3, AO order [1s(A), 1s(B), 2s(A)]):\nS = [[1.0, 0.25, 0.10],\n     [0.25, 1.0, 0.05],\n     [0.10, 0.05, 1.0]]\n\nReplicate B (7×7, water minimal basis):\nS = [[1.000, 0.235, 0.210, 0.000, 0.000, 0.045, 0.045],\n     [0.235, 1.000, 0.180, 0.020, 0.020, 0.030, 0.030],\n     [0.210, 0.180, 1.000, 0.015, 0.015, 0.025, 0.025],\n     [0.000, 0.020, 0.015, 1.000, 0.300, 0.010, 0.005],\n     [0.000, 0.020, 0.015, 0.300, 1.000, 0.005, 0.010],\n     [0.045, 0.030, 0.025, 0.010, 0.005, 1.000, 0.150],\n     [0.045, 0.030, 0.025, 0.005, 0.010, 0.150, 1.000]]\n\nReturn the coefficient matrices c for the replicates that satisfy the consistency criteria (one c per retained replicate).", "answers": "[{\"name\":\"pyscf_lo_orth_weight_orth\",\"arguments\":{\"s\":[[1.0,0.25,0.1],[0.25,1.0,0.05],[0.1,0.05,1.0]],\"weight\":[0.8,0.8,1.0]}},{\"name\":\"pyscf_lo_orth_weight_orth\",\"arguments\":{\"s\":[[1.0,0.235,0.21,0.0,0.0,0.045,0.045],[0.235,1.0,0.18,0.02,0.02,0.03,0.03],[0.21,0.18,1.0,0.015,0.015,0.025,0.025],[0.0,0.02,0.015,1.0,0.3,0.01,0.005],[0.0,0.02,0.015,0.3,1.0,0.005,0.01],[0.045,0.03,0.025,0.01,0.005,1.0,0.15],[0.045,0.03,0.025,0.005,0.01,0.15,1.0]],\"weight\":[1.2,1.2,1.2,0.8,0.8,1.2,1.2]}}]"}
{"func_name": "pyscf_mcpdft__libxc_split_x_c_comma", "func_desc": "Split an xc code string into two separate identifiers: one for exchange and one for\n    correlation, used by the PySCF mcpdft._libxc plumbing that maps user-specified\n    XC identifiers to LibXC exchange/correlation components. The function locates a\n    comma in the input string or in any alias/lookup it resolves, upper-cases the\n    input for alias matching, consults module tables for numeric LibXC codes and\n    aliases (XC_ALIAS, XC_CODES, XC_KEYS, INTCODES_TYPES, INTCODES_HYB), and enforces\n    a recursion limit when expanding aliases. This is used when parsing an\n    xc specification passed to higher-level MCPDFT/DFT routines so that the exchange\n    and correlation parts can be handled separately by LibXC or other backends.", "tools": [{"function": {"description": "Split an xc code string into two separate identifiers: one for exchange and one for\ncorrelation, used by the PySCF mcpdft._libxc plumbing that maps user-specified\nXC identifiers to LibXC exchange/correlation components. The function locates a\ncomma in the input string or in any alias/lookup it resolves, upper-cases the\ninput for alias matching, consults module tables for numeric LibXC codes and\naliases (XC_ALIAS, XC_CODES, XC_KEYS, INTCODES_TYPES, INTCODES_HYB), and enforces\na recursion limit when expanding aliases. This is used when parsing an\nxc specification passed to higher-level MCPDFT/DFT routines so that the exchange\nand correlation parts can be handled separately by LibXC or other backends.", "name": "pyscf_mcpdft__libxc_split_x_c_comma", "parameters": {"properties": {"xc": {"type": "string", "description": "The user-supplied XC code string to split. In practical PySCF\nuse this is the textual identifier for a density-functional\napproximation (for example a LibXC name, an alias, or a numeric code\nmapped to a LibXC functional). The function treats an empty string\nspecially (returns two empty strings), converts the input to upper\ncase for alias/code lookup, and accepts strings that already contain a\ncomma separating exchange and correlation. The parameter is read-only;\nno other parameters are modified.", "default": ""}}, "required": ["xc"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed-provenance XC manifest from an MC-PDFT campaign where operators typed functionals inconsistently. The raw entries are:\n\n- \" b88 , lyp \"\n- \"B88/LYP\"\n- \"B88+LYP\"\n- \"B88,\"\n- \",LYP\"\n- \"B88 LYP\"\n- \"B88,LYP,\"\n- \"B88,LYP\"\n\nRun two independent parsing replicates (A and B) through the LibXC-style XC splitter utility, but only for records that unambiguously encode exactly one exchange identifier and exactly one correlation identifier using a single comma delimiter after trimming surrounding whitespace; the parsing should rely on the splitter’s own alias/normalization behavior rather than pre-normalizing the tokens. For each eligible record, submit the exact raw string as observed to the splitter in replicate A, and submit it again in replicate B (independent check).", "answers": "[{\"name\":\"pyscf_mcpdft__libxc_split_x_c_comma\",\"arguments\":{\"xc\":\" b88 , lyp \"}},{\"name\":\"pyscf_mcpdft__libxc_split_x_c_comma\",\"arguments\":{\"xc\":\"B88,LYP\"}},{\"name\":\"pyscf_mcpdft__libxc_split_x_c_comma\",\"arguments\":{\"xc\":\" b88 , lyp \"}},{\"name\":\"pyscf_mcpdft__libxc_split_x_c_comma\",\"arguments\":{\"xc\":\"B88,LYP\"}}]"}
{"func_name": "pyscf_mcpdft_mspdft_get_diabfns", "func_desc": "Return a pair of callables (diabatizer, diabatize) appropriate for the requested\n    MS-PDFT method name. The pair implements (1) evaluation of the MS-PDFT\n    objective function and its first and second derivatives (the second-order power\n    series used by Newton-like optimizers) and (2) the transformation/optimization\n    that produces CI vectors in the optimized intermediate-state (diabatic) basis.\n    This helper is used in the MS-PDFT (multistate pair-density functional theory)\n    components of PySCF to select method-specific implementations: for \"CMS\" the\n    function returns the Coulomb-energy-based diabatizer and a local Newton solver\n    for intermediate-state optimization; for \"XMS\" the function returns the\n    state-averaged Fock-based diabatizer and its corresponding solver.", "tools": [{"function": {"description": "Return a pair of callables (diabatizer, diabatize) appropriate for the requested\nMS-PDFT method name. The pair implements (1) evaluation of the MS-PDFT\nobjective function and its first and second derivatives (the second-order power\nseries used by Newton-like optimizers) and (2) the transformation/optimization\nthat produces CI vectors in the optimized intermediate-state (diabatic) basis.\nThis helper is used in the MS-PDFT (multistate pair-density functional theory)\ncomponents of PySCF to select method-specific implementations: for \"CMS\" the\nfunction returns the Coulomb-energy-based diabatizer and a local Newton solver\nfor intermediate-state optimization; for \"XMS\" the function returns the\nstate-averaged Fock-based diabatizer and its corresponding solver.", "name": "pyscf_mcpdft_mspdft_get_diabfns", "parameters": {"properties": {"obj": {"type": "string", "description": "Name of the MS-PDFT method to select. This string controls which\nimplementation pair is returned. The comparison is case-insensitive.\nCurrently supported values (exactly as implemented in the source code)\nare \"CMS\" and \"XMS\". When obj.upper() == \"CMS\", the function returns\npyscf.mcpdft.cmspdft.e_coul as the diabatizer and the local si_newton\nsolver as diabatize; these callables are intended to: take model-space\nCI vectors given in a trial intermediate-state basis, evaluate the\nCMS-specific objective function (energy-like quantity) and provide its\nfirst and second derivatives (diabatizer), and perform Newton-style\noptimization to produce CI vectors in the optimized intermediate-state\nbasis (diabatize). When obj.upper() == \"XMS\", the function returns\npyscf.mcpdft.xmspdft.safock_energy as the diabatizer and\npyscf.mcpdft.xmspdft.solve_safock as diabatize; these are the XMS\nequivalents used to evaluate the state-averaged Fock objective and to\ncompute the optimized intermediate-state CI vectors. The function\nperforms dynamic imports of the selected implementation modules as a\nside effect.", "default": ""}}, "required": ["obj"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing MS-PDFT diabatization method selection in a messy multistate benchmark harness where each record corresponds to a replicate from different sources. The raw method tags come from a metadata merge and may include mixed case, leading/trailing whitespace, and occasional version-like suffixes (e.g., \"CMS_v1\", \"xms-2024\"). Given the following replicate tags in arrival order:\n\n[' cms ', 'XMS', 'Cms_v1', 'xms-2024', '   CMS', 'Xms ']\n\nFor each replicate, normalize the tag (trim whitespace; case-insensitive) and map it to the requested MS-PDFT method name by taking the leading alphabetic method code only (so any suffix after the method code is ignored). Then retrieve the appropriate (diabatizer, diabatize) callable pair via the helper for each mapped method, preserving the original arrival order so we can replay selections exactly during reruns. This should exercise both the Coulomb-energy-based CMS pathway and the state-averaged Fock-based XMS pathway multiple times under realistic metadata noise.", "answers": "[{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"CMS\"}},{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"XMS\"}},{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"CMS\"}},{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"XMS\"}},{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"CMS\"}},{\"name\":\"pyscf_mcpdft_mspdft_get_diabfns\",\"arguments\":{\"obj\":\"XMS\"}}]"}
{"func_name": "pyscf_mcpdft_otfnal_make_hybrid_fnal", "func_desc": "make_hybrid_fnal generates a PySCF-style hybrid exchange–correlation (XC) functional specification by combining a base libxc functional code (xc_code) with a small set of \"hybridization\" parameters (hyb) according to one of several predefined combination rules (hyb_type). This convenience wrapper is used in the mcpdft.otfnal code path of PySCF to produce hybrid functionals suitable for multiconfigurational pair-density functional theory workflows; it produces a functional specification that is ultimately passed to make_scaled_fnal and then consumed by PySCF's DFT machinery (pyscf.dft.libxc) for evaluation of exchange and correlation contributions.", "tools": [{"function": {"description": "make_hybrid_fnal generates a PySCF-style hybrid exchange–correlation (XC) functional specification by combining a base libxc functional code (xc_code) with a small set of \"hybridization\" parameters (hyb) according to one of several predefined combination rules (hyb_type). This convenience wrapper is used in the mcpdft.otfnal code path of PySCF to produce hybrid functionals suitable for multiconfigurational pair-density functional theory workflows; it produces a functional specification that is ultimately passed to make_scaled_fnal and then consumed by PySCF's DFT machinery (pyscf.dft.libxc) for evaluation of exchange and correlation contributions.\n", "name": "pyscf_mcpdft_otfnal_make_hybrid_fnal", "parameters": {"properties": {"xc_code": {"type": "string", "description": "The base functional identifier as used by pyscf.dft.libxc. This string names the underlying semilocal functional (for example a libxc label) whose exchange and correlation parts will be combined with exact (Hartree–Fock) exchange according to the hybridization rule. The function will raise an exception if xc_code already denotes a hybrid-type functional or if it contains a kinetic-energy functional component (these checks are performed by downstream routines that validate the libxc specification).", "default": ""}, "hyb": {"type": "float", "description": "Parameter(s) defining the hybridization. By default this is documented as a float scalar value, and the implementation will accept a scalar or a sequence-like object. If a scalar is provided it is coerced to a single-element sequence internally. The entries of hyb control the weight of exact exchange (HF) and the scaling of the semilocal exchange and correlation components according to the chosen hyb_type. Different hyb_type values impose specific length requirements:\n- For hyb_type 0, 1, 2, or 3: len(hyb) must be 1 (a single float); the single value is denoted hyb[0] in the formulas below.\n- For hyb_type 4: len(hyb) must be 2; the first element hyb[0] is the primary mixing parameter a and hyb[1] is used to define an exponent for correlation scaling.", "default": ""}, "hyb_type": {"type": "integer", "description": "The hybrid-construction rule to apply. The default is 1. hyb_type may be supplied as an integer code or as one of the string names below (case-sensitive, matched against the internal mapping). The implementation accepts the following codes and applies the corresponding construction; assertion failures will occur if the supplied hyb sequence length does not match the requirement of the selected type, a KeyError will occur if a string name is not recognized, and a RuntimeError is raised for an undefined numeric type outside the documented set.\n- 0 or 'translation': \"translation\" hybrid. Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=1). Requires a single-element hyb.\n- 1 or 'average': \"average\" hybrid (default). Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + (1-hyb)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=(1-hyb[0])). Requires a single-element hyb. This choice is motivated by the idea that hyb = 1 should recover the underlying wave function energy.\n- 2 or 'diagram': \"diagram\" hybrid. Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = c_code (no HF mixing in correlation). Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=0, fnal_x=(1-hyb[0]), fnal_c=1). Requires a single-element hyb.\n- 3 or 'lambda': \"lambda\" hybrid (as used in arXiv:1911.11162v1 / double-hybrid style constructions). Construction used: exchange part = hyb*HF + (1-hyb)*x_code; correlation part = hyb*HF + (1-hyb^2)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=hyb[0], hyb_c=hyb[0], fnal_x=(1-hyb[0]), fnal_c=(1-(hyb[0]*hyb[0]))). Requires a single-element hyb.\n- 4 or 'scaling': \"scaling\" hybrid motivated by density-scaling inequalities of Levy and Perdew. Let a = hyb[0] and define b = a**(1 + hyb[1]). Construction used: exchange part = a*HF + (1-a)*x_code; correlation part = a*HF + (1-b)*c_code. Implementation call: make_scaled_fnal(xc_code, hyb_x=a, hyb_c=a, fnal_x=(1-a), fnal_c=(1-b)). Requires two-element hyb (hyb[0] and hyb[1]) and implements an empirical correlation exponent 1+hyb[1].", "default": 1}}, "required": ["xc_code", "hyb", "hyb_type"], "type": "any"}}, "type": "function"}], "query": "We’re curating a small “as-run” XC provenance table from an MC-PDFT (mcpdft.otfnal) benchmark where the semilocal starting point was recorded inconsistently across replicates. The raw labels are: [\"PBE,PBE\", \"PBE\", \"PBE\", \"PBE,PBE\"]. Prepare hybrid XC functional specifications with make_hybrid_fnal using a data-dependent branching protocol:\n- Treat entries that explicitly specify exchange and correlation separately (contain a comma) as *component-resolved* and use the default/average hybridization rule (hyb_type = 1).\n- Treat entries given as a single combined functional label (no comma) as *monolithic* and use the alternative combination rule (hyb_type = 2).\nFor all entries, set the exact-exchange mixing fraction (hyb) to 0.25. Return the resulting hybrid functional specifications for each row in the provenance table, in the same order as the input list.", "answers": "[{\"name\":\"pyscf_mcpdft_otfnal_make_hybrid_fnal\",\"arguments\":{\"xc_code\":\"PBE,PBE\",\"hyb\":0.25,\"hyb_type\":1}},{\"name\":\"pyscf_mcpdft_otfnal_make_hybrid_fnal\",\"arguments\":{\"xc_code\":\"PBE\",\"hyb\":0.25,\"hyb_type\":2}},{\"name\":\"pyscf_mcpdft_otfnal_make_hybrid_fnal\",\"arguments\":{\"xc_code\":\"PBE\",\"hyb\":0.25,\"hyb_type\":2}},{\"name\":\"pyscf_mcpdft_otfnal_make_hybrid_fnal\",\"arguments\":{\"xc_code\":\"PBE,PBE\",\"hyb\":0.25,\"hyb_type\":1}}]"}
{"func_name": "pyscf_md_set_seed", "func_desc": "Sets the seed for the random number generator used by the md module in PySCF.\n    \n    This function initializes or replaces the module-level random number generator used by pyscf.md routines by creating a numpy.random.Generator backed by numpy.random.PCG64 initialized with the given integer seed. In the context of the PySCF (Python-based Simulations of Chemistry Framework) md submodule, this seed controls all subsequent pseudo-random behaviour that relies on the module's rng variable, for example random initial velocities, stochastic forces, Monte Carlo sampling, or other randomized procedures that may appear in molecular dynamics or related stochastic workflows within PySCF. Calling this function ensures deterministic, reproducible sequences of pseudo-random numbers for those md module operations when the same seed and the same numpy/PySCF environment are used.", "tools": [{"function": {"description": "Sets the seed for the random number generator used by the md module in PySCF.\n\nThis function initializes or replaces the module-level random number generator used by pyscf.md routines by creating a numpy.random.Generator backed by numpy.random.PCG64 initialized with the given integer seed. In the context of the PySCF (Python-based Simulations of Chemistry Framework) md submodule, this seed controls all subsequent pseudo-random behaviour that relies on the module's rng variable, for example random initial velocities, stochastic forces, Monte Carlo sampling, or other randomized procedures that may appear in molecular dynamics or related stochastic workflows within PySCF. Calling this function ensures deterministic, reproducible sequences of pseudo-random numbers for those md module operations when the same seed and the same numpy/PySCF environment are used.", "name": "pyscf_md_set_seed", "parameters": {"properties": {"seed": {"type": "integer", "description": "Integer seed value to initialize the underlying PCG64 pseudo-random number generator. This parameter must be an integer as required by numpy.random.PCG64; it is used directly to construct PCG64(seed) and therefore determines the exact pseudo-random sequence produced by the Generator. Choosing and documenting the seed is important for reproducibility of stochastic molecular-dynamics or sampling tasks performed with the md module.", "default": ""}}, "required": ["seed"], "type": "any"}}, "type": "function"}], "query": "We’re rerunning a mixed-protocol MD reproducibility audit where each trajectory must be re-seeded immediately before its stochastic steps. Use the following run-manifest (each entry is a replicate ID with an intended provenance tag):\n\n- rep01: provenance=production\n- rep02: provenance=production\n- rep03: provenance=production\n- rep04: provenance=legacy_patch\n- rep05: provenance=legacy_patch\n- rep06: provenance=legacy_patch\n\nSeeding rule: set the PySCF `md` module RNG to the cohort seed selected by provenance: use seed 20241216 for trajectories tagged `production`, and seed 20240112 for trajectories tagged `legacy_patch`. Apply the seed for each replicate right before generating random initial velocities so that repeated reruns reproduce identical stochastic trajectories within each provenance cohort.", "answers": "[{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20241216}},{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20241216}},{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20241216}},{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20240112}},{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20240112}},{\"name\":\"pyscf_md_set_seed\",\"arguments\":{\"seed\":20240112}}]"}
{"func_name": "pyscf_pbc_gto_cell_pgf_rcut", "func_desc": "pyscf.pbc.gto.cell.pgf_rcut estimates cutoff radii for primitive Gaussian functions used in real-space integral screening in periodic electronic structure calculations (PySCF PBC module). The routine solves for rcut from the asymptotic relation c * rcut^(l+2) * exp(-alpha * rcut^2) ~ precision, where c = log(coeff/precision), and returns a radius beyond which the primitive Gaussian amplitude falls below the requested precision. This cutoff is used to truncate Gaussian tails for efficient integral evaluation and grid-based operations in periodic simulations.", "tools": [{"function": {"description": "pyscf.pbc.gto.cell.pgf_rcut estimates cutoff radii for primitive Gaussian functions used in real-space integral screening in periodic electronic structure calculations (PySCF PBC module). The routine solves for rcut from the asymptotic relation c * rcut^(l+2) * exp(-alpha * rcut^2) ~ precision, where c = log(coeff/precision), and returns a radius beyond which the primitive Gaussian amplitude falls below the requested precision. This cutoff is used to truncate Gaussian tails for efficient integral evaluation and grid-based operations in periodic simulations.\n", "name": "pyscf_pbc_gto_cell_pgf_rcut", "parameters": {"properties": {"l": {"type": "integer", "description": "Angular momentum quantum number (non-negative integer) of the primitive Gaussian. In the PySCF PBC gto context, l determines the polynomial prefactor r^(l) of the real-space Gaussian; the function uses l+2 in the asymptotic estimate because the screened quantity scales as r^(l+2)*exp(-alpha*r^2). Practical significance: higher l increases the algebraic prefactor and typically requires a larger cutoff to reach the same amplitude threshold.", "default": ""}, "alpha": {"type": "float", "description": "Gaussian exponent controlling the exponential decay exp(-alpha * r^2). This parameter must be positive in practice; it sets the spatial extent of the primitive function. Larger alpha produces faster decay and thus smaller cutoffs for the same precision.", "default": ""}, "coeff": {"type": "float", "description": "Multiplicative coefficient of the asymptotic model for the primitive Gaussian tail. The routine computes c = log(coeff/precision), so coeff must be positive for the logarithm to be defined. In PySCF, coeff represents the prefactor amplitude of the primitive Gaussian contribution being screened.", "default": ""}, "precision": {"type": "float", "description": "Target amplitude threshold for the primitive Gaussian in real space. The routine finds rcut such that the tail amplitude is approximately at this level. Default is 1e-08 (the module constant INTEGRAL_PRECISION in the source), chosen as a typical numerical threshold for integral screening in PySCF. precision must be positive; smaller values yield larger rcut.", "default": 1e-08}, "rcut": {"type": "float", "description": "Initial cutoff radius (in the same length units used by the cell) used as a lower bound or starting guess for the fixed-point iteration. Default is 0. If provided, the returned value will be at least this initial rcut. This parameter allows callers to enforce a minimum radius (for example, to ensure consistency across shells or grids).", "default": 0}, "max_cycle": {"type": "integer", "description": "Maximum number of fixed-point iteration cycles to attempt when solving the nonlinear relation for rcut. Default is 10 (the module constant RCUT_MAX_CYCLE). If convergence is not reached within max_cycle iterations the routine emits a warning and returns the last iterate.", "default": 10}, "eps": {"type": "float", "description": "Convergence tolerance for the change in rcut between successive iterations. Default is 0.001 (the module constant RCUT_EPS). The code tightens eps relative to an analytic lower-bound rmin (eps := min(rmin/10, eps)) to avoid excessive early termination. eps is specified in the same length units as rcut.", "default": 0.001}}, "required": ["l", "alpha", "coeff", "rcut", "max_cycle", "eps", "precision"], "type": "any"}}, "type": "function"}], "query": "We’re tuning real-space integral screening for a mixed-quality set of candidate d-type (l=2) primitive Gaussians coming out of an automated basis contraction step in a periodic PySCF PBC workflow. The raw primitive list is:\n\n- P1: alpha=0.85, coeff=1.2\n- P2: alpha=0.80, coeff=0.05\n- P3: alpha=0.60, coeff=1e-14\n- P4: alpha=0.00, coeff=0.9\n- P5: alpha=1.50, coeff=-0.2\n\nUse a unified screening threshold precision=1e-10, but only calibrate cutoff radii for primitives that represent physically meaningful decaying Gaussians and are not already below the screening threshold at the origin. For the remaining primitives, choose the numerical protocol by intrinsic tail-risk: if coeff/alpha > 1, start from rcut=6.0 bohr with max_cycle=20 and eps=5e-4; otherwise start from rcut=3.0 bohr with max_cycle=15 and eps=1e-4. Return the converged rcut values for all primitives that pass the sieve so we can feed them into the periodic real-space truncation settings.", "answers": "[{\"name\":\"pyscf_pbc_gto_cell_pgf_rcut\",\"arguments\":{\"l\":2,\"alpha\":0.85,\"coeff\":1.2,\"precision\":1e-10,\"rcut\":6.0,\"max_cycle\":20,\"eps\":0.0005}},{\"name\":\"pyscf_pbc_gto_cell_pgf_rcut\",\"arguments\":{\"l\":2,\"alpha\":0.8,\"coeff\":0.05,\"precision\":1e-10,\"rcut\":3.0,\"max_cycle\":15,\"eps\":0.0001}}]"}
{"func_name": "pyscf_pbc_gto_pseudo_pp_Ylm", "func_desc": "Spherical harmonic Y_l^m for use in PySCF PBC pseudopotential and angular-projection routines.\n    \n    This function evaluates the complex-valued spherical harmonic Y_l^m(theta, phi) using SciPy's scipy.special.sph_harm implementation and the same angle ordering/convention as SciPy. In the PySCF periodic-boundary-condition (pbc) gto.pseudo.pp code path, spherical harmonics are used to represent the angular dependence of pseudopotential projectors and to project atomic-like angular components; this function provides the atomic-centred angular factor that appears in those expansions. The implementation calls scipy.special.sph_harm(m, l, phi, theta) so the azimuthal angle phi is passed first to SciPy and the polar (colatitude) angle theta is passed second, matching SciPy's expected argument order.", "tools": [{"function": {"description": "Spherical harmonic Y_l^m for use in PySCF PBC pseudopotential and angular-projection routines.\n\nThis function evaluates the complex-valued spherical harmonic Y_l^m(theta, phi) using SciPy's scipy.special.sph_harm implementation and the same angle ordering/convention as SciPy. In the PySCF periodic-boundary-condition (pbc) gto.pseudo.pp code path, spherical harmonics are used to represent the angular dependence of pseudopotential projectors and to project atomic-like angular components; this function provides the atomic-centred angular factor that appears in those expansions. The implementation calls scipy.special.sph_harm(m, l, phi, theta) so the azimuthal angle phi is passed first to SciPy and the polar (colatitude) angle theta is passed second, matching SciPy's expected argument order.", "name": "pyscf_pbc_gto_pseudo_pp_Ylm", "parameters": {"properties": {"l": {"type": "integer", "description": "Degree (nonnegative integer) of the spherical harmonic, corresponding to the total angular momentum quantum number l used in pseudopotential angular expansion. In practice l selects the angular frequency of the returned function; larger l yields more oscillatory angular dependence. Passing a non-integer or a negative value will be rejected by SciPy or produce a runtime error.", "default": ""}, "m": {"type": "integer", "description": "Order (integer) of the spherical harmonic, with the mathematical requirement -l <= m <= l. m determines the azimuthal dependence and complex phase of the result and is used in PySCF to select a specific magnetic component of an angular projector. If m lies outside the allowed range or is not an integer, the underlying SciPy call may raise an exception.", "default": ""}, "theta": {"type": "float", "description": "Polar (colatitude) angle in radians, measured from the positive z-axis. This argument represents the standard spherical-coordinate polar angle used in atomic-centred expansions in the pbc.gto.pseudo.pp context. Typical values lie in the interval [0, pi]; values outside this range are allowed numerically but may be interpreted modulo 2*pi by trigonometric functions.", "default": ""}, "phi": {"type": "float", "description": "Azimuthal angle in radians, measured in the x-y plane from the positive x-axis. This argument is the standard spherical-coordinate azimuthal angle used by PySCF code that consumes Y_l^m; typical values lie in [0, 2*pi). Note that when calling SciPy, phi is passed as the first angular argument.", "default": ""}}, "required": ["l", "m", "theta", "phi"], "type": "any"}}, "type": "function"}], "query": "We’re doing an angular-channel sanity check for the PySCF PBC pseudopotential projector (SciPy sph_harm convention) on a small batch of candidate probe directions that came out of a symmetry-reduction step. Each record is (theta, phi) in radians but may contain wrap-around values from upstream. For the fixed projector channel (l=3, m=−2), evaluate Y_l^m only for directions that correspond to physically meaningful spherical coordinates after canonicalization: map theta into [0, π] via theta' = arccos(cos(theta)) and map phi into [0, 2π) via phi' = phi mod (2π). Keep only those probes whose canonicalized theta' lies in the northern hemisphere (theta' ≤ π/2). Use the SciPy/PySCF argument ordering (phi first, then theta) when computing Y_l^m. Candidate probes: A=(1.1, 2.4), B=(1.0471975512, 2.3561944902), C=(2.2, 2.4), D=(1.1, -3.8831853072).", "answers": "[{\"name\":\"pyscf_pbc_gto_pseudo_pp_Ylm\",\"arguments\":{\"l\":3,\"m\":-2,\"theta\":1.1,\"phi\":2.4}},{\"name\":\"pyscf_pbc_gto_pseudo_pp_Ylm\",\"arguments\":{\"l\":3,\"m\":-2,\"theta\":1.0471975512,\"phi\":2.3561944902}},{\"name\":\"pyscf_pbc_gto_pseudo_pp_Ylm\",\"arguments\":{\"l\":3,\"m\":-2,\"theta\":1.1,\"phi\":2.4}}]"}
{"func_name": "pyscf_pbc_gw_krgw_ac_AC_twopole_diag", "func_desc": "Analytic continuation to the real-frequency axis using a two-pole model for orbital self-energies in periodic GW calculations.\n    \n    This function is used in the pyscf.pbc.gw.krgw_ac module to fit a compact two-pole analytic model to orbital-resolved self-energy data sampled on the imaginary (Matsubara) frequency axis. The fitted model parameters can be evaluated on the real axis to obtain a continued self-energy for subsequent spectral analysis, quasiparticle energy estimation, or other post-processing steps in periodic-boundary-condition (PBC) GW workflows.", "tools": [{"function": {"description": "Analytic continuation to the real-frequency axis using a two-pole model for orbital self-energies in periodic GW calculations.\n\nThis function is used in the pyscf.pbc.gw.krgw_ac module to fit a compact two-pole analytic model to orbital-resolved self-energy data sampled on the imaginary (Matsubara) frequency axis. The fitted model parameters can be evaluated on the real axis to obtain a continued self-energy for subsequent spectral analysis, quasiparticle energy estimation, or other post-processing steps in periodic-boundary-condition (PBC) GW workflows.", "name": "pyscf_pbc_gw_krgw_ac_AC_twopole_diag", "parameters": {"properties": {"sigma": {"type": "array", "items": {"type": "float"}, "description": "Numeric array of self-energy samples on the imaginary-frequency axis. The function expects sigma to have shape (norbs, nw) where norbs is the number of orbitals (bands) and nw is the number of sampled frequencies per orbital. Each row sigma[p] contains the self-energy values for orbital p evaluated at the frequencies given by the corresponding row of omega. The values may be real or complex depending on upstream GW output. Sigma provides the target data that the two-pole model is fit to for analytic continuation.", "default": ""}, "omega": {"type": "array", "items": {"type": "float"}, "description": "Frequency grid array that corresponds to sigma. omega must be indexable in the same way as sigma (typically shape (norbs, nw)), so that omega[p] gives the frequency samples associated with sigma[p]. Each omega[p] supplies the imaginary-axis frequencies (Matsubara points) used during the nonlinear least-squares fit for orbital p. The function uses these frequencies as the independent variable in the two-pole fitting procedure.", "default": ""}, "orbs": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional array of orbital indices or band labels (integer-like) with length equal to the number of orbitals (norbs). The routine uses orbs[p] in a simple occupancy test (orbs[p] < nocc) to choose an occupancy-dependent initial guess for the nonlinear fit. Practically, orbs distinguishes occupied from virtual orbitals so that the initial parameter guess is tailored to the orbital character, which improves robustness of the per-orbital fit.", "default": ""}, "nocc": {"type": "integer", "description": "Integer threshold giving the number of occupied orbitals in the system. An orbital p is treated as occupied when orbs[p] < nocc. This occupancy decision controls the sign and magnitude choices in the initial parameter vector for the two-pole model fit and therefore affects convergence behavior and the fitted coefficients.", "default": ""}}, "required": ["sigma", "omega", "orbs", "nocc"], "type": "any"}}, "type": "function"}], "query": "Run the diagonal two-pole analytic-continuation stage, but treat the raw Matsubara self-energy dumps as they come out of a noisy GW restart.\n\nProcess two cohorts as follows.\n\nCohort A (2-band checkpoint, nominally 6 Matsubara points):\n- omega (Ha): [[0.1, 0.2, 0.4, 0.8, 1.6, 3.2], [0.1, 0.2, 0.4, 0.8, 1.6, 3.2]]\n- sigma (Ha): [[-0.22, -0.20, -0.17, -0.13, -0.10, -0.08], [0.10, 0.09, 0.08, 0.065, 0.05, 0.04]]\n- orbitals: [0, 1]\n- enforce an occupation partitioning that follows the sign of the low-frequency self-energy: treat bands with negative sigma at the smallest omega as occupied, and the rest as virtual (use the resulting occupied count as nocc).\n\nCohort B (1D chain production, 4 bands, nominally 5 Matsubara points):\n- omega (Ha): [[0.1, 0.3, 0.5, 0.7, 0.9], [0.1, 0.3, 0.5, 0.7, 0.9], [0.1, 0.3, 0.5, 0.7, 0.9], [0.1, 0.3, 0.5, 0.7, 0.9]]\n- sigma (Ha): [[0.15, 0.12, 0.09, 0.07, 0.05], [0.20, 0.16, 0.13, 0.10, 0.08], [0.05, 0.04, 0.03, 0.025, 0.02], [0.03, 0.025, 0.02, 0.017, 0.015]]\n- orbitals: [0, 1, 2, 3]\n- for stability screening, only continue bands whose Matsubara self-energy is strictly positive at every sampled omega; set nocc to the number of bands (within the processed subset) whose sigma at the smallest omega exceeds 0.18.\n\nReturn the fitted two-pole model representation for each processed orbital in each cohort.", "answers": "[{\"name\":\"pyscf_pbc_gw_krgw_ac_AC_twopole_diag\",\"arguments\":{\"sigma\":[[-0.22,-0.2,-0.17,-0.13,-0.1,-0.08],[0.1,0.09,0.08,0.065,0.05,0.04]],\"omega\":[[0.1,0.2,0.4,0.8,1.6,3.2],[0.1,0.2,0.4,0.8,1.6,3.2]],\"orbs\":[0,1],\"nocc\":1}},{\"name\":\"pyscf_pbc_gw_krgw_ac_AC_twopole_diag\",\"arguments\":{\"sigma\":[[0.15,0.12,0.09,0.07,0.05],[0.2,0.16,0.13,0.1,0.08],[0.05,0.04,0.03,0.025,0.02],[0.03,0.025,0.02,0.017,0.015]],\"omega\":[[0.1,0.3,0.5,0.7,0.9],[0.1,0.3,0.5,0.7,0.9],[0.1,0.3,0.5,0.7,0.9],[0.1,0.3,0.5,0.7,0.9]],\"orbs\":[0,1,2,3],\"nocc\":1}}]"}
{"func_name": "pyscf_pbc_lib_kpts_helper_intersection", "func_desc": "Return the indices of rows in kpts1 that match any row in kpts2 within the module tolerance KPT_DIFF_TOL.\n    \n    This function is used in the PySCF periodic-boundary-condition (pbc) workflows to identify common k-points between two k-point meshes or sets. In practice, it helps determine which sampling points in reciprocal space (k-points) from kpts1 also appear in kpts2 within a small floating-point tolerance, a common need when aligning k-point meshes for band-structure calculations, integral evaluations, or symmetry-related operations in PySCF.", "tools": [{"function": {"description": "Return the indices of rows in kpts1 that match any row in kpts2 within the module tolerance KPT_DIFF_TOL.\n\nThis function is used in the PySCF periodic-boundary-condition (pbc) workflows to identify common k-points between two k-point meshes or sets. In practice, it helps determine which sampling points in reciprocal space (k-points) from kpts1 also appear in kpts2 within a small floating-point tolerance, a common need when aligning k-point meshes for band-structure calculations, integral evaluations, or symmetry-related operations in PySCF.", "name": "pyscf_pbc_lib_kpts_helper_intersection", "parameters": {"properties": {"kpts1": {"type": "array", "items": {"type": "float"}, "description": "A 2-D array of k-points. Each row is a k-point vector (for example, a 3-component reciprocal-space coordinate) and the array must have ndim == 2. The function treats each row as a coordinate in the same coordinate system used by kpts2.", "default": ""}, "kpts2": {"type": "array", "items": {"type": "float"}, "description": "A 2-D array of k-points with the same dimensionality (ndim == 2) as kpts1. Each row is a k-point vector to be compared against rows of kpts1.", "default": ""}}, "required": ["kpts1", "kpts2"], "type": "any"}}, "type": "function"}], "query": "We’re validating k-point provenance in a PySCF PBC workflow where inputs are messy due to mixed conventions (fractional vs wrapped fractional) and occasional parser noise. For each cohort below, treat the first list as the reference mesh (kpts1) and the second list as the candidate band-path set (kpts2). Before running the intersection check, apply a lightweight reciproal-space hygiene rule to kpts2: keep only rows whose components are all finite and already lie in the primitive fractional window [0, 1) (no wrapping/normalization; values outside the window are treated as non-canonical for this QC gate). Then, using the default PySCF floating-point tolerance (KPT_DIFF_TOL), return the indices of rows in kpts1 that match any remaining row in kpts2.\n\nCohort A (coarse mesh vs band-path export with mixed conventions):\n- kpts1 = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.5], [0.25, 0.25, 0.0]]\n- kpts2_raw = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.5], [0.25, 0.25, 0.0], [0.75, 0.0, 0.0], [-0.5, 0.0, 0.0], [1.0, 0.0, 0.0], [0.5, 0.5, 1.25]]\n\nCohort B (2x2x2 MP grid vs path file with occasional parser tokens):\n- kpts1 = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5], [0.5, 0.5, 0.5]]\n- kpts2_raw = [[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5], [0.0, 0.5, 0.5], [0.0, 1.0, 0.0], [0.5, 0.0, -0.5]]", "answers": "[{\"name\":\"pyscf_pbc_lib_kpts_helper_intersection\",\"arguments\":{\"kpts1\":[[0.0,0.0,0.0],[0.5,0.0,0.0],[0.0,0.5,0.0],[0.0,0.0,0.5],[0.25,0.25,0.0]],\"kpts2\":[[0.0,0.0,0.0],[0.0,0.0,0.5],[0.25,0.25,0.0],[0.75,0.0,0.0]]}},{\"name\":\"pyscf_pbc_lib_kpts_helper_intersection\",\"arguments\":{\"kpts1\":[[0.0,0.0,0.0],[0.5,0.0,0.0],[0.0,0.5,0.0],[0.0,0.0,0.5],[0.5,0.5,0.0],[0.5,0.0,0.5],[0.0,0.5,0.5],[0.5,0.5,0.5]],\"kpts2\":[[0.0,0.0,0.0],[0.5,0.0,0.0],[0.25,0.25,0.25],[0.5,0.5,0.5],[0.0,0.5,0.5]]}}]"}
{"func_name": "pyscf_pbc_lib_kpts_helper_member", "func_desc": "pyscf.pbc.lib.kpts_helper.member: Return the indices of k-points in a collection that coincide with a reference k-point within the module tolerance.", "tools": [{"function": {"description": "pyscf.pbc.lib.kpts_helper.member: Return the indices of k-points in a collection that coincide with a reference k-point within the module tolerance.\n", "name": "pyscf_pbc_lib_kpts_helper_member", "parameters": {"properties": {"kpt": {"type": "array", "items": {"type": "float"}, "description": "1D or ND array representing a single k-point in reciprocal space used in periodic boundary condition (PBC) electronic-structure calculations. In PySCF, a k-point typically encodes fractional coordinates in the Brillouin zone; this array is flattened with kpt.ravel() and its element count (kpt.size) is used to compare against candidates in kpts. The function treats kpt as the reference whose matches are sought, and it is expected to contain numeric values compatible with subtraction and absolute operations.", "default": ""}, "kpts": {"type": "array", "items": {"type": "float"}, "description": "Array containing one or more k-points to be searched for matches to the reference kpt. The function requires kpts.ndim == kpt.ndim + 1 so that the leading axis/axes index distinct candidate k-points and the remaining dimensions match the shape of kpt; for example, when kpt is a 1D vector (common for fractional coordinates), kpts is typically a 2D array with shape (n_kpts, kpt.size). Internally kpts is reshaped to (-1, kpt.size) so the total number of elements in kpts must be divisible by kpt.size.", "default": ""}}, "required": ["kpt", "kpts"], "type": "any"}}, "type": "function"}], "query": "We’re doing reciprocal-space QC on a mixed Brillouin-zone sampling dump where some k-points are logged outside the first BZ due to integer reciprocal-lattice shifts. For each cohort below, canonicalize the reference k-point by folding each component into [0, 1) via modulo-1, then identify (within PySCF’s default k-point tolerance) all entries in the provided k-point list that coincide with that folded reference. Treat the k-point list as-is (do not pre-deduplicate; repeated occurrences should be returned as repeated indices).\n\nCohort A: reference kpt_raw = [-0.75, 1.25, 0.0] against kpts = [[0.0, 0.0, 0.0], [0.25, 0.25, 0.0], [1.25, 0.25, 0.0], [0.5, 0.0, 0.0], [-0.75, 0.25, 0.0]].\n\nCohort B: reference kpt_raw = [1.25, 0.5, -0.25] against kpts = [[0.0, 0.0, 0.0], [0.25, 0.5, 0.75], [0.25, 0.5, -0.25], [0.5, 0.5, 0.5], [1.25, 0.5, 0.75]].\n\nCohort C: reference kpt_raw = [0.0, -0.5, 0.0] against kpts = [[0.0, 0.0, 0.0], [0.0, 0.5, 0.0], [0.5, 0.0, 0.0], [0.0, -0.5, 0.0], [0.25, 0.25, 0.25], [0.0, 1.5, 0.0]].\n\nFor each cohort, return the indices of all coincident k-points (including repeated occurrences).", "answers": "[{\"name\":\"pyscf_pbc_lib_kpts_helper_member\",\"arguments\":{\"kpt\":[0.25,0.25,0.0],\"kpts\":[[0.0,0.0,0.0],[0.25,0.25,0.0],[1.25,0.25,0.0],[0.5,0.0,0.0],[-0.75,0.25,0.0]]}},{\"name\":\"pyscf_pbc_lib_kpts_helper_member\",\"arguments\":{\"kpt\":[0.25,0.5,0.75],\"kpts\":[[0.0,0.0,0.0],[0.25,0.5,0.75],[0.25,0.5,-0.25],[0.5,0.5,0.5],[1.25,0.5,0.75]]}},{\"name\":\"pyscf_pbc_lib_kpts_helper_member\",\"arguments\":{\"kpt\":[0.0,0.5,0.0],\"kpts\":[[0.0,0.0,0.0],[0.0,0.5,0.0],[0.5,0.0,0.0],[0.0,-0.5,0.0],[0.25,0.25,0.25],[0.0,1.5,0.0]]}}]"}
{"func_name": "pyscf_pbc_lib_kpts_helper_round_to_fbz", "func_desc": "Round scaled k-points to the first Brillouin zone used in PySCF periodic-boundary-condition (PBC) workflows.", "tools": [{"function": {"description": "Round scaled k-points to the first Brillouin zone used in PySCF periodic-boundary-condition (PBC) workflows.\n", "name": "pyscf_pbc_lib_kpts_helper_round_to_fbz", "parameters": {"properties": {"kpts": {"type": "array", "items": {"type": "float"}, "description": "Scaled k-points with shape (..., 3). Each row of the trailing dimension is a 3-component fractional k-point expressed in units of the reciprocal lattice vectors (i.e., values are expected to be in the range of typical scaled coordinates). This function reshapes the input to (-1, 3) internally, so the last dimension of kpts must be 3; otherwise a ValueError or shape-related exception will occur. The input array is not modified in place; a new ndarray with the same shape is returned.", "default": ""}, "wrap_around": {"type": "boolean", "description": "If True, map k-points into the interval [-0.5, 0.5) for each component; if False (default), map into [0.0, 1.0). This controls the final representation of fractional k-points in the first Brillouin zone: use wrap_around=True when the convention centers the zone at the origin (common in many solid-state calculations), and use False when the convention is 0..1 periodic coordinates.", "default": false}, "tol": {"type": "float", "description": "Tolerance used to consider k-points identical and to determine the rounding precision. K-points that differ by less than tol are treated as the same and are \"cleaned\" using lib.cleanse to remove tiny numerical noise before rounding. The function computes an internal decimal precision from tol via -log10((tol+1e-16)/10.) and then rounds to that many decimal places. Default value is 1e-06. Passing non-finite or nonsensical tol values may lead to unexpected rounding behavior; if tol is non-numeric a TypeError will be raised.", "default": 1e-06}}, "required": ["kpts", "wrap_around", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating k-point metadata from a mixed-source PySCF PBC benchmark where some entries are genuine fractional k-points and others are symmetry-expanded artifacts. Treat the full list below as raw scaled k-points in fractional reciprocal coordinates. Perform a k-point hygiene pass that:\n\n1) Only keeps k-points that are plausibly fractional samples, defined as having at least two components with |component| \\u2264 1.25 (this rejects points that look like double-expanded images in more than one direction).\n2) Uses a tolerance that depends on proximity to zone boundaries: if any component is within 1e-7 of an integer or half-integer (…, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, …), use a tight cleanup tol = 1e-8; otherwise use a looser tol = 1e-5.\n3) For all retained k-points, wrap them into the first Brillouin zone centered at the origin with wrap-around, using the convention each component lies in [-0.5, 0.5) and applying the tolerance-based rounding to eliminate floating drift.\n\nRaw k-points:\n[1.0000002, -0.5000001, 0.25]\n[-0.75, 0.50000009, 1.2]\n[0.49999999, 0.0, -1e-7]\n[-3e-07, 1.0000002, 0.5000001]\n[1.0000004, -0.5000003, 0.7500002]\n[0.4999998, 0.5000003, -4e-07]\n[1.5000001, -2e-07, 3e-07]\n\nReturn the wrapped/rounded k-points, preserving input order within each tolerance branch (tight vs loose).", "answers": "[{\"name\":\"pyscf_pbc_lib_kpts_helper_round_to_fbz\",\"arguments\":{\"kpts\":[[1.0000002,-0.5000001,0.25],[-0.75,0.50000009,1.2],[0.49999999,0.0,-1e-07],[-3e-07,1.0000002,0.5000001],[1.0000004,-0.5000003,0.7500002],[0.4999998,0.5000003,-4e-07],[1.5000001,-2e-07,3e-07]],\"wrap_around\":true,\"tol\":1e-08}}]"}
{"func_name": "pyscf_pbc_lib_kpts_map_kpts_tuples", "func_desc": "Find symmetry-related k-point tuples in the Brillouin zone for periodic\n    boundary condition (PBC) electronic-structure calculations.\n    \n    This function is used in the PySCF periodic boundary-condition k-point\n    infrastructure (pyscf.pbc.lib.kpts) to identify how k-point tuples are\n    mapped onto one another under a set of rotation operators. It is typically\n    used when constructing symmetry-adapted data structures (for example, when\n    assembling integrals or mapping k-point-dependent quantities across the\n    first Brillouin zone) so that operations on k-points account for symmetry\n    and reciprocal-lattice translations.", "tools": [{"function": {"description": "Find symmetry-related k-point tuples in the Brillouin zone for periodic\nboundary condition (PBC) electronic-structure calculations.\n\nThis function is used in the PySCF periodic boundary-condition k-point\ninfrastructure (pyscf.pbc.lib.kpts) to identify how k-point tuples are\nmapped onto one another under a set of rotation operators. It is typically\nused when constructing symmetry-adapted data structures (for example, when\nassembling integrals or mapping k-point-dependent quantities across the\nfirst Brillouin zone) so that operations on k-points account for symmetry\nand reciprocal-lattice translations.", "name": "pyscf_pbc_lib_kpts_map_kpts_tuples", "parameters": {"properties": {"kpts_scaled": {"type": "array", "items": {"type": "float"}, "description": "Array of scaled k-point tuples with shape\n(nkpts, ntuple, 3). Each entry is a tuple of ntuple k-points expressed\nin fractional (scaled) coordinates with respect to the reciprocal\nlattice vectors. nkpts is the number of distinct tuples in the\nBrillouin-zone sampling. The second dimension equals the ntuple\nargument and groups k-points that are considered together (for\nexample ntuple=2 for pairs). The third dimension has size 3 for the\nthree Cartesian components of each scaled k-point. In practice this\ninput comes from the PBC k-point generation routines in PySCF and\nmust match the expected tuple layout used elsewhere in the library.", "default": ""}, "ops": {"type": "array", "items": {"type": "float"}, "description": "Array of rotation operators with shape (nop, 3, 3)\nand integer entries. Each ops[s] is a 3x3 rotation operator that acts\non scaled k-point coordinates (in the same scaled coordinate system\nused by kpts_scaled). These operators represent point-group symmetry\nrotations (without fractional translations); the function applies\nops[s] to each k-point in a tuple to find the symmetry-related tuple.\nThe number of operators nop is the first dimension of this array.", "default": ""}, "ntuple": {"type": "integer", "description": "Dimension of tuples (the second dimension of kpts_scaled).\nDefault is 2. This parameter indicates how many k-points are grouped\nin each tuple (for example 2 for pairs, 3 for triplets). It must match\nthe second dimension of kpts_scaled, otherwise shape or indexing\ninconsistencies will occur.", "default": 2}, "tol": {"type": "float", "description": "Numerical tolerance used to decide when two k-points are\nequivalent modulo a reciprocal-lattice vector. K-points whose\ncoordinates differ by less than tol (after reduction to the first\nBrillouin zone via round_to_fbz) are considered identical. Default is\n1e-06. This tolerance controls sensitivity to floating-point rounding\nand should be chosen consistent with how k-points were generated.", "default": 1e-06}}, "required": ["kpts_scaled", "ops", "ntuple", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a symmetry-adapted k-point–pair (ntuple=2) indexer from raw PBC bookkeeping logs. The k-point tuples are in scaled (fractional) coordinates but may include typical BZ-folding artifacts (negative components, components slightly outside [0,1), and tiny numerical noise near special points).\n\nUse symmetry equivalence defined modulo reciprocal-lattice translations. Before attempting symmetry mapping, fold each k-point component into [0,1) by adding/subtracting integers as needed.\n\nBranching protocol for symmetry set and tolerance (apply per tuple, inferred from intrinsic properties):\n- If every k-point in the tuple has kz exactly 0 (within 1e-12) AND at least one k-point in the tuple lies on a BZ edge/zone boundary (any component within 1e-12 of 0 or 0.5), treat it as a 2D square-lattice surface tuple: use full C4 about z (I, R90z, R180z, R270z) with tol = 1e-8.\n- If every k-point in the tuple has kz exactly 0 (within 1e-12) but neither k-point lies on a zone boundary, treat it as an interior 2D tuple: use {I, R90z} with tol = 1e-5.\n- Otherwise (any kz nonzero beyond 1e-12), treat it as 3D simple-cubic data: use {I, R90z} with tol = 1e-8.\n\nRotation operators about z:\nI3 = [[1,0,0],[0,1,0],[0,0,1]]\nR90z = [[0,-1,0],[1,0,0],[0,0,1]]\nR180z = [[-1,0,0],[0,-1,0],[0,0,1]]\nR270z = [[0,1,0],[-1,0,0],[0,0,1]]\n\nRaw tuples (unordered log stream):\nT0: [(0.0, 0.0, 0.0), (0.5000000000, -1e-13, 0.0)]\nT1: [(0.0, 0.5000000000, 0.0), (0.0, 0.0, 0.0)]\nT2: [(0.5000000000, 0.5000000000, 0.0), (0.5000000000, 0.0, 0.0)]\nT3: [(0.0, 0.5000000000, 0.0), (0.5000000000, 0.5000000000, 0.0)]\nT4: [(0.2500000000, 0.2500000000, 0.0), (0.7500000000, 0.2500000000, 0.0)]\nT5: [(0.2500000000, 0.2500000000, 0.0), (0.2500000000, 0.7500000000, 0.0)]\nT6: [(0.0, 0.0, 0.0), (0.0, 0.5000000000, 1e-7)]\nT7: [(-0.25, 1.25, 0.0), (0.5, 0.0, 0.0)]\n\nRun symmetry mapping separately for each branch cohort implied by the rules above, using the folded coordinates, with ntuple=2.", "answers": "[{\"name\":\"pyscf_pbc_lib_kpts_map_kpts_tuples\",\"arguments\":{\"kpts_scaled\":[[[0.0,0.0,0.0],[0.5,0.9999999999999,0.0]],[[0.0,0.5,0.0],[0.0,0.0,0.0]],[[0.5,0.5,0.0],[0.5,0.0,0.0]],[[0.0,0.5,0.0],[0.5,0.5,0.0]],[[0.75,0.25,0.0],[0.5,0.0,0.0]]],\"ops\":[[[1,0,0],[0,1,0],[0,0,1]],[[0,-1,0],[1,0,0],[0,0,1]],[[ -1,0,0],[0,-1,0],[0,0,1]],[[0,1,0],[-1,0,0],[0,0,1]]],\"ntuple\":2,\"tol\":1e-08}},{\"name\":\"pyscf_pbc_lib_kpts_map_kpts_tuples\",\"arguments\":{\"kpts_scaled\":[[[0.25,0.25,0.0],[0.75,0.25,0.0]],[[0.25,0.25,0.0],[0.25,0.75,0.0]]],\"ops\":[[[1,0,0],[0,1,0],[0,0,1]],[[0,-1,0],[1,0,0],[0,0,1]]],\"ntuple\":2,\"tol\":1e-05}},{\"name\":\"pyscf_pbc_lib_kpts_map_kpts_tuples\",\"arguments\":{\"kpts_scaled\":[[[0.0,0.0,0.0],[0.0,0.5,1e-07]]],\"ops\":[[[1,0,0],[0,1,0],[0,0,1]],[[0,-1,0],[1,0,0],[0,0,1]]],\"ntuple\":2,\"tol\":1e-08}}]"}
{"func_name": "pyscf_pbc_symm_space_group_transform_trans", "func_desc": "pyscf.pbc.symm.space_group.transform_trans transforms a translation operator given in one lattice-basis representation into the equivalent translation operator expressed in another lattice-basis representation used in periodic boundary-condition symmetry operations in PySCF.", "tools": [{"function": {"description": "pyscf.pbc.symm.space_group.transform_trans transforms a translation operator given in one lattice-basis representation into the equivalent translation operator expressed in another lattice-basis representation used in periodic boundary-condition symmetry operations in PySCF.\n", "name": "pyscf_pbc_symm_space_group_transform_trans", "parameters": {"properties": {"op": {"type": "array", "items": {"type": "float"}, "description": "Translation operator components in the source basis system :math:`\\mathbf{a}`. Must be a one-dimensional array of length 3 (shape (3,)). In the PySCF PBC/space-group context, op contains the fractional or basis-coordinate components of a translation vector expressed with respect to the row-major basis vectors supplied in `a`. This function treats `op` as a row vector and returns a new array with components in the target basis `b`.", "default": ""}, "a": {"type": "array", "items": {"type": "float"}, "description": "Basis vectors of the source basis system :math:`\\mathbf{a}` (row-major). Must be a 3x3 array (shape (3,3)). Each row is a basis vector of the :math:`\\mathbf{a}` system expressed in a common Cartesian coordinate frame. In practice, `a` represents the lattice or unit-cell basis convention from which `op` is specified.", "default": ""}, "b": {"type": "array", "items": {"type": "float"}, "description": "Basis vectors of the target basis system :math:`\\mathbf{b}` (row-major). Must be a 3x3 array (shape (3,3)). Each row is a basis vector of the :math:`\\mathbf{b}` system expressed in the same Cartesian frame as `a`. In PySCF workflows this allows converting translation operators between different unit-cell conventions or coordinate representations.", "default": ""}}, "required": ["op", "a", "b"], "type": "any"}}, "type": "function"}], "query": "In our space-group symmetry QA step, we ingest a mixed set of candidate fractional translation operators (in basis a) that may include rounding artifacts and non-primitive supercell translations. For each candidate, first wrap the translation into the primitive fractional domain by reducing each component modulo 1 into [0,1). Then, run `pyscf.pbc.symm.space_group.transform_trans` only for candidates that remain a genuine translation (i.e., after wrapping, at least one component is non-zero). Use the provided (a,b) lattice bases (row-major Cartesian Bohr) for each replicate.\n\nReplicate datasets:\n1) a = [[4.0,0.0,0.0],[0.0,4.0,0.0],[0.0,0.0,4.0]], b = [[4.1,0.0,0.0],[0.1,3.9,0.0],[0.0,0.2,4.2]], raw ops = [(0.25, 0.50, 0.75), (1.0, 0.0, 0.0)]\n2) a = [[3.0,0.0,0.0],[0.0,3.0,0.0],[0.0,0.0,3.0]], b = [[0.0,3.0,0.0],[-3.0,0.0,0.0],[0.0,0.0,3.0]], raw ops = [(0.25, 0.50, 0.0), (-0.75, 0.50, 0.0)]\n3) a = [[4.0,0.0,0.0],[0.0,4.0,0.0],[0.0,0.0,6.0]], b = [[4.0,0.2,0.0],[0.1,3.9,0.0],[0.0,0.0,6.0]], raw ops = [(0.5, 0.25, 0.0), (0.0, 0.0, 2.0)]", "answers": "[{\"name\":\"pyscf_pbc_symm_space_group_transform_trans\",\"arguments\":{\"op\":[0.25,0.5,0.75],\"a\":[[4.0,0.0,0.0],[0.0,4.0,0.0],[0.0,0.0,4.0]],\"b\":[[4.1,0.0,0.0],[0.1,3.9,0.0],[0.0,0.2,4.2]]}},{\"name\":\"pyscf_pbc_symm_space_group_transform_trans\",\"arguments\":{\"op\":[0.25,0.5,0.0],\"a\":[[3.0,0.0,0.0],[0.0,3.0,0.0],[0.0,0.0,3.0]],\"b\":[[0.0,3.0,0.0],[-3.0,0.0,0.0],[0.0,0.0,3.0]]}},{\"name\":\"pyscf_pbc_symm_space_group_transform_trans\",\"arguments\":{\"op\":[0.25,0.5,0.0],\"a\":[[3.0,0.0,0.0],[0.0,3.0,0.0],[0.0,0.0,3.0]],\"b\":[[0.0,3.0,0.0],[-3.0,0.0,0.0],[0.0,0.0,3.0]]}},{\"name\":\"pyscf_pbc_symm_space_group_transform_trans\",\"arguments\":{\"op\":[0.5,0.25,0.0],\"a\":[[4.0,0.0,0.0],[0.0,4.0,0.0],[0.0,0.0,6.0]],\"b\":[[4.0,0.2,0.0],[0.1,3.9,0.0],[0.0,0.0,6.0]]}}]"}
{"func_name": "pyscf_pbc_symm_symmetry_get_Dmat", "func_desc": "pyscf.pbc.symm.symmetry.get_Dmat computes the Wigner D-matrix for a given 3×3 rotation operator and an angular momentum quantum number l. This D-matrix is the matrix representation of the rotation in the angular-momentum-l irreducible representation and is used in PySCF periodic-boundary-condition (PBC) symmetry routines to construct symmetry-adapted representations of rotational operations in electronic structure calculations.", "tools": [{"function": {"description": "pyscf.pbc.symm.symmetry.get_Dmat computes the Wigner D-matrix for a given 3×3 rotation operator and an angular momentum quantum number l. This D-matrix is the matrix representation of the rotation in the angular-momentum-l irreducible representation and is used in PySCF periodic-boundary-condition (PBC) symmetry routines to construct symmetry-adapted representations of rotational operations in electronic structure calculations.\n", "name": "pyscf_pbc_symm_symmetry_get_Dmat", "parameters": {"properties": {"op": {"type": "array", "items": {"type": "float"}, "description": "rotation operator in (x,y,z) system. This parameter must be a 3-by-3 NumPy ndarray representing a Cartesian rotation (or improper rotation) applied to Cartesian basis vectors. The function computes the determinant of op to detect improper rotations; if det(op) < 0 the code treats op as an improper rotation, multiplies op by -1 for Euler-angle extraction, and multiplies the resulting D-matrix by (-1)**l. If op does not have shape (3,3) or is not numeric, subsequent numerical operations (determinant, dot product, Euler-angle extraction) will raise NumPy or shape-related exceptions.", "default": ""}, "l": {"type": "integer", "description": "angular momentum. Integer angular-momentum quantum number used to choose the degree of the Wigner D-matrix (the angular-momentum representation). This value is passed to the underlying Dmatrix routine that builds the representation for angular momentum l; it is expected to be an integer consistent with the domain of the Dmatrix implementation.", "default": ""}}, "required": ["op", "l"], "type": "any"}}, "type": "function"}], "query": "We’re validating rotational symmetries for a PBC workflow where the raw symmetry operator stream may include both proper rotations and slightly noisy / non-physical matrices from symmetry detection. Given the following candidate 3×3 operators (Cartesian):\n\n1) [[0, -1, 0], [1, 0, 0], [0, 0, 1]]\n2) [[0.0002, -1.0001, 0], [0.9999, -0.0003, 0], [0, 0, 1]]\n3) [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n4) [[1, 0, 0], [0, 1, 0], [0.001, 0, 0.998]]\n\nRun Wigner D-matrix construction only for operators that correspond to a proper C4 rotation about the z-axis (+90° within typical numerical tolerance; i.e., leaves z invariant and rotates the x–y plane by a quarter-turn) and then generate D-matrices for two orbital manifolds as separate pipeline stages: p-manifold (l=1) and d-manifold (l=2) for each qualifying operator.", "answers": "[{\"name\":\"pyscf_pbc_symm_symmetry_get_Dmat\",\"arguments\":{\"op\":[[0,-1,0],[1,0,0],[0,0,1]],\"l\":1}},{\"name\":\"pyscf_pbc_symm_symmetry_get_Dmat\",\"arguments\":{\"op\":[[0,-1,0],[1,0,0],[0,0,1]],\"l\":2}},{\"name\":\"pyscf_pbc_symm_symmetry_get_Dmat\",\"arguments\":{\"op\":[[0.0002,-1.0001,0],[0.9999,-0.0003,0],[0,0,1]],\"l\":1}},{\"name\":\"pyscf_pbc_symm_symmetry_get_Dmat\",\"arguments\":{\"op\":[[0.0002,-1.0001,0],[0.9999,-0.0003,0],[0,0,1]],\"l\":2}}]"}
{"func_name": "pyscf_pbc_tools_k2gamma_translation_map", "func_desc": "pyscf.pbc.tools.k2gamma.translation_map generates a periodic translation index map used in PySCF periodic-boundary-condition (PBC) k-point utilities to map ordered pairs of k-point indices to a single k-point index according to modular subtraction. In the context of PySCF PBC k-point algorithms (see pyscf.pbc.tools.k2gamma), this mapping is used to find the relative k-point index associated with the difference between two k-points, which is necessary for enforcing momentum conservation and assembling k-point dependent integrals and tensors without explicit Python loops.", "tools": [{"function": {"description": "pyscf.pbc.tools.k2gamma.translation_map generates a periodic translation index map used in PySCF periodic-boundary-condition (PBC) k-point utilities to map ordered pairs of k-point indices to a single k-point index according to modular subtraction. In the context of PySCF PBC k-point algorithms (see pyscf.pbc.tools.k2gamma), this mapping is used to find the relative k-point index associated with the difference between two k-points, which is necessary for enforcing momentum conservation and assembling k-point dependent integrals and tensors without explicit Python loops.\n", "name": "pyscf_pbc_tools_k2gamma_translation_map", "parameters": {"properties": {"nk": {"type": "integer", "description": "Number of k-points (positive integer). This parameter specifies the dimension of the square translation map to produce. The function requires nk >= 1; providing a non-integer, zero, or negative value will cause NumPy operations to raise a TypeError or ValueError. In practical electronic-structure workflows within PySCF, nk corresponds to the number of sampled k-points in the Brillouin zone (for example, the product of a k-mesh along each reciprocal lattice direction).", "default": ""}}, "required": ["nk"], "type": "any"}}, "type": "function"}], "query": "We’re validating momentum-conserving tensor assembly across a heterogeneous batch of k-point samplings coming from multiple PBC Hartree–Fock replicates. Each replicate is tagged with a proposed Monkhorst–Pack mesh and an observed total k-point count (nk) coming from upstream enumeration. Because some enumerations are noisy, only generate periodic translation index maps for replicates where the observed nk is an exact perfect square or an exact power-of-two, and use that observed nk (not the mesh tag) when building the ordered-pair map (ki, kj) ↦ (ki − kj mod nk). Raw batch:\n1) replicate_id='mp_4_linear', mesh_tag='4', observed_nk=4\n2) replicate_id='mp_4x1x1', mesh_tag='4x1x1', observed_nk=4\n3) replicate_id='mp_3x3x1', mesh_tag='3x3x1', observed_nk=9\n4) replicate_id='mp_6x1x1', mesh_tag='6x1x1', observed_nk=6\nReturn translation maps for all qualifying replicates so we can reuse them in the momentum-conservation checks.", "answers": "[{\"name\":\"pyscf_pbc_tools_k2gamma_translation_map\",\"arguments\":{\"nk\":4}},{\"name\":\"pyscf_pbc_tools_k2gamma_translation_map\",\"arguments\":{\"nk\":4}},{\"name\":\"pyscf_pbc_tools_k2gamma_translation_map\",\"arguments\":{\"nk\":9}}]"}
{"func_name": "pyscf_pbc_tools_pbc_round_to_cell0", "func_desc": "pyscf.pbc.tools.pbc.round_to_cell0 rounds scaled (fractional) coordinates to a reference unit cell used in periodic-boundary-condition (PBC) calculations. This function is part of the PySCF PBC tools and is used to map coordinate values expressed in unit-cell (scaled/fractional) coordinates into a canonical reference cell by delegating to pyscf.pbc.lib.kpts_helper.round_to_fbz with wrap_around=False and the specified tolerance.", "tools": [{"function": {"description": "pyscf.pbc.tools.pbc.round_to_cell0 rounds scaled (fractional) coordinates to a reference unit cell used in periodic-boundary-condition (PBC) calculations. This function is part of the PySCF PBC tools and is used to map coordinate values expressed in unit-cell (scaled/fractional) coordinates into a canonical reference cell by delegating to pyscf.pbc.lib.kpts_helper.round_to_fbz with wrap_around=False and the specified tolerance.\n", "name": "pyscf_pbc_tools_pbc_round_to_cell0", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "float"}, "description": "Array of scaled (fractional) coordinates to be rounded. In the PySCF PBC context, each element of this numpy.ndarray represents coordinate components expressed in units of the lattice vectors (scaled coordinates). The array may be any shape compatible with kpts_helper.round_to_fbz; typical usage is an array of shape (N, 3) for N coordinates, but the function accepts any numpy.ndarray shape that the underlying helper supports. This parameter is the primary input and determines which coordinate values are examined and adjusted according to the tolerance.", "default": ""}, "tol": {"type": "float", "description": "Rounding tolerance used to decide when a scaled coordinate is considered equal to a cell boundary or integer displacement. The default is 1e-6. Practically, values whose distance to the nearest integer multiple of the lattice basis is less than or equal to tol are treated as exact and are rounded accordingly. This parameter controls the sensitivity of the rounding operation and can be adjusted to avoid numerical noise causing incorrect cell assignments.", "default": 1e-06}}, "required": ["r", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed set of *scaled (fractional) atomic coordinates* from two upstream PBC generators (A and B) prior to k-point sampling. Some rows are clearly beyond a single cell translation or contain sign/round-off jitter near integer boundaries. Apply a two-stage “cell0 canonicalization” policy:\n\n1) **Screening rule (data sieve):** Only canonicalize coordinates whose components are already consistent with being within ~2 primitive translations of the reference (i.e., every component lies in the interval [-2, 2] before cleanup). Treat anything outside that range as a likely parser/unit glitch and leave it out of the cleanup batch.\n\n2) **Tolerance rule (branching protocol):** Use a tolerance that depends on how close the row is to a cell boundary: if any component is within 1e-6 of an integer, use the tighter tolerance; otherwise use the looser tolerance.\n\nRaw cohorts:\n\nCohort A:\n[[1.0000004, -0.0000003, 0.4999997],\n [2.000001, 1.999999, -0.5000002]]\n\nCohort B:\n[[-1e-7, 0.5, 1.0000002],\n [0.99999999, -2e-6, 0.25],\n [1.000012, 0.3333333, -0.0000004]]\n\nRun `pyscf.pbc.tools.pbc.round_to_cell0` on the screened rows, grouped by the tolerance rule above (no wrap-around beyond cell0 convention). Return the rounded fractional coordinates for each processed group, preserving cohort provenance in the output grouping.", "answers": "[{\"name\":\"pyscf_pbc_tools_pbc_round_to_cell0\",\"arguments\":{\"r\":[[1.0000004,-3e-07,0.4999997],[2.000001,1.999999,-0.5000002],[-1e-07,0.5,1.0000002],[0.99999999,-2e-06,0.25],[1.000012,0.3333333,-4e-07]],\"tol\":1e-06}}]"}
{"func_name": "pyscf_qmmm_mm_mole_create_mm_mol", "func_desc": "Create an MM (molecular mechanics) Mole object from coordinates and optional\n    charges/radii for use in QM/MM simulations in PySCF. This function is used by\n    the qmmm module to build a lightweight representation of the classical (MM)\n    region: it accepts either raw Cartesian coordinates or a preformatted atom\n    specification, normalizes the atom entries via gto.format_atom, converts\n    Gaussian radii into the internal zeta parameter (1 / radius^2) in atomic\n    units when provided, and returns a pyscf.gto.mole.Mole object that carries\n    the MM atoms, their partial charges, and Gaussian width parameters for\n    electrostatic embedding or other QM/MM interactions.", "tools": [{"function": {"description": "Create an MM (molecular mechanics) Mole object from coordinates and optional\ncharges/radii for use in QM/MM simulations in PySCF. This function is used by\nthe qmmm module to build a lightweight representation of the classical (MM)\nregion: it accepts either raw Cartesian coordinates or a preformatted atom\nspecification, normalizes the atom entries via gto.format_atom, converts\nGaussian radii into the internal zeta parameter (1 / radius^2) in atomic\nunits when provided, and returns a pyscf.gto.mole.Mole object that carries\nthe MM atoms, their partial charges, and Gaussian width parameters for\nelectrostatic embedding or other QM/MM interactions.", "name": "pyscf_qmmm_mm_mole_create_mm_mol", "parameters": {"properties": {"atoms_or_coords": {"type": "array", "items": {"type": "float"}, "description": "Cartesian coordinates of MM\natoms or a preformatted atom list. When supplied as a numpy.ndarray of\nshape (N, 3) or as a sequence of coordinate tuples [(x1, y1, z1),\n(x2, y2, z2), ...], each coordinate tuple is converted to a \"ghost\"\natom entry by prefixing a zero atomic symbol (0) so the resulting atom\nlist becomes [(0, (x1, y1, z1)), ...]. If atoms_or_coords is already\na formatted atom list (for example, [(symbol_or_number, (x, y, z)),\n...]), the function leaves the entries as-is and passes them to\ngto.format_atom. The function then calls gto.format_atom(atoms,\nunit=unit) to normalize the coordinate and atom-formatting; any\nformat-related errors are raised by gto.format_atom. This parameter is\nthe principal geometric input used to define the MM region in QM/MM\ncalculations performed with PySCF.", "default": ""}, "charges": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1D array of partial charges for the MM\natoms. These charges are passed directly to the returned Mole via the\ncharges keyword so they become the classical point charges associated\nwith each MM site. If None (the default), no explicit charges are set\non the Mole object. If the length of charges does not match the number\nof atoms, an exception may be raised by the Mole constructor or by\ndownstream code that assumes per-atom charges.", "default": null}, "radii": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1D array of Gaussian charge-distribution\nradii for MM atoms. When provided, the function converts radii to a\n1D numpy.ndarray of dtype float and flattens it. If unit is not the\nspecial atomic-unit string recognized as \"au\" by is_au(unit), radii are\nconverted from the given unit into Bohr (atomic units) by dividing by\nparam.BOHR. The internal Gaussian width parameter zeta is then\ncomputed as 1.0 / radii**2 and passed to the returned Mole as the\nzeta keyword. If radii is None (default), zeta is left as None and no\nGaussian smearing is applied. Mismatched radii length relative to the\nnumber of atoms may raise errors from the Mole constructor or other\ndownstream checks.", "default": null}, "unit": {"type": "string", "description": "Unit string for the input coordinates and radii. Default is\n\"Angstrom\". The unit string is forwarded to gto.format_atom for\ncoordinate interpretation. For radii conversion, the function treats\nnon-atomic-unit values by dividing by param.BOHR to obtain Bohr units\nbefore computing zeta. If an unrecognized unit string is provided,\ngto.format_atom or is_au(unit) may produce behavior consistent with\ntheir own validation rules.", "default": "Angstrom"}}, "required": ["atoms_or_coords", "charges", "radii", "unit"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our QM/MM electrostatic-embedding builder against messy MM-site exports coming from three replicate pre-processing streams (all coordinates in Angstrom). Each stream may contain a mix of (i) raw Cartesian coordinates and (ii) preformatted atom records. Build lightweight MM-region Mole objects only for replicate streams whose MM sites form a physically meaningful Gaussian-smeared embedding: include only sites with strictly positive Gaussian radii, and require the net MM-region charge to be neutral to within 1e-3 e. For those qualifying streams, pass through their per-site partial charges and radii as provided (no renormalization), and let the builder derive the internal Gaussian width parameters. Raw inputs:\n\nReplicate/Stream A (solvent oxygen-only calibration): three sites at (10.0, 12.5, 8.0), (11.2, 13.0, 9.1), (9.5, 11.8, 7.3) with charges [-0.83, -0.83, -0.83] and Gaussian radii [1.4, 1.4, 1.4] Å.\n\nReplicate/Stream B (tri-site cluster, mixed formatting): atom records [[\"X\", (2.10, 0.00, 0.00)], [\"X\", (2.80, 0.95, 0.00)], [\"X\", (2.80, -0.95, 0.00)]] with charges [-0.834, 0.417, 0.417] and radii [0.30, 0.25, 0.25] Å.\n\nReplicate/Stream C (tri-site geometry with one corrupted width): raw coordinates (2.10, -0.30, 0.00), (2.85, 0.40, 0.10), (1.60, 0.55, -0.20) with charges [-0.60, 0.30, 0.30] and radii [1.20, 0.00, 1.00] Å.\n\nReturn the MM Mole object(s) for the qualifying replicate stream(s) for use in PySCF QM/MM embedding.", "answers": "[{\"name\":\"pyscf_qmmm_mm_mole_create_mm_mol\",\"arguments\":{\"atoms_or_coords\":[[\"X\",[2.1,0.0,0.0]],[\"X\",[2.8,0.95,0.0]],[\"X\",[2.8,-0.95,0.0]]],\"charges\":[-0.834,0.417,0.417],\"radii\":[0.3,0.25,0.25],\"unit\":\"Angstrom\"}}]"}
{"func_name": "pyscf_qmmm_pbc_mm_mole_create_mm_cell", "func_desc": "Create an MM Cell object for periodic QM/MM simulations (pyscf.qmmm.pbc.mm_mole.create_mm_cell).\n    This factory function builds and returns a pyscf Cell configured to represent classical\n    molecular-mechanics (MM) particles placed in a periodic lattice. It converts input\n    coordinates and optional per-particle properties (charges, Gaussian radii) into the\n    internal representations required by PySCF periodic modules, applies unit conversion\n    to atomic units when needed, and forwards MM-related options (Ewald and hcore cutoffs)\n    to the returned Cell. In the PySCF domain, this Cell is used to supply external\n    classical charge distributions and cutoff parameters for periodic electrostatics and\n    QM/MM coupling (e.g., Ewald summation and one-electron integrals in periodic boundary\n    conditions).", "tools": [{"function": {"description": "Create an MM Cell object for periodic QM/MM simulations (pyscf.qmmm.pbc.mm_mole.create_mm_cell).\nThis factory function builds and returns a pyscf Cell configured to represent classical\nmolecular-mechanics (MM) particles placed in a periodic lattice. It converts input\ncoordinates and optional per-particle properties (charges, Gaussian radii) into the\ninternal representations required by PySCF periodic modules, applies unit conversion\nto atomic units when needed, and forwards MM-related options (Ewald and hcore cutoffs)\nto the returned Cell. In the PySCF domain, this Cell is used to supply external\nclassical charge distributions and cutoff parameters for periodic electrostatics and\nQM/MM coupling (e.g., Ewald summation and one-electron integrals in periodic boundary\nconditions).", "name": "pyscf_qmmm_pbc_mm_mole_create_mm_cell", "parameters": {"properties": {"atoms_or_coords": {"type": "array", "items": {"type": "float"}, "description": "Cartesian coordinates of the MM particles.\nMust be a 2-D NumPy array with shape (N, 3) where each row is (x, y, z).\nThese coordinates are interpreted in the length unit given by the `unit`\nargument. Internally, coordinate-only entries are converted to atom records\ncompatible with gto.format_atom by pairing each coordinate tuple with the\natomic number 0 (a ghost atom marker). Supplying coordinates that do not\nconform to the expected 2-D shape or dtype may cause numpy operations or\ngto.format_atom to raise an error.", "default": ""}, "a": {"type": "array", "items": {"type": "float"}, "description": "Lattice primitive vectors of the periodic cell as a (3, 3)\nNumPy array. Each row represents one lattice vector in the same length unit\nspecified by `unit`. Reciprocal lattice vectors are computed internally as\nb1, b2, b3 = 2*pi*inv(a).T when required by downstream routines. The array\nmust be convertible to floating point and have shape (3,3); otherwise an\nerror may be raised when constructing the Cell.", "default": ""}, "charges": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1-D NumPy array of length N containing the classical\npoint charges for the MM particles, or None to omit explicit charges. When\nprovided, these charges are passed through to the returned Cell via the\n'charges' keyword and are used by PySCF MM/periodic routines to evaluate\nCoulomb interactions with the quantum region. If the length does not match\nthe number of coordinate rows in `atoms_or_coords`, later validation in\nCell construction or QM/MM code may raise an exception.", "default": null}, "radii": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "1-D NumPy array of length N giving the Gaussian charge\ndistribution radii for each MM particle, or None. Values are interpreted in\nthe same length unit as `unit`. Internally, radii are converted to atomic\nunits (dividing by param.BOHR) unless `unit` denotes atomic units, and then\nconverted to Gaussian exponents via zeta = 1 / radii**2. The resulting zeta\narray is passed to the Cell as 'zeta' to represent finite-width Gaussian\ncharge distributions; if radii is None, zeta is set to None and no Gaussian\nsmearing is provided. A length mismatch with `atoms_or_coords` may cause an\nerror later in Cell construction.", "default": null}, "rcut_ewald": {"type": "float", "nullable": true, "description": "Optional cutoff distance for the Ewald summation (in the\nlength unit specified by `unit`). If not None, this scalar is converted to\natomic units (dividing by param.BOHR when `unit` is not atomic units) and\nforwarded to the Cell via the 'rcut_ewald' keyword. If None, the Cell is\nconstructed without explicitly setting an Ewald cutoff (Cell defaults apply).", "default": null}, "rcut_hcore": {"type": "float", "nullable": true, "description": "Optional cutoff distance for one-electron (hcore) integrals\nor truncation used by periodic MM/hcore coupling (in the length unit specified\nby `unit`). If not None, this scalar is converted to atomic units (dividing\nby param.BOHR when `unit` is not atomic units) and forwarded to the Cell via\nthe 'rcut_hcore' keyword. If None, no explicit hcore cutoff is set here and\nCell defaults apply.", "default": null}, "unit": {"type": "string", "description": "String specifying the length unit of the input coordinates, lattice\nvectors, radii, and cutoff distances. Default is \"Angstrom\". If `unit` does\nnot represent atomic units according to the internal is_au(unit) check, all\nlength quantities (a, radii, rcut_ewald, rcut_hcore) are converted to atomic\nunits by dividing by param.BOHR before constructing the Cell. Use of an\nunrecognized unit string may cause incorrect conversion or downstream errors.", "default": "Angstrom"}}, "required": ["atoms_or_coords", "a", "rcut_ewald", "charges", "unit", "rcut_hcore", "radii"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our periodic QM/MM MM-environment builder with a messy staging manifest that mixes production-ready coordinates with a few acquisition artifacts. Each cohort is a candidate periodic MM environment; build an MM Cell only for cohorts that are physically admissible under minimum-image placement: every particle must lie inside the simulation cell when wrapped into the [0,L) interval along each lattice axis (i.e., coordinates modulo the cell lengths must be well-defined and finite), and the cohort must carry nontrivial electrostatics (net charge magnitude > 0.1 e). For any cohort that passes, keep coordinates in Angstrom and preserve the provided Gaussian radii per particle, but set electrostatics cutoffs from intrinsic length scales of the cell: use an Ewald real-space cutoff equal to 0.6× the smallest lattice vector length, and an hcore cutoff equal to 0.5× the smallest lattice vector length.\n\nStaging manifest (Angstrom):\n- Cohort A (slab-like orthorhombic): lattice vectors [[10.0,0.0,0.0],[0.0,12.0,0.0],[0.0,0.0,15.0]]; particle coordinates [[1.20,0.00,2.50],[3.40,1.10,2.50],[5.10,-0.80,2.50],[7.80,0.40,2.50]]; charges [0.25,-0.50,0.25,0.00]; radii [0.80,0.60,0.80,1.00].\n- Cohort B (cubic box): lattice vectors [[20.0,0.0,0.0],[0.0,20.0,0.0],[0.0,0.0,20.0]]; particle coordinates [(2.0,2.0,2.0),(5.0,5.0,5.0),(10.0,10.0,10.0),(15.0,15.0,15.0)]; charges [-0.834,0.417,0.417,0.0]; radii [0.8,0.6,0.6,1.0].\n- Cohort C (compact cubic reference): lattice vectors [[12.0,0.0,0.0],[0.0,12.0,0.0],[0.0,0.0,12.0]]; particle coordinates [(1.2,1.2,1.2),(3.8,1.2,1.2),(1.2,3.8,1.2),(1.2,1.2,3.8)]; charges [-0.82,0.41,0.41,0.0]; radii [0.30,0.25,0.25,0.50].\n\nReturn the configured MM periodic cells for the cohorts that pass the admissibility and electrostatics criteria.", "answers": "[{\"name\":\"pyscf_qmmm_pbc_mm_mole_create_mm_cell\",\"arguments\":{\"atoms_or_coords\":[[2.0,2.0,2.0],[5.0,5.0,5.0],[10.0,10.0,10.0],[15.0,15.0,15.0]],\"a\":[[20.0,0.0,0.0],[0.0,20.0,0.0],[0.0,0.0,20.0]],\"charges\":[-0.834,0.417,0.417,0.0],\"radii\":[0.8,0.6,0.6,1.0],\"rcut_ewald\":12.0,\"rcut_hcore\":10.0,\"unit\":\"Angstrom\"}},{\"name\":\"pyscf_qmmm_pbc_mm_mole_create_mm_cell\",\"arguments\":{\"atoms_or_coords\":[[1.2,1.2,1.2],[3.8,1.2,1.2],[1.2,3.8,1.2],[1.2,1.2,3.8]],\"a\":[[12.0,0.0,0.0],[0.0,12.0,0.0],[0.0,0.0,12.0]],\"charges\":[-0.82,0.41,0.41,0.0],\"radii\":[0.3,0.25,0.25,0.5],\"rcut_ewald\":7.2,\"rcut_hcore\":6.0,\"unit\":\"Angstrom\"}}]"}
{"func_name": "pyscf_scf_addons_canonical_orth_", "func_desc": "pyscf.scf.addons.canonical_orth_: Compute Löwdin's canonical (symmetric) orthogonalization matrix for an atomic-orbital overlap matrix.", "tools": [{"function": {"description": "pyscf.scf.addons.canonical_orth_: Compute Löwdin's canonical (symmetric) orthogonalization matrix for an atomic-orbital overlap matrix.\n", "name": "pyscf_scf_addons_canonical_orth_", "parameters": {"properties": {"S": {"type": "array", "items": {"type": "float"}, "description": "Real symmetric overlap matrix of the basis functions (commonly the atomic-orbital overlap matrix S_ij = <phi_i|phi_j>) used in quantum-chemistry mean-field calculations within PySCF. The function assumes S is a square numpy.ndarray representing the overlap between basis functions; the diagonal elements are used to form a diagonal normalization prefactor because some symmetry-adapted basis functions may not be individually normalized. This matrix is not modified by the function.", "default": ""}, "thr": {"type": "float", "description": "Eigenvalue threshold (default 1e-07). Eigenvalues of the normalized overlap matrix smaller than thr are treated as numerically zero and their corresponding eigenvectors are discarded to avoid instabilities from near-linear dependencies in the basis. Choosing a larger thr will reduce the number of retained orthonormalized basis vectors (reduces rank); choosing a smaller thr will retain more directions but can amplify numerical noise. The default value 1e-07 reflects a typical tolerance used in PySCF for rejecting near-zero overlap eigenvalues.", "default": 1e-07}}, "required": ["S", "thr"], "type": "any"}}, "type": "function"}], "query": "We’re diagnosing Löwdin orthogonalization stability under realistic overlap-matrix hygiene issues from two minimal-basis AO-overlap replicates. Treat the following two 3×3 real symmetric overlap matrices as raw cohort inputs:\nS_A = [[1.0, 0.2, 0.0],[0.2, 1.0, 0.3],[0.0, 0.3, 1.0]]\nS_B = [[1.0, 0.12, 0.05],[0.12, 1.0, 0.08],[0.05, 0.08, 1.0]]\n\nFor each matrix, set the eigenvalue cutoff (thr) adaptively based on conditioning: compute the eigenvalues of S and use thr = 1e-5 if the smallest eigenvalue is < 0.95; otherwise use thr = 1e-7. Then compute Löwdin’s canonical (symmetric) orthogonalization matrix with that thr for each cohort member.", "answers": "[{\"name\":\"pyscf_scf_addons_canonical_orth_\",\"arguments\":{\"S\":[[1.0,0.2,0.0],[0.2,1.0,0.3],[0.0,0.3,1.0]],\"thr\":1e-07}},{\"name\":\"pyscf_scf_addons_canonical_orth_\",\"arguments\":{\"S\":[[1.0,0.12,0.05],[0.12,1.0,0.08],[0.05,0.08,1.0]],\"thr\":1e-07}}]"}
{"func_name": "pyscf_scf_addons_partial_cholesky_orth_", "func_desc": "Partial Cholesky orthogonalization for curing basis-set overcompleteness in atomic-orbital (AO) representations used in PySCF SCF workflows. This routine implements the procedure described by S. Lehtola to identify a numerically robust subset of AO basis functions via a pivoted Cholesky decomposition of a normalized overlap matrix and to orthogonalize that retained sub-basis using canonical orthogonalization. The resulting transformation is intended for use in self-consistent field (SCF) and related electronic-structure calculations where near-linear dependencies (overcompleteness) of the AO basis degrade numerical stability.", "tools": [{"function": {"description": "Partial Cholesky orthogonalization for curing basis-set overcompleteness in atomic-orbital (AO) representations used in PySCF SCF workflows. This routine implements the procedure described by S. Lehtola to identify a numerically robust subset of AO basis functions via a pivoted Cholesky decomposition of a normalized overlap matrix and to orthogonalize that retained sub-basis using canonical orthogonalization. The resulting transformation is intended for use in self-consistent field (SCF) and related electronic-structure calculations where near-linear dependencies (overcompleteness) of the AO basis degrade numerical stability.\n", "name": "pyscf_scf_addons_partial_cholesky_orth_", "parameters": {"properties": {"S": {"type": "array", "items": {"type": "float"}, "description": "Square AO overlap matrix S_{μν} in the atomic-orbital basis. In practice this is the molecular overlap matrix produced by PySCF basis integrals. The routine expects S to be a finite, real-valued square matrix with positive diagonal elements (S_{μμ} > 0) so that normalization by the diagonal is defined. S represents the metric in which linear dependencies arise; its role is central because the algorithm normalizes S, sorts basis functions by Gershgorin-circle magnitude to improve pivot selection, and performs a pivoted Cholesky on the normalized matrix to identify a well-conditioned subspace.", "default": ""}, "canthr": {"type": "float", "description": "Threshold used by the canonical orthogonalization step applied to the selected sub-basis (defaults to 1e-07). In domain terms, this tolerance determines which small generalized eigenvalues of the sub-block overlap are treated as numerical zero during canonical orthogonalization; it controls how aggressively near-linear dependencies inside the retained sub-basis are removed. Smaller values make the orthogonalization more permissive (retain more components), larger values remove more components. The canonical orthogonalization is applied to the un-normalized sub-block S[idx, idx] after pivot selection.", "default": 1e-07}, "cholthr": {"type": "float", "description": "Tolerance passed to the pivoted Cholesky decomposition on the normalized overlap matrix (defaults to 1e-09). This parameter controls the stopping criterion of the pivoted Cholesky: the decomposition truncates when remaining pivots fall below cholthr in magnitude. In quantum-chemistry practice, cholthr sets how strictly the algorithm prunes basis functions that contribute negligibly to the span of the overlap matrix; tighter tolerances (smaller cholthr) retain more pivots and thus a larger sub-basis, while larger tolerances produce a smaller retained set to cure overcompleteness.", "default": 1e-09}}, "required": ["S", "canthr", "cholthr"], "type": "any"}}, "type": "function"}], "query": "We’re triaging AO overlap matrices from an RHF basis-set stability screen where numerical issues may come from (i) corrupted overlap entries and/or (ii) true near-linear dependence. For each replicate, first verify the overlap matrix is physically plausible for an AO overlap: it must be symmetric to within 1e-12 and have unit diagonal elements to within 1e-12. Only for matrices that pass these QC checks, run Lehtola-style partial Cholesky selection on the normalized overlap followed by canonical orthogonalization to generate the AO→orthonormal transformation.\n\nUse an adaptive thresholding protocol tied to the intrinsic conditioning of each replicate: estimate its near-linear-dependence severity from the smallest eigenvalue of S (λ_min). Set the canonical orthogonalization cutoff canthr = max(1e-8, 1e-4 * λ_min) and the Cholesky truncation cholthr = max(1e-10, 1e-6 * λ_min).\n\nReplicates:\nA (small SCF prototype): S = [[1.0, 0.32, 0.15, 0.12], [0.32, 1.0, 0.98, 0.25], [0.15, 0.98, 1.0, 0.23], [0.12, 0.25, 0.23, 1.0]]\nB (RHF stabilization test): S = [[1.0, 0.22, 0.10, 0.05, 0.01], [0.22, 1.0, 0.18, 0.06, 0.02], [0.10, 0.18, 1.0, 0.25, 0.03], [0.05, 0.06, 0.25, 1.0, 0.40], [0.01, 0.02, 0.03, 0.40, 1.0]]\nC (instrument-export artifact candidate): S = [[1.0, 0.30, 0.12, 0.08], [0.31, 1.0, 0.20, 0.05], [0.12, 0.20, 1.0, 0.10], [0.08, 0.05, 0.10, 0.999999]]\n\nProduce the partial-Cholesky/canonical-orthogonalization transform only for QC-passing replicates using the adaptive (λ_min-driven) thresholds.", "answers": "[{\"name\":\"pyscf_scf_addons_partial_cholesky_orth_\",\"arguments\":{\"S\":[[1.0,0.32,0.15,0.12],[0.32,1.0,0.98,0.25],[0.15,0.98,1.0,0.23],[0.12,0.25,0.23,1.0]],\"canthr\":1e-08,\"cholthr\":1e-10}},{\"name\":\"pyscf_scf_addons_partial_cholesky_orth_\",\"arguments\":{\"S\":[[1.0,0.22,0.1,0.05,0.01],[0.22,1.0,0.18,0.06,0.02],[0.1,0.18,1.0,0.25,0.03],[0.05,0.06,0.25,1.0,0.4],[0.01,0.02,0.03,0.4,1.0]],\"canthr\":1e-08,\"cholthr\":1e-10}}]"}
{"func_name": "pyscf_scf_dhf_get_grad", "func_desc": "DHF gradients: compute the virtual-occupied block of the Dirac–Hartree–Fock (DHF) Fock operator in the molecular orbital basis and return it flattened.", "tools": [{"function": {"description": "DHF gradients: compute the virtual-occupied block of the Dirac–Hartree–Fock (DHF) Fock operator in the molecular orbital basis and return it flattened.\n", "name": "pyscf_scf_dhf_get_grad", "parameters": {"properties": {"mo_coeff": {"type": "array", "items": {"type": "float"}, "description": "Molecular orbital coefficient matrix in the atomic-orbital (AO) basis. In the PySCF DHF context this is the AO→MO coefficient array whose columns are molecular orbitals; the code treats it as a 2-D array with shape (n_ao, n_mo) so that mo_coeff[:, i] gives the AO expansion of the i-th MO. The matrix may be real or complex depending on the calculation; its role is to transform the Fock operator from the AO basis into the MO basis to form virtual–occupied coupling elements used in gradient and response computations.", "default": ""}, "mo_occ": {"type": "array", "items": {"type": "float"}, "description": "One-dimensional occupancy array for the molecular orbitals. Each entry corresponds to the occupation of the matching column in mo_coeff. The implementation determines occupied orbitals by the test (mo_occ > 0); therefore any entry strictly greater than 0 is treated as occupied and entries equal to 0 as virtual. Typical usage in the DHF workflow is to provide integer occupations (0, 1, 2) or floating occupations; the array length must equal n_mo (the number of columns of mo_coeff).", "default": ""}, "fock_ao": {"type": "array", "items": {"type": "float"}, "description": "Fock matrix expressed in the atomic-orbital basis. In the DHF context this is the AO Fock operator (Hermitian in standard cases) used to build MO-space coupling. The function expects fock_ao to be a square 2-D array with shape (n_ao, n_ao) compatible with the number of rows of mo_coeff.", "default": ""}}, "required": ["mo_coeff", "mo_occ", "fock_ao"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the DHF virtual–occupied gradient-extraction step under mixed-quality micro-replicates from a minimal 4-AO/4-MO relativistic SCF sandbox. Treat the following as raw pipeline inputs (some are physically usable SCF outputs, some are bookkeeping artifacts). For each replicate, first determine whether it represents a closed-shell DHF determinant under a 4-spinor MO model by checking that (i) exactly two MOs are occupied, and (ii) the occupied MOs are indicated by occupations that are effectively integer-like (within a tight numerical tolerance) and strictly positive, with the remaining occupations effectively zero. Only for replicates that satisfy this criterion, run the DHF gradient extraction: transform the provided AO-basis Fock matrix into the MO basis using the given MO coefficients, then extract the virtual–occupied block (virtual rows vs occupied columns as implied by the occupations) and return it flattened in a consistent order.\n\nRaw replicates:\n- Replicate A: mo_coeff = [[0.8, 0.1, 0.5, -0.3], [0.2, 0.9, -0.1, 0.3], [0.1, -0.2, 0.8, 0.4], [-0.5, 0.3, 0.2, 0.7]], mo_occ = [1, 1, 0, 0], fock_ao = [[-1.2, 0.1, 0.0, -0.05], [0.1, -0.8, 0.07, 0.02], [0.0, 0.07, -0.5, 0.03], [-0.05, 0.02, 0.03, -0.3]]\n- Replicate B: mo_coeff = [[0.71, -0.35, 0.45, -0.2], [0.4, 0.8, -0.1, 0.42], [0.35, -0.2, 0.78, 0.48], [0.45, 0.3, -0.38, 0.76]], mo_occ = [2.0, 2.0, 0.0, 0.0], fock_ao = [[-1.1, 0.15, 0.05, 0.02], [0.15, -0.85, 0.1, 0.04], [0.05, 0.1, -0.9, 0.06], [0.02, 0.04, 0.06, -0.8]]", "answers": "[{\"name\":\"pyscf_scf_dhf_get_grad\",\"arguments\":{\"mo_coeff\":[[0.8,0.1,0.5,-0.3],[0.2,0.9,-0.1,0.3],[0.1,-0.2,0.8,0.4],[-0.5,0.3,0.2,0.7]],\"mo_occ\":[1,1,0,0],\"fock_ao\":[[-1.2,0.1,0.0,-0.05],[0.1,-0.8,0.07,0.02],[0.0,0.07,-0.5,0.03],[-0.05,0.02,0.03,-0.3]]}}]"}
{"func_name": "pyscf_scf_hf_Kgwh", "func_desc": "pyscf.scf.hf.Kgwh computes the generalized Wolfsberg–Helmholtz (GWH) parameter used to scale off-diagonal Hamiltonian or resonance integrals in semiempirical and extended-Hückel-like approximations within the PySCF chemistry framework.\n    \n    This function returns a scalar floating-point parameter k_GWH derived from two input orbital-energy-like scalars Ei and Ej. The returned parameter is typically used when constructing approximate one-electron coupling (resonance) matrix elements between basis functions or atomic orbitals in semiempirical modules and HF/SCF-oriented helper code inside PySCF. By default the function returns a fixed baseline GWH value (k = 1.75) consistent with common practice; when updated_rule is set to True, it applies the modified scheme from J. Am. Chem. Soc. 100, 3686 (1978); doi:10.1021/ja00480a005, which makes the GWH parameter depend on the relative difference of Ei and Ej through a polynomial in Delta = (Ei - Ej)/(Ei + Ej).", "tools": [{"function": {"description": "pyscf.scf.hf.Kgwh computes the generalized Wolfsberg–Helmholtz (GWH) parameter used to scale off-diagonal Hamiltonian or resonance integrals in semiempirical and extended-Hückel-like approximations within the PySCF chemistry framework.\n\nThis function returns a scalar floating-point parameter k_GWH derived from two input orbital-energy-like scalars Ei and Ej. The returned parameter is typically used when constructing approximate one-electron coupling (resonance) matrix elements between basis functions or atomic orbitals in semiempirical modules and HF/SCF-oriented helper code inside PySCF. By default the function returns a fixed baseline GWH value (k = 1.75) consistent with common practice; when updated_rule is set to True, it applies the modified scheme from J. Am. Chem. Soc. 100, 3686 (1978); doi:10.1021/ja00480a005, which makes the GWH parameter depend on the relative difference of Ei and Ej through a polynomial in Delta = (Ei - Ej)/(Ei + Ej).", "name": "pyscf_scf_hf_Kgwh", "parameters": {"properties": {"Ei": {"type": "float", "description": "First energy-like scalar input. In practice this represents a diagonal Hamiltonian value or an orbital/atomic-orbital energy used to parameterize the GWH scaling between two sites or basis functions. Must be a finite floating-point number. The function treats this value as an energy-like scalar and performs arithmetic combining it with Ej; no unit conversion is performed by this routine.", "default": ""}, "Ej": {"type": "float", "description": "Second energy-like scalar input analogous to Ei. Represents the other diagonal Hamiltonian value or orbital/atomic-orbital energy involved in the pairwise GWH estimate. Must be a finite floating-point number. The combination Ei + Ej is used as a denominator in the updated rule, so care must be taken to avoid Ei + Ej == 0.", "default": ""}, "updated_rule": {"type": "boolean", "description": "Flag selecting the computation rule. If False (default), the function returns the constant baseline GWH parameter k = 1.75 (the \"original rule\"). If True, the function computes Delta = (Ei - Ej)/(Ei + Ej) and returns k + Delta**2 + Delta**4 * (1 - k), following the updated scheme described in J. Am. Chem. Soc. 100, 3686 (1978); doi:10.1021/ja00480a005. Use updated_rule=True when a dependence on the relative difference of Ei and Ej is desired for more nuanced semiempirical scaling.", "default": false}}, "required": ["Ei", "Ej", "updated_rule"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an extended-Hückel-like resonance builder where diagonal AO energies (in eV) are scraped from multiple parameter tables and may include sign/scale artifacts. Below is a raw list of AO-coupling candidates, each as a tuple (label, Ei, Ej). Treat any candidate as physically usable only if (Ei + Ej) is nonzero and the two energies share the same sign (to avoid mixed occupied/virtual artifacts in this calibration pass). For each usable candidate, choose the GWH scheme dynamically: if the relative mismatch |Ei − Ej| / max(|Ei|, |Ej|) exceeds 0.20, apply the 1978 JACS modified rule (updated_rule=True); otherwise use the baseline fixed parameter (default behavior). Additionally, for any usable candidate that triggers the modified rule, also compute a baseline fixed-k replicate for comparison. Candidates: [('C2p–N2p(pi)', -11.2, -13.5), ('O2p–O2p', -15.3, -10.8), ('artifact_mixed_sign', -11.2, 13.6), ('singular_sum', -12.0, 12.0)].", "answers": "[{\"name\":\"pyscf_scf_hf_Kgwh\",\"arguments\":{\"Ei\":-11.2,\"Ej\":-13.5}},{\"name\":\"pyscf_scf_hf_Kgwh\",\"arguments\":{\"Ei\":-15.3,\"Ej\":-10.8,\"updated_rule\":true}},{\"name\":\"pyscf_scf_hf_Kgwh\",\"arguments\":{\"Ei\":-15.3,\"Ej\":-10.8}}]"}
{"func_name": "pyscf_scf_hf_dot_eri_dm", "func_desc": "Compute Coulomb (J) and exchange (K) matrices from two-electron integrals and density matrix(es) for use in Hartree–Fock and related SCF procedures.\n    \n    This function is used in the PySCF SCF machinery to build the two-electron contribution to the Fock matrix: the Coulomb matrix J and the exchange matrix K. It accepts either a full 4-index eri array (reshaped to (N,N,N,N)) or an 8-fold compressed ERI representation handled by the internal _vhf.incore routine. The density matrix input may be a single density matrix or a stack/list of density matrices; the function returns J and K with matching layout and dtype to the input and computed integrals.", "tools": [{"function": {"description": "Compute Coulomb (J) and exchange (K) matrices from two-electron integrals and density matrix(es) for use in Hartree–Fock and related SCF procedures.\n\nThis function is used in the PySCF SCF machinery to build the two-electron contribution to the Fock matrix: the Coulomb matrix J and the exchange matrix K. It accepts either a full 4-index eri array (reshaped to (N,N,N,N)) or an 8-fold compressed ERI representation handled by the internal _vhf.incore routine. The density matrix input may be a single density matrix or a stack/list of density matrices; the function returns J and K with matching layout and dtype to the input and computed integrals.", "name": "pyscf_scf_hf_dot_eri_dm", "parameters": {"properties": {"eri": {"type": "array", "items": {"type": "float"}, "description": "8-fold or 4-fold electron repulsion integrals (ERIs) or a complex integral array containing N**4 elements, where N is the number of orbitals. If eri.dtype is complex128 or eri.size == nao**4, the array is interpreted as a full (N,N,N,N) tensor and is reshaped accordingly; otherwise the implementation dispatches to pyscf.scf._vhf.incore to process compact ERI formats. The eri determines which computational path is used (direct einsum over the full tensor, or _vhf.incore for packed formats) and therefore influences memory and CPU tradeoffs.", "default": ""}, "dm": {"type": "array", "items": {"type": "float"}, "description": "A single density matrix with shape (..., N, N) or a list/stack of density matrices. The last two dimensions must be square with size N (number of orbitals). The function converts dm to a NumPy array and treats the leading dimension as the index over multiple density matrices; a single (N,N) matrix will produce single (N,N) J and K outputs, while an (M,N,N) array produces M J and M K matrices. If dm is complex128, the function separates real and imaginary parts and combines results (real part processed with the hermi flag, imaginary part processed with hermi=0) so that complex density matrices are supported.", "default": ""}, "hermi": {"type": "integer", "description": "Whether to enforce hermiticity/symmetry in the computed J and K when the internal _vhf.incore path is used. This flag controls how symmetric/antisymmetric contributions are treated when integrals are supplied in packed (non-full-tensor) form.\n0 : no hermitian or symmetric enforcement (default). The function will not assume hermiticity and will compute J/K as-is.\n1 : hermitian. Instructs the underlying routine to return hermitian (conjugate symmetric) J/K consistent with real-symmetric density matrices where applicable.\n2 : anti-hermitian. Instructs the underlying routine to treat results as anti-hermitian when that is required by the integral/density conventions.", "default": 0}, "with_j": {"type": "boolean", "description": "If True (default), compute and return the Coulomb (J) matrix/matrices. If False, skip computing J and return None for the J position. Skipping can save computation if only K is required.", "default": true}, "with_k": {"type": "boolean", "description": "If True (default), compute and return the exchange (K) matrix/matrices. If False, skip computing K and return None for the K position. Skipping can save computation if only J is required.", "default": true}}, "required": ["eri", "dm", "with_j", "with_k", "hermi"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the two-electron build stage of a tiny (N=2) SCF micro-kernel with intentionally messy inputs. You’re given three candidate AO/MO ERI+DM “frames” (same tensors as below), but only frames whose density matrix is physically admissible under a closed-shell idempotency sanity heuristic should be routed through the J/K builder: specifically, require (i) the DM is real-symmetric (treat small asymmetry as numerical noise and enforce hermiticity via `hermi=1` in the contraction) and (ii) both eigenvalues of the DM lie in the occupancy range [0, 2]. For frames that pass, compute both Coulomb (J) and exchange (K) contributions (with_j=true, with_k=true) using the provided full (2,2,2,2) ERI tensors.\n\nCandidate frames:\n(1) ERI = [[[[0.7,0.12],[0.12,0.25]],[[0.11,0.18],[0.05,0.1]]],[[[0.11,0.05],[0.18,0.1]],[[0.3,0.08],[0.08,0.6]]]] with DM = [[1.1,0.2],[0.2,0.9]]\n(2) ERI = [[[[0.8,0.0],[0.0,0.0]],[[0.0,0.0],[0.0,0.3]]],[[[0.0,0.0],[0.0,0.0]],[[0.0,0.3],[0.0,0.75]]]] with DM = [[1.0,0.2],[0.2,0.5]]\n(3) ERI = [[[[0.7,0.12],[0.12,0.65]],[[0.12,0.18],[0.18,0.11]]],[[[0.12,0.18],[0.18,0.11]],[[0.65,0.11],[0.11,0.6]]]] with DM = [[1.2,0.3],[0.3,0.8]]\n\nReturn J and K only for the frames that satisfy the admissibility rule.", "answers": "[{\"name\":\"pyscf_scf_hf_dot_eri_dm\",\"arguments\":{\"eri\":[[[[0.7,0.12],[0.12,0.25]],[[0.11,0.18],[0.05,0.1]]],[[[0.11,0.05],[0.18,0.1]],[[0.3,0.08],[0.08,0.6]]]],\"dm\":[[1.1,0.2],[0.2,0.9]],\"hermi\":1,\"with_j\":true,\"with_k\":true}},{\"name\":\"pyscf_scf_hf_dot_eri_dm\",\"arguments\":{\"eri\":[[[[0.8,0.0],[0.0,0.0]],[[0.0,0.0],[0.0,0.3]]],[[[0.0,0.0],[0.0,0.0]],[[0.0,0.3],[0.0,0.75]]]],\"dm\":[[1.0,0.2],[0.2,0.5]],\"hermi\":1,\"with_j\":true,\"with_k\":true}},{\"name\":\"pyscf_scf_hf_dot_eri_dm\",\"arguments\":{\"eri\":[[[[0.7,0.12],[0.12,0.65]],[[0.12,0.18],[0.18,0.11]]],[[[0.12,0.18],[0.18,0.11]],[[0.65,0.11],[0.11,0.6]]]],\"dm\":[[1.2,0.3],[0.3,0.8]],\"hermi\":1,\"with_j\":true,\"with_k\":true}}]"}
{"func_name": "pyscf_scf_hf_get_grad", "func_desc": "Compute the restricted Hartree–Fock (RHF) orbital gradient vector in the molecular\n    orbital (MO) basis for use in PySCF electronic structure routines.\n    \n    This function is part of the PySCF (Python-based Simulations of Chemistry Framework)\n    toolset and is used to form the occupied–virtual block of the Fock matrix in the MO\n    representation for closed-shell RHF calculations. The output is commonly used by SCF\n    response, orbital-rotation, and post-Hartree–Fock procedures that require the coupling\n    between occupied and virtual spatial orbitals.", "tools": [{"function": {"description": "Compute the restricted Hartree–Fock (RHF) orbital gradient vector in the molecular\norbital (MO) basis for use in PySCF electronic structure routines.\n\nThis function is part of the PySCF (Python-based Simulations of Chemistry Framework)\ntoolset and is used to form the occupied–virtual block of the Fock matrix in the MO\nrepresentation for closed-shell RHF calculations. The output is commonly used by SCF\nresponse, orbital-rotation, and post-Hartree–Fock procedures that require the coupling\nbetween occupied and virtual spatial orbitals.", "name": "pyscf_scf_hf_get_grad", "parameters": {"properties": {"mo_coeff": {"type": "array", "items": {"type": "float"}, "description": "Obital coefficients. Array of molecular orbital\ncoefficients expressed in the atomic orbital (AO) basis with shape\n(n_ao, n_mo). Each column is an MO coefficient vector. The function uses\nthese coefficients to form the occupied and virtual MO subblocks via\nprojection of the AO Fock matrix into the MO basis.", "default": ""}, "mo_occ": {"type": "array", "items": {"type": "float"}, "description": "Orbital occupancy. One-dimensional array of length n_mo\ngiving the occupancy of each MO. Entries with value > 0 are treated as\noccupied (internally occidx = mo_occ > 0) and entries with value <= 0 are\ntreated as virtual. This routine follows the source-code rule that any\npositive occupancy marks an occupied orbital; fractional occupancies > 0\nwill be classified as occupied accordingly.", "default": ""}, "fock_ao": {"type": "array", "items": {"type": "float"}, "description": "Fock matrix in AO representation. Square AO-basis Fock\nmatrix with shape (n_ao, n_ao). This matrix is multiplied by the MO\ncoefficients to obtain the occupied–virtual MO block.", "default": ""}}, "required": ["mo_coeff", "mo_occ", "fock_ao"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an RHF orbital-rotation response stage with mixed-quality MO metadata coming from two independent closed-shell cohorts (toy 3×3 and water-like 7×7). For each cohort, form the MO-basis occupied–virtual (ov) orbital-gradient coupling from the provided AO Fock matrix and MO coefficients, but first apply an occupancy-sanity sieve to decide what is treated as occupied vs virtual: treat an MO as occupied only if its occupation is a physically closed-shell value (approximately 2.0 within tight numerical tolerance) and treat it as virtual only if its occupation is approximately 0.0. Any MO with an intermediate/non-integer occupation should be treated as an artifact and excluded from the occupied/virtual partition (i.e., it must not contribute to the ov coupling block). Use the arrays exactly as provided (no re-diagonalization, no symmetrization, no re-orthogonalization). Return the ov orbital-gradient coupling for each cohort under this rule.\n\nCohort A (toy, 3 AO / 3 MO): mo_coeff = [[0.8, 0.5, 0.1],[0.5,-0.8,0.3],[0.3,0.1,-0.95]], fock_ao = [[-1.1,0.05,0.0],[0.05,-0.7,0.02],[0.0,0.02,-0.3]], raw mo_occ = [2,0,0].\n\nCohort B (water-like, 7 AO / 7 MO): mo_coeff = [[0.99,0.1,0.02,-0.01,0.0,0.0,0.0],[0.1,-0.95,0.25,0.05,0.0,0.0,0.0],[0.01,0.2,0.9,-0.3,0.0,0.0,0.0],[-0.01,0.05,-0.3,-0.95,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,1.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,1.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,1.0]], fock_ao = [[-20.5,-0.3,0.1,0.0,0.0,0.0,0.0],[-0.3,-1.5,-0.2,0.0,0.0,0.0,0.0],[0.1,-0.2,-0.8,-0.1,0.0,0.0,0.0],[0.0,0.0,-0.1,-0.5,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.2,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.3,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.4]], raw mo_occ = [2.0,2.0,2.0,2.0,0.0,0.0,0.0], but assume the 4th MO’s occupation was flagged by upstream parsing as slightly fractional (set it to 1.999 for the purpose of this test) and should therefore be excluded by the sieve.", "answers": "[{\"name\":\"pyscf_scf_hf_get_grad\",\"arguments\":{\"mo_coeff\":[[0.8,0.5,0.1],[0.5,-0.8,0.3],[0.3,0.1,-0.95]],\"mo_occ\":[2,0,0],\"fock_ao\":[[-1.1,0.05,0.0],[0.05,-0.7,0.02],[0.0,0.02,-0.3]]}},{\"name\":\"pyscf_scf_hf_get_grad\",\"arguments\":{\"mo_coeff\":[[0.99,0.1,0.02,-0.01,0.0,0.0,0.0],[0.1,-0.95,0.25,0.05,0.0,0.0,0.0],[0.01,0.2,0.9,-0.3,0.0,0.0,0.0],[-0.01,0.05,-0.3,-0.95,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,1.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,1.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,1.0]],\"mo_occ\":[2.0,2.0,2.0,1.999,0.0,0.0,0.0],\"fock_ao\":[[-20.5,-0.3,0.1,0.0,0.0,0.0,0.0],[-0.3,-1.5,-0.2,0.0,0.0,0.0,0.0],[0.1,-0.2,-0.8,-0.1,0.0,0.0,0.0],[0.0,0.0,-0.1,-0.5,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.2,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.3,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.4]]}}]"}
{"func_name": "pyscf_scf_hf_level_shift", "func_desc": "pyscf.scf.hf.level_shift applies an energy level shift to virtual molecular orbitals in Hartree–Fock self-consistent-field (SCF) procedures to improve convergence by increasing the Fock matrix diagonal elements associated with virtual space.\n    \n    This routine constructs an operator that projects onto the virtual-space block in the atomic-orbital (AO) representation and adds a uniform energy offset Delta (the level shift) to that virtual-space block of the Fock matrix. In PySCF workflows this is used when performing SCF iterations to discourage occupation of virtual orbitals and to stabilize convergence in difficult cases (e.g., near-degeneracies or variational collapse). The implementation follows the algebraic form F_new = F + (S - S D S) * Delta, where S is the AO overlap matrix, D is the AO density matrix (occupied-space projector), and F is the current AO Fock matrix.", "tools": [{"function": {"description": "pyscf.scf.hf.level_shift applies an energy level shift to virtual molecular orbitals in Hartree–Fock self-consistent-field (SCF) procedures to improve convergence by increasing the Fock matrix diagonal elements associated with virtual space.\n\nThis routine constructs an operator that projects onto the virtual-space block in the atomic-orbital (AO) representation and adds a uniform energy offset Delta (the level shift) to that virtual-space block of the Fock matrix. In PySCF workflows this is used when performing SCF iterations to discourage occupation of virtual orbitals and to stabilize convergence in difficult cases (e.g., near-degeneracies or variational collapse). The implementation follows the algebraic form F_new = F + (S - S D S) * Delta, where S is the AO overlap matrix, D is the AO density matrix (occupied-space projector), and F is the current AO Fock matrix.", "name": "pyscf_scf_hf_level_shift", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "float"}, "description": "AO overlap matrix S. A 2D numpy.ndarray containing overlap integrals between atomic-orbital basis functions in the same basis used to form d and f. This matrix is used to form the projector onto the virtual subspace via the expression S - S @ D @ S. The array must be a numeric 2D array compatible with matrix multiplication with d and f; mismatched shapes will raise standard NumPy errors.", "default": ""}, "d": {"type": "array", "items": {"type": "float"}, "description": "AO density matrix D. A 2D numpy.ndarray representing the current SCF density (occupied-space projector) in the same AO basis as s and f. d is used to remove the occupied-space contribution from the identity in the S-weighted projector S - S D S. The array must be numeric and conformable with s and f for matrix multiplication.", "default": ""}, "f": {"type": "array", "items": {"type": "float"}, "description": "AO Fock matrix F. A 2D numpy.ndarray representing the current Fock operator in the AO basis. The function returns a modified copy of this matrix with the level shift added to the virtual-space block; f itself is not modified in place by this function (the operation returns a new array).", "default": ""}, "factor": {"type": "float", "description": "Level shift magnitude Delta. A floating-point scalar with the same energy units as the entries of f. This value is multiplied by the AO virtual-space projector (S - S D S) and added to f to produce the shifted Fock matrix. Choosing a nonzero factor increases virtual orbital energies; very large values can overly bias the solution and impede obtaining a physically meaningful SCF result.", "default": ""}}, "required": ["s", "d", "f", "factor"], "type": "any"}}, "type": "function"}], "query": "We are curating a small SCF “stability triage” set of AO-basis snapshots from independent RHF runs. Each snapshot provides (S, D, F) but the convergence aid should be applied conditionally: apply pyscf.scf.hf.level_shift using the standard virtual-space projector form only for snapshots whose AO overlap matrix exhibits nontrivial basis coupling (i.e., at least one off-diagonal overlap magnitude >= 0.15). For the level-shift magnitude, use an adaptive Delta equal to 0.5 Hartree times the trace of the density matrix (Tr[D]) for that snapshot. Process the following raw snapshots in one batch and output the shifted AO Fock matrix for each snapshot that meets the coupling criterion: (A) 3×3 difficult-case: S=[[1.0,0.2,0.0],[0.2,1.0,0.1],[0.0,0.1,1.0]], D=[[1.0,0.0,0.0],[0.0,0.5,0.0],[0.0,0.0,0.0]], F=[[-1.2,0.1,0.0],[0.1,-0.8,0.05],[0.0,0.05,-0.3]]; (B) 2×2 oscillatory diatomic: S=[[1.0,0.2],[0.2,1.0]], D=[[1.0,0.0],[0.0,0.0]], F=[[-1.0,0.1],[0.1,0.5]]; (C) 3×3 near-orthogonal control: S=[[1.0,0.05,0.0],[0.05,1.0,0.02],[0.0,0.02,1.0]], D=[[1.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]], F=[[-0.9,0.02,0.0],[0.02,-0.7,0.01],[0.0,0.01,0.2]].", "answers": "[{\"name\":\"pyscf_scf_hf_level_shift\",\"arguments\":{\"s\":[[1.0,0.2,0.0],[0.2,1.0,0.1],[0.0,0.1,1.0]],\"d\":[[1.0,0.0,0.0],[0.0,0.5,0.0],[0.0,0.0,0.0]],\"f\":[[-1.2,0.1,0.0],[0.1,-0.8,0.05],[0.0,0.05,-0.3]],\"factor\":0.75}},{\"name\":\"pyscf_scf_hf_level_shift\",\"arguments\":{\"s\":[[1.0,0.2],[0.2,1.0]],\"d\":[[1.0,0.0],[0.0,0.0]],\"f\":[[-1.0,0.1],[0.1,0.5]],\"factor\":0.5}}]"}
{"func_name": "pyscf_solvent_hessian_pcm_gradgrad_switch_h", "func_desc": "Second derivative of the PCM switching function h(x) used in solvent Hessian calculations.\n    \n    This function evaluates the elementwise second derivative h''(x) of the smoothing/switching function h(x) that appears in pyscf.solvent.hessian.pcm. In the domain 0 <= x <= 1 the second derivative is computed from the cubic polynomial\n    h''(x) = 60.0*x - 180.0*x**2 + 120.0*x**3,\n    and values outside the closed interval [0, 1] are clamped to 0.0 (i.e., h''(x) = 0 for x < 0 or x > 1). The switching function h(x) is used in the polarizable continuum model (PCM) machinery to smoothly transition between regions (for example, between solvent-exposed and buried surface regions) when assembling analytic gradients and Hessians of solvation-related quantities; this routine supplies the curvature contribution (second derivative) of that switch, evaluated elementwise for array inputs. The operation is fully vectorized and has no side effects on input arrays.", "tools": [{"function": {"description": "Second derivative of the PCM switching function h(x) used in solvent Hessian calculations.\n\nThis function evaluates the elementwise second derivative h''(x) of the smoothing/switching function h(x) that appears in pyscf.solvent.hessian.pcm. In the domain 0 <= x <= 1 the second derivative is computed from the cubic polynomial\nh''(x) = 60.0*x - 180.0*x**2 + 120.0*x**3,\nand values outside the closed interval [0, 1] are clamped to 0.0 (i.e., h''(x) = 0 for x < 0 or x > 1). The switching function h(x) is used in the polarizable continuum model (PCM) machinery to smoothly transition between regions (for example, between solvent-exposed and buried surface regions) when assembling analytic gradients and Hessians of solvation-related quantities; this routine supplies the curvature contribution (second derivative) of that switch, evaluated elementwise for array inputs. The operation is fully vectorized and has no side effects on input arrays.", "name": "pyscf_solvent_hessian_pcm_gradgrad_switch_h", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input array of scalar switching parameters x for which the second derivative is required. Each element represents a dimensionless switching coordinate (for example a scaled distance or blending parameter used by the PCM cavity/switch function). The function treats the array elementwise and returns an array of the same shape. The implementation uses floating-point arithmetic (literal constants are floats), so integer-valued arrays will be promoted to floating dtype in the result. The API expects a numpy.ndarray; passing non-ndarray objects may raise an exception or rely on numpy's conversion/broadcasting semantics.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing PCM solvent-Hessian curvature assembly with messy cavity switching coordinates coming from three independent surface reconstructions. Each replicate contains a mix of physical switching coordinates (expected to lie in the closed interval [0,1]) plus spurious extrapolated values from the tessellation step. For each replicate: (i) evaluate h''(x) on the full raw coordinate vector to capture clamping behavior; and (ii) additionally evaluate h''(x) on the subset of coordinates that fall within [0,1] (inclusive), preserving their original order, to isolate the in-domain curvature profile. Use these raw replicates: R1 = [-0.2, 0.0, 0.1, 0.35, 0.7, 1.0, 1.3]; R2 = [-0.2, 0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0, 1.2]; R3 = [-0.2, 0.0, 0.15, 0.5, 0.85, 1.0, 1.3].", "answers": "[{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[-0.2,0.0,0.1,0.35,0.7,1.0,1.3]}},{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[0.0,0.1,0.35,0.7,1.0]}},{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[-0.2,0.0,0.1,0.25,0.5,0.75,0.9,1.0,1.2]}},{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[0.0,0.1,0.25,0.5,0.75,0.9,1.0]}},{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[-0.2,0.0,0.15,0.5,0.85,1.0,1.3]}},{\"name\":\"pyscf_solvent_hessian_pcm_gradgrad_switch_h\",\"arguments\":{\"x\":[0.0,0.15,0.5,0.85,1.0]}}]"}
{"func_name": "pyscf_solvent_pcm_switch_h", "func_desc": "switching polynomial used in the polarizable continuum model (PCM) code path of PySCF.\n    This function implements the smooth switching function (Eq. 3.19) used to transition\n    between regions (for example, across a cavity surface discretization) in continuum\n    solvation treatments. The implementation follows the corrected polynomial form\n    y = x**3 * (10.0 - 15.0*x + 6.0*x**2) and applies hard bounds outside the unit\n    interval. Reference: J. Chem. Phys. 133, 244111 (2010) — note that the original\n    paper contains a typographical error in the printed formula; this function encodes\n    the corrected form used in PySCF's PCM routines.", "tools": [{"function": {"description": "switching polynomial used in the polarizable continuum model (PCM) code path of PySCF.\nThis function implements the smooth switching function (Eq. 3.19) used to transition\nbetween regions (for example, across a cavity surface discretization) in continuum\nsolvation treatments. The implementation follows the corrected polynomial form\ny = x**3 * (10.0 - 15.0*x + 6.0*x**2) and applies hard bounds outside the unit\ninterval. Reference: J. Chem. Phys. 133, 244111 (2010) — note that the original\npaper contains a typographical error in the printed formula; this function encodes\nthe corrected form used in PySCF's PCM routines.", "name": "pyscf_solvent_pcm_switch_h", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input array of dimensionless switching coordinates.\nEach element of x represents a normalized coordinate along which the\nswitching is evaluated (typically a scaled distance or overlap-like\nmeasure used in PCM cavity/surface constructions). The array must be a\nnumpy.ndarray (any shape is accepted) and the operation is applied\nelementwise. The function is vectorized: the output has the same shape\nas x. Numerical dtype is determined by numpy broadcasting and operations;\ninteger inputs will be upcast to a floating type during computation.\nValues outside the closed interval [0, 1] are clamped as described below.\nIf x contains NaN or infinite values, the corresponding outputs follow\nnumpy arithmetic rules (NaN/infs will generally propagate).", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "We’re validating the PCM cavity smoothing stage against a mixed-quality set of normalized signed-distance samples coming from three independent surface discretizations (each array retains its original shape). Apply the PySCF PCM switching polynomial only on samples that are physically interpretable as *near-surface interpolation coordinates*, i.e., within a relaxed transition window of [-0.10, 1.10]; values outside that window should be treated as upstream mesh/normalization artifacts and excluded from this validation. The three raw cohorts are:\nA) 1D probe trace: xA = [0.0, 0.2, 0.5, 0.8, 1.0, 1.3, -0.4]\nB) 2D two-patch replicate grid: xB = [[-0.2, 0.0, 0.25, 0.5], [0.75, 1.0, 1.2, 2.0]]\nC) 1D boundary-stress debug sweep: xC = [-0.2, 0.0, 0.15, 0.5, 0.85, 1.0, 1.3]\n\nEvaluate the switching function on the subset of entries from each cohort that fall in the validation window, preserving ordering within each cohort (but dropping invalid points).", "answers": "[{\"name\":\"pyscf_solvent_pcm_switch_h\",\"arguments\":{\"x\":[0.0,0.2,0.5,0.8,1.0]}},{\"name\":\"pyscf_solvent_pcm_switch_h\",\"arguments\":{\"x\":[[0.0,0.25,0.5],[0.75,1.0]]}},{\"name\":\"pyscf_solvent_pcm_switch_h\",\"arguments\":{\"x\":[0.0,0.15,0.5,0.85,1.0]}}]"}
{"func_name": "pyscf_solvent_smd_experiment_atomic_surface_tension", "func_desc": "pyscf.solvent.smd_experiment.atomic_surface_tension computes per-atom surface-tension contributions used by the SMD (Solvation Model based on Density) experiment code path in PySCF. It evaluates atom-centered contributions for a molecular solute given atomic types, Cartesian coordinates, and three scalar solvent descriptors, combining per-atom and pairwise bond descriptors from module-level lookup tables to produce a numpy.ndarray of tensions aligned with the input atom ordering.\n    \n    This function is used within the implicit-solvent SMD implementation to estimate how each atom of a solute contributes to the solute–solvent surface tension term. It assembles contributions from (1) atom-type-specific base tensions and (2) pairwise bond/neighbor-dependent terms that are activated by interatomic distances and precomputed radius/switch parameters. The computation relies on module-level dictionaries and helper utilities defined elsewhere in the same module: sigma_water, sigma_n, sigma_alpha, sigma_beta, r_zz, and swtich_function. It also uses scipy.spatial.distance.cdist to form the NxN matrix of interatomic distances.", "tools": [{"function": {"description": "pyscf.solvent.smd_experiment.atomic_surface_tension computes per-atom surface-tension contributions used by the SMD (Solvation Model based on Density) experiment code path in PySCF. It evaluates atom-centered contributions for a molecular solute given atomic types, Cartesian coordinates, and three scalar solvent descriptors, combining per-atom and pairwise bond descriptors from module-level lookup tables to produce a numpy.ndarray of tensions aligned with the input atom ordering.\n\nThis function is used within the implicit-solvent SMD implementation to estimate how each atom of a solute contributes to the solute–solvent surface tension term. It assembles contributions from (1) atom-type-specific base tensions and (2) pairwise bond/neighbor-dependent terms that are activated by interatomic distances and precomputed radius/switch parameters. The computation relies on module-level dictionaries and helper utilities defined elsewhere in the same module: sigma_water, sigma_n, sigma_alpha, sigma_beta, r_zz, and swtich_function. It also uses scipy.spatial.distance.cdist to form the NxN matrix of interatomic distances.", "name": "pyscf_solvent_smd_experiment_atomic_surface_tension", "parameters": {"properties": {"symbols": {"type": "array", "items": {"type": "float"}, "description": "Ordered list of atomic symbols (strings) for the molecule, e.g. ['C', 'H', 'H', ...]. Each entry identifies the element of the corresponding atom and determines which lookup values and bond/neighbor rules are applied. Only atoms with symbols in ['H', 'C', 'N', 'O', 'F', 'Si', 'S', 'Cl', 'Br'] are processed with nonzero contributions; other symbols yield a tension of 0 in the output.", "default": ""}, "coords": {"type": "array", "items": {"type": "float"}, "description": "Two-dimensional array of atomic Cartesian coordinates with shape (N, 3), where N == len(symbols). Coordinates are expected in Angstrom units (original source referred to \"Anstrong\"); pairwise distances are computed with scipy.spatial.distance.cdist. A mismatched length between symbols and coords or a coords array not shaped (N, 3) will raise an error (for example, IndexError or a shape-related exception from cdist) and represents a failure mode.", "default": ""}, "n": {"type": "float", "description": "Scalar solvent descriptor (named n in the source). This descriptor is combined with module-level bond/atom sensitivity coefficients stored in sigma_n to form part of the non-water solvent contribution. When the optional water flag is True, this parameter is not used because per-atom/bond sigma_water values are selected instead.", "default": ""}, "alpha": {"type": "float", "description": "Scalar solvent descriptor (named alpha in the source). This descriptor is multiplied by coefficients in the module-level sigma_alpha dictionary to contribute to atom/bond tensions for non-water solvents. As with n, if water is True the sigma_water values override usage of this descriptor.", "default": ""}, "beta": {"type": "float", "description": "Scalar solvent descriptor (named beta in the source). This descriptor is multiplied by coefficients in the module-level sigma_beta dictionary and combined into atom/bond tensions for non-water solvents. It is ignored when water is True.", "default": ""}, "water": {"type": "boolean", "description": "Optional boolean flag (default True) that selects whether to use water-specific per-atom and per-bond tensions from the module-level sigma_water dictionary. If water is True, the function returns tensions determined solely by sigma_water lookups (falling back to 0.0 for missing keys) and ignores n, alpha, and beta. If water is False, tensions are assembled as linear combinations of sigma_n, sigma_alpha, and sigma_beta entries and the provided scalar descriptors. This flag does not change the required shapes of symbols or coords.", "default": true}}, "required": ["symbols", "coords", "n", "alpha", "beta", "water"], "type": "any"}}, "type": "function"}], "query": "I’m validating an SMD implicit-solvent surface-tension pipeline where incoming geometries may contain distorted bonds that should not be used for atom-resolved tension comparisons. I have three candidate solute snapshots (two methanol conformers and one acetone). For each snapshot, first screen the geometry using only heavy-atom (non-H) distances: keep the snapshot for SMD surface-tension evaluation only if every heavy atom has at least one other heavy-atom neighbor within 1.80 Å (to reject fragmented/invalid heavy-atom connectivity). For each snapshot that passes, compute per-atom SMD surface-tension contributions using the provided atom ordering, treating the solvent as non-water. Solvent descriptors must be assigned per-snapshot from the geometry itself: set n to 1.30 + 0.01×(number of atoms); set alpha to 0.10×(number of hetero atoms, i.e., non-C/non-H); set beta to 0.40 + 0.02×(number of oxygen atoms). Snapshots: (A) Methanol, symbols ['C','O','H','H','H','H'], coords [[0.0,0.0,0.0],[1.43,0.0,0.0],[-0.54,0.935,0.0],[-0.54,-0.467,0.809],[-0.54,-0.467,-0.809],[1.93,0.89,0.0]]; (B) Methanol geometry variant, symbols ['C','O','H','H','H','H'], coords [[0.0,0.0,0.0],[1.43,0.0,0.0],[-0.54,0.94,0.0],[-0.54,-0.47,0.815],[-0.54,-0.47,-0.815],[1.93,0.76,0.0]]; (C) Acetone, symbols ['C','C','C','O','H','H','H','H','H','H'], coords [[0.0,0.0,0.0],[1.52,0.0,0.0],[-1.52,0.0,0.0],[0.0,1.23,0.0],[2.09,0.93,0.0],[2.09,-0.93,0.0],[-2.09,0.93,0.0],[-2.09,-0.93,0.0],[0.0,-0.54,0.93],[0.0,-0.54,-0.93]].", "answers": "[{\"name\":\"pyscf_solvent_smd_experiment_atomic_surface_tension\",\"arguments\":{\"symbols\":[\"C\",\"O\",\"H\",\"H\",\"H\",\"H\"],\"coords\":[[0.0,0.0,0.0],[1.43,0.0,0.0],[-0.54,0.935,0.0],[-0.54,-0.467,0.809],[-0.54,-0.467,-0.809],[1.93,0.89,0.0]],\"n\":1.36,\"alpha\":0.1,\"beta\":0.42,\"water\":false}},{\"name\":\"pyscf_solvent_smd_experiment_atomic_surface_tension\",\"arguments\":{\"symbols\":[\"C\",\"O\",\"H\",\"H\",\"H\",\"H\"],\"coords\":[[0.0,0.0,0.0],[1.43,0.0,0.0],[-0.54,0.94,0.0],[-0.54,-0.47,0.815],[-0.54,-0.47,-0.815],[1.93,0.76,0.0]],\"n\":1.36,\"alpha\":0.1,\"beta\":0.42,\"water\":false}}]"}
{"func_name": "pyscf_symm_Dmatrix_Dmatrix", "func_desc": "pyscf.symm.Dmatrix.Dmatrix returns the Wigner rotation D-matrix for angular momentum l and a rotation specified by Euler angles in the z-y-z convention. The matrix implements D_{m m'} = <l m | R(alpha, beta, gamma) | l m'> and is commonly used in PySCF for rotating spherical-harmonic angular-momentum basis functions (for example when transforming molecular orbital or integral representations under symmetry operations).", "tools": [{"function": {"description": "pyscf.symm.Dmatrix.Dmatrix returns the Wigner rotation D-matrix for angular momentum l and a rotation specified by Euler angles in the z-y-z convention. The matrix implements D_{m m'} = <l m | R(alpha, beta, gamma) | l m'> and is commonly used in PySCF for rotating spherical-harmonic angular-momentum basis functions (for example when transforming molecular orbital or integral representations under symmetry operations).\n", "name": "pyscf_symm_Dmatrix_Dmatrix", "parameters": {"properties": {"l": {"type": "integer", "description": "The angular momentum quantum number l >= 0. This determines the size of the returned square matrix, which has shape (2*l+1, 2*l+1) and indices corresponding to magnetic quantum numbers m = -l, -l+1, ..., l. The function expects a non-negative integer; providing a negative value or a non-integer will lead to incorrect behavior or runtime errors.", "default": ""}, "alpha": {"type": "float", "description": "The first Euler angle (rotation about z) in radians. In the z-y-z convention used here, this angle contributes a phase factor e^{-i * alpha * m} multiplying the left side (row index m) of the small-d matrix.", "default": ""}, "beta": {"type": "float", "description": "The second Euler angle (rotation about y) in radians. This angle is passed to the internal small-d matrix generator dmatrix(l, beta) to produce the real-valued Wigner small-d matrix d_{m m'}(beta) used between the two z-rotations.", "default": ""}, "gamma": {"type": "float", "description": "The third Euler angle (rotation about z) in radians. In the z-y-z convention, this angle contributes a phase factor e^{-i * gamma * m'} multiplying the right side (column index m') of the small-d matrix.", "default": ""}, "reorder_p": {"type": "boolean", "description": "Whether to reorder p functions (l == 1) into Cartesian (x, y, z) ordering. Default is False. Practical significance: different parts of PySCF and external integral conventions expect p-function ordering either in the spherical-harmonic sequence m = -1,0,1 or in Cartesian x,y,z order. When reorder_p is True and l == 1, the returned matrix is permuted by the index mapping [2,0,1] applied to both rows and columns, so that the output matches the (x,y,z) ordering. For l != 1 this parameter is ignored (no reordering is applied). Note: internally the small-d and real-basis conversion routines are called with their own reorder flags set to False; the only explicit permutation performed by this function is the final [2,0,1] permutation when l == 1 and reorder_p is True.", "default": false}}, "required": ["l", "alpha", "beta", "gamma", "reorder_p"], "type": "any"}}, "type": "function"}], "query": "We’re validating a symmetry-rotation stage in a PySCF AO-basis transformation pipeline where the rotation protocol depends on the manifold. Given two candidate rotation records (angles are in radians, z–y–z convention):\n\n1) manifold tag: \"p-like\" with (alpha, beta, gamma) = (0.7853981633974483, 1.2309594173407747, -0.5235987755982988)\n2) manifold tag: \"d-like\" with (alpha, beta, gamma) = (1.0471975512, 0.7853981634, 0.5235987756)\n\nGenerate Wigner D-matrices only for manifolds where l is consistent with the tag (p-like → l=1, d-like → l=2). Additionally, if the manifold is p-like, emit the D-matrix in the Cartesian p-function ordering (x,y,z); otherwise keep the default spherical ordering. Treat each qualifying manifold as an independent pipeline stage output.", "answers": "[{\"name\":\"pyscf_symm_Dmatrix_Dmatrix\",\"arguments\":{\"l\":1,\"alpha\":0.7853981633974483,\"beta\":1.2309594173407747,\"gamma\":-0.5235987755982988,\"reorder_p\":true}},{\"name\":\"pyscf_symm_Dmatrix_Dmatrix\",\"arguments\":{\"l\":2,\"alpha\":1.0471975512,\"beta\":0.7853981634,\"gamma\":0.5235987756}}]"}
{"func_name": "pyscf_symm_Dmatrix_dmatrix", "func_desc": "Wigner small-d matrix (z-y-z convention).\n    \n    Computes the Wigner small-d matrix d^l(beta) for angular-momentum rank l and rotation\n    angle beta using the z-y-z Euler-convention commonly used in quantum chemistry and\n    angular-momentum algebra. In PySCF this function is used to construct rotation\n    operators and symmetry-transformation matrices for spherical tensors, spherical\n    harmonics, and spin-orbital block rotations that appear in symmetry-adapted\n    electronic-structure calculations. For small l (0, 1, 2) the implementation uses\n    closed-form expressions; for larger l it evaluates the standard factorial-based\n    sum (vectorized with NumPy) that implements the conventional definition of the\n    Wigner small-d matrix elements d^l_{m1,m2}(beta) = <l,m1| e^{-i beta J_y} |l,m2>.", "tools": [{"function": {"description": "Wigner small-d matrix (z-y-z convention).\n\nComputes the Wigner small-d matrix d^l(beta) for angular-momentum rank l and rotation\nangle beta using the z-y-z Euler-convention commonly used in quantum chemistry and\nangular-momentum algebra. In PySCF this function is used to construct rotation\noperators and symmetry-transformation matrices for spherical tensors, spherical\nharmonics, and spin-orbital block rotations that appear in symmetry-adapted\nelectronic-structure calculations. For small l (0, 1, 2) the implementation uses\nclosed-form expressions; for larger l it evaluates the standard factorial-based\nsum (vectorized with NumPy) that implements the conventional definition of the\nWigner small-d matrix elements d^l_{m1,m2}(beta) = <l,m1| e^{-i beta J_y} |l,m2>.", "name": "pyscf_symm_Dmatrix_dmatrix", "parameters": {"properties": {"l": {"type": "integer", "description": "The angular-momentum rank (non-negative integer) of the Wigner\nsmall-d matrix. Physically, l corresponds to the total angular-momentum\nquantum number (for example, l=0 for s, l=1 for p, l=2 for d functions).\nThe returned matrix has shape (2*l+1, 2*l+1), with rows/columns indexed\nby magnetic quantum numbers m = -l,...,+l in increasing order. Passing\na negative or non-integer l will cause internal NumPy array construction\nto fail (ValueError or related exception); callers should ensure l is a\nnon-negative int.", "default": ""}, "beta": {"type": "float", "description": "The rotation angle (in the same units used by numpy trigonometric\nfunctions, i.e. radians) for the rotation about the y axis in the\nz-y-z Euler convention. The function computes cos(beta/2) and sin(beta/2)\ninternally and uses these values in closed-form (l<=2) or factorial-sum\n(l>2) formulas. Extremely large |beta| are allowed (periodicity applies),\nbut numerical cancellation can occur for large l or angles producing\nvery small matrix elements.", "default": ""}, "reorder_p": {"type": "boolean", "description": "Optional flag (default False). For l == 1 only, if True the\nfunction reorders rows and columns by the permutation [2, 0, 1] before\nreturning. This reordering changes the magnetic-quantum-number indexing\nconvention and is provided to match alternative conventions used in some\nparts of quantum-chemistry code (it permutes the basis ordering for p\norbitals). For l != 1 this flag has no effect (the code does not apply\nany reordering).", "default": false}}, "required": ["l", "beta", "reorder_p"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mixed-l rotation stage in a symmetry-adapted PySCF workflow using a messy calibration log from two manifolds (d and f). Each record contains an intended y-axis rotation, but some entries are instrument/encoding artifacts. Apply this protocol:\n\n- Treat any angle explicitly marked in degrees as a physical rotation and convert it to radians before evaluation.\n- Treat any angle already provided as a radian float as-is.\n- Only evaluate rotations that lie strictly within the open interval (0, pi/2).\n- For even-l manifolds, preserve the default m-ordering and explicitly disable any p-orbital reordering behavior; for odd-l manifolds, leave all ordering/permutation settings at defaults.\n\nRaw calibration records:\n1) manifold l=2, angle tag: 45 deg\n2) manifold l=3, angle tag: 1.0471975512 rad\n3) manifold l=2, angle tag: 0 deg\n4) manifold l=3, angle tag: 2.0943951024 rad\n\nGenerate the Wigner small-d matrices for the records that pass the protocol.", "answers": "[{\"name\":\"pyscf_symm_Dmatrix_dmatrix\",\"arguments\":{\"l\":2,\"beta\":0.7853981633974483,\"reorder_p\":false}},{\"name\":\"pyscf_symm_Dmatrix_dmatrix\",\"arguments\":{\"l\":3,\"beta\":1.0471975512}}]"}
{"func_name": "pyscf_symm_addons_eigh", "func_desc": "Solve the standard Hermitian (or real symmetric) eigenvalue problem by exploiting known symmetry labels of the basis to block-diagonalize the matrix before diagonalization.\n    \n    This function is a PySCF convenience wrapper used in the context of molecular electronic structure calculations (see the PySCF README). It delegates to pyscf.lib.linalg_helper.eigh_by_blocks to partition the input matrix into symmetry blocks defined by the orbital symmetry labels and then solves the eigenvalue problem block-wise. In practice this speeds up and clarifies diagonalization of operators represented in a basis (for example, one-electron integrals or Fock matrices transformed into a symmetry-adapted orbital basis) by returning eigenvalues and eigenvectors consistent with the supplied symmetry labeling.", "tools": [{"function": {"description": "Solve the standard Hermitian (or real symmetric) eigenvalue problem by exploiting known symmetry labels of the basis to block-diagonalize the matrix before diagonalization.\n\nThis function is a PySCF convenience wrapper used in the context of molecular electronic structure calculations (see the PySCF README). It delegates to pyscf.lib.linalg_helper.eigh_by_blocks to partition the input matrix into symmetry blocks defined by the orbital symmetry labels and then solves the eigenvalue problem block-wise. In practice this speeds up and clarifies diagonalization of operators represented in a basis (for example, one-electron integrals or Fock matrices transformed into a symmetry-adapted orbital basis) by returning eigenvalues and eigenvectors consistent with the supplied symmetry labeling.", "name": "pyscf_symm_addons_eigh", "parameters": {"properties": {"h": {"type": "array", "items": {"type": "float"}, "description": "Square matrix to diagonalize. In PySCF workflows this is typically an operator represented in an orbital or atomic basis (for example, c.T * intor('int1e_*') * c in the example from the original docstring). The array must be two-dimensional and shape (n, n). The matrix is treated as Hermitian (or real symmetric); behavior for non-Hermitian input is delegated to the underlying eigh_by_blocks implementation.", "default": ""}, "orbsym": {"type": "array", "items": {"type": "float"}, "description": "Sequence of symmetry labels for each basis function (length n). Each entry identifies the symmetry (irreducible representation) of the corresponding row/column in h; these labels are used to group rows/columns into blocks. In PySCF this list is produced by routines such as symm.label_orb_symm and must correspond elementwise to the ordering of basis/orbitals used to build h.", "default": ""}}, "required": ["h", "orbsym"], "type": "any"}}, "type": "function"}], "query": "We’re validating symmetry-resolved eigensolves in an SCF-like workflow where the orbital symmetry labels come from an upstream irrep classifier and can include mixed-quality labels. For each candidate Fock matrix below, first enforce a symmetry-label sanity rule: only run the symmetry-blocked Hermitian diagonalization if (a) the matrix is real-symmetric as given, and (b) the provided `orbsym` actually partitions the basis into at least two distinct irreps (i.e., more than one unique symmetry label is present). Additionally, treat “two-irrep” and “multi-irrep” cases differently: if the number of unique irreps is exactly 2, run one diagonalization as-is; if there are more than 2 unique irreps, run one diagonalization per irrep-collapsing experiment where you remap the least-populated irrep label onto the most-populated irrep label (to mimic an aggressive symmetry-merging heuristic) and diagonalize that remapped labeling. Candidate jobs: (A) 4×4 C2v-like real-symmetric hA = [[-0.8, 0.1, 0.0, 0.0],[0.1, -0.6, 0.0, 0.0],[0.0, 0.0, -0.5, 0.05],[0.0, 0.0, 0.05, -0.4]] with orbsymA = [0,0,2,2]; (B) an intended replicate with the same hA but with an upstream labeler that collapsed everything to a single irrep, orbsymB = [0,0,0,0]; (C) a 6×6 real-symmetric hC = [[-0.8,0.1,0.0,0.0,0.0,0.0],[0.1,-0.3,0.0,0.0,0.0,0.0],[0.0,0.0,0.2,0.05,0.0,0.0],[0.0,0.0,0.05,0.6,0.0,0.0],[0.0,0.0,0.0,0.0,1.1,0.02],[0.0,0.0,0.0,0.0,0.02,1.3]] with orbsymC = [0,0,1,1,1,1]. Return eigenpairs consistent with the (possibly remapped) symmetry labels used for block partitioning.", "answers": "[{\"name\":\"pyscf_symm_addons_eigh\",\"arguments\":{\"h\":[[-0.8,0.1,0.0,0.0],[0.1,-0.6,0.0,0.0],[0.0,0.0,-0.5,0.05],[0.0,0.0,0.05,-0.4]],\"orbsym\":[0,0,2,2]}},{\"name\":\"pyscf_symm_addons_eigh\",\"arguments\":{\"h\":[[-0.8,0.1,0.0,0.0,0.0,0.0],[0.1,-0.3,0.0,0.0,0.0,0.0],[0.0,0.0,0.2,0.05,0.0,0.0],[0.0,0.0,0.05,0.6,0.0,0.0],[0.0,0.0,0.0,0.0,1.1,0.02],[0.0,0.0,0.0,0.0,0.02,1.3]],\"orbsym\":[0,0,1,1,1,1]}}]"}
{"func_name": "pyscf_symm_addons_irrep_name2id", "func_desc": "Convert an irreducible-representation (irrep) symbol used in molecular point-group symmetry to the internal integer irrep ID used throughout PySCF's symmetry machinery.\n    \n    This function is part of PySCF's symmetry utilities for quantum-chemistry calculations (see project README). It accepts human-readable point-group and irrep symbols, canonicalizes them with std_symb, and maps them to the internal integer identifier that PySCF uses to index symmetry blocks, orbitals, basis-function blocks, and other symmetry-dependent data structures. The mapping behavior depends on the canonicalized point group: SO3 irreps are handled by basis.so3_irrep_symb2id, linear-molecule groups Dooh/Coov are handled by basis.linearmole_irrep_symb2id, and all other groups are looked up in param.IRREP_ID_TABLE.", "tools": [{"function": {"description": "Convert an irreducible-representation (irrep) symbol used in molecular point-group symmetry to the internal integer irrep ID used throughout PySCF's symmetry machinery.\n\nThis function is part of PySCF's symmetry utilities for quantum-chemistry calculations (see project README). It accepts human-readable point-group and irrep symbols, canonicalizes them with std_symb, and maps them to the internal integer identifier that PySCF uses to index symmetry blocks, orbitals, basis-function blocks, and other symmetry-dependent data structures. The mapping behavior depends on the canonicalized point group: SO3 irreps are handled by basis.so3_irrep_symb2id, linear-molecule groups Dooh/Coov are handled by basis.linearmole_irrep_symb2id, and all other groups are looked up in param.IRREP_ID_TABLE.", "name": "pyscf_symm_addons_irrep_name2id", "parameters": {"properties": {"gpname": {"type": "string", "description": "The point-group symbol for the molecular symmetry (for example 'C2v', 'D3h', 'SO3', 'Dooh', or 'Coov'). This string is normalized by std_symb before lookup to produce a canonical point-group name; the canonical name determines which internal table or helper is used to convert the irrep symbol to an integer ID. Providing the correct point-group label is required because different groups use different mapping tables or helper functions.", "default": ""}, "symb": {"type": "string", "description": "The irrep symbol within the given point group (for example 'A1', 'E', 'T2g', or other group-specific labels). This string is also normalized by std_symb before lookup. The symbol must be a valid irrep name for the (canonicalized) gpname; the returned integer corresponds to the internal ID for that particular irrep in PySCF.", "default": ""}}, "required": ["gpname", "symb"], "type": "any"}}, "type": "function"}], "query": "We’re triaging symmetry labels coming out of an automated MO-block assignment + QC sanity-check pipeline for a nominally C2v water job. The raw labels are messy due to mixed-case logging and occasional numeric irrep IDs leaking into the text stream. From the following raw irrep tokens, canonicalize and convert to PySCF’s internal integer irrep IDs **only for entries that are valid C2v Mulliken irrep symbols** (i.e., A1/A2/B1/B2 after canonicalization). Use this to (i) generate the MO-block IDs for the first two valid tokens encountered, and then (ii) re-run the conversion for the earliest-occurring A1 token as a consistency check. Raw tokens (in order): [' a1 ', 'B2', 'b3', '2', 'A-1', 'A1'] with point group recorded as 'c2V'. Return the resulting IDs in the order: first valid token, second valid token, then the earliest-occurring A1 token re-mapped.", "answers": "[{\"name\":\"pyscf_symm_addons_irrep_name2id\",\"arguments\":{\"gpname\":\"c2V\",\"symb\":\" a1 \",\"\" : \"\"}}, {\"name\":\"pyscf_symm_addons_irrep_name2id\",\"arguments\":{\"gpname\":\"c2V\",\"symb\":\"B2\"}}, {\"name\":\"pyscf_symm_addons_irrep_name2id\",\"arguments\":{\"gpname\":\"c2V\",\"symb\":\" a1 \"}}]"}
{"func_name": "pyscf_symm_sph_real_sph_vec", "func_desc": "Computes real (Cartesian) spherical harmonics evaluated on a set of 3D vectors for all angular momenta up to the specified maximum lmax.\n    \n    This function is used in PySCF to generate the angular basis functions (real spherical harmonics) that appear in multipole expansions and atomic-orbital-like angular components used across molecular integrals, multipole moment evaluation, and symmetry-adapted basis constructions. The implementation normalizes input Cartesian vectors to unit length (so the angular coordinates are taken on the unit sphere), converts the usual complex spherical harmonics to the real-valued linear combinations, and delegates to the internal multipoles helper to produce the final array layout. The returned data structure matches the convention used elsewhere in PySCF for angular-channel sampling (one entry per l from 0..lmax).", "tools": [{"function": {"description": "Computes real (Cartesian) spherical harmonics evaluated on a set of 3D vectors for all angular momenta up to the specified maximum lmax.\n\nThis function is used in PySCF to generate the angular basis functions (real spherical harmonics) that appear in multipole expansions and atomic-orbital-like angular components used across molecular integrals, multipole moment evaluation, and symmetry-adapted basis constructions. The implementation normalizes input Cartesian vectors to unit length (so the angular coordinates are taken on the unit sphere), converts the usual complex spherical harmonics to the real-valued linear combinations, and delegates to the internal multipoles helper to produce the final array layout. The returned data structure matches the convention used elsewhere in PySCF for angular-channel sampling (one entry per l from 0..lmax).", "name": "pyscf_symm_sph_real_sph_vec", "parameters": {"properties": {"r": {"type": "array", "items": {"type": "float"}, "description": "Real-valued Cartesian coordinate array of the points where the spherical harmonics are evaluated. Expected shape is (npoints, 3), with columns corresponding to x, y, z. The function normalizes each row to unit length internally; the input array is not modified in-place. Supplying zero vectors (rows with norm == 0) will lead to division by zero and produce invalid values or raise an error.", "default": ""}, "lmax": {"type": "integer", "description": "Non-negative integer giving the maximum angular momentum l to include. All angular channels from l = 0 up to and including l = lmax are computed. The returned list therefore has length lmax + 1. Passing a negative value will be treated as invalid and will raise an error.", "default": ""}, "reorder_p": {"type": "boolean", "description": "Boolean flag forwarded to the internal multipoles routine that controls the ordering of the l = 1 (p) components in the returned arrays. When False (default), the spherical-harmonic ordering of magnetic quantum number m (from -l to +l) is preserved. When True, the p-channel components are reordered to match PySCF's internal Cartesian p-orbital ordering convention (practical when these harmonic values are consumed by routines expecting Cartesian p = {px, py, pz} ordering). This flag does not change the numerical values, only the ordering of indices for l = 1.", "default": false}}, "required": ["r", "lmax", "reorder_p"], "type": "any"}}, "type": "function"}], "query": "We’re validating angular-basis stability in a multipole-expansion workflow using raw direction vectors coming from two upstream sources (geometry gradients and bond-direction heuristics). The raw stream contains both physically meaningful directions and degenerate/near-degenerate artifacts from numerics.\n\nDataset A (gradient-derived probes) is:\n[ [1,0,0], [0,1,0], [0,0,1], [1,1,1], [-2,1,0.5], [0,0,0], [1e-14,-1e-14,2e-14] ].\nFor this dataset, evaluate real (Cartesian) spherical harmonics up to lmax=3 only on directions whose Euclidean norm is large enough to define a stable unit direction (i.e., not effectively the zero vector at double precision). Because these are used to cross-check AO-like angular components, return the p-channel explicitly in Cartesian (px, py, pz) order.\n\nDataset B (bond-direction candidates) is:\n[ [1,0,0], [0,1,0], [0,0,1], [1,1,1], [2,2,2], [-1,-1,-1], [1,-1,0], [0.5,0.5,0.5] ].\nFor this dataset, treat vectors that are collinear with another vector in the same set (same or opposite direction after normalization) as redundant and keep only one representative per unique direction. Compute real spherical harmonics up to lmax=3 on the retained set, using the default p-channel ordering (no Cartesian reordering).", "answers": "[{\"name\":\"pyscf_symm_sph_real_sph_vec\",\"arguments\":{\"r\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0],[1.0,1.0,1.0],[-2.0,1.0,0.5]],\"lmax\":3,\"reorder_p\":true}},{\"name\":\"pyscf_symm_sph_real_sph_vec\",\"arguments\":{\"r\":[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0],[1.0,1.0,1.0],[1.0,-1.0,0.0]],\"lmax\":3,\"reorder_p\":false}}]"}
{"func_name": "pyscf_tools_mo_mapping_mo_1to1map", "func_desc": "Compute a one-to-one mapping from row indices i to column indices j using the absolute overlap matrix <i|j>, typically used in PySCF (Python-based Simulations of Chemistry Framework) to associate molecular orbitals from one set (bras) to the closest matching orbitals in another set (kets). In the quantum chemistry workflow of PySCF, this function is used to produce a simple, greedy correspondence between two orbital bases (for example, when comparing orbitals from two calculations or projecting orbitals between different basis sets) by selecting the largest absolute overlap for each row and preventing reuse of the same column index until all columns are exhausted.\n    \n    The algorithm is greedy: for each row i (0..s.shape[0]-1) it selects the column index k with the largest absolute value in that row, records k as the match for i, and then sets that entire column to zero in an internal working copy so that subsequent rows prefer different columns. The input array is not modified; the function operates on an absolute-value copy of s. Because the method is greedy rather than globally optimal, it may produce suboptimal assignments in cases of strongly tied overlaps or when the number of rows exceeds the number of columns.", "tools": [{"function": {"description": "Compute a one-to-one mapping from row indices i to column indices j using the absolute overlap matrix <i|j>, typically used in PySCF (Python-based Simulations of Chemistry Framework) to associate molecular orbitals from one set (bras) to the closest matching orbitals in another set (kets). In the quantum chemistry workflow of PySCF, this function is used to produce a simple, greedy correspondence between two orbital bases (for example, when comparing orbitals from two calculations or projecting orbitals between different basis sets) by selecting the largest absolute overlap for each row and preventing reuse of the same column index until all columns are exhausted.\n\nThe algorithm is greedy: for each row i (0..s.shape[0]-1) it selects the column index k with the largest absolute value in that row, records k as the match for i, and then sets that entire column to zero in an internal working copy so that subsequent rows prefer different columns. The input array is not modified; the function operates on an absolute-value copy of s. Because the method is greedy rather than globally optimal, it may produce suboptimal assignments in cases of strongly tied overlaps or when the number of rows exceeds the number of columns.", "name": "pyscf_tools_mo_mapping_mo_1to1map", "parameters": {"properties": {"s": {"type": "array", "items": {"type": "float"}, "description": "A 2-D numeric array representing overlaps <i|j>, where rows index the \"bra\" orbitals i and columns index the \"ket\" orbitals j. The function uses abs(s) internally, so negative values are treated by magnitude. The function expects a 2-D array; behavior for inputs that are not 2-D, contain NaNs or non-numeric entries, or otherwise violate the expected numeric array semantics is undefined and may raise numpy exceptions.", "default": ""}}, "required": ["s"], "type": "any"}}, "type": "function"}], "query": "In our PySCF MO-tracking QA, we have a mixed bag of braket overlap matrices i|j coming from different run types (baseline, displaced geometry, and a sanity recalculation). Some runs are known to be unreliable when the overlap matrix shows obvious numerical corruption or when the correspondence is underdetermined. Apply the greedy one-to-one MO mapping (absolute-value working copy; per row pick the column with maximum |overlap| and then zero that entire column) only to overlap matrices that satisfy both QC gates: (1) square matrix with dimension N between 4 and 6 inclusive, and (2) every row has at least one |overlap| \u001e 0.75 (so each bra orbital has at least one plausible ket match). Use the matrices below as the raw cohort:\n\nA) Water baseline (4\u0017): [[0.98, -0.01, 0.10, 0.00], [0.05, 0.95, -0.02, 0.01], [0.00, 0.10, 0.90, -0.05], [-0.02, 0.00, 0.03, 0.99]]\nB) Displaced geometry cohort (5\u0017): [[0.02, -0.91, 0.12, 0.05, 0.10], [0.88, 0.15, -0.05, 0.03, 0.02], [0.06, 0.14, 0.85, -0.10, 0.04], [-0.04, 0.08, 0.20, 0.79, -0.02], [0.05, 0.07, -0.02, 0.11, 0.76]]\nC) Recalculation check (4\u0017): [[0.95, -0.02, 0.10, 0.05], [0.01, 0.88, -0.15, 0.20], [0.05, 0.12, 0.90, -0.03], [0.10, 0.25, -0.05, 0.80]]\nD) Suspect truncation artifact (4\u00173): [[0.90, 0.01, -0.02], [0.03, 0.87, 0.04], [-0.01, 0.02, 0.92], [0.05, -0.03, 0.06]]\nE) Noise-dominated run (4\u00174): [[0.40, -0.10, 0.05, 0.02], [0.12, 0.30, -0.08, 0.10], [0.05, 0.11, 0.45, -0.09], [0.02, 0.06, -0.07, 0.20]]\n\nReturn the greedy braket index mapping for every matrix that passes the gates.", "answers": "[{\"name\":\"pyscf_tools_mo_mapping_mo_1to1map\",\"arguments\":{\"s\":[[0.98,-0.01,0.1,0.0],[0.05,0.95,-0.02,0.01],[0.0,0.1,0.9,-0.05],[-0.02,0.0,0.03,0.99]]}},{\"name\":\"pyscf_tools_mo_mapping_mo_1to1map\",\"arguments\":{\"s\":[[0.02,-0.91,0.12,0.05,0.1],[0.88,0.15,-0.05,0.03,0.02],[0.06,0.14,0.85,-0.1,0.04],[-0.04,0.08,0.2,0.79,-0.02],[0.05,0.07,-0.02,0.11,0.76]]}},{\"name\":\"pyscf_tools_mo_mapping_mo_1to1map\",\"arguments\":{\"s\":[[0.95,-0.02,0.1,0.05],[0.01,0.88,-0.15,0.2],[0.05,0.12,0.9,-0.03],[0.1,0.25,-0.05,0.8]]}}]"}
{"func_name": "pyscf_tools_qcschema_recreate_mol_obj", "func_desc": "Recreate a PySCF Mole object from a QCSchema-format dictionary. This function is used in the PySCF tools.qcschema module to translate a QCSchema-compliant molecular description (the JSON-like dictionary used across quantum chemistry tools and data exchange) into an instantiated pyscf.gto.Mole object that can be used for PySCF electronic structure calculations. The routine extracts molecular charge, multiplicity, basis information, basis Cartesian/pure flag, and the atomic geometry (via load_qcschema_molecule), converts units if requested, constructs the pyscf.gto.Mole using the extracted settings, and calls mol.build(False, False) to finalize internal data structures required by downstream PySCF routines.", "tools": [{"function": {"description": "Recreate a PySCF Mole object from a QCSchema-format dictionary. This function is used in the PySCF tools.qcschema module to translate a QCSchema-compliant molecular description (the JSON-like dictionary used across quantum chemistry tools and data exchange) into an instantiated pyscf.gto.Mole object that can be used for PySCF electronic structure calculations. The routine extracts molecular charge, multiplicity, basis information, basis Cartesian/pure flag, and the atomic geometry (via load_qcschema_molecule), converts units if requested, constructs the pyscf.gto.Mole using the extracted settings, and calls mol.build(False, False) to finalize internal data structures required by downstream PySCF routines.\n", "name": "pyscf_tools_qcschema_recreate_mol_obj", "parameters": {"properties": {"qcschema_dict": {"type": "any", "description": "A QCSchema-format dictionary describing the molecule and model. The function reads qcschema_dict[\"molecule\"][\"molecular_charge\"] (converted to int) to set the PySCF charge; qcschema_dict[\"molecule\"][\"molecular_multiplicity\"] (converted to int and then transformed to PySCF spin as multiplicity - 1) to set the PySCF spin (number of unpaired electrons); qcschema_dict[\"model\"][\"basis\"] (converted to str) to set the basis name passed to both the Mole.basis and Mole.ecp arguments in the created Mole; and qcschema_dict[\"keywords\"][\"basisSet\"][\"cartesian\"] (converted to bool) to set the cartesian flag indicating whether the basis uses Cartesian functions. The function also calls load_qcschema_molecule(qcschema_dict, to_Angstrom, False) to obtain the atom specification accepted by pyscf.gto.Mole. If qcschema_dict is missing required keys or contains values of unexpected types, the function will raise the corresponding KeyError or TypeError/ValueError coming from these extractions or from load_qcschema_molecule.", "default": ""}, "to_Angstrom": {"type": "boolean", "description": "Optional flag (default False) that controls unit conversion for geometry. If False (default), the QCSchema geometry is interpreted in Bohr (QCSchema default) and the resulting pymol unit passed to pyscf.gto.Mole is 'B'. If True, the geometry is converted to Angstrom by load_qcschema_molecule and the unit passed to pyscf.gto.Mole is 'A'. Setting this flag affects only how the atomic coordinates are interpreted/converted; it does not otherwise change basis, charge, spin, or cartesian settings.", "default": false}}, "required": ["qcschema_dict", "to_Angstrom"], "type": "any"}}, "type": "function"}], "query": "We’re validating our QCSchema→PySCF ingestion step on a messy micro-cohort of nominal “water” records coming from two upstream sources. Each record is a QCSchema-like dictionary but may differ in unit annotation and metadata.\n\nCohort (three raw records):\n1) Record A:\n{\n  \"molecule\": {\n    \"symbols\": [\"O\",\"H\",\"H\"],\n    \"geometry\": [0.0,0.0,0.0, 0.0,0.75716,0.58626, 0.0,-0.75716,0.58626],\n    \"molecular_charge\": 0,\n    \"molecular_multiplicity\": 1\n  },\n  \"model\": {\"basis\": \"cc-pVDZ\"},\n  \"keywords\": {\"basisSet\": {\"cartesian\": false}}\n}\n\n2) Record B:\n{\n  \"molecule\": {\n    \"symbols\": [\"O\",\"H\",\"H\"],\n    \"geometry\": [0.0,0.0,0.0, 1.43233673,1.10715266,0.0, -1.43233673,1.10715266,0.0],\n    \"molecular_charge\": 0,\n    \"molecular_multiplicity\": 1\n  },\n  \"model\": {\"basis\": \"cc-pVDZ\", \"method\": \"HF\"},\n  \"keywords\": {\"basisSet\": {\"cartesian\": false}}\n}\n\n3) Record C (alternate basis convention):\n{\n  \"molecule\": {\n    \"symbols\": [\"O\",\"H\",\"H\"],\n    \"geometry\": [0.0,0.0,0.0, 0.0,1.4310,1.1070, 0.0,-1.4310,1.1070],\n    \"molecular_charge\": 0,\n    \"molecular_multiplicity\": 1\n  },\n  \"model\": {\"basis\": \"cc-pVDZ\"},\n  \"keywords\": {\"basisSet\": {\"cartesian\": true}}\n}\n\nIngest only records that are already configured for a pure/spherical basis representation (basisSet.cartesian=false). For the selected records, recreate a PySCF gto.Mole from the QCSchema dictionaries and finalize internal structures for downstream calculations. Use geometry conversion behavior that depends on intrinsic scale: if the maximum absolute coordinate component in the provided geometry exceeds 1.0, treat it as Bohr and convert to Angstrom during construction; otherwise, assume the geometry is already in Angstrom and do not request conversion. (Method metadata, if present, should not block ingestion.)", "answers": "[{\"name\":\"pyscf_tools_qcschema_recreate_mol_obj\",\"arguments\":{\"qcschema_dict\":{\"molecule\":{\"symbols\":[\"O\",\"H\",\"H\"],\"geometry\":[0.0,0.0,0.0,0.0,0.75716,0.58626,0.0,-0.75716,0.58626],\"molecular_charge\":0,\"molecular_multiplicity\":1},\"model\":{\"basis\":\"cc-pVDZ\"},\"keywords\":{\"basisSet\":{\"cartesian\":false}}},\"to_Angstrom\":false}},{\"name\":\"pyscf_tools_qcschema_recreate_mol_obj\",\"arguments\":{\"qcschema_dict\":{\"molecule\":{\"symbols\":[\"O\",\"H\",\"H\"],\"geometry\":[0.0,0.0,0.0,1.43233673,1.10715266,0.0,-1.43233673,1.10715266,0.0],\"molecular_charge\":0,\"molecular_multiplicity\":1},\"model\":{\"basis\":\"cc-pVDZ\",\"method\":\"HF\"},\"keywords\":{\"basisSet\":{\"cartesian\":false}}},\"to_Angstrom\":true}}]"}
{"func_name": "rdkit_Chem_AtomPairs_Torsions_ExplainPathScore", "func_desc": "rdkit.Chem.AtomPairs.Torsions.ExplainPathScore decodes an encoded integer path score (the integer produced by the AtomPairs torsion scoring code such as pyScorePath) into a per-atom explanation useful for human inspection and cheminformatics debugging. The function is used in RDKit to recover, for each atom along a scored path, the atom symbol, an adjusted branch/substituent count used by the torsion scoring routine, and the number of pi-electrons/features that were encoded for that atom.", "tools": [{"function": {"description": "rdkit.Chem.AtomPairs.Torsions.ExplainPathScore decodes an encoded integer path score (the integer produced by the AtomPairs torsion scoring code such as pyScorePath) into a per-atom explanation useful for human inspection and cheminformatics debugging. The function is used in RDKit to recover, for each atom along a scored path, the atom symbol, an adjusted branch/substituent count used by the torsion scoring routine, and the number of pi-electrons/features that were encoded for that atom.\n", "name": "rdkit_Chem_AtomPairs_Torsions_ExplainPathScore", "parameters": {"properties": {"score": {"type": "integer", "description": "The encoded path score integer produced by pyScorePath or other AtomPairs torsion scoring routines. This integer contains bit-packed per-atom codes (each codeSize bits wide where codeSize is taken from rdMolDescriptors.AtomPairsParameters.codeSize). The function extracts these per-atom codes from the least-significant bits upward and decodes them with Utils.ExplainAtomCode. The value must be a non-negative integer that was produced by the corresponding scoring routine; negative values or integers encoded with a different codeSize will produce incorrect or undefined decoded results.", "default": ""}, "size": {"type": "integer", "description": "The number of atoms expected in the path (the path length). This defaults to 4. The function will decode exactly size atom codes from the integer (producing a tuple of length size). If size does not match the length used when the score was produced, the decoded information will not correspond to the original path (higher-order codes beyond the encoded bit-length will appear as zeros). size must be an integer.", "default": 4}}, "required": ["score", "size"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a torsion-scoring reproducibility issue from mixed AtomPairs torsion exports where the path-length metadata is sometimes wrong. Below is a small QC bundle of candidate encoded path-score integers that should correspond to a single torsion (4 atoms) when the encoding is plausible. For each candidate, infer the torsion path size from its encoding width: use size=4 when the score is a 9-digit integer, size=5 when it’s a 12-digit integer. Decode only the candidates that are strictly positive integers and whose digit-length matches one of those two widths (9 or 12). For each decoded score, return the per-atom explanation (atom symbol, adjusted substituent count used by the torsion routine, and the encoded pi-electron/features) so we can compare across extraction variants.\n\nQC bundle (raw): [987654321, 100200300400, -555555555, 12345678, 314159265, 444444444444, 0, 900000000]", "answers": "[{\"name\":\"rdkit_Chem_AtomPairs_Torsions_ExplainPathScore\",\"arguments\":{\"score\":987654321,\"size\":4}},{\"name\":\"rdkit_Chem_AtomPairs_Torsions_ExplainPathScore\",\"arguments\":{\"score\":100200300400,\"size\":5}},{\"name\":\"rdkit_Chem_AtomPairs_Torsions_ExplainPathScore\",\"arguments\":{\"score\":314159265,\"size\":4}},{\"name\":\"rdkit_Chem_AtomPairs_Torsions_ExplainPathScore\",\"arguments\":{\"score\":444444444444,\"size\":5}},{\"name\":\"rdkit_Chem_AtomPairs_Torsions_ExplainPathScore\",\"arguments\":{\"score\":900000000,\"size\":4}}]"}
{"func_name": "rdkit_Chem_AtomPairs_Utils_BitsInCommon", "func_desc": "Compute the number of bit identifiers that occur in common between two sorted bit‑id vectors used as molecular fingerprints.\n    \n    This function is part of RDKit's AtomPairs.Utils utilities and is used in cheminformatics tasks that compare fingerprint-like representations of molecules. Each input vector represents the positions (IDs) of set bits in a fingerprint; the function counts how many bit IDs are shared between the two vectors. In practical use within RDKit this count can serve as the numerator for similarity measures (for example, the intersection used when computing Tanimoto similarity) or as a fast overlap check between atom-pair/fingerprint bit lists.", "tools": [{"function": {"description": "Compute the number of bit identifiers that occur in common between two sorted bit‑id vectors used as molecular fingerprints.\n\nThis function is part of RDKit's AtomPairs.Utils utilities and is used in cheminformatics tasks that compare fingerprint-like representations of molecules. Each input vector represents the positions (IDs) of set bits in a fingerprint; the function counts how many bit IDs are shared between the two vectors. In practical use within RDKit this count can serve as the numerator for similarity measures (for example, the intersection used when computing Tanimoto similarity) or as a fast overlap check between atom-pair/fingerprint bit lists.", "name": "rdkit_Chem_AtomPairs_Utils_BitsInCommon", "parameters": {"properties": {"v1": {"type": "any", "description": "A tuple of bit identifiers (integers) representing set bits in the first fingerprint vector. The tuple must be sorted in non-decreasing (ascending) order. Duplicate bit IDs are allowed and are significant: each duplicate occurrence in v1 contributes separately to the count if matched in v2. The function iterates over v1 and advances a pointer through v2 to identify matches efficiently.", "default": ""}, "v2": {"type": "any", "description": "A tuple of bit identifiers (integers) representing set bits in the second fingerprint vector. The tuple must be sorted in non-decreasing (ascending) order. Duplicate bit IDs are allowed and are counted multiple times when matched against duplicates in v1. The function assumes v2 supports len() and index access for forward scanning.", "default": ""}}, "required": ["v1", "v2"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing our atom-pair overlap stage under realistic registry noise. You’re given four candidate molecule-pair records; each record provides two *sorted* atom-pair bit-ID vectors (duplicates may appear due to atom-pair degeneracy). Before computing overlap, apply a data-sieve rule consistent with our production fingerprints: treat any bit-ID outside the canonical RDKit atom-pair bit range [0, 1023] as nonphysical and drop it. Then, for each record that still has at least one retained bit-ID in both vectors, compute the intersection count (bits in common, honoring duplicate multiplicity) to use as the numerator for similarity screening.\n\nRecords:\nA: v1=[12, 47, 47, 103, 250, 999], v2=[5, 47, 103, 103, 250, 400, 999, 1200]\nB: v1=[-3, 0, 1, 1, 2, 2048], v2=[-1, 1, 2, 2, 3, 4096]\nC: v1=[1024, 2048], v2=[7, 8, 9]\nD: v1=[12, 45, 45, 87, 120, 305, 450, 450, 451], v2=[10, 45, 87, 200, 305, 450, 452]\n\nReturn the overlap counts for the qualifying records (i.e., after range-filtering and empty-vector checks).", "answers": "[{\"name\":\"rdkit_Chem_AtomPairs_Utils_BitsInCommon\",\"arguments\":{\"v1\":[12,47,47,103,250,999],\"v2\":[5,47,103,103,250,400,999]}},{\"name\":\"rdkit_Chem_AtomPairs_Utils_BitsInCommon\",\"arguments\":{\"v1\":[0,1,1,2],\"v2\":[1,2,2,3]}},{\"name\":\"rdkit_Chem_AtomPairs_Utils_BitsInCommon\",\"arguments\":{\"v1\":[12,45,45,87,120,305,450,450,451],\"v2\":[10,45,87,200,305,450,452]}}]"}
{"func_name": "rdkit_Chem_AtomPairs_Utils_CosineSimilarity", "func_desc": "rdkit.Chem.AtomPairs.Utils.CosineSimilarity computes the cosine similarity between two fingerprint vectors represented as sorted tuples of bit identifiers. This function implements the metric recommended in the LaSSI paper and is intended for comparing molecular fingerprints or other non-negative integer bit-id encodings used in RDKit for similarity searching, clustering, and descriptor-based machine learning.", "tools": [{"function": {"description": "rdkit.Chem.AtomPairs.Utils.CosineSimilarity computes the cosine similarity between two fingerprint vectors represented as sorted tuples of bit identifiers. This function implements the metric recommended in the LaSSI paper and is intended for comparing molecular fingerprints or other non-negative integer bit-id encodings used in RDKit for similarity searching, clustering, and descriptor-based machine learning.\n", "name": "rdkit_Chem_AtomPairs_Utils_CosineSimilarity", "parameters": {"properties": {"v1": {"type": "any", "description": "A tuple of integers representing the first fingerprint vector as sorted bit identifiers. Each element corresponds to a set or multiset occurrence of a fingerprint bit; duplicates are allowed and are treated according to the Dot() implementation used internally. The tuple must be sorted in non-decreasing order for the expected behavior; the function does not sort its inputs and may produce incorrect results if the inputs are not pre-sorted.", "default": ""}, "v2": {"type": "any", "description": "A tuple of integers representing the second fingerprint vector as sorted bit identifiers. Same conventions and requirements as v1 apply.", "default": ""}}, "required": ["v1", "v2"], "type": "any"}}, "type": "function"}], "query": "We’re doing ligand-cohort replicate QC using RDKit atom-pair fingerprint bit-ID multisets, but the export stream is messy: each record contains two sorted bit-ID tuples that may include placeholder/sentinel IDs from failed featurization. For this LaSSI cosine run, treat each tuple as a count vector (multiplicity matters) and compute cosine similarity only for replicate comparisons where (i) both tuples are non-empty after cleaning and (ii) all remaining bit IDs are in the chemically meaningful range 1–1024 (inclusive). Any replicate that violates the range constraint should be excluded by the rule. Apply this QC rule to the following three replicate comparisons and return cosine similarities for the ones that qualify:\n- Replicate A: v1=(12, 12, 45, 130, 900) vs v2=(12, 45, 45, 131, 900, 1200)\n- Replicate B: v1=(10, 10, 25, 40, 40, 40, 72, 101) vs v2=(10, 20, 25, 25, 40, 90, 101, 150)\n- Replicate C: v1=(12, 45, 45, 101, 305, 900) vs v2=(12, 45, 77, 101, 305, 305, 1200)", "answers": "[{\"name\":\"rdkit_Chem_AtomPairs_Utils_CosineSimilarity\",\"arguments\":{\"v1\":[10,10,25,40,40,40,72,101],\"v2\":[10,20,25,25,40,90,101,150]}},{\"name\":\"rdkit_Chem_AtomPairs_Utils_CosineSimilarity\",\"arguments\":{\"v1\":[12,12,45,130,900],\"v2\":[12,45,45,131,900]}},{\"name\":\"rdkit_Chem_AtomPairs_Utils_CosineSimilarity\",\"arguments\":{\"v1\":[12,45,45,101,305,900],\"v2\":[12,45,77,101,305,305]}}]"}
{"func_name": "rdkit_Chem_ChemUtils_DescriptorUtilities_setDescriptorVersion", "func_desc": "rdkit.Chem.ChemUtils.DescriptorUtilities.setDescriptorVersion sets a version string on a descriptor function and returns a decorator that applies that metadata to the function. This is intended for use in the RDKit cheminformatics library to annotate descriptor-calculation callables (functions used to compute molecular descriptors and features for machine learning and database storage) with a stable version identifier that can be inspected by downstream tooling, descriptor registries, or serialization/database cartridges.", "tools": [{"function": {"description": "rdkit.Chem.ChemUtils.DescriptorUtilities.setDescriptorVersion sets a version string on a descriptor function and returns a decorator that applies that metadata to the function. This is intended for use in the RDKit cheminformatics library to annotate descriptor-calculation callables (functions used to compute molecular descriptors and features for machine learning and database storage) with a stable version identifier that can be inspected by downstream tooling, descriptor registries, or serialization/database cartridges.\n", "name": "rdkit_Chem_ChemUtils_DescriptorUtilities_setDescriptorVersion", "parameters": {"properties": {"version": {"type": "string", "description": "The version string to attach to a descriptor function. This argument is the metadata value that will be assigned to the decorated function as its .version attribute. The default value is \"1.0.0\". In RDKit workflows this string is typically used to indicate the descriptor implementation or schema version (for example, following semantic versioning) so consumers of descriptor values (machine-learning pipelines, descriptor registries, or database cartridges) can detect changes in descriptor calculation logic or provenance.", "default": "1.0.0"}}, "required": ["version"], "type": "any"}}, "type": "function"}], "query": "We’re hardening our QSAR descriptor registry for reproducible retraining and DB cartridge compatibility. Below is a mixed batch of descriptor implementations coming from different code paths (new development branch, legacy registry, and hotfix patches). Each entry has a current `__version__` string (may be missing or malformed) and a target release train. Apply RDKit’s descriptor metadata mechanism to attach a stable semantic version string, but only for callables whose target version is a valid SemVer `MAJOR.MINOR.PATCH` and whose release train is either `stable` or `registry`.\n\nBatch manifest (descriptor_name, release_train, target_version):\n- MolComplexityScore, stable, 2.3.1\n- InHouseRegistryEntry, registry, 2.1.0\n- MolPolarityScore, stable, 2.3.1\n- ExperimentalFragmentCount, nightly, 2.4.0-alpha\n- LegacyTopoIndex, stable, 2.1\n\nAnnotate each qualifying descriptor implementation with the requested version using RDKit’s version-tagging decorator factory.", "answers": "[{\"name\":\"rdkit_Chem_ChemUtils_DescriptorUtilities_setDescriptorVersion\",\"arguments\":{\"version\":\"2.3.1\"}},{\"name\":\"rdkit_Chem_ChemUtils_DescriptorUtilities_setDescriptorVersion\",\"arguments\":{\"version\":\"2.1.0\"}},{\"name\":\"rdkit_Chem_ChemUtils_DescriptorUtilities_setDescriptorVersion\",\"arguments\":{\"version\":\"2.3.1\"}}]"}
{"func_name": "rdkit_Chem_EState_AtomTypes_BuildPatts", "func_desc": "BuildPatts compiles SMARTS patterns into RDKit Mol pattern objects and stores them in the module-level esPatterns list used by the EState atom-typing routines.\n    \n    This function is part of RDKit's cheminformatics EState (electrotopological state) machinery and is intended for internal use: it translates a sequence of named SMARTS pattern specifications into RDKit molecule pattern objects (Chem.Mol) that other EState code uses to match atom types. If rawV is omitted, the function reads the pattern specifications from the module-level _rawD. The function has no return value; instead it updates the global esPatterns variable to a list with the same length as rawV where each element is either None (if compilation failed or the pattern was skipped) or a tuple (name, patt) with patt being the RDKit Mol object produced by Chem.MolFromSmarts(sma). On SMARTS compilation failures the function writes a warning message to sys.stderr and leaves the corresponding esPatterns entry as None. Typical callers are other EState descriptor/atom-typing routines that expect esPatterns to contain compiled patterns for matching against molecule atoms.", "tools": [{"function": {"description": "BuildPatts compiles SMARTS patterns into RDKit Mol pattern objects and stores them in the module-level esPatterns list used by the EState atom-typing routines.\n\nThis function is part of RDKit's cheminformatics EState (electrotopological state) machinery and is intended for internal use: it translates a sequence of named SMARTS pattern specifications into RDKit molecule pattern objects (Chem.Mol) that other EState code uses to match atom types. If rawV is omitted, the function reads the pattern specifications from the module-level _rawD. The function has no return value; instead it updates the global esPatterns variable to a list with the same length as rawV where each element is either None (if compilation failed or the pattern was skipped) or a tuple (name, patt) with patt being the RDKit Mol object produced by Chem.MolFromSmarts(sma). On SMARTS compilation failures the function writes a warning message to sys.stderr and leaves the corresponding esPatterns entry as None. Typical callers are other EState descriptor/atom-typing routines that expect esPatterns to contain compiled patterns for matching against molecule atoms.", "name": "rdkit_Chem_EState_AtomTypes_BuildPatts", "parameters": {"properties": {"rawV": {"type": "any", "nullable": true, "description": "A list of pattern specifications. Each element must be a two-element sequence (name, sma) where name is a textual identifier (usually a string) for the pattern and sma is a SMARTS pattern string that describes the atom or substructure to match. If rawV is None (the default), the function reads the pattern list from the module-level _rawD. The function uses len(rawV) to size the esPatterns list and iterates over rawV with enumerate, so rawV must be a sequence with a well-defined length and must yield 2-tuples on iteration.", "default": null}}, "required": ["rawV"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an EState atom-typing SMARTS compilation cache under realistic curation rules. We have three incoming “panels” of (label, SMARTS) pairs from different teams, but the source data is messy: labels may be duplicated with different casing, SMARTS may contain leading/trailing whitespace, and some entries are known to be instrumented negative-controls (they embed an unmatched bracket so compilation should fail and land as a None slot while emitting a warning). Run three independent BuildPatts batches, each time updating the module-level EState esPatterns cache.\n\nBatch 1 (functional-group intake):\n- AromaticCarbon = \"  [c]  \"\n- CarbonylOxygen = \"[O]=[C]\"\n- TertiaryAmine = \"[N+](C)(C)C\"\n- BrokenPattern = \"[C\"  (negative-control)\n\nCuration rule for Batch 1: normalize labels by stripping whitespace; normalize SMARTS by stripping whitespace; keep the original ordering.\n\nBatch 2 (core atom-typing intake):\n- alkane_carbon = \"[CX4]\"\n- aromatic_carbon = \"c\"\n- hydroxyl_oxygen = \"[OX2H]\"\n- broken_pattern = \" [C\"  (negative-control with leading space)\n\nCuration rule for Batch 2: treat labels as case-insensitive for deduplication; if duplicates occur, retain the first occurrence; preserve the remaining order.\n\nBatch 3 (subtype-focused intake):\n- alkyl_C_sp3 = \"[CX4H3,H2][CH3]\"\n- aryl_C_sp2 = \"c1ccccc1\"\n\nCuration rule for Batch 3: compile only patterns whose SMARTS contains an explicit ring closure digit; others are still carried through as-is in the panel but should not be compiled in this batch.\n\nCompile each batch after applying its curation rule and update the global EState pattern cache each time (three BuildPatts calls total).", "answers": "[{\"name\":\"rdkit_Chem_EState_AtomTypes_BuildPatts\",\"arguments\":{\"rawV\":[[\"AromaticCarbon\",\"[c]\"],[\"CarbonylOxygen\",\"[O]=[C]\"],[\"TertiaryAmine\",\"[N+](C)(C)C\"],[\"BrokenPattern\",\"[C\"]]}},{\"name\":\"rdkit_Chem_EState_AtomTypes_BuildPatts\",\"arguments\":{\"rawV\":[[\"alkane_carbon\",\"[CX4]\"],[\"aromatic_carbon\",\"c\"],[\"hydroxyl_oxygen\",\"[OX2H]\"],[\"broken_pattern\",\"[C\"]]}},{\"name\":\"rdkit_Chem_EState_AtomTypes_BuildPatts\",\"arguments\":{\"rawV\":[[\"aryl_C_sp2\",\"c1ccccc1\"]]}}]"}
{"func_name": "rdkit_Chem_EState_EState_GetPrincipleQuantumNumber", "func_desc": "rdkit.Chem.EState.EState.GetPrincipleQuantumNumber maps an atomic number to the corresponding principal quantum number used by RDKit's EState (electrotopological state) calculations. This function is a small utility in the EState module that converts an element's atomic number (Z) into the principal quantum number (n) that represents the electron shell level used in descriptor calculations.", "tools": [{"function": {"description": "rdkit.Chem.EState.EState.GetPrincipleQuantumNumber maps an atomic number to the corresponding principal quantum number used by RDKit's EState (electrotopological state) calculations. This function is a small utility in the EState module that converts an element's atomic number (Z) into the principal quantum number (n) that represents the electron shell level used in descriptor calculations.\n", "name": "rdkit_Chem_EState_EState_GetPrincipleQuantumNumber", "parameters": {"properties": {"atNum": {"type": "integer", "description": "The atomic number (Z) of the element for which the principal quantum number is required. In the RDKit EState context this is the integer element number used when computing electrotopological state descriptors. The function expects an integer input; typical inputs are positive integers corresponding to chemical elements (e.g., 1 for hydrogen, 6 for carbon). The implementation does not perform explicit type validation, so passing a non-integer or a type that does not support integer comparisons may raise a TypeError in normal Python execution. Values less than or equal to 2 are treated the same as atomic numbers 1 and 2 (see behavior below).", "default": ""}}, "required": ["atNum"], "type": "any"}}, "type": "function"}], "query": "As part of an EState/QSAR ingestion QC step, we receive a mixed list of atom annotations extracted from drug-like scaffolds where some entries are malformed or outside the physically meaningful element range. Use RDKit’s EState principal quantum number mapping only for entries whose atomic number corresponds to (i) a second-row p-block heteroatom frequently used as a hydrogen-bonding center and (ii) a third-row halogen commonly used in substituent scans; then immediately repeat the same two lookups as an independent replicate to verify determinism. Finally, among the remaining valid annotations, identify the heaviest halogen present in the batch and run the same mapping for it to confirm behavior on high-Z halogens.\n\nRaw annotations (Z): [7, 17, 0, -1, 118, 119, 53, 16, null, 7]", "answers": "[{\"name\":\"rdkit_Chem_EState_EState_GetPrincipleQuantumNumber\",\"arguments\":{\"atNum\":7}},{\"name\":\"rdkit_Chem_EState_EState_GetPrincipleQuantumNumber\",\"arguments\":{\"atNum\":17}},{\"name\":\"rdkit_Chem_EState_EState_GetPrincipleQuantumNumber\",\"arguments\":{\"atNum\":7}},{\"name\":\"rdkit_Chem_EState_EState_GetPrincipleQuantumNumber\",\"arguments\":{\"atNum\":17}},{\"name\":\"rdkit_Chem_EState_EState_GetPrincipleQuantumNumber\",\"arguments\":{\"atNum\":53}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_GetAllCombinations", "func_desc": "rdkit.Chem.Pharm2D.Utils.GetAllCombinations enumerates all possible combinations formed by picking one element from each sequence in a list of sequences. In the context of RDKit (a cheminformatics library) and the Pharm2D utilities, this function is used to generate all possible tuples of pharmacophoric/feature choices when constructing 2D pharmacophore descriptors or fingerprints; these enumerated combinations are the raw candidate feature vectors that downstream code will score or encode into descriptors.", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.GetAllCombinations enumerates all possible combinations formed by picking one element from each sequence in a list of sequences. In the context of RDKit (a cheminformatics library) and the Pharm2D utilities, this function is used to generate all possible tuples of pharmacophoric/feature choices when constructing 2D pharmacophore descriptors or fingerprints; these enumerated combinations are the raw candidate feature vectors that downstream code will score or encode into descriptors.\n", "name": "rdkit_Chem_Pharm2D_Utils_GetAllCombinations", "parameters": {"properties": {"choices": {"type": "array", "items": {"type": "float"}, "description": "A list (sequence) of sequences; each element of choices is itself a sequence (for example, a tuple or list) containing alternative values/options for that position. The function returns combinations that pick exactly one element from each sequence in choices, preserving the original ordering of positions. Practical significance: in Pharm2D generation this models the alternative features or atom/group choices at successive positions when building feature tuples.", "default": ""}, "noDups": {"type": "integer", "description": "If nonzero, combinations that would contain duplicate values (for example, a constructed combination like [1, 1, 0] where the same value appears in more than one selected position) are omitted from the result. If zero, duplicates are allowed. Default is 1 (duplicates suppressed). This parameter enables callers to avoid degenerate or redundant tuples when elements represent identical chemical features.", "default": 1}, "which": {"type": "integer", "description": "Internal recursion index used to track the current position in choices; callers should normally leave this as the default 0. The function recurses by incrementing which until it reaches the last sequence. Practical significance: callers may pass a different value only for advanced recursive use; passing values >= len(choices) causes the function to return an empty list. Negative or out-of-range values are allowed by the implementation but are intended for internal recursion control and may lead to empty results or other non-useful outputs.", "default": 0}}, "required": ["choices", "noDups", "which"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing Pharm2D feature-vocabulary enumeration against messy, mixed provenance vocabularies coming from three assay teams. Each team provides 3 pharmacophore “slots” (position-1..3), but the curation policy differs by how much semantic overlap exists within that team’s slot lists.\n\nTeam A (legacy abbreviated vocabulary):\n- position-1 candidates: {HBA, HBD}\n- position-2 candidates: {AROM, HYD}\n- position-3 candidates: {POS, NEG}\nEnumerate all possible 3-tuples by selecting exactly one label per position.\n\nTeam B (cross-validation vocabulary, spelled-out labels):\n- position-1 candidates: {HBA, HBD}\n- position-2 candidates: {Aromatic, Hydrophobe}\n- position-3 candidates: {PosIon, NegIon}\nEnumerate all possible 3-tuples by selecting exactly one label per position.\n\nTeam C (stress-test panel with intra-tuple redundancy risk):\n- position-1 candidates: {HBD, HBA}\n- position-2 candidates: {AROM, POS}\n- position-3 candidates: {HYD, HBA}\nBecause this panel can produce tuples where the same feature label appears in more than one position, enumerate candidates only under a strict uniqueness constraint per tuple (i.e., retain only tuples whose chosen labels are all distinct).\n\nGenerate the full list of candidate tuples for each team for downstream scoring/encoding.", "answers": "[{\"name\":\"rdkit_Chem_Pharm2D_Utils_GetAllCombinations\",\"arguments\":{\"choices\":[[\"HBA\",\"HBD\"],[\"AROM\",\"HYD\"],[\"POS\",\"NEG\"]],\"noDups\":0,\"which\":0}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_GetAllCombinations\",\"arguments\":{\"choices\":[[\"HBA\",\"HBD\"],[\"Aromatic\",\"Hydrophobe\"],[\"PosIon\",\"NegIon\"]],\"noDups\":0,\"which\":0}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_GetAllCombinations\",\"arguments\":{\"choices\":[[\"HBD\",\"HBA\"],[\"AROM\",\"POS\"],[\"HYD\",\"HBA\"]],\"noDups\":1,\"which\":0}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_GetUniqueCombinations", "func_desc": "rdkit.Chem.Pharm2D.Utils.GetUniqueCombinations returns the set of unique, deterministic mappings between provided class identifiers and one selected item from each corresponding choices list while avoiding selections that reuse the same item across multiple class positions. In the RDKit Pharm2D context this utility is used when generating combinatorial assignments of pharmacophore or feature labels (classes) to candidate feature instances (choices) for descriptor/fingerprint enumeration, ensuring each returned combination maps each class to a distinct feature and that equivalent assignments are deduplicated and returned in a stable order.", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.GetUniqueCombinations returns the set of unique, deterministic mappings between provided class identifiers and one selected item from each corresponding choices list while avoiding selections that reuse the same item across multiple class positions. In the RDKit Pharm2D context this utility is used when generating combinatorial assignments of pharmacophore or feature labels (classes) to candidate feature instances (choices) for descriptor/fingerprint enumeration, ensuring each returned combination maps each class to a distinct feature and that equivalent assignments are deduplicated and returned in a stable order.\n", "name": "rdkit_Chem_Pharm2D_Utils_GetUniqueCombinations", "parameters": {"properties": {"choices": {"type": "array", "items": {"type": "float"}, "description": "A list with one element per class position; each element should be a sequence (for example a list) of candidate items that may be selected for that class. The function uses itertools.product(*choices) to iterate over every possible selection of one item from each element of choices. The practical significance in Pharm2D workflows is that choices enumerates alternative feature instances (e.g., different atoms or pharmacophore points) that could be assigned to each label when building 2D pharmacophore combinations.", "default": ""}, "classes": {"type": "array", "items": {"type": "float"}, "description": "A list of class identifiers corresponding position-by-position to entries of choices. len(classes) must equal len(choices); the function asserts this and will raise AssertionError if lengths differ. In the Pharm2D domain these are the feature labels (for example pharmacophore types or descriptor slots) that will be paired with chosen items from the matching entry of choices.", "default": ""}}, "required": ["choices", "classes"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing Pharm2D feature-instance assignment determinism under realistic curation rules. For each cohort below, first sanitize the per-class candidate lists as if they came from noisy feature detection: (i) treat repeated candidate IDs within the same class list as redundant detections and collapse them to unique IDs while preserving first-seen order; (ii) treat any non-integer or negative candidate IDs as extraction artifacts and ignore them. After sanitization, decide whether the cohort is eligible for enumeration using an intrinsic feasibility gate: only enumerate cohorts where the union of all remaining candidate IDs contains at least as many distinct IDs as there are classes (otherwise the no-reuse constraint cannot be satisfied). For each eligible cohort, enumerate all unique, deterministic class→instance assignments under the no-reuse constraint and return them in stable order.\n\nCohort 1 (toy atom-index mapping; includes redundant detections): classes [\"A\",\"B\",\"C\"] with raw candidate indices per class [[0,2,2],[1,2],[0,3,3]].\nCohort 2 (2D pharmacophore enumerator output; includes a bad artifact and redundancy): classes [\"HBA\",\"HBD\",\"AROM\"] with raw candidate IDs [[101,102,102],[-1,102,103],[101,104]].\nCohort 3 (degenerate detection scenario to probe feasibility gating): classes [\"POS\",\"NEG\",\"HYD\"] with raw candidate IDs [[7,7],[7],[7,7,7]].", "answers": "[{\"name\":\"rdkit_Chem_Pharm2D_Utils_GetUniqueCombinations\",\"arguments\":{\"choices\":[[0,2],[1,2],[0,3]],\"classes\":[\"A\",\"B\",\"C\"]}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_GetUniqueCombinations\",\"arguments\":{\"choices\":[[101,102],[102,103],[101,104]],\"classes\":[\"HBA\",\"HBD\",\"AROM\"]}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_NumCombinations", "func_desc": "rdkit.Chem.Pharm2D.Utils.NumCombinations: Compute the number of unordered combinations with repetition for placing a set of items into a fixed number of slots. This function implements the \"stars and bars\" combinatorial formula used in RDKit's Pharm2D utilities to count how many distinct multisets of size nSlots can be formed from nItems types (for example, counting ways to assign pharmacophore feature types to fingerprint slots when order does not matter and repeats are allowed).", "tools": [{"function": {"description": "rdkit.Chem.Pharm2D.Utils.NumCombinations: Compute the number of unordered combinations with repetition for placing a set of items into a fixed number of slots. This function implements the \"stars and bars\" combinatorial formula used in RDKit's Pharm2D utilities to count how many distinct multisets of size nSlots can be formed from nItems types (for example, counting ways to assign pharmacophore feature types to fingerprint slots when order does not matter and repeats are allowed).\n", "name": "rdkit_Chem_Pharm2D_Utils_NumCombinations", "parameters": {"properties": {"nItems": {"type": "integer", "description": "The number of distinct item types (N in the combinatorial formula). In the Pharm2D context this corresponds to the number of distinct feature categories or bins that can be placed into slots. This must be an integer; passing values that are not integers or that lead to invalid binomial parameters will raise the underlying exception from the comb implementation (typically TypeError or ValueError).", "default": ""}, "nSlots": {"type": "integer", "description": "The number of slots to fill (S in the combinatorial formula). In the Pharm2D context this is the number of positions in a descriptor or fingerprint that will be populated from the nItems types. This must be an integer; invalid values (negative or those that make the binomial arguments invalid) will cause the underlying comb call to raise an exception.", "default": ""}}, "required": ["nItems", "nSlots"], "type": "any"}}, "type": "function"}], "query": "We’re sanity-checking Pharm2D feature-space growth under messy, evolving feature alphabets before launching a large-scale fingerprint grid search. Below is a mixed manifest of candidate pharmacophore feature sets coming from different assay teams; some entries are incomplete snapshots or misconfigured exports.\n\nManifest (each entry lists the feature labels it *claims* to include):\n1) [HBA, HBD, aromatic, hydrophobe, positive, negative, halogen]\n2) [HBA, HBD, aromatic, hydrophobe, positive, negative]\n3) [HBA, HBD, aromatic, hydrophobe, positive, negative]  \n4) [HBA, HBD, aromatic, hydrophobe, positive]  \n5) [HBA, HBD, aromatic, hydrophobe, positive, negative, metal]\n\nFor the audit, treat an entry as eligible only if it represents a complete, canonical alphabet snapshot used in production (i.e., it contains exactly the canonical 6 labels or exactly the canonical 7-label extension where the only extra label beyond the 6 is ‘halogen’).\n\nFor each eligible entry, compute the number of distinct unordered 4-slot multisets (combinations with repetition; stars-and-bars) implied by the number of feature types in that entry. Keep the two identical canonical-6 entries as independent replicates (so both should be counted separately).", "answers": "[{\"name\":\"rdkit_Chem_Pharm2D_Utils_NumCombinations\",\"arguments\":{\"nItems\":7,\"nSlots\":4}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_NumCombinations\",\"arguments\":{\"nItems\":6,\"nSlots\":4}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_NumCombinations\",\"arguments\":{\"nItems\":6,\"nSlots\":4}}]"}
{"func_name": "rdkit_Chem_Pharm2D_Utils_UniquifyCombinations", "func_desc": "UniquifyCombinations in rdkit.Chem.Pharm2D.Utils deduplicates a list of combinations by treating each inner combination as an unordered set of elements. This utility is useful in the RDKit Pharm2D context (cheminformatics/pharmacophore 2D utilities) when generating or comparing sets of pharmacophore point combinations or other element combinations where element order does not matter: it returns one representative tuple for each unique collection of elements regardless of their order in the input.", "tools": [{"function": {"description": "UniquifyCombinations in rdkit.Chem.Pharm2D.Utils deduplicates a list of combinations by treating each inner combination as an unordered set of elements. This utility is useful in the RDKit Pharm2D context (cheminformatics/pharmacophore 2D utilities) when generating or comparing sets of pharmacophore point combinations or other element combinations where element order does not matter: it returns one representative tuple for each unique collection of elements regardless of their order in the input.\n", "name": "rdkit_Chem_Pharm2D_Utils_UniquifyCombinations", "parameters": {"properties": {"combos": {"type": "array", "items": {"type": "float"}, "description": "A list whose elements are sequences representing combinations (for example, lists of atom or pharmacophore point identifiers). Each inner sequence is expected to support slicing (combo[:]) and an in-place sort method (k.sort()) or otherwise be convertible to a mutable, sortable sequence prior to calling this function. The function treats each inner sequence as an unordered collection of values and uses a sorted form of the inner sequence as the uniqueness key. In practice within RDKit Pharm2D code, combos will typically be a list of lists of integers or strings identifying features; if inner sequences are not sortable or contain incomparable elements, a TypeError or AttributeError will be raised.", "default": ""}}, "required": ["combos"], "type": "any"}}, "type": "function"}], "query": "We’re QC’ing a Pharm2D feature-combination enumeration dump coming from two generators (one emits integer feature indices, the other emits feature IDs). Before deduplication, apply a sieve consistent with our 2D pharmacophore constraints: keep only triplets that represent *3 distinct pharmacophore points* (i.e., no repeated element within a triplet). Then, within each cohort, collapse redundancy where ordering is irrelevant by uniquifying the remaining triplets as unordered multisets (so any permutations map to the same representative). Cohort A (index triplets): [[0,2,5],[5,0,2],[1,3,4],[4,1,3],[2,2,7],[7,2,2]]. Cohort B (feature-ID triplets): [[\"HBA1\",\"AROM3\",\"HBD2\"],[\"HBD2\",\"HBA1\",\"AROM3\"],[\"POS5\",\"HBA1\",\"AROM3\"],[\"AROM3\",\"POS5\",\"HBA1\"],[\"HBA1\",\"HBA4\",\"NEG6\"]]. Return one representative per unique unordered combination for each cohort after QC filtering.", "answers": "[{\"name\":\"rdkit_Chem_Pharm2D_Utils_UniquifyCombinations\",\"arguments\":{\"combos\":[[0,2,5],[5,0,2],[1,3,4],[4,1,3]]}},{\"name\":\"rdkit_Chem_Pharm2D_Utils_UniquifyCombinations\",\"arguments\":{\"combos\":[[\"HBA1\",\"AROM3\",\"HBD2\"],[\"HBD2\",\"HBA1\",\"AROM3\"],[\"POS5\",\"HBA1\",\"AROM3\"],[\"AROM3\",\"POS5\",\"HBA1\"],[\"HBA1\",\"HBA4\",\"NEG6\"]]}}]"}
{"func_name": "rdkit_Chem_Pharm3D_EmbedLib_CombiEnum", "func_desc": "rdkit.Chem.Pharm3D.EmbedLib.CombiEnum generates all combinations that pick one element from each subsequence in a tuple of subsequences. This generator is a small utility used in RDKit's Pharm3D embedding and pharmacophore-related code to enumerate alternative assignments (for example, alternative placements or feature choices) without materializing the full Cartesian product in memory; it yields each combination as a list in the same order as the input subsequences.", "tools": [{"function": {"description": "rdkit.Chem.Pharm3D.EmbedLib.CombiEnum generates all combinations that pick one element from each subsequence in a tuple of subsequences. This generator is a small utility used in RDKit's Pharm3D embedding and pharmacophore-related code to enumerate alternative assignments (for example, alternative placements or feature choices) without materializing the full Cartesian product in memory; it yields each combination as a list in the same order as the input subsequences.\n", "name": "rdkit_Chem_Pharm3D_EmbedLib_CombiEnum", "parameters": {"properties": {"sequence": {"type": "any", "description": "A tuple of subsequences (each subsequence is expected to be an iterable such as a tuple or list) that provide candidate values for each position in the combination. The role of this parameter in the Pharm3D/embedding domain is to represent, for each pharmacophore position or decision point, the set of alternatives to be enumerated. The function uses len(), indexing and slicing on this tuple. If the top-level tuple is empty (len(sequence) == 0) the generator yields a single empty list (one valid combination of zero choices). If any subsequence is empty, the generator will produce no combinations (i.e., it will be empty), because there is no valid choice for that position.", "default": ""}}, "required": ["sequence"], "type": "any"}}, "type": "function"}], "query": "I’m enumerating alternative pharmacophore embeddings for two replicate 3-point hypotheses, but the upstream feature picker is noisy. For each cohort, treat each decision list as raw candidates and first apply a lightweight QC sieve: within each decision list, keep only entries that are (i) integer-valued and (ii) strictly positive. If a cohort ends up with fewer than 3 non-empty decision lists after QC, skip enumeration for that cohort; otherwise, stream (generator-style) all alternative assignment triplets by selecting exactly one surviving option from each of the three ordered decision lists, yielding each combined assignment as a list in the same positional order as the input (do not materialize the full Cartesian product).\n\nCohort A raw decision lists (atom indices): feature1 [2, 5, -1], feature2 [7, 0], feature3 [11, 13, 17].\nCohort B raw decision lists (feature IDs): donor [101, 102], acceptor [201, -202, 203], hydrophobe [0, 301, 302].", "answers": "[{\"name\":\"rdkit_Chem_Pharm3D_EmbedLib_CombiEnum\",\"arguments\":{\"sequence\":[[2,5],[7],[11,13,17]]}},{\"name\":\"rdkit_Chem_Pharm3D_EmbedLib_CombiEnum\",\"arguments\":{\"sequence\":[[101,102],[201,203],[301,302]]}}]"}
{"func_name": "rdkit_Chem_QuickSmartsMatch", "func_desc": "rdkit.Chem.QuickSmartsMatch quickly matches a SMARTS substructure pattern against a SMILES molecule string and returns the atom index matches found in the molecule.", "tools": [{"function": {"description": "rdkit.Chem.QuickSmartsMatch quickly matches a SMARTS substructure pattern against a SMILES molecule string and returns the atom index matches found in the molecule.\n", "name": "rdkit_Chem_QuickSmartsMatch", "parameters": {"properties": {"smi": {"type": "string", "description": "The input molecule specified as a SMILES string. In the RDKit cheminformatics context (see README), SMILES is a compact, ASCII representation of a molecule used as input to RDKit's parser (MolFromSmiles). This function parses smi to an RDKit Mol internally; if parsing fails (invalid SMILES), the function may raise an exception when attempting to perform the substructure search. The returned atom indices refer to the parsed molecule's atom ordering.", "default": ""}, "sma": {"type": "string", "description": "The SMARTS substructure pattern to match against the molecule, expressed as a SMARTS string. SMARTS is the RDKit-supported pattern language for specifying substructures used in substructure searches (MolFromSmarts). If sma cannot be parsed as a valid SMARTS pattern, parsing will fail and the function may raise an exception when attempting to match.", "default": ""}, "unique": {"type": "boolean", "description": "Optional; defaults to True. When True, only unique matches are returned: matches that are equivalent under symmetry or automorphism of the query/molecule are filtered so duplicate-equivalent mappings are omitted. When False, all distinct mappings found by RDKit's internal substructure matching engine are returned, which can include symmetry-related duplicates. Use False when you need every mapping instance; use True for a concise set of representative matches.", "default": true}, "display": {"type": "boolean", "description": "Optional; defaults to False. This parameter is accepted for API compatibility but is ignored by the implementation. It has no effect on parsing, matching behavior, return values, or side effects.", "default": false}}, "required": ["smi", "sma", "display", "unique"], "type": "any"}}, "type": "function"}], "query": "We’re doing substructure-mapping QC on a mixed, partially messy aromatic analgesic micro-benchmark where some structures are provided in different aromaticity/tautomer encodings. Use the following raw SMILES list (in this order):\n1) \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n2) \"c1ccccc1\"\n3) \"CC(=O)Oc1ccccc1C(=O)O\"\n\nApply an adaptive SMARTS screen:\n- If a molecule contains at least one explicit carboxylic-acid motif, run a carboxyl scan using SMARTS \"C(=O)O\" and keep all matches including symmetry-related ones (no deduplication). For the first such molecule, allow normal display; for any additional carboxyl-containing molecules, suppress display output.\n- Otherwise (no carboxylic acid present), treat it as an aromatic calibrant and scan for aromatic C–C connectivity using SMARTS \"c:c\" with unique matches only (deduplicate).", "answers": "[{\"name\":\"rdkit_Chem_QuickSmartsMatch\",\"arguments\":{\"smi\":\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"sma\":\"C(=O)O\",\"unique\":false}},{\"name\":\"rdkit_Chem_QuickSmartsMatch\",\"arguments\":{\"smi\":\"c1ccccc1\",\"sma\":\"c:c\",\"unique\":true}},{\"name\":\"rdkit_Chem_QuickSmartsMatch\",\"arguments\":{\"smi\":\"CC(=O)Oc1ccccc1C(=O)O\",\"sma\":\"C(=O)O\",\"unique\":false,\"display\":false}}]"}
{"func_name": "rdkit_Chem_TorsionFingerprints_CalculateTFD", "func_desc": "Calculate the torsion deviation fingerprint (TFD) between two sets of torsion angles for conformer comparison.\n    \n    This function is used in the RDKit cheminformatics context to quantify how different two molecular conformations are in terms of their torsion (dihedral) angles. It computes, for each corresponding torsion, the minimal circular angular difference (in degrees) between any representative angles for that torsion in the two conformers, normalizes that difference by a per-torsion normalization factor, and returns an average (or weighted average) of these normalized deviations. This metric can be used as a descriptor/fingerprint for comparing 3D molecular conformations in similarity searches, clustering, or as an input feature for machine-learning models.", "tools": [{"function": {"description": "Calculate the torsion deviation fingerprint (TFD) between two sets of torsion angles for conformer comparison.\n\nThis function is used in the RDKit cheminformatics context to quantify how different two molecular conformations are in terms of their torsion (dihedral) angles. It computes, for each corresponding torsion, the minimal circular angular difference (in degrees) between any representative angles for that torsion in the two conformers, normalizes that difference by a per-torsion normalization factor, and returns an average (or weighted average) of these normalized deviations. This metric can be used as a descriptor/fingerprint for comparing 3D molecular conformations in similarity searches, clustering, or as an input feature for machine-learning models.", "name": "rdkit_Chem_TorsionFingerprints_CalculateTFD", "parameters": {"properties": {"torsions1": {"type": "array", "items": {"type": "float"}, "description": "A list describing torsions for conformation 1. Each element must be an indexable 2-element sequence where the first element is an iterable of torsion angles (numeric, in degrees) representing equivalent angle values for that torsion (e.g., due to symmetry or multiple representations), and the second element is a numeric normalization value (divisor) used to scale the raw angular deviation for that torsion. The function iterates over tors1[0] and uses tors1[1] as the divisor; if tors1[1] is zero a ZeroDivisionError will be raised. This structure and role are required because the algorithm computes minimal circular differences among the angle lists and divides by the provided normalization factor to produce a normalized per-torsion deviation.", "default": ""}, "torsions2": {"type": "array", "items": {"type": "float"}, "description": "A list describing torsions for conformation 2 with the same structure and semantics as torsions1. torsions1 and torsions2 must have identical lengths and corresponding elements represent the same torsional degrees of freedom in the two conformations. If the two lists have different lengths a ValueError is raised. Elements must contain numeric angle values in degrees; non-numeric or improperly structured elements may raise TypeError or IndexError.", "default": ""}, "weights": {"type": "any", "nullable": true, "description": "Optional list of numeric weights, one per torsion, used to compute a weighted average of the normalized deviations. If provided, weights must have the same length as torsions1 and torsions2, otherwise a ValueError is raised. If weights is None (the default), all torsions are equally weighted (the average is taken over the number of torsions). If the sum of weights is zero, the function will avoid division by zero and return the un-divided sum of weighted deviations (i.e., sum(deviations) without normalization by sum_weights), which reflects an implementation detail to be aware of.", "default": null}}, "required": ["torsions1", "torsions2", "weights"], "type": "any"}}, "type": "function"}], "query": "We’re running conformer-comparison QC for the same ligand across two independent candidate-pair replicates, but the torsion annotations coming out of the dihedral enumerator are messy. Each replicate provides three corresponding torsions for Conformer A vs Conformer B, where a torsion is given as (a set of representative dihedral angles in degrees, plus its per-torsion normalization divisor). For each replicate, compute a weighted TFD using only torsions whose normalization divisor matches between A and B for that torsion index (these are the only torsions considered reliably parameterized across both conformers). For retained torsions, use the minimal circular angular difference (degrees) between any representative angle from A and any representative angle from B, normalize by the shared divisor, then combine using the provided weights (use the weight for that torsion index; weights should be renormalized implicitly by the weighted averaging in the TFD implementation). Replicate 1: Conformer A torsions = ([60.0, -300.0], norm 180.0), ([179.0], norm 180.0), ([-45.0, 315.0], norm 180.0); Conformer B torsions = ([75.0], norm 180.0), ([-170.0, 190.0], norm 180.0), ([-30.0], norm 180.0); weights = [1.0, 2.0, 1.0]. Replicate 2: Conformer A torsions = ([-60.0, 300.0], norm 180.0), ([170.0], norm 180.0), ([45.0], norm 120.0); Conformer B torsions = ([-75.0], norm 180.0), ([-175.0, 185.0], norm 180.0), ([90.0], norm 180.0); weights = [1.0, 0.5, 2.0]. Return the two replicate TFD values for downstream clustering/selection.", "answers": "[{\"name\":\"rdkit_Chem_TorsionFingerprints_CalculateTFD\",\"arguments\":{\"torsions1\":[[[60.0,-300.0],180.0],[[179.0],180.0],[[-45.0,315.0],180.0]],\"torsions2\":[[[75.0],180.0],[[-170.0,190.0],180.0],[[-30.0],180.0]],\"weights\":[1.0,2.0,1.0]}},{\"name\":\"rdkit_Chem_TorsionFingerprints_CalculateTFD\",\"arguments\":{\"torsions1\":[[[-60.0,300.0],180.0],[[170.0],180.0]],\"torsions2\":[[[-75.0],180.0],[[-175.0,185.0],180.0]],\"weights\":[1.0,0.5]}}]"}
{"func_name": "rdkit_Chem_UnitTestSurf_readRegressionData", "func_desc": "Return entries from a regression dataset file bundled with RDKit and used by RDKit unit tests.\n    \n    This generator function opens the file named by filename located in the repository source tree under RDConfig.RDCodeDir/Chem/test_data, reads it line by line, and for each non-comment line yields a _TestData tuple containing the 1-based line number, the SMILES string, the parsed RDKit molecule, and the expected numeric value parsed from column col. It is intended for use in RDKit's unit/regression testing workflows to feed expected values (floats) and molecules (Chem.Mol) into test code that compares computed properties against baseline values stored in the test_data files.", "tools": [{"function": {"description": "Return entries from a regression dataset file bundled with RDKit and used by RDKit unit tests.\n\nThis generator function opens the file named by filename located in the repository source tree under RDConfig.RDCodeDir/Chem/test_data, reads it line by line, and for each non-comment line yields a _TestData tuple containing the 1-based line number, the SMILES string, the parsed RDKit molecule, and the expected numeric value parsed from column col. It is intended for use in RDKit's unit/regression testing workflows to feed expected values (floats) and molecules (Chem.Mol) into test code that compares computed properties against baseline values stored in the test_data files.", "name": "rdkit_Chem_UnitTestSurf_readRegressionData", "parameters": {"properties": {"filename": {"type": "string", "description": "The base filename of the CSV-style regression data file located in RDKit's test data directory. The function resolves this relative to RDConfig.RDCodeDir by joining RDConfig.RDCodeDir, 'Chem', 'test_data', and filename, then opens that file for reading. This parameter must be the exact filename present in that directory (for example 'my_regression.csv'); providing an incorrect filename will raise a FileNotFoundError when the generator is first iterated.", "default": ""}, "col": {"type": "integer", "description": "Zero-based column index into the comma-separated fields on each non-comment line from which the expected numeric value will be read. The function uses split(',') on each line and accesses splitL[col] to obtain the value; therefore col must be an integer indexing an existing column in every data line. If col is out of range for a given line an IndexError will be raised during iteration.", "default": ""}}, "required": ["filename", "col"], "type": "any"}}, "type": "function"}], "query": "Run a single regression-validation sweep over RDKit’s bundled Chem/test_data CSV baselines, but treat them as heterogeneous assay panels rather than uniform files. Use this dynamic protocol per dataset filename: (1) For any dataset whose name indicates fragments/atom-constants (e.g., Crippen-style), read the expected target from the column whose header matches the first occurrence of either “MR” or “logP” (prefer “MR” if both exist). (2) For any dataset whose name indicates solubility benchmarking, read the expected target from the column whose header contains “solubility” or “logS” (case-insensitive), choosing the rightmost match if multiple exist. (3) For all other datasets in this run, read the expected target from the column whose header contains “ASA” or “surface” (case-insensitive). In all three cohorts, iterate line-by-line over non-comment entries and yield the standard _TestData tuple (1-based line number, SMILES, parsed molecule, expected float). Apply the protocol to these cohorts in the same run: UnitTestSurf.csv, Crippen.csv, and solubility_regression.csv.", "answers": "[{\"name\":\"rdkit_Chem_UnitTestSurf_readRegressionData\",\"arguments\":{\"filename\":\"UnitTestSurf.csv\",\"col\":2}},{\"name\":\"rdkit_Chem_UnitTestSurf_readRegressionData\",\"arguments\":{\"filename\":\"Crippen.csv\",\"col\":1}},{\"name\":\"rdkit_Chem_UnitTestSurf_readRegressionData\",\"arguments\":{\"filename\":\"solubility_regression.csv\",\"col\":2}}]"}
{"func_name": "rdkit_Chem_inchi_InchiToInchiKey", "func_desc": "rdkit.Chem.inchi.InchiToInchiKey: Return the InChIKey for a given InChI string.\n    \n    This function is a thin RDKit Python wrapper around the underlying rdinchi binding that converts a text InChI (IUPAC International Chemical Identifier) into its corresponding InChIKey, a compact hashed identifier commonly used in cheminformatics for indexing, deduplication, database keys, and fast exact matching of chemical structures. The function performs no modification of the input string; it delegates conversion to the rdinchi.InchiToInchiKey implementation and returns its result if the conversion is successful.", "tools": [{"function": {"description": "rdkit.Chem.inchi.InchiToInchiKey: Return the InChIKey for a given InChI string.\n\nThis function is a thin RDKit Python wrapper around the underlying rdinchi binding that converts a text InChI (IUPAC International Chemical Identifier) into its corresponding InChIKey, a compact hashed identifier commonly used in cheminformatics for indexing, deduplication, database keys, and fast exact matching of chemical structures. The function performs no modification of the input string; it delegates conversion to the rdinchi.InchiToInchiKey implementation and returns its result if the conversion is successful.", "name": "rdkit_Chem_inchi_InchiToInchiKey", "parameters": {"properties": {"inchi": {"type": "string", "description": "The input InChI string to be converted. This should be a textual IUPAC InChI representing a chemical structure (typically starts with the prefix \"InChI=\" for standard InChI values). The parameter is passed verbatim to the underlying rdinchi binding; no normalization or validation is performed by this wrapper beyond what rdinchi implements.", "default": ""}}, "required": ["inchi"], "type": "any"}}, "type": "function"}], "query": "We’re reconciling vendor-supplied caffeine InChI records into our registry, but the feed is messy: it contains intended structure records alongside malformed/partial identifiers. Use a strict “acceptance sieve” before keying: only generate an InChIKey for records that (i) begin with the canonical prefix `InChI=1S/`, (ii) contain exactly one `/c` layer and one `/h` layer, and (iii) have no whitespace anywhere in the string (common copy/paste contamination). Treat each accepted string as an independent record and do not edit/normalize any text. For the following raw records, return InChIKeys in the original order for the subset that passes the sieve:\n1) InChI=1S/C8H10N4O2/c1-10-4-9-6(10)7(13)12(2)8(14)11(3)5-15-12/h4-5H,1-3H3\n2) InChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7(13)12(3)8(14)11(2)6/h4H,1-3H3\n3) InChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7(13)12(3)8(14)11(6)2/h4H,1-3H3\n4) InChI=1S/C8H10N4O2/c1-10-4-9-6(10)7(13)12(2)8(14)11(3)5-15-12\n5) inchi=1S/C8H10N4O2/c1-10-4-9-6(10)7(13)12(2)8(14)11(3)5-15-12/h4-5H,1-3H3\n6) InChI=1S/C8H10N4O2 /c1-10-4-9-6(10)7(13)12(2)8(14)11(3)5-15-12/h4-5H,1-3H3", "answers": "[{\"name\":\"rdkit_Chem_inchi_InchiToInchiKey\",\"arguments\":{\"inchi\":\"InChI=1S/C8H10N4O2/c1-10-4-9-6(10)7(13)12(2)8(14)11(3)5-15-12/h4-5H,1-3H3\"}},{\"name\":\"rdkit_Chem_inchi_InchiToInchiKey\",\"arguments\":{\"inchi\":\"InChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7(13)12(3)8(14)11(2)6/h4H,1-3H3\"}},{\"name\":\"rdkit_Chem_inchi_InchiToInchiKey\",\"arguments\":{\"inchi\":\"InChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7(13)12(3)8(14)11(6)2/h4H,1-3H3\"}}]"}
{"func_name": "rdkit_Dbase_DbUtils_GetTypeStrings", "func_desc": "rdkit.Dbase.DbUtils.GetTypeStrings returns a list of SQL column type declaration strings constructed from parallel lists of column headings and type descriptors. This function is used by RDKit database utilities (for example, when generating CREATE TABLE column specifications for the RDKit PostgreSQL cartridge and other code that needs textual SQL column definitions). It maps Python type markers in colTypes to SQL type keywords: float -> \"double precision\", int -> \"integer\", and any other marker -> \"varchar(n)\" where n is supplied in the type descriptor.", "tools": [{"function": {"description": "rdkit.Dbase.DbUtils.GetTypeStrings returns a list of SQL column type declaration strings constructed from parallel lists of column headings and type descriptors. This function is used by RDKit database utilities (for example, when generating CREATE TABLE column specifications for the RDKit PostgreSQL cartridge and other code that needs textual SQL column definitions). It maps Python type markers in colTypes to SQL type keywords: float -> \"double precision\", int -> \"integer\", and any other marker -> \"varchar(n)\" where n is supplied in the type descriptor.\n", "name": "rdkit_Dbase_DbUtils_GetTypeStrings", "parameters": {"properties": {"colHeadings": {"type": "array", "items": {"type": "float"}, "description": "A list of column name strings. Each entry is used verbatim as the column identifier prefixing the SQL type token for the corresponding column. The i-th element of colHeadings is paired with the i-th element of colTypes to produce a single SQL type declaration string (for example, \"mol double precision\" or \"name varchar(50)\").", "default": ""}, "colTypes": {"type": "array", "items": {"type": "float"}, "description": "A list of type descriptor tuples. Each element is expected to be an indexable sequence whose first element (typ[0]) is a Python type object used as a marker: the code tests for typ[0] == float and typ[0] == int. If typ[0] == float the resulting SQL type is \"double precision\"; if typ[0] == int the resulting SQL type is \"integer\"; otherwise the function formats a varchar with the length taken from typ[1] (i.e., \"varchar(%d)\" % typ[1]). The function iterates over range(len(colTypes)) and indexes colHeadings with the same index, so colTypes controls the number and order of output strings.", "default": ""}, "keyCol": {"type": "string", "nullable": true, "description": "Optional. A column name (one of the strings in colHeadings) to mark as the table primary key. If keyCol is equal to colHeadings[i] for some i, the corresponding SQL declaration is postfixed with \" not null primary key\" (for example, \"id integer not null primary key\"). The default is None, which leaves all columns unmarked as primary key.", "default": null}}, "required": ["colHeadings", "colTypes", "keyCol"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an RDKit-to-PostgreSQL schema autogeneration step using a messy, partially curated set of candidate column specs for three tables (compound registry, assay results, and simplified molecule). Each column spec is a (heading, type_descriptor) pair where type_descriptor is either a Python type marker (int/float) or a varchar length. Before generating SQL type declaration strings, apply these curation rules:\n\n1) Only keep headings that are valid SQL identifiers for our conventions: lowercase letters, digits, and underscores only; must not start with a digit.\n2) If the same heading appears multiple times within a table after curation, keep only the first occurrence.\n3) For varchar length descriptors: treat any non-positive or missing length as unspecified and set it to 128; if length is greater than 255, clamp to 255.\n4) For numeric type markers: interpret int-like markers as integer and float-like markers as double precision (per RDKit DbUtils rules).\n5) The key column for each table is defined as: the first remaining heading that ends with “_id”; if none remain, use the first remaining heading. Generate the SQL column type declaration strings for each table using rdkit.Dbase.DbUtils.GetTypeStrings, with the computed keyCol.\n\nRaw candidate specs:\n\nA) compound registry candidates (in order):\n- ('compound_id', int)\n- ('smiles', 300)\n- ('exact_mw', float)\n- ('supplier', 80)\n- ('Supplier', 80)\n- ('1_badcol', 16)\n\nB) assay results candidates (in order):\n- ('assay_id', int)\n- ('compound_id', int)\n- ('activity_value', float)\n- ('activity_unit', 16)\n- ('source', 64)\n- ('activity_unit', 8)\n- ('source-url', 128)\n\nC) simplified molecule candidates (in order):\n- ('id', int)\n- ('smiles', 255)\n- ('mol_weight', float)\n- ('source', 0)\n- ('notes', 500)\n\nReturn the CREATE TABLE-style column type declaration strings as produced by GetTypeStrings for each curated table definition.", "answers": "[{\"name\":\"rdkit_Dbase_DbUtils_GetTypeStrings\",\"arguments\":{\"colHeadings\":[\"compound_id\",\"smiles\",\"exact_mw\",\"supplier\"],\"colTypes\":[[\"int\",0],[\"str\",255],[\"float\",0],[\"str\",80]],\"keyCol\":\"compound_id\"}},{\"name\":\"rdkit_Dbase_DbUtils_GetTypeStrings\",\"arguments\":{\"colHeadings\":[\"assay_id\",\"compound_id\",\"activity_value\",\"activity_unit\",\"source\"],\"colTypes\":[[\"int\",0],[\"int\",0],[\"float\",0],[\"str\",16],[\"str\",64]],\"keyCol\":\"assay_id\"}},{\"name\":\"rdkit_Dbase_DbUtils_GetTypeStrings\",\"arguments\":{\"colHeadings\":[\"id\",\"smiles\",\"mol_weight\",\"source\",\"notes\"],\"colTypes\":[[\"int\",0],[\"str\",255],[\"float\",0],[\"str\",128],[\"str\",255]],\"keyCol\":\"id\"}}]"}
{"func_name": "rdkit_Dbase_StorageUtils_RDIdToInt", "func_desc": "rdkit.Dbase.StorageUtils.RDIdToInt converts a canonical RDId string used in RDKit storage utilities into the integer index that RDKit uses for internal indexing, database keys, and compact numeric representations in molecular storage and retrieval workflows.\n    \n    This function is used in RDKit's database/storage utilities (for example, the molecular database cartridge and other persistence layers) to map human-readable RDId identifiers like \"RDCmpd-009-000-009-8\" or \"RDData_000_009_9\" to a single integer index. The conversion treats the components between the first and last dash (or underscore, which is normalized to a dash) as fixed-width 3-digit blocks that form a base-1000 little-endian number: the rightmost interior block is the least significant block (multiplied by 1000^0), the next block to the left is multiplied by 1000^1, and so on. Algorithmic steps: if validation is enabled the function calls ValidateRDId(ID) and raises ValueError(\"Bad RD Id\") if validation fails; underscores are replaced with hyphens; the ID is split on '-' and the tokens between the first and last are parsed in reverse order as decimal integers; the final integer is the sum of term_int * (1000**position) for each term. Typical practical significance: the returned integer is suitable for compact storage, numeric comparisons, indexing, and as a deterministic numeric key derived from the RDId string.", "tools": [{"function": {"description": "rdkit.Dbase.StorageUtils.RDIdToInt converts a canonical RDId string used in RDKit storage utilities into the integer index that RDKit uses for internal indexing, database keys, and compact numeric representations in molecular storage and retrieval workflows.\n\nThis function is used in RDKit's database/storage utilities (for example, the molecular database cartridge and other persistence layers) to map human-readable RDId identifiers like \"RDCmpd-009-000-009-8\" or \"RDData_000_009_9\" to a single integer index. The conversion treats the components between the first and last dash (or underscore, which is normalized to a dash) as fixed-width 3-digit blocks that form a base-1000 little-endian number: the rightmost interior block is the least significant block (multiplied by 1000^0), the next block to the left is multiplied by 1000^1, and so on. Algorithmic steps: if validation is enabled the function calls ValidateRDId(ID) and raises ValueError(\"Bad RD Id\") if validation fails; underscores are replaced with hyphens; the ID is split on '-' and the tokens between the first and last are parsed in reverse order as decimal integers; the final integer is the sum of term_int * (1000**position) for each term. Typical practical significance: the returned integer is suitable for compact storage, numeric comparisons, indexing, and as a deterministic numeric key derived from the RDId string.", "name": "rdkit_Dbase_StorageUtils_RDIdToInt", "parameters": {"properties": {"ID": {"type": "string", "description": "The RDId string to convert. This is the canonical identifier used in RDKit storage contexts (examples from source: \"RDCmpd-000-009-9\", \"RDCmpd-009-000-009-8\", \"RDData_000_009_9\"). Underscores in this string are treated as hyphens; the function extracts the tokens between the first and last hyphen after normalization and interprets each token as a zero-padded 3-digit decimal block. If the provided ID does not conform to expected RDId formatting and validation is enabled, a ValueError is raised.", "default": ""}, "validate": {"type": "boolean", "description": "Whether to validate the ID format before conversion. The default value in the function signature is 1 (truthy), so validation is performed by default. When True (or any truthy value), the function calls ValidateRDId(ID) and raises ValueError(\"Bad RD Id\") if validation fails. When False (or falsy), the function skips the ValidateRDId check and proceeds to parse and convert the ID; in that case, malformed interior blocks may still cause a ValueError during integer conversion (for example, if a block is not numeric).", "default": 1}}, "required": ["ID", "validate"], "type": "any"}}, "type": "function"}], "query": "We’re triaging RDKit storage identifiers coming from two acquisition channels before loading into the compound registry. Channel A (instrument export) is noisy: identifiers may use underscores, and anything from this channel must be validated prior to key generation. Channel B (curated backfill) is assumed pre-sanitized: only identifiers whose interior numeric blocks are strictly increasing left-to-right should bypass validation; all other curated entries should still be validated. Given this mixed batch: [\"RDCmpd-009-000-009-8\", \"RDData_000_009_9\", \"RDCmpd-012-345-678-9\"], normalize delimiters where needed and convert each ID to its deterministic internal integer key using the appropriate validation rule per channel/structure.", "answers": "[{\"name\":\"rdkit_Dbase_StorageUtils_RDIdToInt\",\"arguments\":{\"ID\":\"RDCmpd-009-000-009-8\",\"validate\":true}},{\"name\":\"rdkit_Dbase_StorageUtils_RDIdToInt\",\"arguments\":{\"ID\":\"RDData_000_009_9\",\"validate\":true}},{\"name\":\"rdkit_Dbase_StorageUtils_RDIdToInt\",\"arguments\":{\"ID\":\"RDCmpd-012-345-678-9\",\"validate\":false}}]"}
{"func_name": "rdkit_ML_Cluster_Resemblance_FindMinValInList", "func_desc": "Finds the minimum value in a condensed metric matrix and returns the zero-based pair indices and the minimum value.\n    \n    This function is used in the RDKit ML clustering/ressemblance code to locate the smallest pairwise distance (or resemblance score) when the pairwise values for nObjs objects are stored in a condensed 1-D numpy array (upper-triangle excluding the diagonal). It is typically used during agglomerative clustering or similarity search steps to pick the closest pair of objects (for example, molecules or descriptors) as described by the RDKit ML Cluster Resemblance utilities. The implementation decodes a linear index in the condensed representation into a (row, column) pair with row < column.", "tools": [{"function": {"description": "Finds the minimum value in a condensed metric matrix and returns the zero-based pair indices and the minimum value.\n\nThis function is used in the RDKit ML clustering/ressemblance code to locate the smallest pairwise distance (or resemblance score) when the pairwise values for nObjs objects are stored in a condensed 1-D numpy array (upper-triangle excluding the diagonal). It is typically used during agglomerative clustering or similarity search steps to pick the closest pair of objects (for example, molecules or descriptors) as described by the RDKit ML Cluster Resemblance utilities. The implementation decodes a linear index in the condensed representation into a (row, column) pair with row < column.", "name": "rdkit_ML_Cluster_Resemblance_FindMinValInList", "parameters": {"properties": {"mat": {"type": "array", "items": {"type": "float"}, "description": "A 1-D numpy array containing the condensed metric matrix values in row-major order for the upper triangle of an nObjs-by-nObjs symmetric matrix excluding the diagonal. The array length must equal nObjs * (nObjs - 1) / 2; an AssertionError is raised if this condition is not met. Elements are numeric values (distances or resemblance scores) and the function returns one of these elements as the minimum value.", "default": ""}, "nObjs": {"type": "integer", "description": "The number of original objects (nodes) whose pairwise values are represented in mat. This value is used to validate the length of mat and to decode the condensed index into a pair of zero-based indices (row, column). Practically, nObjs corresponds to the number of molecules or feature vectors in a clustering or similarity computation.", "default": ""}, "minIdx": {"type": "integer", "nullable": true, "description": "Optional precomputed index in the condensed array mat that points to the minimum value. If provided, it must be a valid index into mat (0 <= minIdx < len(mat)); if it is out of bounds, an IndexError may be raised when accessing mat[minIdx]. If None (the default), numpy.argmin(mat) is used to compute the index of the first occurrence of the minimum value; when there are ties, numpy.argmin selects the first minimal element encountered, which this function then decodes.", "default": null}}, "required": ["mat", "nObjs", "minIdx"], "type": "any"}}, "type": "function"}], "query": "You’re triaging two independent 6-compound similarity cohorts before an agglomerative pre-merge step. Each cohort provides a condensed (upper-triangle, no diagonal; row-major) 1-D Tanimoto distance vector. Because some compound pairs are known duplicates or registration collisions, treat any distance <= 0.12 as a suspect artifact and ignore it by selecting the minimum distance strictly greater than 0.12 for that cohort, then decode that selected minimum’s linear position into the corresponding zero-based (i,j) indices (i<j) and report (i,j,value). Cohort A distances: [0.35, 0.72, 0.18, 0.55, 0.40, 0.60, 0.22, 0.47, 0.51, 0.33, 0.29, 0.38, 0.44, 0.26, 0.31]. Cohort B distances: [0.42, 0.15, 0.33, 0.27, 0.58, 0.21, 0.19, 0.40, 0.36, 0.12, 0.50, 0.29, 0.25, 0.31, 0.44].", "answers": "[{\"name\":\"rdkit_ML_Cluster_Resemblance_FindMinValInList\",\"arguments\":{\"mat\":[0.35,0.72,0.18,0.55,0.4,0.6,0.22,0.47,0.51,0.33,0.29,0.38,0.44,0.26,0.31],\"nObjs\":6}},{\"name\":\"rdkit_ML_Cluster_Resemblance_FindMinValInList\",\"arguments\":{\"mat\":[0.42,0.15,0.33,0.27,0.58,0.21,0.19,0.4,0.36,0.12,0.5,0.29,0.25,0.31,0.44],\"nObjs\":6,\"minIdx\":1}}]"}
{"func_name": "rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap", "func_desc": "rdkit.ML.Data.DataUtils.CalcNPossibleUsingMap: calculate the number of possible discrete values for each variable in a dataset, using an ordering map and optional quantization bounds. This function is used in RDKit machine-learning and descriptor-processing workflows to determine how many discrete categories each variable can take, either by reading provided quantization bounds or by scanning integer-valued data. The result is commonly used when preparing molecular descriptors or other features for algorithms that require knowledge of categorical cardinality (for example, encoder sizing, histogram binning, or discrete-feature models).\n    \n    This function examines each variable index defined by the ordering map and returns a list of counts corresponding to the number of possible values for that variable. For variables that have an entry in qBounds (a list of quantization buckets), the function uses the length of that entry as the count. For variables without qBounds, the function inspects the dataset values (accessed according to order) and, if all observed values for that variable are integer-valued numbers of one of the recognized numeric types, computes the count as max_integer_value_seen + 1 (interpreting integer values as zero-based categories). Variables that are declared non-quantized via nQBounds (non-zero entry) are excluded from computation and yield the sentinel count produced by the function logic. The function can print diagnostic information when silent is False.", "tools": [{"function": {"description": "rdkit.ML.Data.DataUtils.CalcNPossibleUsingMap: calculate the number of possible discrete values for each variable in a dataset, using an ordering map and optional quantization bounds. This function is used in RDKit machine-learning and descriptor-processing workflows to determine how many discrete categories each variable can take, either by reading provided quantization bounds or by scanning integer-valued data. The result is commonly used when preparing molecular descriptors or other features for algorithms that require knowledge of categorical cardinality (for example, encoder sizing, histogram binning, or discrete-feature models).\n\nThis function examines each variable index defined by the ordering map and returns a list of counts corresponding to the number of possible values for that variable. For variables that have an entry in qBounds (a list of quantization buckets), the function uses the length of that entry as the count. For variables without qBounds, the function inspects the dataset values (accessed according to order) and, if all observed values for that variable are integer-valued numbers of one of the recognized numeric types, computes the count as max_integer_value_seen + 1 (interpreting integer values as zero-based categories). Variables that are declared non-quantized via nQBounds (non-zero entry) are excluded from computation and yield the sentinel count produced by the function logic. The function can print diagnostic information when silent is False.", "name": "rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "A list of examples (rows). Each example is an indexable sequence (for example, a tuple or list) of variable values. In RDKit ML workflows this typically contains molecular descriptor vectors or feature rows; values are accessed as data[row_index][order[col_index]] according to the mapping in order. The function iterates over these examples to infer maximum integer values for variables that lack explicit quantization bounds.", "default": ""}, "order": {"type": "array", "items": {"type": "float"}, "description": "A list mapping the function's variable indices (0..nVars-1) to the column indices in each example of data. For variable index i used inside this function, the corresponding value in a given example is data[row][order[i]]. The length of order determines the number of variables processed and must match the length of qBounds or nQBounds as required by the assertion below.", "default": ""}, "qBounds": {"type": "array", "items": {"type": "float"}, "description": "A list of quantization bounds for variables. Each entry qBounds[i] is expected to be a sequence (possibly empty) that lists quantization buckets for variable i; if qBounds[i] is non-empty, the function reads len(qBounds[i]) and uses that as the number of possible values for variable i without scanning data. qBounds must be provided (truthy) and have the same length as order unless nQBounds is provided and satisfies the assertion described below.", "default": ""}, "nQBounds": {"type": "any", "nullable": true, "description": "Optional list used to mark variables as non-quantized. If provided and nQBounds[i] != 0 for a variable i, that variable is excluded from counting and assigned the internal sentinel value (which results in zero in the final returned count). If nQBounds is provided, it must have the same length as order. If both qBounds and nQBounds are provided, nQBounds is checked first for each variable index.", "default": null}, "silent": {"type": "boolean", "description": "If False, the function prints diagnostic information to standard output (for example, the order, qBounds and messages when a column is excluded during scanning). Default True suppresses these prints. Use False when debugging mapping/quantization issues in descriptor preprocessing.", "default": true}}, "required": ["data", "order", "qBounds", "nQBounds", "silent"], "type": "any"}}, "type": "function"}], "query": "We are validating a mixed-origin RDKit descriptor matrix before sizing discrete encoders. Each cohort contains some continuous drift and some discrete/categorical channels; only channels intended to be discrete should contribute to cardinality. For each cohort, run CalcNPossibleUsingMap using these rules: (1) apply explicit quantization bounds where provided; (2) for any variable without bounds, let the function infer cardinality only if the observed values for that variable are integer-valued (treating them as zero-based categories); (3) any variable flagged as non-quantized is treated as an analog/continuous channel and is excluded from the discrete-cardinality audit. \n\nCohort A (silent): 5-molecule dataset rows [[1,0,3],[0,1,2],[1,0,0],[2,1,1],[0,1,3]]. Use an ordering map so that variable0 reads column2, variable1 reads column0, variable2 reads column1 (order=[2,0,1]). Provide explicit quantization buckets only for variable0 (4 buckets: [0.0,1.0,2.0,3.0]). Leave variable1 without qBounds so it is inferred from the data if eligible. Flag variable2 as non-quantized (continuous) so it is excluded.\n\nCohort B (diagnostic): 5-molecule dataset rows [[0.2,0,1],[1.1,2,0],[2.7,1,3],[3.4,2,2],[0.9,3,1]] with identity ordering (order=[0,1,2]). Provide explicit quantization buckets for variable0 (4 buckets: [0.0,1.0,2.0,3.0]). Leave variable1 without qBounds so it is inferred from integer-valued observations. Flag variable2 as non-quantized (excluded). Print diagnostics for this cohort.", "answers": "[{\"name\":\"rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap\",\"arguments\":{\"data\":[[1,0,3],[0,1,2],[1,0,0],[2,1,1],[0,1,3]],\"order\":[2,0,1],\"qBounds\":[[0.0,1.0,2.0,3.0],[],[]],\"nQBounds\":[0,0,1],\"silent\":true}},{\"name\":\"rdkit_ML_Data_DataUtils_CalcNPossibleUsingMap\",\"arguments\":{\"data\":[[0.2,0,1],[1.1,2,0],[2.7,1,3],[3.4,2,2],[0.9,3,1]],\"order\":[0,1,2],\"qBounds\":[[0.0,1.0,2.0,3.0],[],[]],\"nQBounds\":[0,0,1],\"silent\":false}}]"}
{"func_name": "rdkit_ML_Data_DataUtils_TakeEnsemble", "func_desc": "rdkit.ML.Data.DataUtils.TakeEnsemble extracts a subset of elements from a sequence (vect) according to ensemble member indices (ensembleIds). This utility is used in RDKit's machine-learning data handling to select predictions, feature values, or ensemble member outputs for further processing, evaluation, or storage. When vect is a \"data vector\" (isDataVect=True) that contains metadata at the first and last positions (for example a molecule identifier or label), those sentinel elements are preserved and the ensemble indices are adjusted to account for the metadata layout.", "tools": [{"function": {"description": "rdkit.ML.Data.DataUtils.TakeEnsemble extracts a subset of elements from a sequence (vect) according to ensemble member indices (ensembleIds). This utility is used in RDKit's machine-learning data handling to select predictions, feature values, or ensemble member outputs for further processing, evaluation, or storage. When vect is a \"data vector\" (isDataVect=True) that contains metadata at the first and last positions (for example a molecule identifier or label), those sentinel elements are preserved and the ensemble indices are adjusted to account for the metadata layout.\n", "name": "rdkit_ML_Data_DataUtils_TakeEnsemble", "parameters": {"properties": {"vect": {"type": "array", "items": {"type": "float"}, "description": "The input sequence from which elements will be selected. In RDKit ML workflows this is typically a list containing ensemble outputs or a data vector that mixes metadata and ensemble values. The function does not modify the original list object; it builds and returns a new list with the selected elements. Elements are accessed by index using Python's zero-based indexing.", "default": ""}, "ensembleIds": {"type": "any", "description": "A tuple of integer indices specifying which ensemble member positions to extract from vect. Each element must be an int and is interpreted as a zero-based index into vect. If isDataVect is True, these indices are interpreted as referring to the ensemble portion of a data vector and are incremented by 1 internally to skip the first metadata element.", "default": ""}, "isDataVect": {"type": "boolean", "description": "When False (default), vect is treated as a plain list of ensemble values and the returned list contains exactly the elements vect[x] for each x in ensembleIds, in the same order as ensembleIds. When True, vect is treated as a data vector that stores metadata at vect[0] and vect[-1]; in this mode the function returns a new list composed of vect[0], followed by vect[x+1] for each x in ensembleIds, followed by vect[-1]. Use this mode when vect includes leading/trailing metadata that must be preserved around the selected ensemble members.", "default": false}}, "required": ["vect", "ensembleIds", "isDataVect"], "type": "any"}}, "type": "function"}], "query": "We’re doing a QC-aware post-processing pass over QSAR ensemble outputs for two single-molecule records. Treat each record as a data vector (isDataVect=True) where the first element is the molecule identifier and the last element is the ground-truth label; both sentinel metadata entries must be preserved. Apply an uncertainty-based selection rule per record: compute the median of that record’s ensemble prediction values (i.e., everything between ID and label), then select the ensemble members whose prediction is greater than or equal to the median; keep those selected members in their original ensemble order (increasing member index).\n\nRecord A: [\"CHEMBL12345\", 0.12, 0.18, 0.09, 0.22, 0.15, 1]\nRecord B: [\"Mol_1023\", 0.72, 0.65, 0.81, 0.60, 0.77, 0.69, \"active\"]", "answers": "[{\"name\":\"rdkit_ML_Data_DataUtils_TakeEnsemble\",\"arguments\":{\"vect\":[\"CHEMBL12345\",0.12,0.18,0.09,0.22,0.15,1],\"ensembleIds\":[1,3,4],\"isDataVect\":true}},{\"name\":\"rdkit_ML_Data_DataUtils_TakeEnsemble\",\"arguments\":{\"vect\":[\"Mol_1023\",0.72,0.65,0.81,0.6,0.77,0.69,\"active\"],\"ensembleIds\":[0,2,4],\"isDataVect\":true}}]"}
{"func_name": "rdkit_ML_Data_Quantize_FindVarMultQuantBounds", "func_desc": "FindVarMultQuantBounds finds multiple quantization bounds (cut points) for a single continuous variable so as to maximize information gain for a discrete target.\n    This function is used in RDKit's ML/Data routines when discretizing continuous descriptor or feature values (for example descriptor values computed from molecules) into bins that are informative for a discrete result variable. The routine sorts the input variable together with its associated result codes, identifies valid cut start points, adjusts the requested number of bounds if there are too few candidate boundaries, and searches (via internal helpers) for the set of cuts that yields the highest information gain for the given number of result categories.", "tools": [{"function": {"description": "FindVarMultQuantBounds finds multiple quantization bounds (cut points) for a single continuous variable so as to maximize information gain for a discrete target.\nThis function is used in RDKit's ML/Data routines when discretizing continuous descriptor or feature values (for example descriptor values computed from molecules) into bins that are informative for a discrete result variable. The routine sorts the input variable together with its associated result codes, identifies valid cut start points, adjusts the requested number of bounds if there are too few candidate boundaries, and searches (via internal helpers) for the set of cuts that yields the highest information gain for the given number of result categories.", "name": "rdkit_ML_Data_Quantize_FindVarMultQuantBounds", "parameters": {"properties": {"vals": {"type": "array", "items": {"type": "float"}, "description": "Sequence of variable values to quantize. These are assumed to be numeric (floats) representing a continuous descriptor or feature (e.g., a molecular descriptor value). Each element in vals corresponds positionally to an element in results. The function asserts that len(vals) == len(results) and will raise AssertionError if they differ.", "default": ""}, "nBounds": {"type": "integer", "description": "The requested number of quantization bounds (cut points) to find. This is an integer specifying how many boundaries to attempt to place between sorted values to partition the continuous variable into multiple bins. If the number of valid candidate start points found in the data is smaller than nBounds, the function reduces nBounds (nBounds is set to len(startPoints)-1) and will ensure at least one bound is used when possible.", "default": ""}, "results": {"type": "array", "items": {"type": "float"}, "description": "A list of result codes (should be integers) that give the discrete target/class for each corresponding entry in vals. These are used to compute information gain for candidate quantizations. The values in results should be consistent with nPossibleRes (see below); the function does not coercively validate that each code lies within a specific range but uses them as categorical labels when computing gains.", "default": ""}, "nPossibleRes": {"type": "integer", "description": "An integer specifying the number of possible distinct values for the result variable (the number of classes). This parameter informs the information-gain calculations performed by the internal routines and should match the number of distinct categories represented in results.", "default": ""}}, "required": ["vals", "nBounds", "results", "nPossibleRes"], "type": "any"}}, "type": "function"}], "query": "We’re discretizing a single continuous lipophilicity descriptor for a QSAR decision-tree, but the raw descriptor export includes experimental artifacts and redundant measurements. Treat this as a two-cohort benchmark with independent replicates.\n\nCohort A (12 compounds):\n- cLogP values: [1.2, 3.5, 2.8, 4.1, 5.0, 1.9, 3.0, 4.7, 2.1, 5.3, 3.8, 4.0]\n- activity labels (0/1): [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1]\n\nCohort B (15 compounds):\n- cLogP values: [1.2, 3.5, 2.8, 4.1, 5.0, 1.9, 3.0, 4.8, 2.1, 3.7, 5.3, 1.5, 2.4, 4.0, 3.2]\n- activity labels (0/1): [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]\n\nFor each cohort independently, run the quantization-bound search under a data-sieve + branching protocol:\n1) Only consider compounds whose descriptor is within the physically plausible window for neutral small molecules (0.0 ≤ cLogP ≤ 6.0).\n2) If a cohort has fewer than 14 retained compounds after filtering, request at most 2 bounds; otherwise request up to 3 bounds.\n3) Assume exactly two possible result categories (inactive/active).\n\nReturn the resulting set of quantization cut points for each cohort.", "answers": "[{\"name\":\"rdkit_ML_Data_Quantize_FindVarMultQuantBounds\",\"arguments\":{\"vals\":[1.2,3.5,2.8,4.1,5.0,1.9,3.0,4.7,2.1,5.3,3.8,4.0],\"nBounds\":2,\"results\":[0,1,0,1,1,0,0,1,0,1,1,1],\"nPossibleRes\":2}},{\"name\":\"rdkit_ML_Data_Quantize_FindVarMultQuantBounds\",\"arguments\":{\"vals\":[1.2,3.5,2.8,4.1,5.0,1.9,3.0,4.8,2.1,3.7,5.3,1.5,2.4,4.0,3.2],\"nBounds\":3,\"results\":[0,1,0,1,1,0,0,1,0,1,1,0,0,1,0],\"nPossibleRes\":2}}]"}
{"func_name": "rdkit_ML_Data_Quantize_FindVarQuantBound", "func_desc": "FindVarQuantBound finds a single-variable quantization boundary and its associated gain by delegating to FindVarMultQuantBounds with a multiplicity of 1. This function is a thin, historical wrapper kept for backwards compatibility in RDKit's ML data quantization utilities; it is used when converting continuous descriptor or feature values (vals) into a single discrete cut (one quantization boundary) for downstream machine-learning tasks such as descriptor binning, simple decision splits, or preprocessing of molecular descriptors.", "tools": [{"function": {"description": "FindVarQuantBound finds a single-variable quantization boundary and its associated gain by delegating to FindVarMultQuantBounds with a multiplicity of 1. This function is a thin, historical wrapper kept for backwards compatibility in RDKit's ML data quantization utilities; it is used when converting continuous descriptor or feature values (vals) into a single discrete cut (one quantization boundary) for downstream machine-learning tasks such as descriptor binning, simple decision splits, or preprocessing of molecular descriptors.\n", "name": "rdkit_ML_Data_Quantize_FindVarQuantBound", "parameters": {"properties": {"vals": {"type": "array", "items": {"type": "float"}, "description": "A list of continuous or ordered feature values to be quantized. In the RDKit ML/Quantize context, these are typically descriptor values computed for a set of molecules; the function treats this list as the variable to be partitioned and preserves the element type when returning the boundary (the returned boundary will be the same type as elements of vals where possible).", "default": ""}, "results": {"type": "array", "items": {"type": "float"}, "description": "A list of target outcomes or class labels corresponding to the entries in vals. In supervised quantization for machine learning (as used in RDKit descriptor preprocessing), results provides the observed outcomes used to evaluate candidate boundaries (for example class membership or binned response values). The length and ordering of results are expected to match vals; mismatches may lead to incorrect behavior or exceptions propagated from underlying routines.", "default": ""}, "nPossibleRes": {"type": "integer", "description": "The number of distinct possible result values (classes) present in results. This integer guides the internal evaluation of information gain or impurity when FindVarMultQuantBounds computes candidate boundaries. It must reflect the actual number of categories represented in results; providing an incorrect count may produce incorrect gain estimates.", "default": ""}}, "required": ["vals", "results", "nPossibleRes"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a legacy single-cut quantization step (FindVarQuantBound) in an RDKit-based QSAR preprocessing workflow for logP vs. binary activity. The raw assay exports include common plate artifacts and unit/parse errors, so for each cohort (treated as an independent replicate) apply a data-sieve rule before computing the optimal one-boundary split and information gain:\n\nFor each cohort independently, keep only compound records where:\n- logP is a finite, physically plausible value (0 <= logP <= 6)\n- activity label is a valid binary class in {0,1}\n\nThen, using the filtered logP values and their aligned activity labels, assume exactly 2 possible result classes (inactive/active) and compute the optimal single quantization boundary (low vs high bin) and associated information gain.\n\nRaw cohorts (unordered exports):\n- Cohort A (logP, activity): [(0.12,0), (0.45,0), (0.51,0), (0.87,0), (1.02,1), (1.15,1), (1.43,1), (1.77,1), (2.05,1), (2.31,1), (999,1), (-0.4,0), (1.10,2)]\n- Cohort B (logP, activity): [(1.2,0), (3.5,1), (2.8,1), (4.1,1), (0.9,0), (5.0,1), (3.0,1), (2.2,0), (4.5,1), (1.8,0), (6.8,1), (3.3,-1)]", "answers": "[{\"name\":\"rdkit_ML_Data_Quantize_FindVarQuantBound\",\"arguments\":{\"vals\":[0.12,0.45,0.51,0.87,1.02,1.15,1.43,1.77,2.05,2.31],\"results\":[0,0,0,0,1,1,1,1,1,1],\"nPossibleRes\":2}},{\"name\":\"rdkit_ML_Data_Quantize_FindVarQuantBound\",\"arguments\":{\"vals\":[1.2,3.5,2.8,4.1,0.9,5.0,3.0,2.2,4.5,1.8],\"results\":[0,1,1,1,0,1,1,0,1,0],\"nPossibleRes\":2}}]"}
{"func_name": "rdkit_ML_Data_Quantize_feq", "func_desc": "rdkit.ML.Data.Quantize.feq tests whether two floating-point values are equal within a specified absolute tolerance. This routine is used in RDKit's machine-learning and data-quantization code paths to guard against floating-point round-off when comparing descriptor, fingerprint, or other computed scalar feature values.", "tools": [{"function": {"description": "rdkit.ML.Data.Quantize.feq tests whether two floating-point values are equal within a specified absolute tolerance. This routine is used in RDKit's machine-learning and data-quantization code paths to guard against floating-point round-off when comparing descriptor, fingerprint, or other computed scalar feature values.\n", "name": "rdkit_ML_Data_Quantize_feq", "parameters": {"properties": {"v1": {"type": "float", "description": "The first floating-point value to compare. In the RDKit context this is typically a computed descriptor or feature value extracted from a molecule or dataset. Its role is as one side of the equality test; passing non-finite values (NaN or infinity) will affect the comparison as described below.", "default": ""}, "v2": {"type": "float", "description": "The second floating-point value to compare. As with v1, v2 is usually a computed scalar from cheminformatics calculations or ML feature pipelines. The function computes the absolute difference between v1 and v2 to determine equality.", "default": ""}, "tol": {"type": "float", "description": "The absolute tolerance used for the comparison. The function returns equality when abs(v1 - v2) < tol. The default value is 1e-08, chosen to tolerate typical floating-point rounding noise encountered when computing molecular descriptors or features in RDKit. This is an absolute (not relative) tolerance. Because the comparison uses a strict less-than test, a difference exactly equal to tol is considered not equal.", "default": 1e-08}}, "required": ["v1", "v2", "tol"], "type": "any"}}, "type": "function"}], "query": "We’re validating numerical stability before RDKit ML quantization by comparing replicate descriptor outputs under an adaptive absolute-tolerance rule tied to feature scale. For each molecule/descriptor replicate pair, compute an absolute tolerance as: tol = 1e-7 * max(1, max(|v1|, |v2|)). Treat the pair as consistent only if feq(v1, v2, tol) is true under that per-pair tol. Run this audit on the following replicate pairs: (A) logP: 2.357891234 vs 2.357891240, (B) near-zero descriptor: 0.123456781 vs 0.123456789, (C) large-magnitude descriptor: 152340.1250001 vs 152340.1250000, (D) signed descriptor: -3.00000005 vs -3.00000010, (E) quantization-edge case: 0.99999994 vs 1.00000003. Return the feq decision for each pair using its computed tol.", "answers": "[{\"name\":\"rdkit_ML_Data_Quantize_feq\",\"arguments\":{\"v1\":2.357891234,\"v2\":2.35789124,\"tol\":2.35789124e-07}},{\"name\":\"rdkit_ML_Data_Quantize_feq\",\"arguments\":{\"v1\":0.123456781,\"v2\":0.123456789,\"tol\":1e-07}},{\"name\":\"rdkit_ML_Data_Quantize_feq\",\"arguments\":{\"v1\":152340.1250001,\"v2\":152340.125,\"tol\":0.01523401250001}},{\"name\":\"rdkit_ML_Data_Quantize_feq\",\"arguments\":{\"v1\":-3.00000005,\"v2\":-3.0000001,\"tol\":3.0000001e-07}},{\"name\":\"rdkit_ML_Data_Quantize_feq\",\"arguments\":{\"v1\":0.99999994,\"v2\":1.00000003,\"tol\":1.00000003e-07}}]"}
{"func_name": "rdkit_ML_Data_SplitData_SplitDataSet", "func_desc": "SplitDataSet splits a dataset into two subsets (training and hold-out) for machine-learning workflows in RDKit, typically used when preparing molecular descriptor or fingerprint example lists for model training and evaluation.\n    \n    This function takes a sequence of examples (for example, RDKit-calculated descriptors, fingerprints, or any per-molecule feature objects used in cheminformatics machine learning) and delegates the computation of integer indices to SplitIndices to partition the full index range [0, len(data)). The first returned list contains the examples selected for the \"first\" partition (commonly used as the training set) and the second returned list contains the remaining examples (commonly used as the hold-out/test set). SplitDataSet preserves the element selection order defined by the index lists returned from SplitIndices. SplitDataSet also emits a short summary message to standard output unless printing is suppressed via the silent parameter.", "tools": [{"function": {"description": "SplitDataSet splits a dataset into two subsets (training and hold-out) for machine-learning workflows in RDKit, typically used when preparing molecular descriptor or fingerprint example lists for model training and evaluation.\n\nThis function takes a sequence of examples (for example, RDKit-calculated descriptors, fingerprints, or any per-molecule feature objects used in cheminformatics machine learning) and delegates the computation of integer indices to SplitIndices to partition the full index range [0, len(data)). The first returned list contains the examples selected for the \"first\" partition (commonly used as the training set) and the second returned list contains the remaining examples (commonly used as the hold-out/test set). SplitDataSet preserves the element selection order defined by the index lists returned from SplitIndices. SplitDataSet also emits a short summary message to standard output unless printing is suppressed via the silent parameter.", "name": "rdkit_ML_Data_SplitData_SplitDataSet", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "A Python list of examples to be split. In RDKit ML usage this is typically a list of per-molecule feature vectors, descriptor results, fingerprint objects, or any example objects that are indexable by integer positions. The function calls len(data) and accesses elements by integer indices (data[i]), so a TypeError will occur if the object does not support these operations.", "default": ""}, "frac": {"type": "float", "description": "The fraction in the interval [0.0, 1.0] of the original data to place into the first returned list (the \"first\" partition, e.g., training set). Values are interpreted as a proportion of the total number of examples; non-integer counts are handled by the underlying SplitIndices logic, so the actual number of items in the first partition will be approximately frac * len(data), subject to integer rounding performed by SplitIndices. If frac < 0.0 or frac > 1.0 a ValueError is raised.", "default": ""}, "silent": {"type": "integer", "description": "Controls printing of brief informational messages to standard output. The default value 0 causes the function to print the number of points placed in the first partition and the hold-out set, which is useful for interactive or script-level feedback during dataset preparation. Any non-zero integer value suppresses these messages. Internally, SplitIndices is invoked with silent=1 so that only SplitDataSet's summary output is affected by this parameter.", "default": 0}}, "required": ["data", "frac", "silent"], "type": "any"}}, "type": "function"}], "query": "We’re validating an RDKit QSAR workflow under messy, multi-source feature export conditions. Each cohort below is a raw per-molecule feature matrix, but downstream modeling requires (i) consistent feature dimensionality within a split run and (ii) removal of blank/failed featurizations. For each cohort, first retain only vectors that are non-empty, have a consistent length equal to the most common vector length observed within that cohort, and are not all-zeros across all features. Then perform SplitDataSet on the retained vectors using a dynamic hold-out fraction based on cohort quality: if at least half of the retained vectors contain any negative feature value (indicative of centered/scaled descriptors), use a 60/40 split (frac=0.6); otherwise use a 75/25 split (frac=0.75). Suppress console/status output during all splits. Use these exact cohorts: (A) [[0.12, 1.3, 5.6], [0.45, 1.1, 4.9], [0.78, 0.9, 5.2], [0.33, 1.5, 6.0], [0.56, 1.0, 5.1], [0.67, 1.2, 5.8], [0.89, 0.8, 4.7], [0.23, 1.4, 6.2], [0.34, 1.3, 5.4], [0.91, 0.7, 4.5]]; (B) [[0.12, 1.4, 0.0], [0.33, 0.9, 1.0], [1.02, 0.1, 0.0], [0.58, 1.1, 1.0], [0.77, 0.4, 0.0], [1.25, 1.8, 1.0], [0.05, 0.2, 0.0], [0.91, 1.3, 1.0], [0.44, 0.7, 0.0], [1.1, 1.0, 1.0]]; (C) [[1.2, -0.5, 3.1, 0.0], [0.8, 1.5, -2.3, 4.1], [2.0, 0.0, 0.5, -1.2], [1.1, -1.1, 2.2, 3.3], [0.0, 0.0, 0.0, 0.0], [3.5, 2.2, -0.7, 1.0], [1.7, -0.3, 0.8, 2.5], [2.2, 1.1, -1.5, -0.5], [0.3, -0.8, 1.9, 2.2], [1.0, 0.5, -0.2, 3.0]].", "answers": "[{\"name\":\"rdkit_ML_Data_SplitData_SplitDataSet\",\"arguments\":{\"data\":[[0.12,1.3,5.6],[0.45,1.1,4.9],[0.78,0.9,5.2],[0.33,1.5,6.0],[0.56,1.0,5.1],[0.67,1.2,5.8],[0.89,0.8,4.7],[0.23,1.4,6.2],[0.34,1.3,5.4],[0.91,0.7,4.5]],\"frac\":0.75,\"silent\":1}},{\"name\":\"rdkit_ML_Data_SplitData_SplitDataSet\",\"arguments\":{\"data\":[[0.12,1.4,0.0],[0.33,0.9,1.0],[1.02,0.1,0.0],[0.58,1.1,1.0],[0.77,0.4,0.0],[1.25,1.8,1.0],[0.05,0.2,0.0],[0.91,1.3,1.0],[0.44,0.7,0.0],[1.1,1.0,1.0]],\"frac\":0.75,\"silent\":1}},{\"name\":\"rdkit_ML_Data_SplitData_SplitDataSet\",\"arguments\":{\"data\":[[1.2,-0.5,3.1,0.0],[0.8,1.5,-2.3,4.1],[2.0,0.0,0.5,-1.2],[1.1,-1.1,2.2,3.3],[3.5,2.2,-0.7,1.0],[1.7,-0.3,0.8,2.5],[2.2,1.1,-1.5,-0.5],[0.3,-0.8,1.9,2.2],[1.0,0.5,-0.2,3.0]],\"frac\":0.6,\"silent\":1}}]"}
{"func_name": "rdkit_ML_Data_Stats_MeanAndDev", "func_desc": "rdkit.ML.Data.Stats.MeanAndDev computes the arithmetic mean and standard deviation of a numeric vector. This function is intended for use in RDKit workflows (for example when processing descriptor or fingerprint vectors for machine learning and normalization tasks) and implements the standard formulas used when summarizing a list of numeric descriptor values.", "tools": [{"function": {"description": "rdkit.ML.Data.Stats.MeanAndDev computes the arithmetic mean and standard deviation of a numeric vector. This function is intended for use in RDKit workflows (for example when processing descriptor or fingerprint vectors for machine learning and normalization tasks) and implements the standard formulas used when summarizing a list of numeric descriptor values.\n", "name": "rdkit_ML_Data_Stats_MeanAndDev", "parameters": {"properties": {"vect": {"type": "array", "items": {"type": "float"}, "description": "A list of numeric values representing a 1-D data vector (for example, a descriptor vector produced by RDKit). The function converts this list to a NumPy array with dtype 'd' (double precision) using numpy.array(vect, 'd') before computation. If vect is empty (length 0) the function returns (0., 0.). If vect contains values that cannot be converted to double precision floats, numpy will raise a TypeError or ValueError during conversion; those exceptions are not handled inside this function.", "default": ""}, "sampleSD": {"type": "boolean", "description": "Controls whether the standard deviation is the sample standard deviation (True) or the population standard deviation (False). When True (the default value in the signature is 1, which is truthy and selects the sample estimator), the deviation is computed as sqrt(sum((x - mean)^2) / (n - 1)) for n > 1. When False, the deviation is computed as sqrt(sum((x - mean)^2) / n). For n <= 0 the function returns (0., 0.). For n == 1 the mean is the single value and the deviation is returned as 0.0. This choice matters in statistical preprocessing for machine-learning tasks: sampleSD (True) provides the unbiased estimator for population variance when working with a sample.", "default": 1}}, "required": ["vect", "sampleSD"], "type": "any"}}, "type": "function"}], "query": "We’re re-parameterizing the z-score normalization step for a QSAR build where the upstream descriptor extractor sometimes emits sentinel artifacts and incomplete replicates. We have three raw replicate vectors coming from different descriptor families:\n\nA) General descriptors (may contain failed-calculation sentinels): [0.25, 1.7, 3.4, 2.9, 1.1, 0.0, 4.2, 999]\nB) Fingerprint-derived scalar summaries (may contain missing-value placeholders): [0.12, 1.5, 2.3, 0.9, 1.1, 2.0, -1]\nC) MolWt batch (may include a few implausible entries due to salt/mixture parsing): [180.16, 256.34, 134.13, 312.41, 198.22, 225.29, 0.0]\n\nFor each cohort, compute the arithmetic mean and unbiased sample standard deviation (n−1) **after** applying these QC gates:\n- For general and fingerprint-derived cohorts, retain only values in the closed interval [0, 10].\n- For MolWt, retain only values strictly greater than 50 and strictly less than 1000.\n\nReturn mean and sample SD for each QC-filtered cohort vector.", "answers": "[{\"name\":\"rdkit_ML_Data_Stats_MeanAndDev\",\"arguments\":{\"vect\":[0.25,1.7,3.4,2.9,1.1,0.0,4.2],\"sampleSD\":true}},{\"name\":\"rdkit_ML_Data_Stats_MeanAndDev\",\"arguments\":{\"vect\":[0.12,1.5,2.3,0.9,1.1,2.0],\"sampleSD\":true}},{\"name\":\"rdkit_ML_Data_Stats_MeanAndDev\",\"arguments\":{\"vect\":[180.16,256.34,134.13,312.41,198.22,225.29],\"sampleSD\":true}}]"}
{"func_name": "rdkit_ML_Data_Stats_TransformPoints", "func_desc": "rdkit.ML.Data.Stats.TransformPoints transforms a set of numeric point vectors by centering them on their centroid and applying a linear transformation matrix. This function is intended for machine-learning and cheminformatics workflows in RDKit (for example, operating on 2D/3D atomic coordinate sets used when computing descriptors or aligning molecular point sets) where a set of points must be zero-centered and multiplied by a transformation matrix.", "tools": [{"function": {"description": "rdkit.ML.Data.Stats.TransformPoints transforms a set of numeric point vectors by centering them on their centroid and applying a linear transformation matrix. This function is intended for machine-learning and cheminformatics workflows in RDKit (for example, operating on 2D/3D atomic coordinate sets used when computing descriptors or aligning molecular point sets) where a set of points must be zero-centered and multiplied by a transformation matrix.\n", "name": "rdkit_ML_Data_Stats_TransformPoints", "parameters": {"properties": {"tFormMat": {"type": "array", "items": {"type": "float"}, "description": "A numeric transformation matrix provided as a NumPy array. This matrix is used with numpy.dot to transform each centered point. It must be a 2-D numeric numpy.ndarray whose dimensions are compatible with the dimensionality of the point vectors in pts (i.e., shapes must allow numpy.dot(tFormMat, point) to succeed). The function does not modify tFormMat.", "default": ""}, "pts": {"type": "array", "items": {"type": "float"}, "description": "A sequence of point vectors provided as a Python list. Each element is expected to be a numeric vector (for example a 1-D numpy.ndarray) or pts may be a 2-D numeric array (a list of vectors or an array of shape (n_points, point_dim)). The function converts pts to a numpy.array internally, computes the centroid (mean of the points), subtracts that centroid from each point (zero-centers them), and then applies tFormMat to each centered point.", "default": ""}}, "required": ["tFormMat", "pts"], "type": "any"}}, "type": "function"}], "query": "I’m doing an alignment QC pass before descriptor generation on three raw conformer point clouds (A–C). Because some exported conformers contain a duplicated atom coordinate (a common topology/serialization artifact) that breaks the rigid-alignment assumptions, only run centroid-normalization + linear transform on conformers whose 4-point set contains **no exact duplicate 3D coordinate vector** (all points must be unique).\n\nFor each qualifying conformer: translate its points so the centroid is at the origin, then apply the provided 3×3 transformation matrix.\n\nCohort A: pts = [(1.2, -0.7, 0.0), (2.0, 0.3, 1.1), (0.4, 1.5, -0.2), (1.6, 0.1, 0.8)], tFormMat = [[0.0, -0.5, 0.0], [1.5, 0.0, 0.0], [0.0, 0.0, 1.0]].\n\nCohort B: pts = [[1.2, -0.5, 0.0], [2.0, 0.3, 1.1], [0.4, -1.2, 0.7], [1.6, 0.9, -0.8]], tFormMat = [[0, -1, 0], [1, 0, 0], [0, 0, 1]].\n\nCohort C: pts = [(1.2, -0.5, 0.0), (2.0, 1.0, -1.5), (0.0, 0.5, 1.0), (-1.0, -1.5, 0.5)], tFormMat = [[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]].\n\nNote: treat coordinate vectors as exact matches (no tolerance) when checking for duplicates.", "answers": "[{\"name\":\"rdkit_ML_Data_Stats_TransformPoints\",\"arguments\":{\"tFormMat\":[[0.0,-0.5,0.0],[1.5,0.0,0.0],[0.0,0.0,1.0]],\"pts\":[[1.2,-0.7,0.0],[2.0,0.3,1.1],[0.4,1.5,-0.2],[1.6,0.1,0.8]]}},{\"name\":\"rdkit_ML_Data_Stats_TransformPoints\",\"arguments\":{\"tFormMat\":[[0,-1,0],[1,0,0],[0,0,1]],\"pts\":[[1.2,-0.5,0.0],[2.0,0.3,1.1],[0.4,-1.2,0.7],[1.6,0.9,-0.8]]}},{\"name\":\"rdkit_ML_Data_Stats_TransformPoints\",\"arguments\":{\"tFormMat\":[[0.0,-1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],\"pts\":[[1.2,-0.5,0.0],[2.0,1.0,-1.5],[0.0,0.5,1.0],[-1.0,-1.5,0.5]]}}]"}
{"func_name": "rdkit_ML_Descriptors_Parser_DEV", "func_desc": "rdkit.ML.Descriptors.Parser.DEV: calculate the composition-weighted mean absolute deviation of a descriptor expression across a molecular composition.\n    \n    This function computes the average deviation (mean absolute deviation) of a descriptor value defined by a Python expression string across a composition vector. It is intended for use in cheminformatics descriptor generation (RDKit descriptors and fingerprint features for machine learning). The routine first computes the composition-weighted mean by calling MEAN(strArg, composList, atomDict) and then computes the sum over composition entries of the absolute difference between each per-atom descriptor value and that mean, weighted by the atom counts, divided by the total number of atoms in the composition. The descriptor expression is evaluated with Python's eval after substituting the placeholder token 'DEADBEEF' with an atomic symbol from the composition; therefore the expression must evaluate to a numeric value for each atom symbol.", "tools": [{"function": {"description": "rdkit.ML.Descriptors.Parser.DEV: calculate the composition-weighted mean absolute deviation of a descriptor expression across a molecular composition.\n\nThis function computes the average deviation (mean absolute deviation) of a descriptor value defined by a Python expression string across a composition vector. It is intended for use in cheminformatics descriptor generation (RDKit descriptors and fingerprint features for machine learning). The routine first computes the composition-weighted mean by calling MEAN(strArg, composList, atomDict) and then computes the sum over composition entries of the absolute difference between each per-atom descriptor value and that mean, weighted by the atom counts, divided by the total number of atoms in the composition. The descriptor expression is evaluated with Python's eval after substituting the placeholder token 'DEADBEEF' with an atomic symbol from the composition; therefore the expression must evaluate to a numeric value for each atom symbol.", "name": "rdkit_ML_Descriptors_Parser_DEV", "parameters": {"properties": {"strArg": {"type": "string", "description": "A Python expression in string form that evaluates to a numeric descriptor value for a single atomic symbol. The expression must include the literal placeholder 'DEADBEEF' where the atomic symbol should be inserted (for example, \"atomic_weight('DEADBEEF') * 1.0\" or \"someDescriptor(DEADBEEF)\"), so that the code replaces 'DEADBEEF' with an atom symbol from composList before evaluation. The expression will be evaluated using eval() in the current Python execution environment; any names or functions referenced by the expression must be available in that environment. This function calls MEAN(strArg, composList, atomDict) internally, so strArg must also be acceptable to MEAN.", "default": ""}, "composList": {"type": "array", "items": {"type": "float"}, "description": "A composition vector describing the composition to average over. Each element is expected to be a pair (atom, num) where atom is an atomic identifier (commonly a string atomic symbol such as 'C', 'H', 'O') and num is the count/weight of that atom in the composition (an int or numeric value). The function iterates over composList, replacing 'DEADBEEF' with atom and weighting contributions by num. The total weight is the sum of the num values.", "default": ""}, "atomDict": {"type": "any", "description": "An atomic dictionary passed through the descriptor calculation pipeline. This dictionary typically maps atomic identifiers (keys) to atomic properties or descriptor lookup data used by descriptor expressions or helper functions. Although DEV does not directly index atomDict, it is provided to satisfy the common interface and is forwarded to MEAN(strArg, composList, atomDict) which may require it. The contents and structure of atomDict must be appropriate for the descriptor expressions and for MEAN.", "default": ""}}, "required": ["strArg", "composList", "atomDict"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a composition-based descriptor featurizer on a mixed panel of putative molecular formulas coming from a noisy upstream parser. For each candidate composition, compute the composition-weighted mean absolute deviation (MAD) of Pauling electronegativity using rdkit.ML.Descriptors.Parser.DEV, but only for compositions that are chemically self-consistent: each entry must have a positive integer atom count, each element symbol must exist in the provided electronegativity lookup, and the composition must contain at least two distinct elements (to avoid trivial zero-variance single-element cases). Use an expression-string rule that depends on the atomDict schema: if the atomDict is element-keyed with per-element records, evaluate electronegativity via \"atomDict['DEADBEEF']['eneg']\"; if the atomDict is property-keyed with an 'electronegativity' sub-dictionary, evaluate via \"atomDict['electronegativity']['DEADBEEF']\". Raw candidates (some may be rejected by the rule):\n1) hexose-like: composList [[\"C\",6],[\"H\",12],[\"O\",6]] with atomDict {\"C\":{\"eneg\":2.55},\"H\":{\"eneg\":2.2},\"O\":{\"eneg\":3.44}}\n2) ethanol-like: composList [[\"C\",2],[\"H\",6],[\"O\",1]] with atomDict {\"electronegativity\":{\"H\":2.2,\"C\":2.55,\"O\":3.44}}\n3) corrupted export: composList [[\"C\",0],[\"H\",4],[\"O\",2]] with the same element-keyed atomDict as (1)\n4) single-element artifact: composList [[\"Cl\",2]] with atomDict {\"electronegativity\":{\"Cl\":3.16}}\n5) heteroatom-containing: composList [[\"C\",7],[\"H\",7],[\"N\",1],[\"O\",2]] with atomDict {\"electronegativity\":{\"H\":2.2,\"C\":2.55,\"N\":3.04,\"O\":3.44}}\nReturn the DEV calls needed to execute this protocol on all candidates that pass the self-consistency rules.", "answers": "[{\"name\":\"rdkit_ML_Descriptors_Parser_DEV\",\"arguments\":{\"strArg\":\"atomDict['DEADBEEF']['eneg']\",\"composList\":[[\"C\",6],[\"H\",12],[\"O\",6]],\"atomDict\":{\"C\":{\"eneg\":2.55},\"H\":{\"eneg\":2.2},\"O\":{\"eneg\":3.44}}}},{\"name\":\"rdkit_ML_Descriptors_Parser_DEV\",\"arguments\":{\"strArg\":\"atomDict['electronegativity']['DEADBEEF']\",\"composList\":[[\"C\",2],[\"H\",6],[\"O\",1]],\"atomDict\":{\"electronegativity\":{\"H\":2.2,\"C\":2.55,\"O\":3.44}}}},{\"name\":\"rdkit_ML_Descriptors_Parser_DEV\",\"arguments\":{\"strArg\":\"atomDict['electronegativity']['DEADBEEF']\",\"composList\":[[\"C\",7],[\"H\",7],[\"N\",1],[\"O\",2]],\"atomDict\":{\"electronegativity\":{\"H\":2.2,\"C\":2.55,\"N\":3.04,\"O\":3.44}}}}]"}
{"func_name": "rdkit_ML_InfoTheory_BitRank_FormCounts", "func_desc": "rdkit.ML.InfoTheory.BitRank.FormCounts generates a counts (contingency) matrix for a single fingerprint bit across activity classes; it is used by RDKit's machine-learning/InfoTheory/BitRank utilities to summarize how many occurrences of each bit value co-occur with each activity value when analyzing fingerprint-like integer vectors (IntVectors) for descriptor and fingerprint-based models.\n    \n    This function constructs a 2-D counts matrix with shape (nPossibleBitVals, nPossibleActs) where rows correspond to possible values of the selected bit and columns correspond to possible activity classes. The resulting matrix is typically used for information-theoretic scoring or ranking of fingerprint bits in cheminformatics and machine-learning workflows within RDKit (for example, to compute mutual information or other statistics between bit presence/values and activity labels).", "tools": [{"function": {"description": "rdkit.ML.InfoTheory.BitRank.FormCounts generates a counts (contingency) matrix for a single fingerprint bit across activity classes; it is used by RDKit's machine-learning/InfoTheory/BitRank utilities to summarize how many occurrences of each bit value co-occur with each activity value when analyzing fingerprint-like integer vectors (IntVectors) for descriptor and fingerprint-based models.\n\nThis function constructs a 2-D counts matrix with shape (nPossibleBitVals, nPossibleActs) where rows correspond to possible values of the selected bit and columns correspond to possible activity classes. The resulting matrix is typically used for information-theoretic scoring or ranking of fingerprint bits in cheminformatics and machine-learning workflows within RDKit (for example, to compute mutual information or other statistics between bit presence/values and activity labels).", "name": "rdkit_ML_InfoTheory_BitRank_FormCounts", "parameters": {"properties": {"bitVects": {"type": "array", "items": {"type": "float"}, "description": "a sequence containing IntVectors (RDKit-style integer-indexable vectors representing fingerprint bit values). Each element provides integer values for bits; the function reads the element at index whichBit from each vector to determine the bit value for that sample. The length of this sequence must equal the length of actVals; otherwise a ValueError is raised. Elements must support integer indexing and return integers suitable as indices into the output row dimension.", "default": ""}, "actVals": {"type": "array", "items": {"type": "float"}, "description": "a sequence of integer activity labels (class indices) aligned with bitVects such that actVals[i] is the activity value for the sample represented by bitVects[i]. Values are used as indices into the output column dimension and must lie in the range [0, nPossibleActs-1]; values outside that range will raise an IndexError at runtime.", "default": ""}, "whichBit": {"type": "integer", "description": "the integer index of the bit to examine within each IntVector in bitVects. This selects which position to read from each vector; if this index is out of range for an element in bitVects an IndexError will be raised.", "default": ""}, "nPossibleActs": {"type": "integer", "description": "the integer number of possible activity values (the number of columns in the returned counts matrix). This defines the second dimension of the output and must be consistent with the range of integers found in actVals.", "default": ""}, "nPossibleBitVals": {"type": "integer", "description": "optional integer specifying the maximum number of distinct bit values to account for (the number of rows in the returned counts matrix). By default this is 2, which is appropriate for binary fingerprint bits (0/1). If bitVects contain values outside the range [0, nPossibleBitVals-1], an IndexError will occur when incrementing the corresponding count.", "default": 2}}, "required": ["bitVects", "actVals", "whichBit", "nPossibleActs", "nPossibleBitVals"], "type": "any"}}, "type": "function"}], "query": "As part of a BitRank feature triage audit, we’re merging two replicate cohorts that were featurized with the same 4-position integer fingerprint schema. For robustness, build contingency tables only for descriptor positions that are truly binary across the compounds in each cohort (i.e., the observed values at that position span exactly {0,1} within that cohort), since non-binary drift indicates an encoding artifact and will be handled elsewhere. Use the activity label spaces native to each cohort: Cohort A is a 3-class screen (nPossibleActs=3; classes 0/1/2) with fingerprints [[0,1,1,0],[1,0,1,1],[0,1,0,1],[1,1,1,0],[0,0,0,0],[1,0,1,0]] and labels [0,1,0,2,1,2]. Cohort B is a binary QSAR replicate (nPossibleActs=2; classes 0/1) with fingerprints [[1,0,1,0],[0,1,1,0],[1,1,0,0],[0,0,1,1],[1,0,0,1],[0,1,0,1]] and labels [0,1,0,1,0,1]. For every qualifying bit position per cohort, generate the 2-D counts matrix using rdkit.ML.InfoTheory.BitRank.FormCounts, and explicitly constrain nPossibleBitVals=2 whenever the position qualifies as binary. Return the full set of contingency matrices (one per qualifying bit per cohort) for downstream information-theoretic ranking.", "answers": "[{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[0,1,1,0],[1,0,1,1],[0,1,0,1],[1,1,1,0],[0,0,0,0],[1,0,1,0]],\"actVals\":[0,1,0,2,1,2],\"whichBit\":0,\"nPossibleActs\":3,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[0,1,1,0],[1,0,1,1],[0,1,0,1],[1,1,1,0],[0,0,0,0],[1,0,1,0]],\"actVals\":[0,1,0,2,1,2],\"whichBit\":1,\"nPossibleActs\":3,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[0,1,1,0],[1,0,1,1],[0,1,0,1],[1,1,1,0],[0,0,0,0],[1,0,1,0]],\"actVals\":[0,1,0,2,1,2],\"whichBit\":2,\"nPossibleActs\":3,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[0,1,1,0],[1,0,1,1],[0,1,0,1],[1,1,1,0],[0,0,0,0],[1,0,1,0]],\"actVals\":[0,1,0,2,1,2],\"whichBit\":3,\"nPossibleActs\":3,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[1,0,1,0],[0,1,1,0],[1,1,0,0],[0,0,1,1],[1,0,0,1],[0,1,0,1]],\"actVals\":[0,1,0,1,0,1],\"whichBit\":0,\"nPossibleActs\":2,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[1,0,1,0],[0,1,1,0],[1,1,0,0],[0,0,1,1],[1,0,0,1],[0,1,0,1]],\"actVals\":[0,1,0,1,0,1],\"whichBit\":1,\"nPossibleActs\":2,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[1,0,1,0],[0,1,1,0],[1,1,0,0],[0,0,1,1],[1,0,0,1],[0,1,0,1]],\"actVals\":[0,1,0,1,0,1],\"whichBit\":2,\"nPossibleActs\":2,\"nPossibleBitVals\":2}},{\"name\":\"rdkit_ML_InfoTheory_BitRank_FormCounts\",\"arguments\":{\"bitVects\":[[1,0,1,0],[0,1,1,0],[1,1,0,0],[0,0,1,1],[1,0,0,1],[0,1,0,1]],\"actVals\":[0,1,0,1,0,1],\"whichBit\":3,\"nPossibleActs\":2,\"nPossibleBitVals\":2}}]"}
{"func_name": "rdkit_ML_InfoTheory_entropy_PyInfoGain", "func_desc": "rdkit.ML.InfoTheory.entropy.PyInfoGain computes the information gain (expected reduction in Shannon entropy) for a single discrete variable given a contingency table of observed counts. In the RDKit machine-learning and information-theory utilities this function is used to evaluate how much knowing the value of a chemical descriptor or discrete feature (the variable) reduces uncertainty about an outcome or class label (the result), e.g., during feature selection or decision-tree split evaluation on molecular descriptor/fingerprint data.", "tools": [{"function": {"description": "rdkit.ML.InfoTheory.entropy.PyInfoGain computes the information gain (expected reduction in Shannon entropy) for a single discrete variable given a contingency table of observed counts. In the RDKit machine-learning and information-theory utilities this function is used to evaluate how much knowing the value of a chemical descriptor or discrete feature (the variable) reduces uncertainty about an outcome or class label (the result), e.g., during feature selection or decision-tree split evaluation on molecular descriptor/fingerprint data.\n", "name": "rdkit_ML_InfoTheory_entropy_PyInfoGain", "parameters": {"properties": {"varMat": {"type": "array", "items": {"type": "float"}, "description": "A 2-D numeric array (contingency table) of observed counts where rows correspond to the possible values of the variable and columns correspond to the possible result/outcome values. Each entry varMat[i, j] is the number of occurrences where the variable takes its i-th value and the result is the j-th outcome. For example, for a variable with 4 possible values and a result with 3 possible values, varMat would be shaped (4, 3). The array is interpreted as counts (non-negative numerics). The function sums rows to obtain Sv (the counts for each variable value) and sums columns to obtain S (the overall counts per result), following Mitchell's notation used in the implementation.", "default": ""}}, "required": ["varMat"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a QSAR feature-selection step where a discretized descriptor (3 bins: low/medium/high) may be unreliable in some cohorts due to sparsity. For each cohort’s 3×2 contingency table (rows = bins; columns correspond to class counts but may not be in the same order across cohorts), compute information gain only if every bin has at least 5 total observations (row-sum ≥ 5); treat cohorts failing this minimum-support criterion as unusable for split evaluation. Cohort A and B are already in [inactive, active] column order: A=[[12,3],[8,7],[2,11]]; B=[[12,3],[8,10],[2,15]]. The validation cohort was logged as (active,inactive) per bin: low (10,40), medium (25,25), high (35,15); convert it to [inactive,active] before applying the same minimum-support rule and computing information gain.", "answers": "[{\"name\":\"rdkit_ML_InfoTheory_entropy_PyInfoGain\",\"arguments\":{\"varMat\":[[12,3],[8,10],[2,15]]}},{\"name\":\"rdkit_ML_InfoTheory_entropy_PyInfoGain\",\"arguments\":{\"varMat\":[[40,10],[25,25],[15,35]]}}]"}
{"func_name": "rdkit_ML_SLT_Risk_BurgesRiskBound", "func_desc": "rdkit.ML.SLT.Risk.BurgesRiskBound calculates Burges's formulation of the risk bound (Eqn. 3 in Burges, \"A Tutorial on Support Vector Machines for Pattern Recognition\", Data Mining and Knowledge Discovery, 1998) and returns an upper bound on the true misclassification risk used in machine-learning analyses (for example SVMs) within the RDKit ML/SLT workflow.\n    \n    This function combines an empirical error term (fraction of misclassified training examples) with a structural risk term derived from the VC-dimension to produce a single scalar risk bound. It is intended for use in statistical learning evaluations (binary classification scenarios, as noted in the original implementation) when one wants a theoretically motivated upper bound on generalization error for a hypothesis class characterized by its VC dimension.", "tools": [{"function": {"description": "rdkit.ML.SLT.Risk.BurgesRiskBound calculates Burges's formulation of the risk bound (Eqn. 3 in Burges, \"A Tutorial on Support Vector Machines for Pattern Recognition\", Data Mining and Knowledge Discovery, 1998) and returns an upper bound on the true misclassification risk used in machine-learning analyses (for example SVMs) within the RDKit ML/SLT workflow.\n\nThis function combines an empirical error term (fraction of misclassified training examples) with a structural risk term derived from the VC-dimension to produce a single scalar risk bound. It is intended for use in statistical learning evaluations (binary classification scenarios, as noted in the original implementation) when one wants a theoretically motivated upper bound on generalization error for a hypothesis class characterized by its VC dimension.", "name": "rdkit_ML_SLT_Risk_BurgesRiskBound", "parameters": {"properties": {"VCDim": {"type": "integer", "description": "The Vapnik–Chervonenkis (VC) dimension of the hypothesis class being evaluated. In the context of RDKit's ML utilities, VCDim represents the capacity/complexity of the classifier family (h in Burges's notation). It must be provided as an integer; if supplied incorrectly (non-numeric or not an integer), a TypeError or later numeric-errors may occur.", "default": ""}, "nData": {"type": "integer", "description": "The number of data points (training examples) used to compute the empirical error (l in Burges's notation). This integer is used as a denominator in the empirical error and to scale the structural term. Passing nData == 0 will cause a division-by-zero error in the implementation; non-integer numeric types may lead to unexpected results or exceptions.", "default": ""}, "nWrong": {"type": "integer", "description": "The number of data points from the nData that were misclassified (used to compute empirical risk). This integer is converted to a float fraction rEmp = float(nWrong) / nData. Supplying values inconsistent with nData (e.g., nWrong > nData) will still compute a numeric result but may not be meaningful in a probability interpretation.", "default": ""}, "conf": {"type": "float", "description": "The confidence parameter (denoted eta in the code and in Burges's formulation) used to scale the logarithmic confidence term in the structural risk. This float is used inside a natural logarithm (math.log(eta / 4.0)) and therefore must be such that the argument to the log is positive; improper values can raise a ValueError from math.log. The value represents the confidence level used when deriving the probabilistic bound.", "default": ""}}, "required": ["VCDim", "nData", "nWrong", "conf"], "type": "any"}}, "type": "function"}], "query": "We’re doing a QC-driven generalization-risk audit for a mixed batch of RDKit ML/SLT binary SVM cohort summaries coming from multiple training logs. Each record provides (nData, nWrong, VCDim) and either a stated confidence or a p-value-style alpha; some records are borderline-invalid due to inconsistent confidence conventions. Use Burges’s 1998 VC-dimension risk bound on only those cohorts that satisfy all QC gates: (1) binary-classification sanity: 0 <= nWrong <= nData and nData > 0, (2) VC sanity: VCDim is a positive integer strictly less than nData, (3) confidence must be usable as BurgesRiskBound’s `conf` parameter: if the record provides a confidence in (0,1) but is reported as a percentage (e.g., 95), convert it to a probability; if it provides alpha in (0,1), use alpha directly as `conf`; otherwise it fails QC. Apply the bound to the following raw cohort records (order preserved in reporting for those that pass):\n- Cohort A: nData=500, nWrong=35, VCDim=50, alpha=0.1\n- Cohort B: nData=1500, nWrong=90, VCDim=25, alpha=0.1\n- Cohort C: nData=250, nWrong=18, VCDim=40, confidence=95\n- Cohort D: nData=250, nWrong=260, VCDim=10, alpha=0.1\n- Cohort E: nData=40, nWrong=2, VCDim=40, alpha=0.1\nReturn the Burges risk bound for each cohort that passes QC.", "answers": "[{\"name\":\"rdkit_ML_SLT_Risk_BurgesRiskBound\",\"arguments\":{\"VCDim\":50,\"nData\":500,\"nWrong\":35,\"conf\":0.1}},{\"name\":\"rdkit_ML_SLT_Risk_BurgesRiskBound\",\"arguments\":{\"VCDim\":25,\"nData\":1500,\"nWrong\":90,\"conf\":0.1}},{\"name\":\"rdkit_ML_SLT_Risk_BurgesRiskBound\",\"arguments\":{\"VCDim\":40,\"nData\":250,\"nWrong\":18,\"conf\":0.95}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcBEDROC", "func_desc": "rdkit.ML.Scoring.Scoring.CalcBEDROC computes the BEDROC (Boltzmann-enhanced discrimination of ROC) score used to quantify \"early recognition\" performance in virtual screening campaigns, following Truchon & Bayly, J. Chem. Inf. Model. 47, 488-508 (2007).\n    \n    This function expects a ranked list of samples (e.g., molecules) with per-sample data and a column indicating which samples are considered \"active\" (true positives). It uses an internal RIE (Robust Initial Enhancement) helper to compute the unnormalized enrichment and then normalizes that value to produce the BEDROC score. BEDROC emphasizes retrieval of active compounds at the top of a ranked list and is commonly used in cheminformatics and machine-learning workflows within RDKit to evaluate virtual screening and ranking algorithms.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcBEDROC computes the BEDROC (Boltzmann-enhanced discrimination of ROC) score used to quantify \"early recognition\" performance in virtual screening campaigns, following Truchon & Bayly, J. Chem. Inf. Model. 47, 488-508 (2007).\n\nThis function expects a ranked list of samples (e.g., molecules) with per-sample data and a column indicating which samples are considered \"active\" (true positives). It uses an internal RIE (Robust Initial Enhancement) helper to compute the unnormalized enrichment and then normalizes that value to produce the BEDROC score. BEDROC emphasizes retrieval of active compounds at the top of a ranked list and is commonly used in cheminformatics and machine-learning workflows within RDKit to evaluate virtual screening and ranking algorithms.", "name": "rdkit_ML_Scoring_Scoring_CalcBEDROC", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "float"}, "description": "A two-dimensional sequence (samples × features) containing per-sample data. The 0th index corresponds to the sample (row) and each element scores[sample_id] is an indexable sequence representing that sample's data. The rows in scores must be sorted in ranked order with lower indices representing \"better\" (higher-priority) predictions; the metric assumes the input is pre-ranked. The column specified by col is used to determine true actives: scores[sample_id][col] == True iff that sample is considered active. Providing data that is not 2D, not indexable as described, or not pre-sorted will make the metric meaningless or cause indexing/type errors.", "default": ""}, "col": {"type": "integer", "description": "Integer column index in each sample vector that indicates the true label for that sample. For a given sample_id, scores[sample_id][col] must compare equal to True exactly when the sample is active. If col is out of range for the per-sample vectors, an IndexError (or equivalent) will be raised by the underlying operations.", "default": ""}, "alpha": {"type": "float", "description": "Hyperparameter from the original BEDROC formulation that controls how strongly to weight early enrichment (the \"top\" of the ranked list). Larger values of alpha place more emphasis on top-ranked actives. This float is passed directly into the RIE calculation; extreme values may increase numerical sensitivity because exponential functions are used internally.", "default": ""}}, "required": ["scores", "col", "alpha"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating two ranked docking cohorts for an early-recognition audit, but the activity labels come from mixed sources and we only want to score the label set that is statistically usable for BEDROC. For each cohort (already sorted best→worst by docking score), compute BEDROC with alpha=20.0 using the boolean activity flag in column index 2, but only for cohorts where the active class is neither absent nor trivially dominant (i.e., the fraction of actives is strictly between 0.1 and 0.9). Cohort A rows: [\"mol_001\", -9.8, true], [\"mol_002\", -9.5, true], [\"mol_003\", -9.3, false], [\"mol_004\", -9.1, false], [\"mol_005\", -8.9, true], [\"mol_006\", -8.7, false], [\"mol_007\", -8.5, false], [\"mol_008\", -8.3, true], [\"mol_009\", -8.1, false], [\"mol_010\", -7.9, false]. Cohort B rows: [\"C001\", -11.2, true], [\"C002\", -10.9, false], [\"C003\", -10.7, true], [\"C004\", -10.5, false], [\"C005\", -10.3, false], [\"C006\", -10.1, false], [\"C007\", -9.9, true], [\"C008\", -9.7, false], [\"C009\", -9.6, false], [\"C010\", -9.4, false]. Report BEDROC for each cohort that passes the class-balance gate for cross-run comparability.", "answers": "[{\"name\":\"rdkit_ML_Scoring_Scoring_CalcBEDROC\",\"arguments\":{\"scores\":[[\"mol_001\",-9.8,true],[\"mol_002\",-9.5,true],[\"mol_003\",-9.3,false],[\"mol_004\",-9.1,false],[\"mol_005\",-8.9,true],[\"mol_006\",-8.7,false],[\"mol_007\",-8.5,false],[\"mol_008\",-8.3,true],[\"mol_009\",-8.1,false],[\"mol_010\",-7.9,false]],\"col\":2,\"alpha\":20.0}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcBEDROC\",\"arguments\":{\"scores\":[[\"C001\",-11.2,true],[\"C002\",-10.9,false],[\"C003\",-10.7,true],[\"C004\",-10.5,false],[\"C005\",-10.3,false],[\"C006\",-10.1,false],[\"C007\",-9.9,true],[\"C008\",-9.7,false],[\"C009\",-9.6,false],[\"C010\",-9.4,false]],\"col\":2,\"alpha\":20.0}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcEnrichment", "func_desc": "rdkit.ML.Scoring.Scoring.CalcEnrichment determines enrichment factors for ranked scoring results produced in cheminformatics / virtual screening workflows (as used in RDKit machine-learning and scoring contexts). The function computes, for each requested top fraction of the ranked list, the fold-enrichment of actives recovered in that top fraction relative to the expectation from random selection. This is typically used to quantify early-recovery performance of scoring functions or virtual screening models when a boolean active/inactive indicator is stored in a specific column of each score record.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcEnrichment determines enrichment factors for ranked scoring results produced in cheminformatics / virtual screening workflows (as used in RDKit machine-learning and scoring contexts). The function computes, for each requested top fraction of the ranked list, the fold-enrichment of actives recovered in that top fraction relative to the expectation from random selection. This is typically used to quantify early-recovery performance of scoring functions or virtual screening models when a boolean active/inactive indicator is stored in a specific column of each score record.\n", "name": "rdkit_ML_Scoring_Scoring_CalcEnrichment", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "float"}, "description": "A list of score records (rows) for molecules/items produced by a scoring or ranking procedure. Each element in this list must be an indexable sequence (for example, a tuple or list) such that scores[i][col] yields a truthy value when the i-th record is an active (hit) and a falsy value otherwise. The function uses the order of this list as the ranking (it expects the list to be ordered from best-ranked to worst-ranked so that \"top fractions\" refer to leading entries). The length of this list determines the total number of molecules/items evaluated; an empty list causes a ValueError.", "default": ""}, "col": {"type": "integer", "description": "The integer column index into each score record that identifies activity (the active/inactive flag). This index must be valid for every element of scores; if an element does not support indexing at this position an IndexError or TypeError may be raised. The value at this index is interpreted as boolean-like: truthy means active, falsy means inactive.", "default": ""}, "fractions": {"type": "array", "items": {"type": "float"}, "description": "A list of fractional thresholds (each a numeric value) specifying the proportions of the ranked list at which to report enrichment (for example, 0.01 for the top 1%). Each fraction must satisfy 0 <= fraction <= 1. The function converts each fraction into a count with math.ceil(len(scores) * fraction) to determine how many top-ranked entries to consider for that fraction. An empty fractions list causes a ValueError. Fractions outside [0, 1] cause a ValueError.", "default": ""}}, "required": ["scores", "col", "fractions"], "type": "any"}}, "type": "function"}], "query": "We’re running early-recognition QC on three rank-ordered virtual-screening cohorts, but the exported score tables contain assay-annotation noise. Each record is still a 3-tuple [molecule_id, score, activity_flag], and the activity label is stored in column index 2. Treat each cohort as already sorted best→worst and do not re-sort.\n\nBefore computing enrichment, apply cohort-specific label-sanity rules to decide which records are usable (leave ordering intact after filtering):\n\n1) ZINC20: use the 20-record list [[\"ZINC0001\", -12.3, true], [\"ZINC0002\", -11.8, false], [\"ZINC0003\", -11.5, true], [\"ZINC0004\", -11.2, false], [\"ZINC0005\", -10.9, false], [\"ZINC0006\", -10.7, true], [\"ZINC0007\", -10.5, false], [\"ZINC0008\", -10.3, false], [\"ZINC0009\", -10.0, true], [\"ZINC0010\", -9.8, false], [\"ZINC0011\", -9.6, false], [\"ZINC0012\", -9.4, true], [\"ZINC0013\", -9.2, false], [\"ZINC0014\", -9.0, false], [\"ZINC0015\", -8.8, false], [\"ZINC0016\", -8.6, true], [\"ZINC0017\", -8.4, false], [\"ZINC0018\", -8.2, false], [\"ZINC0019\", -8.0, false], [\"ZINC0020\", -7.8, true]]. Only keep records whose activity_flag is a native boolean (true/false). Compute enrichment at top fractions 0.01, 0.05, and 0.1.\n\n2) CMPD10: use [[\"CMPD_001\", -11.2, 1], [\"CMPD_002\", -10.8, 0], [\"CMPD_003\", -10.5, 1], [\"CMPD_004\", -10.0, 0], [\"CMPD_005\", -9.7, 0], [\"CMPD_006\", -9.3, 1], [\"CMPD_007\", -9.0, 0], [\"CMPD_008\", -8.6, 0], [\"CMPD_009\", -8.2, 0], [\"CMPD_010\", -7.9, 1]]. Only keep records whose activity_flag is encoded as an integer in {0,1}. Compute enrichment at top fractions 0.01, 0.1, 0.2, and 0.5.\n\n3) MOL10: use [[\"mol1\", 9.8, 1], [\"mol2\", 9.5, 0], [\"mol3\", 9.3, 1], [\"mol4\", 9.1, 0], [\"mol5\", 8.9, 1], [\"mol6\", 8.7, 0], [\"mol7\", 8.5, 0], [\"mol8\", 8.3, 1], [\"mol9\", 8.1, 0], [\"mol10\", 7.9, 0]]. This cohort was merged from multiple annotation sources, so only keep records where the activity_flag is an integer in {0,1} AND the score is strictly positive. Compute enrichment at top fractions 0.1, 0.2, and 0.5.\n\nReport the enrichment factors for each cohort using rdkit.ML.Scoring.Scoring.CalcEnrichment on the filtered, order-preserved lists.", "answers": "[{\"name\":\"rdkit_ML_Scoring_Scoring_CalcEnrichment\",\"arguments\":{\"scores\":[[\"ZINC0001\",-12.3,true],[\"ZINC0002\",-11.8,false],[\"ZINC0003\",-11.5,true],[\"ZINC0004\",-11.2,false],[\"ZINC0005\",-10.9,false],[\"ZINC0006\",-10.7,true],[\"ZINC0007\",-10.5,false],[\"ZINC0008\",-10.3,false],[\"ZINC0009\",-10.0,true],[\"ZINC0010\",-9.8,false],[\"ZINC0011\",-9.6,false],[\"ZINC0012\",-9.4,true],[\"ZINC0013\",-9.2,false],[\"ZINC0014\",-9.0,false],[\"ZINC0015\",-8.8,false],[\"ZINC0016\",-8.6,true],[\"ZINC0017\",-8.4,false],[\"ZINC0018\",-8.2,false],[\"ZINC0019\",-8.0,false],[\"ZINC0020\",-7.8,true]],\"col\":2,\"fractions\":[0.01,0.05,0.1]}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcEnrichment\",\"arguments\":{\"scores\":[[\"CMPD_001\",-11.2,1],[\"CMPD_002\",-10.8,0],[\"CMPD_003\",-10.5,1],[\"CMPD_004\",-10.0,0],[\"CMPD_005\",-9.7,0],[\"CMPD_006\",-9.3,1],[\"CMPD_007\",-9.0,0],[\"CMPD_008\",-8.6,0],[\"CMPD_009\",-8.2,0],[\"CMPD_010\",-7.9,1]],\"col\":2,\"fractions\":[0.01,0.1,0.2,0.5]}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcEnrichment\",\"arguments\":{\"scores\":[[\"mol1\",9.8,1],[\"mol2\",9.5,0],[\"mol3\",9.3,1],[\"mol4\",9.1,0],[\"mol5\",8.9,1],[\"mol6\",8.7,0],[\"mol7\",8.5,0],[\"mol8\",8.3,1],[\"mol9\",8.1,0],[\"mol10\",7.9,0]],\"col\":2,\"fractions\":[0.1,0.2,0.5]}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcRIE", "func_desc": "rdkit.ML.Scoring.Scoring.CalcRIE computes the Robust Initial Enhancement (RIE) enrichment metric used to quantify early-recognition performance of ranking/scoring methods in virtual screening (original definition: Sheridan et al., J. Chem. Inf. Comput. Sci. 2001). The function is part of RDKit's ML/Scoring utilities and is used to evaluate how well a scoring list prioritizes “active” entries near the top of a ranked list.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcRIE computes the Robust Initial Enhancement (RIE) enrichment metric used to quantify early-recognition performance of ranking/scoring methods in virtual screening (original definition: Sheridan et al., J. Chem. Inf. Comput. Sci. 2001). The function is part of RDKit's ML/Scoring utilities and is used to evaluate how well a scoring list prioritizes “active” entries near the top of a ranked list.\n", "name": "rdkit_ML_Scoring_Scoring_CalcRIE", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "float"}, "description": "A sequence (Python list) containing per-item records/rows produced by scoring or screening pipelines. Each element is expected to be an indexable record (for example, a tuple, list, or other sequence) from which a column can be selected using the integer index given by col. In cheminformatics/virtual-screening usage, scores typically contains both a predicted score/rank and a label column identifying which entries are actives/decoys; this function reads the column indicated by col to determine activity labels or values used by the RIE calculation. Providing a plain list of numeric scores without a selectable column is not compatible with the col argument.", "default": ""}, "col": {"type": "integer", "description": "Integer column index into each element of scores that identifies the activity indicator or numeric value used by the RIE calculation. In typical use within RDKit screening benchmarks, this selects the column that marks actives (for example 1/0 or True/False) or provides the reference value required by the implementation. The index is zero-based and must be valid for every element in scores.", "default": ""}, "alpha": {"type": "float", "description": "Positive floating point parameter that controls how strongly early-ranking positions are weighted in the RIE calculation. Larger alpha values increase emphasis on the top-ranked portion of the list, making the metric more sensitive to very early enrichment. The value is passed through directly to the underlying _RIEHelper routine; no implicit clipping or defaulting is performed by CalcRIE.", "default": ""}}, "required": ["scores", "col", "alpha"], "type": "any"}}, "type": "function"}], "query": "We’re running an early-recognition QC pass on two top-10 docking cohorts where the activity label may contain assay artifacts. Each record is [compound_id, docking_score, is_active] already in the intended rank order (best-to-worst); keep this order and do not re-sort. Before computing RIE, apply an assay-consistency sieve: only keep cohorts where the activity column (index 2) is strictly binary (0/1) and has at least two actives within the top-10 (otherwise the cohort is considered too sparse/unstable for an RIE readout in this batch).\n\nFor cohorts that pass the sieve, compute RIE using an alpha that adapts to the cohort’s active prevalence to keep the expected early-recognition window comparable across batches: set alpha = 20.0 * (0.3 / f), where f is the fraction of actives in that cohort’s top-10 (using the provided labels). Use col=2.\n\nCohort A (top-10): [[\"C001\",-11.2,1],[\"C002\",-10.8,0],[\"C003\",-10.5,0],[\"C004\",-10.1,1],[\"C005\",-9.9,0],[\"C006\",-9.7,0],[\"C007\",-9.6,0],[\"C008\",-9.4,1],[\"C009\",-9.2,0],[\"C010\",-9.0,0]]\nCohort B (top-10): [[\"C001\",-11.2,1],[\"C002\",-10.8,0],[\"C003\",-10.5,1],[\"C004\",-10.1,0],[\"C005\",-9.9,0],[\"C006\",-9.6,1],[\"C007\",-9.3,0],[\"C008\",-9.0,0],[\"C009\",-8.7,1],[\"C010\",-8.4,0]]", "answers": "[{\"name\":\"rdkit_ML_Scoring_Scoring_CalcRIE\",\"arguments\":{\"scores\":[[\"C001\",-11.2,1],[\"C002\",-10.8,0],[\"C003\",-10.5,0],[\"C004\",-10.1,1],[\"C005\",-9.9,0],[\"C006\",-9.7,0],[\"C007\",-9.6,0],[\"C008\",-9.4,1],[\"C009\",-9.2,0],[\"C010\",-9.0,0]],\"col\":2,\"alpha\":20.0}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcRIE\",\"arguments\":{\"scores\":[[\"C001\",-11.2,1],[\"C002\",-10.8,0],[\"C003\",-10.5,1],[\"C004\",-10.1,0],[\"C005\",-9.9,0],[\"C006\",-9.6,1],[\"C007\",-9.3,0],[\"C008\",-9.0,0],[\"C009\",-8.7,1],[\"C010\",-8.4,0]],\"col\":2,\"alpha\":15.0}}]"}
{"func_name": "rdkit_ML_Scoring_Scoring_CalcROC", "func_desc": "rdkit.ML.Scoring.Scoring.CalcROC: Compute a Receiver Operating Characteristic (ROC) curve from a list of scored records, producing cumulative true positive and false positive rates at each position in the provided list. This function is intended for use in cheminformatics and ML workflows with RDKit (for example virtual screening or classifier evaluation) where a list of scored samples and a column index indicating active/inactive labels are available; it returns the cumulative TPR and FPR vectors that can be plotted or used to compute AUC.", "tools": [{"function": {"description": "rdkit.ML.Scoring.Scoring.CalcROC: Compute a Receiver Operating Characteristic (ROC) curve from a list of scored records, producing cumulative true positive and false positive rates at each position in the provided list. This function is intended for use in cheminformatics and ML workflows with RDKit (for example virtual screening or classifier evaluation) where a list of scored samples and a column index indicating active/inactive labels are available; it returns the cumulative TPR and FPR vectors that can be plotted or used to compute AUC.\n", "name": "rdkit_ML_Scoring_Scoring_CalcROC", "parameters": {"properties": {"scores": {"type": "array", "items": {"type": "float"}, "description": "A list of records (for example tuples or lists) containing scoring/classification results for individual molecules or samples. Each record is indexed with col to determine whether that sample is considered \"active\" (truthy value) or \"inactive\" (falsy value). The function processes this list in the supplied order and computes cumulative counts; for a conventional ROC curve the caller should provide scores sorted by predicted score (e.g., descending predicted activity) so that rates correspond to varying classification thresholds.", "default": ""}, "col": {"type": "integer", "description": "Integer column index into each element of scores that holds the active/inactive indicator used as the ground-truth label. The value at scores[i][col] is interpreted as a boolean: truthy means active (positive class), falsy means inactive (negative class). This parameter selects which position in each record encodes the binary label.", "default": ""}}, "required": ["scores", "col"], "type": "any"}}, "type": "function"}], "query": "We’re integrating three ranked virtual-screening hitlists coming from different vendors/models into a single benchmarking step. Each record is intended to be [molecule_id, predicted_score, is_active], but the raw exports are messy: some entries have missing/invalid labels, some use non-binary encodings, and a few include non-finite scores. For ROC construction with rdkit.ML.Scoring.Scoring.CalcROC, treat the **activity ground truth as the third field** after harmonization.\n\nHarmonization rules (apply independently within each cohort before computing ROC):\n- Keep only records whose predicted_score is a finite number.\n- Map activity labels to binary using: {1, \"1\", True, \"active\", \"A\"} → 1 and {0, \"0\", False, \"inactive\", \"I\"} → 0.\n- Any record whose activity label cannot be mapped to {0,1} is considered unresolved and should not contribute to ROC.\n- Preserve the original ranking order among the retained records (do not re-sort after filtering).\n\nCompute the ROC (cumulative TPR/FPR vectors vs rank position) for each cohort after applying the above rules.\n\nCohort 1 (CHEMBL set, vendor CSV export): [[\"CHEMBL100\", 0.97, 1], [\"CHEMBL101\", 0.93, 0], [\"CHEMBL102\", 0.89, 1], [\"CHEMBL103\", 0.80, 0], [\"CHEMBL104\", 0.78, 1], [\"CHEMBL105\", 0.70, 0], [\"CHEMBL106\", 0.65, \"inactive\"], [\"CHEMBL107\", 0.60, \"active\"], [\"CHEMBL108\", 0.55, null], [\"CHEMBL109\", 0.50, 0]]\n\nCohort 2 (Mol* set, mixed typing from an API): [(\"MolA\", 0.97, True), (\"MolB\", 0.93, False), (\"MolC\", 0.89, \"A\"), (\"MolD\", 0.85, \"I\"), (\"MolE\", 0.82, \"unknown\"), (\"MolF\", 0.78, 1), (\"MolG\", 0.70, 0), (\"MolH\", 0.66, \"1\")]\n\nCohort 3 (CMPD set, assay system export): [[\"CMPD_001\", 9.7, \"active\"], [\"CMPD_002\", 9.1, \"inactive\"], [\"CMPD_003\", 8.9, 1], [\"CMPD_004\", 8.2, 0], [\"CMPD_005\", 7.5, \"A\"], [\"CMPD_006\", 6.8, \"I\"], [\"CMPD_007\", \"NaN\", 0], [\"CMPD_008\", 5.9, 1]]\n\nReturn the ROC vectors for each cohort for downstream plotting/AUC.", "answers": "[{\"name\":\"rdkit_ML_Scoring_Scoring_CalcROC\",\"arguments\":{\"scores\":[[\"CHEMBL100\",0.97,1],[\"CHEMBL101\",0.93,0],[\"CHEMBL102\",0.89,1],[\"CHEMBL103\",0.8,0],[\"CHEMBL104\",0.78,1],[\"CHEMBL105\",0.7,0],[\"CHEMBL106\",0.65,0],[\"CHEMBL107\",0.6,1],[\"CHEMBL109\",0.5,0]],\"col\":2}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcROC\",\"arguments\":{\"scores\":[[\"MolA\",0.97,1],[\"MolB\",0.93,0],[\"MolC\",0.89,1],[\"MolD\",0.85,0],[\"MolF\",0.78,1],[\"MolG\",0.7,0],[\"MolH\",0.66,1]],\"col\":2}},{\"name\":\"rdkit_ML_Scoring_Scoring_CalcROC\",\"arguments\":{\"scores\":[[\"CMPD_001\",9.7,1],[\"CMPD_002\",9.1,0],[\"CMPD_003\",8.9,1],[\"CMPD_004\",8.2,0],[\"CMPD_005\",7.5,1],[\"CMPD_006\",6.8,0],[\"CMPD_008\",5.9,1]],\"col\":2}}]"}
{"func_name": "rdkit_sping_PDF_pdfgeom_bezierArc", "func_desc": "rdkit.sping.PDF.pdfgeom.bezierArc: Compute cubic Bezier control points that approximate an elliptical arc inscribed in the rectangle defined by two corner points. This function is used by RDKit's PDF/graphics geometry routines to convert an arc (used when rendering 2D molecular diagrams and other vector graphics) into one or more cubic Bezier segments suitable for PDF or other vector backends that represent curves with Bezier control points.\n    \n    The function interprets (x1, y1) and (x2, y2) as opposite corners of the enclosing rectangle and normalizes them so the returned ellipse fits that rectangle. The coordinate system assumed by the algorithm has X increasing to the right and Y increasing downwards (typical raster/PDF coordinate orientation used by RDKit PDF geometry code). Angles are expressed in degrees, with 0 degrees pointing to the right (positive X axis) and angles increasing counter-clockwise. The arc starts at startAng and spans extent degrees; a negative extent produces a clockwise sweep. For extents whose absolute value exceeds 90 degrees the arc is subdivided into multiple cubic Bezier segments (each covering at most 90 degrees) to maintain the standard approximation accuracy used in RDKit's PDF geometry code. The returned list contains one tuple per cubic Bezier segment; each tuple has eight floats (x1, y1, x2, y2, x3, y3, x4, y4) representing the start point, the two control points, and the end point of that segment in that order. These coordinates are in the same coordinate system used by the inputs and are ready to be consumed by RDKit's PDF drawing routines or any other renderer that accepts cubic Bezier segments.", "tools": [{"function": {"description": "rdkit.sping.PDF.pdfgeom.bezierArc: Compute cubic Bezier control points that approximate an elliptical arc inscribed in the rectangle defined by two corner points. This function is used by RDKit's PDF/graphics geometry routines to convert an arc (used when rendering 2D molecular diagrams and other vector graphics) into one or more cubic Bezier segments suitable for PDF or other vector backends that represent curves with Bezier control points.\n\nThe function interprets (x1, y1) and (x2, y2) as opposite corners of the enclosing rectangle and normalizes them so the returned ellipse fits that rectangle. The coordinate system assumed by the algorithm has X increasing to the right and Y increasing downwards (typical raster/PDF coordinate orientation used by RDKit PDF geometry code). Angles are expressed in degrees, with 0 degrees pointing to the right (positive X axis) and angles increasing counter-clockwise. The arc starts at startAng and spans extent degrees; a negative extent produces a clockwise sweep. For extents whose absolute value exceeds 90 degrees the arc is subdivided into multiple cubic Bezier segments (each covering at most 90 degrees) to maintain the standard approximation accuracy used in RDKit's PDF geometry code. The returned list contains one tuple per cubic Bezier segment; each tuple has eight floats (x1, y1, x2, y2, x3, y3, x4, y4) representing the start point, the two control points, and the end point of that segment in that order. These coordinates are in the same coordinate system used by the inputs and are ready to be consumed by RDKit's PDF drawing routines or any other renderer that accepts cubic Bezier segments.", "name": "rdkit_sping_PDF_pdfgeom_bezierArc", "parameters": {"properties": {"x1": {"type": "float", "description": "X coordinate of the first corner of the rectangle that encloses the elliptical arc. This input is used to determine the horizontal extent (rx) of the ellipse; the function normalizes x1 and x2 so the left edge is min(x1, x2) and the right edge is max(x1, x2). In RDKit this coordinate is in the same units used for page/canvas coordinates when producing PDF/vector output.", "default": ""}, "y1": {"type": "float", "description": "Y coordinate of the first corner of the rectangle that encloses the elliptical arc. Because the coordinate system used increases downward, the function normalizes the pair (y1, y2) so the top/bottom orientation matches the internal representation (it sets the internal y1 to the larger of the two input Y values). This value, together with y2, determines the vertical radius (ry) of the ellipse for PDF rendering of molecular graphics.", "default": ""}, "x2": {"type": "float", "description": "X coordinate of the second corner of the rectangle that encloses the elliptical arc. Combined with x1 it sets the center and horizontal radius of the ellipse. The function swaps/normalizes x1 and x2 as needed so downstream geometry calculations assume x1 <= x2.", "default": ""}, "y2": {"type": "float", "description": "Y coordinate of the second corner of the rectangle that encloses the elliptical arc. Combined with y1 it sets the center and vertical radius of the ellipse. The function swaps/normalizes y1 and y2 as needed so downstream geometry calculations assume the internal representation where y1 is the larger value (since Y increases downward).", "default": ""}, "startAng": {"type": "float", "description": "Start angle of the arc in degrees. 0 degrees points to the right (positive X axis) and angles increase counter-clockwise. The arc begins at this angle measured from the center of the ellipse defined by the rectangle. The default is 0. In RDKit usage, setting startAng selects the angular position on the ellipse where the arc begins when converting vector shapes for PDF output.", "default": 0}, "extent": {"type": "float", "description": "Angular extent of the arc in degrees (how far the arc spans from startAng). Positive values produce a counter-clockwise sweep, negative values produce a clockwise sweep. If |extent| > 90 the function automatically subdivides the arc into multiple cubic Bezier segments (each with at most 90 degrees) to preserve approximation accuracy, matching the approach used by RDKit's PDF geometry utilities. The default is 90.", "default": 90}}, "required": ["x1", "y1", "x2", "y2", "extent", "startAng"], "type": "any"}}, "type": "function"}], "query": "We’re validating arc-to-Bezier conversion robustness in an RDKit PDF molecular-diagram renderer where upstream layout sometimes emits inconsistent corner ordering and a mix of major/minor sweeps. Given these raw arc primitives (each defined by two opposite rectangle corners plus an arc specification in RDKit’s PDF convention: +X right, +Y down; angles in degrees with 0° along +X and increasing counter-clockwise), convert to cubic Bezier segments using `rdkit.sping.PDF.pdfgeom.bezierArc` only for primitives whose enclosing rectangle has strictly positive area after normalization (i.e., non-degenerate bounds). For each retained primitive, choose the sweep direction and magnitude by a data-driven rule: if the rectangle is wider than it is tall, render the longer-way-around arc (use a counter-clockwise extent whose magnitude is 210°); otherwise render the shorter semicircle (use a clockwise extent of 180°). Also, to stress-test quadrant handling, set `startAng` per primitive from its normalized bounds: use 30° when both normalized corner x-coordinates are even integers; otherwise use 45°. Raw primitives:\n1) corners (250, 300) and (50, 100)\n2) corners (100, 150) and (300, 250)\n3) corners (120, 220) and (120, 260)\nReturn the Bezier control-point tuples for each processed primitive (automatic subdivision for spans > 90° is expected).", "answers": "[{\"name\":\"rdkit_sping_PDF_pdfgeom_bezierArc\",\"arguments\":{\"x1\":250,\"y1\":300,\"x2\":50,\"y2\":100,\"startAng\":30,\"extent\":210}},{\"name\":\"rdkit_sping_PDF_pdfgeom_bezierArc\",\"arguments\":{\"x1\":100,\"y1\":150,\"x2\":300,\"y2\":250,\"startAng\":30,\"extent\":210}}]"}
{"func_name": "rdkit_utils_chemutils_ConfigToNumElectrons", "func_desc": "Counts the number of electrons appearing in an electronic configuration string used in RDKit cheminformatics utilities.\n    \n    This function is used in RDKit code paths that need a simple integer count of valence electrons extracted from an electronic configuration string (for example in heuristics for valence checking, bonding, or charge handling). The implementation parses a space-separated configuration string, sums integer superscript counts from each orbital token after the first token, and optionally ignores full d or f shells when those shells appear and meet the exact fullness criteria used in the code. There are no external side effects; the function returns an integer electron count computed from the input string.", "tools": [{"function": {"description": "Counts the number of electrons appearing in an electronic configuration string used in RDKit cheminformatics utilities.\n\nThis function is used in RDKit code paths that need a simple integer count of valence electrons extracted from an electronic configuration string (for example in heuristics for valence checking, bonding, or charge handling). The implementation parses a space-separated configuration string, sums integer superscript counts from each orbital token after the first token, and optionally ignores full d or f shells when those shells appear and meet the exact fullness criteria used in the code. There are no external side effects; the function returns an integer electron count computed from the input string.", "name": "rdkit_utils_chemutils_ConfigToNumElectrons", "parameters": {"properties": {"config": {"type": "string", "description": "The electronic configuration string to parse. The string is split on space characters and tokens at indices 1..N are interpreted as orbital entries of the form \"<shell><orbital>^<count>\" (for example \"2s^2\" or \"3d^10\"). Note that the implementation intentionally starts processing at the second token (index 1) and therefore the first token (index 0) is ignored; callers should supply a leading token (commonly an element symbol or a placeholder) if they intend all orbital tokens to be considered. A configuration with fewer than two space-separated tokens yields a count of 0. Malformed tokens that do not contain a \"^\" or whose superscript part cannot be converted to int will raise a ValueError or IndexError as produced by the underlying Python operations.", "default": ""}, "ignoreFullD": {"type": "boolean", "description": "If true (nonzero), full d shells are not counted toward the returned electron total. Concretely, when this flag is true and an orbital token contains the letter \"d\" and its superscript integer equals 10, that token's electrons are treated as 0 instead of 10, but only when the configuration has more than two tokens (len(config.split(' ')) > 2). Default is 0 (treated as False in Python); use True/1 to enable ignoring full d shells. This behavior is intended to support domain-specific conventions in counting valence electrons for transition metals within RDKit utilities.", "default": 0}, "ignoreFullF": {"type": "boolean", "description": "If true (nonzero), full f shells are not counted toward the returned electron total. Concretely, when this flag is true and an orbital token contains the letter \"f\" and its superscript integer equals 14, that token's electrons are treated as 0 instead of 14, but only when the configuration has more than two tokens (len(config.split(' ')) > 2). Default is 0 (treated as False in Python); use True/1 to enable ignoring full f shells. This behavior is intended to support domain-specific conventions in counting valence electrons for lanthanides/actinides within RDKit utilities.", "default": 0}}, "required": ["config", "ignoreFullD", "ignoreFullF"], "type": "any"}}, "type": "function"}], "query": "We’re validating an RDKit-derived valence-electron counting heuristic on a messy set of Fe-centered coordination-environment annotations exported from multiple sources. Each record is a space-separated electronic configuration string; some are partially specified, include zero-occupancy orbitals, or have the d-shell placed before/after the s-shell.\n\nPipeline rules:\n1) Only process records whose element label is Fe and whose configuration explicitly contains at least one d-orbital token (to focus on transition-metal cases with an explicit d-manifold).\n2) Enable the RDKit-style option to ignore a completely filled d shell *only* for records where the d superscript equals the formal capacity of the d shell (i.e., it is written as fully occupied in the string). Otherwise keep d contributions.\n3) Enable the option to ignore a completely filled f shell only for records that actually contain an f token; when no f token is present, keep that option disabled.\n\nRaw records:\n- \"Fe 3d^6 4s^2\"\n- \"Fe 3d^10 4s^2 4p^0\"\n- \"Fe 4s^2 3d^10\"\nCompute the electron count for each record that passes the sieve under the above branching rules, returning counts in the same order as the raw records.", "answers": "[{\"name\":\"rdkit_utils_chemutils_ConfigToNumElectrons\",\"arguments\":{\"config\":\"Fe 3d^6 4s^2\",\"ignoreFullD\":false,\"ignoreFullF\":false}},{\"name\":\"rdkit_utils_chemutils_ConfigToNumElectrons\",\"arguments\":{\"config\":\"Fe 3d^10 4s^2 4p^0\",\"ignoreFullD\":true,\"ignoreFullF\":false}},{\"name\":\"rdkit_utils_chemutils_ConfigToNumElectrons\",\"arguments\":{\"config\":\"Fe 4s^2 3d^10\",\"ignoreFullD\":true,\"ignoreFullF\":false}}]"}
{"func_name": "rdkit_utils_chemutils_SplitComposition", "func_desc": "SplitComposition parses a simple chemical composition string into an ordered list of element/count pairs suitable for lightweight stoichiometry and descriptor preprocessing in cheminformatics workflows (RDKit context). The function is intended for very simple, flat composition strings (for example 'Fe3Al') and returns a sequence of element symbols with their associated counts that can be consumed by code that computes elemental contributions, simple composition-based descriptors, or stoichiometric checks.\n    \n    This parser uses a fixed regular expression that recognizes an uppercase letter followed by an optional lowercase letter as the element symbol and an optional numeric portion (digits and optional decimal point) as the count. The numeric portion, when present, is converted to a floating-point value; when absent the count is reported as the integer 1. The method does not modify external state and returns a new list; it is not intended to parse complex chemical formula syntax such as parentheses, hydration/dot notation, charges, isotopic labels, nested groups, or other annotations.", "tools": [{"function": {"description": "SplitComposition parses a simple chemical composition string into an ordered list of element/count pairs suitable for lightweight stoichiometry and descriptor preprocessing in cheminformatics workflows (RDKit context). The function is intended for very simple, flat composition strings (for example 'Fe3Al') and returns a sequence of element symbols with their associated counts that can be consumed by code that computes elemental contributions, simple composition-based descriptors, or stoichiometric checks.\n\nThis parser uses a fixed regular expression that recognizes an uppercase letter followed by an optional lowercase letter as the element symbol and an optional numeric portion (digits and optional decimal point) as the count. The numeric portion, when present, is converted to a floating-point value; when absent the count is reported as the integer 1. The method does not modify external state and returns a new list; it is not intended to parse complex chemical formula syntax such as parentheses, hydration/dot notation, charges, isotopic labels, nested groups, or other annotations.", "name": "rdkit_utils_chemutils_SplitComposition", "parameters": {"properties": {"compStr": {"type": "string", "description": "A simple chemical composition string to be parsed. This should be a flat formula where element symbols are one uppercase letter optionally followed by one lowercase letter, and an optional numeric coefficient (integer or decimal) immediately follows the symbol. Practical examples in the RDKit domain include input used to compute element-based descriptors or to check simple stoichiometries, for example 'Fe3Al' or 'C6H6'. If compStr is not a string a TypeError may be raised by the underlying regex operations. Leading/trailing whitespace is not specially normalized by this function; such whitespace will simply be ignored by the regex if it does not match the pattern.", "default": ""}}, "required": ["compStr"], "type": "any"}}, "type": "function"}], "query": "We’re ingesting a mixed export from a synthesis log into an RDKit-side composition-descriptor preprocessor. The file contains nominally flat composition strings, but some entries include syntax our lightweight regex parser cannot represent (e.g., dot/hydrate notation, parentheses, charges, isotopic labels, or bracketed annotations). From the cohort below, automatically select only those formulas that are strictly a flat concatenation of element symbols (uppercase + optional lowercase) each followed by an optional numeric count (digits with optional decimal point), with no other characters present. For each selected formula, run SplitComposition and return the ordered element/count pairs exactly as parsed (no normalization).\n\nCohort:\n- Ni0.5Fe2O4\n- Li1.2Mn0.8O2\n- La0.7Sr0.3MnO3\n- Fe2(SO4)3\n- CuSO4·5H2O\n- NH4+\n- 13C1H4\n- NaCl", "answers": "[{\"name\":\"rdkit_utils_chemutils_SplitComposition\",\"arguments\":{\"compStr\":\"Ni0.5Fe2O4\"}},{\"name\":\"rdkit_utils_chemutils_SplitComposition\",\"arguments\":{\"compStr\":\"Li1.2Mn0.8O2\"}},{\"name\":\"rdkit_utils_chemutils_SplitComposition\",\"arguments\":{\"compStr\":\"La0.7Sr0.3MnO3\"}},{\"name\":\"rdkit_utils_chemutils_SplitComposition\",\"arguments\":{\"compStr\":\"NaCl\"}}]"}
{"func_name": "rdkit_utils_listutils_CompactListRepr", "func_desc": "CompactListRepr provides a compact, human-readable string representation of a sequence by collapsing consecutive identical elements into run-length notation. This function is part of rdkit.utils.listutils and is intended as a small utility within the RDKit cheminformatics toolkit to make lists that frequently occur in cheminformatics workflows (for example, atom index lists, fingerprint bit arrays, or descriptor sequences) easier to inspect in logs, debugging output, or textual summaries.", "tools": [{"function": {"description": "CompactListRepr provides a compact, human-readable string representation of a sequence by collapsing consecutive identical elements into run-length notation. This function is part of rdkit.utils.listutils and is intended as a small utility within the RDKit cheminformatics toolkit to make lists that frequently occur in cheminformatics workflows (for example, atom index lists, fingerprint bit arrays, or descriptor sequences) easier to inspect in logs, debugging output, or textual summaries.\n", "name": "rdkit_utils_listutils_CompactListRepr", "parameters": {"properties": {"lst": {"type": "array", "items": {"type": "float"}, "description": "The input sequence to summarize. By signature this is documented as a Python list; the implementation operates on any object that supports len() and indexing and compares elements with != (the original examples in the source show that tuples and strings are handled as sequences of elements/characters). The function inspects the sequence element by element, groups consecutive equal elements, and uses repr() on each representative element when building the output. The caller should pass the sequence to be summarized; passing non-sequence objects that do not implement len() and indexing will raise a TypeError or other Python exceptions coming from the attempted operations.", "default": ""}}, "required": ["lst"], "type": "any"}}, "type": "function"}], "query": "In our RDKit debug logs we want a compact run-length signature of only the tracks that look like real chemistry-derived streams rather than sentinel/noise artifacts. We have four candidate per-molecule tracks:\n\n1) fingerprint-bit stream: [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n2) atom-type label track: [\"C\", \"C\", \"C\", \"N\", \"N\", \"O\", \"O\", \"O\", \"O\", \"C\"]\n3) bad bit stream from a failed export: [999, 999, 999, 1, 0]\n4) atom labels from a corrupted annotation pass: [\"C\", \"Xx\", \"N\"]\n\nGenerate CompactListRepr summaries only for tracks that contain no sentinel values (treat 999 as a sentinel) and, for label tracks, only if every label is a valid organic-element symbol from {B,C,N,O,F,P,S,Cl,Br,I}.", "answers": "[{\"name\":\"rdkit_utils_listutils_CompactListRepr\",\"arguments\":{\"lst\":[0,0,0,1,1,0,0,1,1,1,1]}},{\"name\":\"rdkit_utils_listutils_CompactListRepr\",\"arguments\":{\"lst\":[\"C\",\"C\",\"C\",\"N\",\"N\",\"O\",\"O\",\"O\",\"O\",\"C\"]}}]"}
{"func_name": "robert_report_utils_combine_cols", "func_desc": "robert.report_utils.combine_cols constructs an HTML fragment that arranges a sequence of column contents into a single horizontal (multi-column) line using inline CSS flex layout. In the ROBERT project this helper is used when assembling pieces of the HTML-based report (for example the ROBERT_report.pdf styling pipeline) so that multiple data elements can be displayed side-by-side as equally weighted columns in report sections.", "tools": [{"function": {"description": "robert.report_utils.combine_cols constructs an HTML fragment that arranges a sequence of column contents into a single horizontal (multi-column) line using inline CSS flex layout. In the ROBERT project this helper is used when assembling pieces of the HTML-based report (for example the ROBERT_report.pdf styling pipeline) so that multiple data elements can be displayed side-by-side as equally weighted columns in report sections.\n", "name": "robert_report_utils_combine_cols", "parameters": {"properties": {"columns": {"type": "array", "items": {"type": "float"}, "description": "A list (sequence) of objects whose textual representation will be placed into separate column containers. Each item in this list is inserted into a child <div> element via Python string formatting (f'{column}'), so the object's string form (str(column) or its __format__ result) will appear verbatim inside the generated HTML. The caller is responsible for providing the columns in the desired order; no additional escaping or sanitization is performed by this function.", "default": ""}}, "required": ["columns"], "type": "any"}}, "type": "function"}], "query": "We’re assembling ROBERT’s HTML report from a noisy batch of ML experiment summary snippets coming from multiple trackers. Each report section needs a single-row, three-column flex fragment built from the *first three renderable* snippets after applying these rules:\n\n- Treat a snippet as renderable only if it is a non-empty string that does not contain placeholder/error tokens (case-insensitive) like: \"N/A\", \"NA\", \"None\", \"null\", \"nan\", \"TBD\", \"todo\", \"—\", \"--\", \"???\", or \"ERROR\".\n- KPI/evaluation rows must include only regression-style metrics (R²/R2, RMSE, MAE, MSE). If the same metric appears multiple times, keep the first occurrence.\n- Classification rows must include only classification descriptors/metrics (Model, Accuracy, F1) and should preserve any existing HTML tags.\n- Model ‘KPI-card’ rows must include only fields using <strong>…</strong> labels and must prefer the model identifier first if present.\n\nRaw mixed snippets (order matters):\n\n1) Regression eval section candidates:\n[\"R²: 0.93\", \"RMSE: 1.27\", \"\", \"MAE: 0.88\", \"N/A\", \"RMSE: 1.30 (rerun)\"]\n\n2) Classification section candidates:\n[\"<b>Model:</b> RandomForest (v2.3)\", \"<b>Accuracy:</b> 0.932\", \"TBD\", \"<b>F1 Score:</b> 0.918\", \"--\"]\n\n3) Model KPI-card section candidates:\n[\"<strong>Model:</strong> XGBoost v2.1\", \"<strong>R²:</strong> 0.93\", \"null\", \"<strong>RMSE:</strong> 1.27\", \"<b>Accuracy:</b> 0.932\"]\n\nGenerate the three flex-style single-row fragments by combining the selected three snippets for each section (in the order they are selected) using robert.report_utils.combine_cols.", "answers": "[{\"name\":\"robert_report_utils_combine_cols\",\"arguments\":{\"columns\":[\"R²: 0.93\",\"RMSE: 1.27\",\"MAE: 0.88\"]}},{\"name\":\"robert_report_utils_combine_cols\",\"arguments\":{\"columns\":[\"<b>Model:</b> RandomForest (v2.3)\",\"<b>Accuracy:</b> 0.932\",\"<b>F1 Score:</b> 0.918\"]}},{\"name\":\"robert_report_utils_combine_cols\",\"arguments\":{\"columns\":[\"<strong>Model:</strong> XGBoost v2.1\",\"<strong>R²:</strong> 0.93\",\"<strong>RMSE:</strong> 1.27\"]}}]"}
{"func_name": "robert_report_utils_get_col_score", "func_desc": "get_col_score\n    Gather and format the HTML column that summarizes a model score for inclusion in the ROBERT report (PDF/HTML). This function is used by the robert.report_utils module to assemble a small HTML fragment that displays the model title, model type, partition/proportion label, descriptor point counts, and an HTML-formatted block of evaluation text. In the ROBERT workflow (see README), the output is intended to be embedded into the final report (the ROBERT_report.pdf/HTML) to present the score and brief metadata for either a \"No PFI\" or \"PFI\" model.", "tools": [{"function": {"description": "get_col_score\nGather and format the HTML column that summarizes a model score for inclusion in the ROBERT report (PDF/HTML). This function is used by the robert.report_utils module to assemble a small HTML fragment that displays the model title, model type, partition/proportion label, descriptor point counts, and an HTML-formatted block of evaluation text. In the ROBERT workflow (see README), the output is intended to be embedded into the final report (the ROBERT_report.pdf/HTML) to present the score and brief metadata for either a \"No PFI\" or \"PFI\" model.", "name": "robert_report_utils_get_col_score", "parameters": {"properties": {"score_info": {"type": "string", "description": "An HTML-formatted string containing the detailed evaluation text or metric block to be inserted in the column. This value is placed near the bottom of the returned HTML fragment and is expected to already contain any necessary HTML tags (for example line breaks, paragraphs, or inline formatting) because the function does not sanitize or reformat its content. Supplying non-HTML plain text is allowed but may produce suboptimal rendering in the report.", "default": ""}, "data_score": {"type": "any", "description": "A dictionary with model-specific metadata and precomputed strings required to populate the column. The function reads the following keys from this dict: f'robert_score_{suffix}' (a printable score string inserted into the title), 'proportion_ratio_print' (a string containing a substring \"-  Proportion \" that is split to obtain the partitions ratio), 'ML_model' (the model name placed in the fragment), and f'points_descp_ratio_{suffix}' (the descriptor points counts for the chosen suffix). Missing keys will raise a KeyError. The dictionary is supplied by the reporting pipeline in ROBERT and typically originates from model-evaluation routines.", "default": ""}, "suffix": {"type": "string", "description": "A string indicating which score column to build; the code recognizes and handles exactly two domain-specific values: 'No PFI' or 'PFI'. When suffix == 'No PFI' the function uses the global title_no_pfi template to build the caption; when suffix == 'PFI' it uses the global title_pfi template. If suffix has any other value, caption will not be defined and the function will raise an error (UnboundLocalError). The suffix is also used to select the per-suffix keys inside data_score described above.", "default": ""}, "spacing": {"type": "string", "description": "A short HTML snippet or spacing string that is interpolated into inline paragraph templates used for consistent visual spacing in the generated HTML fragment. The caller (report generation code) supplies spacing to control indentation or inline spacing in the final report. The function does not validate this string beyond inserting it into the HTML templates ML_line_format and part_line_format.", "default": ""}, "eval_only": {"type": "boolean", "description": "When False (the default reporting mode), the function builds a title_line using the caption derived from the appropriate global title template and the robert_score_{suffix} value; when True the function overrides the title and returns a concise fixed title_line 'Summary and score of your model (No PFI)'. In practical use, eval_only True is intended to produce a compact summary suitable for evaluation-only displays. This flag only affects the title text; all other fields are still read from data_score.", "default": ""}}, "required": ["score_info", "data_score", "suffix", "spacing", "eval_only"], "type": "any"}}, "type": "function"}], "query": "We’re assembling the PFI section of a ROBERT_report from heterogeneous scoring logs exported by two training backends. Each log contains a PFI score field that is sometimes numeric-only and sometimes prefixed (e.g., “Score: …”), plus a method tag and a compact “points/descriptors” string.\n\nRaw run records (unordered):\n1) ML_model = \"Gradient Boosting Regressor\"; proportion_ratio_print = \"Training/Test -  Proportion 80/20\"; points_descp_ratio_PFI = \"250 descriptors (train), 62 descriptors (test)\"; robert_score_PFI = \"0.87\"; score_info = \"<p><b>RMSE:</b> 0.42<br><b>R²:</b> 0.87<br><b>Validation folds:</b> 5</p>\"; spacing_hint = \"<span style='margin-left:4px;'>&nbsp;</span>\".\n2) ML_model = \"Random Forest Regressor\"; proportion_ratio_print = \"Train/Test -  Proportion 80/20\"; points_descp_ratio_PFI = \"120 pts / 64 desc\"; robert_score_PFI = \"Score: 0.91\"; score_info = \"<p><b>RMSE:</b> 0.82<br><b>R²:</b> 0.91<br><i>5-fold CV</i></p>\"; spacing_hint = \"&nbsp;&nbsp;\".\n\nGenerate HTML “score summary” column fragments using get_col_score with suffix \"PFI\" for the subset of runs that are reportable under these QC rules:\n- Treat a run as reportable only if its PFI score field contains a parseable numeric value in the closed interval [0, 1].\n- For reportable runs, switch to eval-only mode when the numeric score is < 0.90; otherwise keep full formatting.\n- Use each run’s provided spacing_hint verbatim.\n\nReturn the ordered list of get_col_score calls needed to produce the fragments for all reportable runs.", "answers": "[{\"name\":\"robert_report_utils_get_col_score\",\"arguments\":{\"score_info\":\"<p><b>RMSE:</b> 0.42<br><b>R²:</b> 0.87<br><b>Validation folds:</b> 5</p>\",\"data_score\":{\"robert_score_PFI\":\"0.87\",\"proportion_ratio_print\":\"Training/Test -  Proportion 80/20\",\"ML_model\":\"Gradient Boosting Regressor\",\"points_descp_ratio_PFI\":\"250 descriptors (train), 62 descriptors (test)\"},\"suffix\":\"PFI\",\"spacing\":\"<span style='margin-left:4px;'>&nbsp;</span>\",\"eval_only\":true}},{\"name\":\"robert_report_utils_get_col_score\",\"arguments\":{\"score_info\":\"<p><b>RMSE:</b> 0.82<br><b>R²:</b> 0.91<br><i>5-fold CV</i></p>\",\"data_score\":{\"robert_score_PFI\":\"Score: 0.91\",\"proportion_ratio_print\":\"Train/Test -  Proportion 80/20\",\"ML_model\":\"Random Forest Regressor\",\"points_descp_ratio_PFI\":\"120 pts / 64 desc\"},\"suffix\":\"PFI\",\"spacing\":\"&nbsp;&nbsp;\",\"eval_only\":false}}]"}
{"func_name": "robert_report_utils_get_col_text", "func_desc": "robert.report_utils.get_col_text returns an HTML-formatted column string containing fixed abbreviation mappings used in the report \"score\" and \"abbreviation\" sections of the ROBERT PDF/HTML report. This helper assembles a sequence of hard-coded abbreviation descriptions (for example, \"ACC: accuracy\", \"RF: random forest\", \"SHAP: Shapley additive explanations\") into paragraph (<p>) elements with specific inline styles so the resulting string can be embedded directly into the report layout produced by ROBERT (a library for bridging machine learning and chemistry workflows described in the project README).\n    \n    The function is used during report generation to present concise explanatory text for abbreviations and threshold labels that appear in model evaluation and method-description sections of ROBERT-generated reports. It returns a single str containing multiple HTML paragraph elements; the first paragraph uses a larger negative top margin to reduce spacing relative to section headers and subsequent paragraphs use a smaller negative top margin to compact the column.", "tools": [{"function": {"description": "robert.report_utils.get_col_text returns an HTML-formatted column string containing fixed abbreviation mappings used in the report \"score\" and \"abbreviation\" sections of the ROBERT PDF/HTML report. This helper assembles a sequence of hard-coded abbreviation descriptions (for example, \"ACC: accuracy\", \"RF: random forest\", \"SHAP: Shapley additive explanations\") into paragraph (<p>) elements with specific inline styles so the resulting string can be embedded directly into the report layout produced by ROBERT (a library for bridging machine learning and chemistry workflows described in the project README).\n\nThe function is used during report generation to present concise explanatory text for abbreviations and threshold labels that appear in model evaluation and method-description sections of ROBERT-generated reports. It returns a single str containing multiple HTML paragraph elements; the first paragraph uses a larger negative top margin to reduce spacing relative to section headers and subsequent paragraphs use a smaller negative top margin to compact the column.", "name": "robert_report_utils_get_col_text", "parameters": {"properties": {"type_thres": {"type": "string", "description": "A case-sensitive selector that chooses which predefined abbreviation group to format into an HTML column. Accepted literal values (taken from the source code) are 'abbrev_1', 'abbrev_2', and 'abbrev_3', each corresponding to a distinct hard-coded list of abbreviation HTML fragments used by the report. 'abbrev_1' includes items such as '<strong>ACC:</strong> accuracy' and '<strong>GP:</strong> gaussian process'. 'abbrev_2' includes items such as '<strong>KN:</strong> k-nearest neighbors' and '<strong>R2:</strong> coefficient of determination'. 'abbrev_3' includes items such as '<strong>RF:</strong> random forest' and '<strong>SHAP:</strong> Shapley additive explanations'. The caller must supply one of these exact strings; the function does not accept other values, does not perform normalization (for example, lowercasing or stripping), and is case-sensitive.", "default": ""}}, "required": ["type_thres"], "type": "any"}}, "type": "function"}], "query": "We’re auto-assembling ROBERT report pages from heterogeneous templates coming from multiple collaborators. Each template provides a `legend_slot` record with a `section` label and an `anchor_margin_policy`. Only generate abbreviation-column HTML for slots whose `anchor_margin_policy` indicates the compact negative-top-margin legend style is required, and choose the abbreviation legend group based on the slot’s `section`: if the section is a Methods/model-description legend (e.g., contains method/model abbreviations like RF and SHAP), use the methods/model abbreviation group; if the section is a Scores/metrics legend (e.g., contains performance abbreviations like ACC/AUC/GP), use the metrics/threshold abbreviation group. Dataset of incoming legend slots:\n\n- slot_id: L1 | section: \"Methods — right column\" | anchor_margin_policy: \"compact-neg-top\" \n- slot_id: L2 | section: \"Scores & Abbreviations — left column\" | anchor_margin_policy: \"compact-neg-top\" \n- slot_id: L3 | section: \"Scores & Abbreviations — parallel HTML layout\" | anchor_margin_policy: \"compact-neg-top\" \n\nReturn the exact HTML multi-<p> strings from ROBERT for each qualifying slot, in the same order as the slots above, suitable for verbatim embedding in both HTML and PDF layouts.", "answers": "[{\"name\":\"robert_report_utils_get_col_text\",\"arguments\":{\"type_thres\":\"abbrev_3\"}},{\"name\":\"robert_report_utils_get_col_text\",\"arguments\":{\"type_thres\":\"abbrev_1\"}},{\"name\":\"robert_report_utils_get_col_text\",\"arguments\":{\"type_thres\":\"abbrev_1\"}}]"}
{"func_name": "robert_report_utils_get_col_transpa", "func_desc": "get_col_transpa generates an HTML snippet (column) that summarizes model-related parameters for the Reproducibility section of a ROBERT report.", "tools": [{"function": {"description": "get_col_transpa generates an HTML snippet (column) that summarizes model-related parameters for the Reproducibility section of a ROBERT report.\n", "name": "robert_report_utils_get_col_transpa", "parameters": {"properties": {"params_dict": {"type": "any", "description": "Dictionary of model/run parameters produced or consumed by ROBERT report utilities. This dict is expected to contain keys such as 'type' (model family type, e.g., 'reg' or 'clas'), 'error_type' (used to exclude a combined error entry), 'model' (short code for the chosen estimator, e.g., 'RF', 'GB'), and optionally 'params' (a string representation of a Python dict containing estimator hyperparameters). The function reads these keys to decide which parameters to include in the returned HTML and to map short model codes to human-readable sklearn-like names. Pass the exact params_dict used by other ROBERT report code; missing required keys (for example 'type' or 'error_type') will raise a KeyError in this function.", "default": ""}, "suffix": {"type": "string", "description": "Caption selector for the column. The function recognizes the literal values 'No PFI' and 'PFI' and will select global caption variables title_no_pfi and title_pfi respectively to render the bold caption line in the HTML. If a different string is passed, the caption variable will not be set and a NameError or UnboundLocalError may occur; callers should pass exactly 'No PFI' or 'PFI' as used by the ROBERT reporting pipeline.", "default": ""}, "section": {"type": "string", "description": "Logical section selector that controls which keys from params_dict are rendered into the column. The function checks for values such as 'model_section' and 'misc_section'. When section == 'model_section', the function will (1) translate a 'model' code from params_dict into a human-readable sklearn model name according to an internal mapping (models_dict), and (2) if a 'params' key is present it will parse it via ast.literal_eval to extract and render individual hyperparameter lines. When section == 'misc_section', only keys listed in the function's misc_params list (['type','error_type','kfold','repeat_kfolds','seed']) are rendered. Use the same section strings used by ROBERT report generation code to ensure consistent output.", "default": ""}, "spacing": {"type": "string", "description": "A string used for spacing/indentation inserted into rendered HTML lines (for example a number of spaces). spacing is concatenated into inline HTML text to visually indent caption and parameter lines in the generated column. Provide the spacing string used elsewhere in the report generator to preserve consistent layout.", "default": ""}}, "required": ["params_dict", "suffix", "section", "spacing"], "type": "any"}}, "type": "function"}], "query": "We’re assembling the Reproducibility → model_section transparency table from a noisy batch export of Random Forest classification runs where metric naming and indentation are inconsistent across sources. Use suffix “PFI” for all generated columns. From the run records below, generate transparency HTML columns only for entries that (i) are classification runs (type=clas), (ii) specify model=RF, and (iii) use exactly 500 estimators (n_estimators=500) in the params payload. For indentation, use a two-space prefix when the record’s indent_style is “spaces2”; use the literal HTML non-breaking sequence “&nbsp;&nbsp;&nbsp;&nbsp;” when indent_style is “nbsp4”. Preserve each run’s error_type string exactly as written and include the estimator name plus the full hyperparameter dict string exactly as provided. Also, include CV controls (kfold, repeat_kfolds, seed) only when those keys are present in the run record.\n\nRaw run records (as exported):\n1) {type: 'clas', error_type: 'mcc', model: 'RF', params: \"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 13}\", kfold: 5, repeat_kfolds: 2, seed: 13, indent_style: 'spaces2'}\n2) {type: 'clas', error_type: 'accuracy', model: 'RF', params: \"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'class_weight': 'balanced', 'random_state': 42}\", indent_style: 'nbsp4'}\n3) {type: 'clas', error_type: 'acc', model: 'RF', params: \"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 42}\", indent_style: 'spaces2'}\n4) {type: 'clas', error_type: 'mcc', model: 'RF', params: \"{'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 13}\", indent_style: 'spaces2'}\n5) {type: 'regr', error_type: 'rmse', model: 'RF', params: \"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 13}\", indent_style: 'spaces2'}", "answers": "[{\"name\":\"robert_report_utils_get_col_transpa\",\"arguments\":{\"params_dict\":{\"type\":\"clas\",\"error_type\":\"mcc\",\"model\":\"RF\",\"params\":\"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 13}\",\"kfold\":5,\"repeat_kfolds\":2,\"seed\":13},\"suffix\":\"PFI\",\"section\":\"model_section\",\"spacing\":\"  \"}},{\"name\":\"robert_report_utils_get_col_transpa\",\"arguments\":{\"params_dict\":{\"type\":\"clas\",\"error_type\":\"accuracy\",\"model\":\"RF\",\"params\":\"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'class_weight': 'balanced', 'random_state': 42}\"},\"suffix\":\"PFI\",\"section\":\"model_section\",\"spacing\":\"&nbsp;&nbsp;&nbsp;&nbsp;\"}},{\"name\":\"robert_report_utils_get_col_transpa\",\"arguments\":{\"params_dict\":{\"type\":\"clas\",\"error_type\":\"acc\",\"model\":\"RF\",\"params\":\"{'n_estimators': 500, 'max_depth': 12, 'min_samples_split': 4, 'random_state': 42}\"},\"suffix\":\"PFI\",\"section\":\"model_section\",\"spacing\":\"  \"}}]"}
{"func_name": "robert_report_utils_get_predict_scores", "func_desc": "robert.report_utils.get_predict_scores calculates and aggregates numeric scores parsed from the textual output produced by the PREDICT module of the ROBERT package. It parses lines in dat_predict (a list of strings) to extract model type, cross-validation and test results (R2/RMSE for regression or MCC for classification), datapoints:descriptors ratio and outlier/proportion text, and computes derived metrics used by ROBERT for model quality assessment and report generation (scaled RMSE, combined scores, penalties, CV SD coverage). This function is used by the ROBERT reporting pipeline to convert raw PREDICT output into the standardized set of score fields stored in data_score for downstream PDF report generation and model comparison.", "tools": [{"function": {"description": "robert.report_utils.get_predict_scores calculates and aggregates numeric scores parsed from the textual output produced by the PREDICT module of the ROBERT package. It parses lines in dat_predict (a list of strings) to extract model type, cross-validation and test results (R2/RMSE for regression or MCC for classification), datapoints:descriptors ratio and outlier/proportion text, and computes derived metrics used by ROBERT for model quality assessment and report generation (scaled RMSE, combined scores, penalties, CV SD coverage). This function is used by the ROBERT reporting pipeline to convert raw PREDICT output into the standardized set of score fields stored in data_score for downstream PDF report generation and model comparison.\n", "name": "robert_report_utils_get_predict_scores", "parameters": {"properties": {"dat_predict": {"type": "array", "items": {"type": "float"}, "description": "The raw PREDICT output as a list of text lines (strings). Each element is one printed line from the PREDICT module output. The function searches for specific substrings and line offsets (for example '------- ', '(No PFI)', 'with PFI', '   - Model:', 'o  Summary of results', '-fold CV : R2 =', 'Test : R2 =', '-  y range of dataset', '5-fold', '-  Test :', and '-  Average SD in test set') to identify and extract numeric values. The caller must supply the exact sequence of lines produced by PREDICT; malformed, truncated or reordered lines can produce missing keys or conversion errors (ValueError) when numeric parsing is attempted.", "default": ""}, "suffix": {"type": "string", "description": "A short label string used to select which block of PREDICT output to parse and to namespace keys written into data_score. Typical values used in ROBERT are 'No PFI' and 'PFI' to indicate whether permutation feature importance was used. The function uses suffix both to determine when to start parsing blocks (it looks for markers that include '(No PFI)' or 'with PFI' together with '------- ') and to compose output keys such as 'rmse_score_{suffix}', 'r2_cv_{suffix}', 'cv_score_combined_{suffix}', etc. The suffix is required and is directly embedded in resulting data_score keys, so use consistent suffix values when calling this function.", "default": ""}, "pred_type": {"type": "string", "description": "Prediction task type, exactly 'reg' for regression or 'clas' for classification. For 'reg', the function extracts R2 and RMSE (from CV and Test lines) and computes scaled RMSE (RMSE divided by y range of the dataset times 100), combined scores that include RMSE-based scores and R2 penalties, and a stability/difference score between CV and Test RMSE. For 'clas', the function extracts MCC values from CV and Test lines and computes scores using the same score_rmse_mcc helper (which interprets MCC when pred_type is 'clas'); combined scores for classification are taken equal to the MCC-based scores (no additional penalty). Passing any other pred_type will prevent the function from executing the documented parsing branches and may lead to incomplete keys in data_score.", "default": ""}, "data_score": {"type": "any", "description": "Mutable mapping that will be updated in place with parsed values and derived scores. The function initializes or sets a number of keys namespaced by suffix (for example 'rmse_score_{suffix}', 'cv_type_{suffix}', 'ML_model', 'proportion_ratio_print', 'points_descp_ratio_{suffix}', 'r2_cv_{suffix}', 'r2_test_{suffix}', 'rmse_cv_{suffix}', 'rmse_test_{suffix}', 'y_range_{suffix}', 'scaled_rmse_cv_{suffix}', 'scaled_rmse_test_{suffix}', 'cv_score_rmse_{suffix}', 'test_score_rmse_{suffix}', 'cv_penalty_r2_{suffix}', 'test_penalty_r2_{suffix}', 'cv_score_combined_{suffix}', 'test_score_combined_{suffix}', 'factor_scaled_rmse_{suffix}', 'diff_scaled_rmse_score_{suffix}', 'cv_4sd_{suffix}', 'cv_range_cov_{suffix}', 'cv_sd_score_{suffix}'). The function both reads and writes data_score; it expects the caller to provide a dict (possibly pre-populated) and will mutate it. The function relies on external helper functions score_rmse_mcc(pred_type, value) and calc_penalty_r2(value) being available in the same module; if these helpers are missing a NameError will be raised.", "default": ""}}, "required": ["dat_predict", "suffix", "pred_type", "data_score"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing ROBERT’s PREDICT-report parsing under a realistic multi-batch regression benchmarking scenario where console logs are messy and not every block is suitable for scoring. We have three raw PREDICT console captures (each capture is one cohort; treat each capture as the full PREDICT output with one line per list element). Start from an initially empty scoring dict for each cohort and run `robert.report_utils.get_predict_scores` only for cohorts that are *reportable* under the following QC rule: a cohort is reportable if (i) it contains both a fold-CV metric line and a Test metric line, (ii) it includes a y-range line with two numeric bounds, and (iii) it includes a datapoints-to-descriptors ratio line. For each reportable cohort, parse/aggregate all ROBERT reporting fields (model identifier, CV and test metrics, y-range, average test-set SD, datapoints:descriptors ratio, outlier/proportion text) and compute all derived quality metrics (scaled RMSE, combined/penalized scores, CV SD coverage), writing results into the scoring dict using the cohort’s provided suffix exactly as written (suffixes are intentionally inconsistent). Use `pred_type='reg'` for all cohorts.\n\nCohort A (suffix: \"No PFI\"):\n\"\"\"\n-------  (No PFI)  -------\n   - Model: Random Forest Regressor\n   -  y range of dataset : 0.15 .. 8.92\n   -fold CV : R2 = 0.81 ; RMSE = 0.73\n   -  Average SD in test set : 0.12 (5-fold)\n   -  Test : R2 = 0.78 ; RMSE = 0.81\n   -  Summary of results\n      o  Summary of results\n      o  Datapoints:descriptors ratio : 4.2\n      o  Outliers removed: 3 (proportion 0.04)\n\"\"\"\n\nCohort B (suffix: \"NoPFI\"):\n\"\"\"\n-------  (No PFI)\n   - Model: Random Forest (regression)\no  Summary of results (5-fold CV)\n-  5-fold CV : R2 = 0.78, RMSE = 0.52\n-  Test : R2 = 0.74, RMSE = 0.60\n-  y range of dataset : 0.15 to 3.15\n-  Datapoints/Descriptors ratio : 6.4\n-  Outliers removed: 3 (2.5%)\n-  Average SD in test set : 0.08\n\"\"\"\n\nCohort C (suffix: \"No PFI - rerun\"):\n\"\"\"\n-------  (No PFI)  -------\n   - Model: Random Forest Regressor\n   -fold CV : R2 = 0.79 ; RMSE = 0.70\n   -  Average SD in test set : 0.11 (5-fold)\n   -  Summary of results\n      o  Datapoints:descriptors ratio : 5.1\n      o  Outliers removed: 2 (proportion 0.03)\n\"\"\"\n\nReturn the updated scoring dictionaries for the reportable cohorts only.", "answers": "[{\"name\":\"robert_report_utils_get_predict_scores\",\"arguments\":{\"dat_predict\":[\"-------  (No PFI)  -------\",\"   - Model: Random Forest Regressor\",\"   -  y range of dataset : 0.15 .. 8.92\",\"   -fold CV : R2 = 0.81 ; RMSE = 0.73\",\"   -  Average SD in test set : 0.12 (5-fold)\",\"   -  Test : R2 = 0.78 ; RMSE = 0.81\",\"   -  Summary of results\",\"      o  Summary of results\",\"      o  Datapoints:descriptors ratio : 4.2\",\"      o  Outliers removed: 3 (proportion 0.04)\"],\"suffix\":\"No PFI\",\"pred_type\":\"reg\",\"data_score\":{}}},{\"name\":\"robert_report_utils_get_predict_scores\",\"arguments\":{\"dat_predict\":[\"-------  (No PFI)\",\"   - Model: Random Forest (regression)\",\"o  Summary of results (5-fold CV)\",\"-  5-fold CV : R2 = 0.78, RMSE = 0.52\",\"-  Test : R2 = 0.74, RMSE = 0.60\",\"-  y range of dataset : 0.15 to 3.15\",\"-  Datapoints/Descriptors ratio : 6.4\",\"-  Outliers removed: 3 (2.5%)\",\"-  Average SD in test set : 0.08\"],\"suffix\":\"NoPFI\",\"pred_type\":\"reg\",\"data_score\":{}}}]"}
{"func_name": "robert_report_utils_revert_list", "func_desc": "robert.report_utils.revert_list swaps the order of a two-element list when the second element contains the literal substring 'No_PFI'. This helper is used by ROBERT report generation utilities to enforce a consistent ordering for two-component entries (for example, entries that may indicate presence/absence of permutation feature importance data) so downstream report formatting and comparisons behave predictably.\n    \n    This function implements a targeted conditional swap: it only changes the ordering when the input is a list of exactly two items and the second item contains 'No_PFI'. The implementation avoids in-place mutation with list.reverse() (a previously observed issue) by constructing and returning a new two-element list when swapping is needed; otherwise it returns the original list object unchanged.", "tools": [{"function": {"description": "robert.report_utils.revert_list swaps the order of a two-element list when the second element contains the literal substring 'No_PFI'. This helper is used by ROBERT report generation utilities to enforce a consistent ordering for two-component entries (for example, entries that may indicate presence/absence of permutation feature importance data) so downstream report formatting and comparisons behave predictably.\n\nThis function implements a targeted conditional swap: it only changes the ordering when the input is a list of exactly two items and the second item contains 'No_PFI'. The implementation avoids in-place mutation with list.reverse() (a previously observed issue) by constructing and returning a new two-element list when swapping is needed; otherwise it returns the original list object unchanged.", "name": "robert_report_utils_revert_list", "parameters": {"properties": {"list_tuple": {"type": "array", "items": {"type": "float"}, "description": "A list object expected to contain two components. In the ROBERT reporting context this represents a paired entry (for example, a label and a status or data placeholder). The function inspects the second element (index 1) for the literal substring 'No_PFI' and, if found and the list has length exactly 2, returns a new list with the two elements swapped. The parameter must be a Python list; if it is not a list the behavior is not guaranteed by this function. If the second element is not an iterable or does not support the membership test ('No_PFI' in element), a TypeError may be raised.", "default": ""}}, "required": ["list_tuple"], "type": "any"}}, "type": "function"}], "query": "During ROBERT report generation, we ingest a mixed cohort of two-component PFI indicator pairs coming from heterogeneous templates. Normalize only those entries that are valid two-element lists and represent an explicit PFI-absence flag in the second field (detected by the literal substring 'No_PFI'). Treat entries indicating PFI presence/availability in the second field as already canonical. Run the normalization on this raw batch: ['PFI_available', 'No_PFI_for_baseline'], ['No_PFI_expected', 'PFI_available'], ['PFI_available', 'PFI_computed_ok'], ['PFI_available', 'No_PFI_for_model_X'], ['PFI_available', 'NoPFI_typo_model_Y'].", "answers": "[{\"name\":\"robert_report_utils_revert_list\",\"arguments\":{\"list_tuple\":[\"PFI_available\",\"No_PFI_for_baseline\"]}},{\"name\":\"robert_report_utils_revert_list\",\"arguments\":{\"list_tuple\":[\"No_PFI_expected\",\"PFI_available\"]}},{\"name\":\"robert_report_utils_revert_list\",\"arguments\":{\"list_tuple\":[\"PFI_available\",\"PFI_computed_ok\"]}},{\"name\":\"robert_report_utils_revert_list\",\"arguments\":{\"list_tuple\":[\"PFI_available\",\"No_PFI_for_model_X\"]}},{\"name\":\"robert_report_utils_revert_list\",\"arguments\":{\"list_tuple\":[\"PFI_available\",\"NoPFI_typo_model_Y\"]}}]"}
{"func_name": "robert_utils_correct_hidden_layers", "func_desc": "robert.utils.correct_hidden_layers corrects and normalizes the 'hidden_layer_sizes' entry inside a parameter dictionary that is typically loaded from JSON for use with neural-network-based regression tools in ROBERT. The function ensures the value stored under the 'hidden_layer_sizes' key becomes an explicit Python list of integers representing the number of neurons per hidden layer (the same semantic used by scikit-learn MLP estimators), and mutates the input dictionary in place for downstream model construction and hyperparameter handling in the ROBERT regression pipeline.\n    \n    This function is used when JSON-serialized parameter sets store hidden layer sizes as strings (for example \"[64,32]\" or \"64,32\") or as lists of numeric/string elements; it attempts to parse those representations and produce a canonical list[int] that downstream code (e.g., model builders or hyperparameter evaluators in ROBERT) expects. The function implements the following concrete behavior from the source:\n    - If params['hidden_layer_sizes'] is not an int, it treats the value as either a string or a list. If the value is a string that begins with '[' or ends with ']', the function strips the leading/trailing bracket characters. If the (possibly bracket-stripped) value is a string, it splits on commas and converts non-empty segments to ints. If the value is a list, it iterates over elements and converts non-empty elements to ints. Empty strings (''), if present in the split/list, are skipped. Parsed integers are collected into layer_arrays and then assigned back into params['hidden_layer_sizes'].\n    - If params['hidden_layer_sizes'] is an int, the function reaches a code path that attempts to assign an undefined local variable to the output (this is an implementation bug in the current source) which will raise an UnboundLocalError at runtime.\n    \n    Behavioral notes, side effects, and failure modes:\n    - The function mutates the input dictionary params in place by replacing params['hidden_layer_sizes'] with the parsed list of integers. It also returns the same dictionary.\n    - Accepted input forms for params['hidden_layer_sizes'] (as handled by the current implementation) include:\n      - a string representing a bracketed list, e.g. \"[64,32]\"\n      - a comma-separated string without brackets, e.g. \"64,32\"\n      - a list of elements (elements may be numeric or string-representations of integers)\n      - an integer (handled only by the buggy branch described above; see failure modes)\n    - Conversions use int(ele) for each non-empty element and therefore will raise ValueError if any non-empty element cannot be parsed as an integer (for example \"64a\" or \"sixtyfour\").\n    - If the params dictionary does not contain the key 'hidden_layer_sizes', a KeyError will be raised by the implementation.\n    - If params is not a dict, typical attribute or indexing errors (TypeError) will be raised.\n    - If params['hidden_layer_sizes'] is already an int, the current implementation contains a bug that will raise UnboundLocalError because the code assigns layer_arrays = ele where ele is not defined in that branch. This is a known failure mode of the current source and must be handled by callers or fixed in the implementation.\n    - Empty items produced by splitting strings (empty strings) are skipped and not included in the resulting list.", "tools": [{"function": {"description": "robert.utils.correct_hidden_layers corrects and normalizes the 'hidden_layer_sizes' entry inside a parameter dictionary that is typically loaded from JSON for use with neural-network-based regression tools in ROBERT. The function ensures the value stored under the 'hidden_layer_sizes' key becomes an explicit Python list of integers representing the number of neurons per hidden layer (the same semantic used by scikit-learn MLP estimators), and mutates the input dictionary in place for downstream model construction and hyperparameter handling in the ROBERT regression pipeline.\n\nThis function is used when JSON-serialized parameter sets store hidden layer sizes as strings (for example \"[64,32]\" or \"64,32\") or as lists of numeric/string elements; it attempts to parse those representations and produce a canonical list[int] that downstream code (e.g., model builders or hyperparameter evaluators in ROBERT) expects. The function implements the following concrete behavior from the source:\n- If params['hidden_layer_sizes'] is not an int, it treats the value as either a string or a list. If the value is a string that begins with '[' or ends with ']', the function strips the leading/trailing bracket characters. If the (possibly bracket-stripped) value is a string, it splits on commas and converts non-empty segments to ints. If the value is a list, it iterates over elements and converts non-empty elements to ints. Empty strings (''), if present in the split/list, are skipped. Parsed integers are collected into layer_arrays and then assigned back into params['hidden_layer_sizes'].\n- If params['hidden_layer_sizes'] is an int, the function reaches a code path that attempts to assign an undefined local variable to the output (this is an implementation bug in the current source) which will raise an UnboundLocalError at runtime.\n\nBehavioral notes, side effects, and failure modes:\n- The function mutates the input dictionary params in place by replacing params['hidden_layer_sizes'] with the parsed list of integers. It also returns the same dictionary.\n- Accepted input forms for params['hidden_layer_sizes'] (as handled by the current implementation) include:\n  - a string representing a bracketed list, e.g. \"[64,32]\"\n  - a comma-separated string without brackets, e.g. \"64,32\"\n  - a list of elements (elements may be numeric or string-representations of integers)\n  - an integer (handled only by the buggy branch described above; see failure modes)\n- Conversions use int(ele) for each non-empty element and therefore will raise ValueError if any non-empty element cannot be parsed as an integer (for example \"64a\" or \"sixtyfour\").\n- If the params dictionary does not contain the key 'hidden_layer_sizes', a KeyError will be raised by the implementation.\n- If params is not a dict, typical attribute or indexing errors (TypeError) will be raised.\n- If params['hidden_layer_sizes'] is already an int, the current implementation contains a bug that will raise UnboundLocalError because the code assigns layer_arrays = ele where ele is not defined in that branch. This is a known failure mode of the current source and must be handled by callers or fixed in the implementation.\n- Empty items produced by splitting strings (empty strings) are skipped and not included in the resulting list.", "name": "robert_utils_correct_hidden_layers", "parameters": {"properties": {"params": {"type": "any", "description": "A parameters dictionary used by ROBERT for regression model configuration. This dictionary is expected to include the key 'hidden_layer_sizes' whose value may be a string (e.g., \"[64,32]\" or \"64,32\"), a list of elements (strings or numbers), or an int. The function will parse and normalize that entry into a list of integers representing the number of neurons in each hidden layer (the same semantics used by scikit-learn MLP estimators). The dictionary is modified in place; callers should provide a mutable dict and be aware that parsing errors (ValueError, KeyError, UnboundLocalError when the value is int) can be raised by this routine.", "default": ""}}, "required": ["params"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating a heterogeneous batch of ROBERT MLPRegressor hyperparameter dictionaries scraped from multiple JSON exports before launching an automated architecture sweep. Each record should be standardized only if it is a valid multi-layer architecture encoding: the value under `hidden_layer_sizes` must represent at least two layers when interpreted as a comma-separated sequence (after tolerating optional surrounding brackets and whitespace). Records that encode a single-layer architecture (i.e., only one neuron-count token) should be left untouched at this stage to avoid triggering known downstream edge cases in our pipeline.\n\nApply `robert.utils.correct_hidden_layers` in-place to the subset that meets the multi-layer criterion for the following raw parameter dictionaries:\n1) {\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"[256, 128,64]\",\"activation\":\"relu\",\"solver\":\"adam\",\"random_state\":7}\n2) {\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"64\",\"alpha\":0.0001,\"max_iter\":400,\"random_state\":7}\n3) {\"hidden_layer_sizes\":[\"128\",\"\", \"32\"],\"activation\":\"tanh\",\"solver\":\"lbfgs\",\"random_state\":13}\n4) {\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"[ 100 , 50 ]\",\"early_stopping\":true,\"validation_fraction\":0.1,\"random_state\":21}\n5) {\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":128,\"activation\":\"relu\",\"random_state\":9}\n\nPreserve all other hyperparameters exactly as provided for any processed record.", "answers": "[{\"name\":\"robert_utils_correct_hidden_layers\",\"arguments\":{\"params\":{\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"[256, 128,64]\",\"activation\":\"relu\",\"solver\":\"adam\",\"random_state\":7}}},{\"name\":\"robert_utils_correct_hidden_layers\",\"arguments\":{\"params\":{\"hidden_layer_sizes\":[\"128\",\"\",\"32\"],\"activation\":\"tanh\",\"solver\":\"lbfgs\",\"random_state\":13}}},{\"name\":\"robert_utils_correct_hidden_layers\",\"arguments\":{\"params\":{\"model\":\"MLPRegressor\",\"hidden_layer_sizes\":\"[ 100 , 50 ]\",\"early_stopping\":true,\"validation_fraction\":0.1,\"random_state\":21}}}]"}
{"func_name": "robert_utils_load_minimal_model", "func_desc": "Load and return the predefined minimal hyperparameter set for a named estimator used by REFCV within the ROBERT package.", "tools": [{"function": {"description": "Load and return the predefined minimal hyperparameter set for a named estimator used by REFCV within the ROBERT package.\n", "name": "robert_utils_load_minimal_model", "parameters": {"properties": {"model": {"type": "string", "description": "Identifier of the minimalist model whose parameter set will be returned. Accepted exact values (case-sensitive) are 'RF', 'GB', 'NN', 'ADAB', 'GP', and 'MVL'. This argument selects one of the small, predefined hyperparameter dictionaries embedded in ROBERT that are intended for use by the REFCV routine (the package's reference cross-validation / low-resource benchmarking workflow used in the ROBERT machine-learning-for-chemistry tooling). The choice of model controls which set of hyperparameters is returned; the caller should pass this string verbatim.", "default": ""}}, "required": ["model"], "type": "any"}}, "type": "function"}], "query": "In our ROBERT REFCV ablation, we’re ingesting a messy estimator registry exported from several notebooks. Each record has a short tag we want to resolve to the REFCV minimal preset. Use the following raw list:\n\n- {\"tag\": \"baseline_tree\", \"family\": \"boosting\", \"notes\": \"use as baseline\"}\n- {\"tag\": \"gp_rep1\", \"family\": \"bayesian\", \"kernel\": \"RBF\", \"notes\": \"duplicate run\"}\n- {\"tag\": \"gp_rep2\", \"family\": \"bayesian\", \"kernel\": \"RBF\", \"notes\": \"duplicate run\"}\n- {\"tag\": \"linear_probe\", \"family\": \"linear\", \"notes\": \"not supported in this minimal preset loader\"}\n- {\"tag\": \"gb_typo\", \"family\": \"boosting \", \"notes\": \"trailing whitespace in family field\"}\n\nProtocol: load minimal presets only for records that can be mapped cleanly to supported estimator families. Map any record whose normalized family indicates gradient boosting to the minimal GB preset. Map any record whose normalized family indicates a Gaussian process / bayesian regressor to the minimal GP preset. Keep replicate intent: if multiple records map to the same supported family, load that minimal preset once per record so downstream REFCV runs preserve the registry multiplicity. Return the minimal preset dictionaries in the same order as the records that qualify under these rules.", "answers": "[{\"name\":\"robert_utils_load_minimal_model\",\"arguments\":{\"model\":\"GB\"}},{\"name\":\"robert_utils_load_minimal_model\",\"arguments\":{\"model\":\"GP\"}},{\"name\":\"robert_utils_load_minimal_model\",\"arguments\":{\"model\":\"GP\"}},{\"name\":\"robert_utils_load_minimal_model\",\"arguments\":{\"model\":\"GB\"}}]"}
{"func_name": "robert_utils_sort_n_load", "func_desc": "robert.utils.sort_n_load sorts the training feature and target arrays contained in Xy_data to produce a reproducible, stable ordering of rows for downstream regression model training and evaluation in the ROBERT workflow.\n    \n    This function is used in ROBERT's machine-learning/regression pipelines to ensure that when the same database is loaded with different row orders (for example because of file or OS-dependent ordering), the resulting X and y arrays are reordered deterministically so that model training, cross-validation splits, and results are reproducible across runs and platforms. The implementation converts inputs to numpy arrays, computes a stable argsort on the target values, and reindexes the feature matrix to preserve the original feature-target pairing.", "tools": [{"function": {"description": "robert.utils.sort_n_load sorts the training feature and target arrays contained in Xy_data to produce a reproducible, stable ordering of rows for downstream regression model training and evaluation in the ROBERT workflow.\n\nThis function is used in ROBERT's machine-learning/regression pipelines to ensure that when the same database is loaded with different row orders (for example because of file or OS-dependent ordering), the resulting X and y arrays are reordered deterministically so that model training, cross-validation splits, and results are reproducible across runs and platforms. The implementation converts inputs to numpy arrays, computes a stable argsort on the target values, and reindexes the feature matrix to preserve the original feature-target pairing.", "name": "robert_utils_sort_n_load", "parameters": {"properties": {"Xy_data": {"type": "any", "description": "A dictionary expected to contain at least the keys 'X_train_scaled' and 'y_train' used by ROBERT. 'X_train_scaled' should be the training feature matrix after any scaling step (for example, a 2D array-like object where rows correspond to samples and columns to scaled features). 'y_train' should be the corresponding training target values for regression (typically a 1D array-like of numeric target values). The function will convert these values to numpy.ndarray internally. The practical role of Xy_data is to carry the scaled features and target for model fitting; this function enforces a stable, reproducible row order for that data. Passing a non-dict will raise a TypeError; omission of the required keys will raise a KeyError.", "default": ""}}, "required": ["Xy_data"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a reproducible regression benchmark from multiple vendor exports of a housing-price cohort. Each export is a scaled feature matrix paired with a target vector (sale price), but the raw dumps include mixed numeric regimes (some targets are in k$ units, some in $), plus occasional corrupt rows (non-finite targets). Before model training, we need deterministic ordering for each export using ROBERT’s `robert.utils.sort_n_load`, but only after applying the following cohort curation rules:\n\n1) Treat a replicate as eligible for the reproducibility step only if all target values are finite and strictly positive.\n2) Because vendors sometimes export prices in “k$” units, for any eligible replicate where the median of y_train is < 1000, rescale y_train by multiplying by 1000 (leave X_train_scaled unchanged).\n3) After these checks/normalization, run `robert.utils.sort_n_load` to stable-sort each eligible replicate by the (possibly rescaled) y_train while preserving feature–target pairing.\n\nRaw replicates:\n- Replicate A (5 samples):\n  X_train_scaled = [[0.5, -1.2, 0.3], [1.1, 0.0, -0.7], [-0.8, 0.9, 1.5], [0.2, -0.5, -0.3], [-1.0, 1.3, 0.8]]\n  y_train = [320.0, 450.0, 275.0, 390.0, 310.0]\n- Replicate B (8 samples):\n  X_train_scaled = [[0.5, -1.2, 1.0], [1.3, 0.0, -0.7], [-0.8, 2.1, 0.3], [0.0, -0.5, 0.8], [2.0, 1.5, -1.1], [-1.5, -0.3, 0.2], [0.7, 0.9, -0.4], [-0.2, 1.8, 0.6]]\n  y_train = [320000.0, 450000.0, 210000.0, 300000.0, 520000.0, 190000.0, 410000.0, 275000.0]\n- Replicate C (4 samples):\n  X_train_scaled = [[0.25, -1.2, 0.0], [1.1, 0.5, -0.3], [-0.75, -0.8, 1.4], [0.4, 1.2, -0.9]]\n  y_train = [350000.0, 275000.0, 500000.0, 300000.0]\n\nReturn the sorted-and-loaded Xy_data outputs for the eligible replicates, using the above rules to decide whether and how to normalize targets prior to sorting.", "answers": "[{\"name\":\"robert_utils_sort_n_load\",\"arguments\":{\"Xy_data\":{\"X_train_scaled\":[[0.5,-1.2,0.3],[1.1,0.0,-0.7],[-0.8,0.9,1.5],[0.2,-0.5,-0.3],[-1.0,1.3,0.8]],\"y_train\":[320000.0,450000.0,275000.0,390000.0,310000.0]}}},{\"name\":\"robert_utils_sort_n_load\",\"arguments\":{\"Xy_data\":{\"X_train_scaled\":[[0.5,-1.2,1.0],[1.3,0.0,-0.7],[-0.8,2.1,0.3],[0.0,-0.5,0.8],[2.0,1.5,-1.1],[-1.5,-0.3,0.2],[0.7,0.9,-0.4],[-0.2,1.8,0.6]],\"y_train\":[320000.0,450000.0,210000.0,300000.0,520000.0,190000.0,410000.0,275000.0]}}},{\"name\":\"robert_utils_sort_n_load\",\"arguments\":{\"Xy_data\":{\"X_train_scaled\":[[0.25,-1.2,0.0],[1.1,0.5,-0.3],[-0.75,-0.8,1.4],[0.4,1.2,-0.9]],\"y_train\":[350000.0,275000.0,500000.0,300000.0]}}}]"}
{"func_name": "scanpy_preprocessing__utils_sparse_mean_var_minor_axis", "func_desc": "Compute per-minor-axis (column) means and variances for a CSR-format sparse matrix.\n\nThis function is used in Scanpy preprocessing routines to compute per-feature (per-gene) summary\nstatistics across observations (cells) when the expression matrix is stored in CSR (compressed sparse row)\nformat. It iterates over the nonzero entries provided by the CSR arrays and returns the mean and\npopulation variance for each minor-axis element (column/gene) across the major axis (rows/cells).\nThe implementation is numba njit-compiled and uses parallel accumulation across n_threads to reduce\ncontention; intermediate per-thread accumulators are combined at the end.", "tools": [{"function": {"description": "Compute per-minor-axis (column) means and variances for a CSR-format sparse matrix.\n\nThis function is used in Scanpy preprocessing routines to compute per-feature (per-gene) summary\nstatistics across observations (cells) when the expression matrix is stored in CSR (compressed sparse row)\nformat. It iterates over the nonzero entries provided by the CSR arrays and returns the mean and\npopulation variance for each minor-axis element (column/gene) across the major axis (rows/cells).\nThe implementation is numba njit-compiled and uses parallel accumulation across n_threads to reduce\ncontention; intermediate per-thread accumulators are combined at the end.", "name": "scanpy_preprocessing__utils_sparse_mean_var_minor_axis", "parameters": {"properties": {"data": {"type": "array", "items": {"type": "float"}, "description": "1-D array of nonzero values for the CSR matrix, in row-major order.\nEach entry data[j] is the nonzero value corresponding to column index indices[j]\nin the row described by indptr. Values are treated as numeric (floating) and are used\ndirectly in sum and squared-sum accumulation.", "default": ""}, "indices": {"type": "array", "items": {"type": "float"}, "description": "1-D integer array of column indices for each entry in data.\nFor each j in range(len(data)), indices[j] is the minor-axis (column) index of data[j].\nIndices that are greater than or equal to minor_len are ignored (skipped) by this function.", "default": ""}, "indptr": {"type": "array", "items": {"type": "float"}, "description": "1-D integer index pointer array of length rows + 1 for the CSR representation.\nFor row r (0 <= r < rows) the nonzero entries are data[indptr[r]:indptr[r+1]] with\ncorresponding indices[indptr[r]:indptr[r+1]]. The function computes rows = len(indptr) - 1\nand iterates r from 0 to rows-1. indptr must therefore represent the CSR row structure of the data.", "default": ""}, "major_len": {"type": "integer", "description": "Number of elements along the major axis (rows). This integer is used as the\ndenominator when computing means and variances: mean = sum / major_len and variance =\n(sum_of_squares / major_len) - (mean ** 2). In Scanpy usage, major_len typically equals\nthe number of observations (cells). major_len must be positive; passing 0 will cause a\ndivision-by-zero at return time.", "default": ""}, "minor_len": {"type": "integer", "description": "Number of elements along the minor axis (columns). The function computes one\nmean and one variance for each minor index c in range(minor_len). The returned arrays have\nlength minor_len. Entries in indices that are >= minor_len are skipped; mismatches between\nminor_len and the actual maximum index in indices will affect which entries are aggregated.", "default": ""}, "n_threads": {"type": "integer", "description": "Number of threads used for parallel accumulation with numba.prange. The function\ncreates temporary per-thread accumulators of shape (n_threads, minor_len) to accumulate sums\nand squared sums independently before combining them. n_threads must be a positive integer;\nusing more threads than rows is allowed (some threads will process no rows) but increases\ntemporary memory proportional to n_threads * minor_len.", "default": ""}}, "required": ["data", "indices", "indptr", "major_len", "minor_len", "n_threads"], "type": "any"}}, "type": "function"}], "query": "I’m doing a Scanpy-like QC pass across three scRNA-seq cohorts stored as CSR. Before computing per-gene mean and population variance across cells, apply a cohort-specific contamination screen: for any cohort where the median of the nonzero expression values exceeds 3.0 (a signature of ambient RNA spikes), run the summary-statistics kernel with 4 threads; otherwise use 2 threads. Keep each cohort’s matrix shape as provided and compute per-gene (minor-axis) mean and population variance across all cells. Cohort A: 4 cells × 5 genes with CSR arrays data=[1.0, 3.5, 2.0, 4.0, 5.5], indices=[0, 2, 1, 3, 4], indptr=[0, 2, 3, 4, 5]. Cohort B: 3 cells × 4 genes with data=[1.0, 2.5, 3.0, 4.5], indices=[0, 2, 1, 3], indptr=[0, 2, 3, 4]. Cohort C: 3 cells × 4 genes with data=[2.0, 5.0, 3.0, 4.0, 1.0], indices=[0, 2, 1, 2, 3], indptr=[0, 2, 3, 5]. Return the per-gene mean and population variance vectors for each cohort.", "answers": "[{\"name\":\"scanpy_preprocessing__utils_sparse_mean_var_minor_axis\",\"arguments\":{\"data\":[1.0,3.5,2.0,4.0,5.5],\"indices\":[0,2,1,3,4],\"indptr\":[0,2,3,4,5],\"major_len\":4,\"minor_len\":5,\"n_threads\":4}},{\"name\":\"scanpy_preprocessing__utils_sparse_mean_var_minor_axis\",\"arguments\":{\"data\":[1.0,2.5,3.0,4.5],\"indices\":[0,2,1,3],\"indptr\":[0,2,3,4],\"major_len\":3,\"minor_len\":4,\"n_threads\":2}},{\"name\":\"scanpy_preprocessing__utils_sparse_mean_var_minor_axis\",\"arguments\":{\"data\":[2.0,5.0,3.0,4.0,1.0],\"indices\":[0,2,1,2,3],\"indptr\":[0,2,3,5],\"major_len\":3,\"minor_len\":4,\"n_threads\":4}}]"}
{"func_name": "scanpy_readwrite_convert_bool", "func_desc": "scanpy.readwrite.convert_bool checks whether a given string is the literal boolean \"True\" or \"False\" and returns a pair that signals recognition and the corresponding boolean value. This utility is intended for read/write and parsing code in Scanpy (the single-cell gene expression analysis toolkit) where boolean values may be represented as text in configuration files, metadata fields, or I/O parameters; it provides a deterministic, case-sensitive test and a simple encoded result that calling code can use to decide how to interpret or coerce string-valued inputs.", "tools": [{"function": {"description": "scanpy.readwrite.convert_bool checks whether a given string is the literal boolean \"True\" or \"False\" and returns a pair that signals recognition and the corresponding boolean value. This utility is intended for read/write and parsing code in Scanpy (the single-cell gene expression analysis toolkit) where boolean values may be represented as text in configuration files, metadata fields, or I/O parameters; it provides a deterministic, case-sensitive test and a simple encoded result that calling code can use to decide how to interpret or coerce string-valued inputs.\n", "name": "scanpy_readwrite_convert_bool", "parameters": {"properties": {"string": {"type": "string", "description": "The input text to test. The function performs an exact, case-sensitive equality comparison against the two Python literal strings \"True\" and \"False\". In the Scanpy I/O and metadata context this parameter represents a textual boolean candidate read from a file, user input, or annotation field.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "We’re merging boolean-like flags from heterogeneous Scanpy inputs (YAML config, AnnData `.uns` exports, and spreadsheet-derived sample sheets). The raw strings are case-sensitive and may include common serialization artifacts. Given the candidate flag values:\n\n- use_raw: \"False\"\n- use_highly_variable: \"FALSE\"\n- log1p: \"True\"\n- regress_out: \"true\"\n- batch_correct: \"False \" \n- doublet_filter: \"False\"\n- use_rep: \"None\"\n\nRun `scanpy.readwrite.convert_bool` on each raw string exactly as received. Only flags that are recognized as literal booleans (exactly \"True\"/\"False\") should be treated as safely coercible downstream; report recognition/boolean decoding via the function’s encoded result for each candidate value.", "answers": "[{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"False\"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"FALSE\"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"True\"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"true\"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"False \"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"False\"}},{\"name\":\"scanpy_readwrite_convert_bool\",\"arguments\":{\"string\":\"None\"}}]"}
{"func_name": "scanpy_readwrite_is_int", "func_desc": "scanpy.readwrite.is_int checks whether a text string represents an integer according to Python's built-in int() conversion.\n\nThis utility is intended for use in Scanpy's read/write and parsing routines when deciding whether a token (for example, a field from a text file, a row/column label, a barcode fragment, a layer index, or other metadata encountered while loading single-cell gene expression data) should be interpreted as an integer. The function performs a single, non-destructive conversion attempt and has no side effects on program state or input objects.", "tools": [{"function": {"description": "scanpy.readwrite.is_int checks whether a text string represents an integer according to Python's built-in int() conversion.\n\nThis utility is intended for use in Scanpy's read/write and parsing routines when deciding whether a token (for example, a field from a text file, a row/column label, a barcode fragment, a layer index, or other metadata encountered while loading single-cell gene expression data) should be interpreted as an integer. The function performs a single, non-destructive conversion attempt and has no side effects on program state or input objects.", "name": "scanpy_readwrite_is_int", "parameters": {"properties": {"string": {"type": "string", "description": "The string to test for integer-ness. This should be a Python str containing the characters to evaluate (for example, \"42\", \"-7\", or \"  +3  \"). The function attempts to convert this exact string with Python's built-in int(string). Valid representations follow int() semantics for base-10 integers (optional leading sign, optional surrounding whitespace). Strings that include a decimal point, numeric separators like underscores, or prefixes such as \"0x\" or \"0b\" will not be accepted by int() in its default usage and therefore will cause this function to report False. Although the signature requires a str, passing a non-str value may lead to a TypeError raised by int(); such errors are not caught by this function.", "default": ""}}, "required": ["string"], "type": "any"}}, "type": "function"}], "query": "During ingestion of a heterogeneous scRNA-seq annotation table, we’re standardizing tokens before type inference. We only attempt integer casting on tokens that are plausible numeric indices/IDs: after stripping leading/trailing whitespace, the token must match an optional sign followed by digits only (no decimal points, exponent notation, thousands separators, unit suffixes, or embedded whitespace). For the following raw tokens exactly as extracted, apply `scanpy.readwrite.is_int` only to those that satisfy this plausibility gate, and return the corresponding boolean results for the tokens that were actually eligible: ['  +103  ', '  -5  ', '  +304  ', '1e3', '042', '  + 7', '3.0', 'gene_12', '', '  -0  '].", "answers": "[{\"name\":\"scanpy_readwrite_is_int\",\"arguments\":{\"string\":\"  +103  \"}},{\"name\":\"scanpy_readwrite_is_int\",\"arguments\":{\"string\":\"  -5  \"}},{\"name\":\"scanpy_readwrite_is_int\",\"arguments\":{\"string\":\"  +304  \"}},{\"name\":\"scanpy_readwrite_is_int\",\"arguments\":{\"string\":\"042\"}},{\"name\":\"scanpy_readwrite_is_int\",\"arguments\":{\"string\":\"  -0  \"}}]"}
{"func_name": "scanpy_tools__sim_check_nocycles", "func_desc": "Check_nocycles verifies that the directed graph encoded by a square adjacency matrix contains no directed cycles.", "tools": [{"function": {"description": "Check_nocycles verifies that the directed graph encoded by a square adjacency matrix contains no directed cycles.\n", "name": "scanpy_tools__sim_check_nocycles", "parameters": {"properties": {"Adj": {"type": "array", "items": {"type": "float"}, "description": "Square adjacency matrix of shape (n, n) describing a directed graph used in scanpy.tools._sim (for example, a simulated gene regulatory network or other directed interaction graph used during single-cell data simulation). Each entry Adj[i, j] represents the weight or presence of an edge from node j to node i as used by the implementation (the function multiplies the matrix by a column vector). The matrix must be a numpy.ndarray with numeric dtype and must be square (number of rows equals number of columns). If Adj is not square or not a numpy.ndarray with compatible numeric shape, the function will raise a numpy/linear-algebra error when performing dot products. Self-loops (nonzero diagonal entries) are treated as cycles of length 1 and will be detected.", "default": ""}, "verbosity": {"type": "integer", "description": "Verbosity control for diagnostic output. Default is 2. When verbosity > 2, the function emits diagnostic messages via settings.m (it prints the adjacency matrix and a message indicating the detected cycle length and starting node). When verbosity <= 2 the function performs only the cycle check without printing diagnostics. The function does not otherwise modify Adj.", "default": 2}}, "required": ["Adj", "verbosity"], "type": "any"}}, "type": "function"}], "query": "We’re validating inferred GRNs coming out of two different inference backends where edge weights can include sign (activation/inhibition) and occasional self-loop artifacts. Treat the raw adjacency as column j → row i. Before cycle QC, build an “effective adjacency” per network by (i) interpreting any nonzero magnitude as an edge (ignore sign), (ii) zeroing the diagonal (self-regulation is considered an artifact for this QC gate), and (iii) dropping edges whose absolute weight is < 1.0 as weak, likely-spurious interactions. Run cycle-check only on effective adjacencies that still contain at least one edge after this filtering. Use verbosity=1. Network A (weighted): [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.0], [0.0, 2.0, 0.0, 0.0]]. Network B (binary): [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]].", "answers": "[{\"name\":\"scanpy_tools__sim_check_nocycles\",\"arguments\":{\"Adj\":[[0.0,0.0,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0],[0.0,2.0,0.0,0.0]],\"verbosity\":1}},{\"name\":\"scanpy_tools__sim_check_nocycles\",\"arguments\":{\"Adj\":[[0,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,0,0]],\"verbosity\":1}}]"}
{"func_name": "selfies_decoder_decoder", "func_desc": "selfies.decoder.decoder: Translate a SELFIES string into its corresponding SMILES string.\n    \n    Translates a SELFIES molecular representation into a SMILES string deterministically under the library's current semantic constraints (the constraints configured via selfies.set_semantic_constraints). This function is used throughout the SELFIES package and in downstream machine-learning workflows (for example, generative models and random-molecule generation) to convert the robust SELFIES token sequence back into a conventional SMILES string that can be consumed by cheminformatics tools. The output SMILES is guaranteed by the decoder implementation to be syntactically valid and to represent a molecule that obeys the active semantic constraints. The decoder processes dot-separated fragments (SELFIES segments separated by \".\") and assembles rings and branches according to those constraints. The implementation constructs an internal MolecularGraph, forms rings bilocally, and then serializes the graph to SMILES; when attribution is requested the function also tracks which input SELFIES tokens contributed to each output SMILES token.", "tools": [{"function": {"description": "selfies.decoder.decoder: Translate a SELFIES string into its corresponding SMILES string.\n\nTranslates a SELFIES molecular representation into a SMILES string deterministically under the library's current semantic constraints (the constraints configured via selfies.set_semantic_constraints). This function is used throughout the SELFIES package and in downstream machine-learning workflows (for example, generative models and random-molecule generation) to convert the robust SELFIES token sequence back into a conventional SMILES string that can be consumed by cheminformatics tools. The output SMILES is guaranteed by the decoder implementation to be syntactically valid and to represent a molecule that obeys the active semantic constraints. The decoder processes dot-separated fragments (SELFIES segments separated by \".\") and assembles rings and branches according to those constraints. The implementation constructs an internal MolecularGraph, forms rings bilocally, and then serializes the graph to SMILES; when attribution is requested the function also tracks which input SELFIES tokens contributed to each output SMILES token.", "name": "selfies_decoder_decoder", "parameters": {"properties": {"selfies": {"type": "string", "description": "The input SELFIES string to be translated. This must be a sequence of SELFIES symbols (for example, \"[C][=O][Branch1]...\") possibly containing multiple fragments separated by \".\". SELFIES is a robust molecular string representation designed to be used directly as input to machine-learning models; providing a valid SELFIES string ensures deterministic decoding. The function will tokenize the provided string and derive a MolecularGraph from those tokens. Common practical uses include translating SELFIES produced by generative models back into standard SMILES for downstream validation, visualization, or property prediction.", "default": ""}, "compatible": {"type": "boolean", "description": "If True, accept deprecated SELFIES symbols from previous releases when tokenizing and deriving the molecular graph. This is a permissive compatibility mode intended to help migrate older SELFIES strings, but it does not guarantee exact backward compatibility across major releases and the decoded result may differ from historical behavior. When this flag is True the function emits a runtime warning advising the user to update SELFIES to the current alphabet. Default is False.", "default": false}, "attribute": {"type": "boolean", "description": "If True, produce an attribution mapping in addition to the SMILES string that explains which input SELFIES tokens contributed to each output SMILES token. This is useful for explainability and debugging (for example, to trace how branched tokens or ring tokens in SELFIES map to specific SMILES characters). When False (the default), only the SMILES string is returned and no attribution structures are created; when True, the decoder builds and returns attribution information while deriving the MolecularGraph, which increases memory/work but enables downstream inspection.", "default": false}}, "required": ["selfies", "attribute", "compatible"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a mixed-batch of SELFIES emitted by two molecular generators (new vs. legacy) where some candidates contain explicit topology operators and multi-fragment notation. Treat the following raw outputs as independent replicates:\n\n1) \"[C][C][O][C][=O][C]\"\n2) \"[C][C][O][Branch1][C][=O][O][Ring1][C]\"\n3) \"[C][C][O]\"\n\nQC protocol:\n- Decode every candidate to SMILES.\n- If a candidate contains any topology-control tokens (any token starting with \"[Branch\" or \"[Ring\"), decode it in legacy-compatible mode and request token-level attribution so we can audit how those operators materialize in SMILES.\n- If a candidate has no topology-control tokens, decode it under the default constraints; request token-level attribution only for candidates whose decoded molecule would be considered “non-trivial” for downstream inspection (operationally: inputs with more than 3 SELFIES tokens).\n\nReturn the decoded SMILES (and attribution where requested) for each replicate according to the above rules so we can feed results into RDKit and debug token-to-structure mappings.", "answers": "[{\"name\":\"selfies_decoder_decoder\",\"arguments\":{\"selfies\":\"[C][C][O][C][=O][C]\",\"attribute\":true}},{\"name\":\"selfies_decoder_decoder\",\"arguments\":{\"selfies\":\"[C][C][O][Branch1][C][=O][O][Ring1][C]\",\"compatible\":true,\"attribute\":true}},{\"name\":\"selfies_decoder_decoder\",\"arguments\":{\"selfies\":\"[C][C][O]\"}}]"}
{"func_name": "selfies_encoder_encoder", "func_desc": "selfies.encoder.encoder translates a SMILES string into its corresponding SELFIES string. This function is the encoder entry point in the selfies package and is used in the chemical string-processing workflow described in the README: it produces a deterministic, machine-learning-friendly SELFIES representation (robust molecular string representation) from a SMILES input, preserving the input atom order so randomized SMILES yield randomized SELFIES.", "tools": [{"function": {"description": "selfies.encoder.encoder translates a SMILES string into its corresponding SELFIES string. This function is the encoder entry point in the selfies package and is used in the chemical string-processing workflow described in the README: it produces a deterministic, machine-learning-friendly SELFIES representation (robust molecular string representation) from a SMILES input, preserving the input atom order so randomized SMILES yield randomized SELFIES.\n", "name": "selfies_encoder_encoder", "parameters": {"properties": {"smiles": {"type": "string", "description": "The input SMILES string to be translated into SELFIES. In practice this string should be a chemically valid SMILES (for example validated with RDKit) because the function first parses the SMILES via smiles_to_mol(smiles, attributable=attribute). The parser may fail for syntactically invalid SMILES or unsupported constructs; such failures are converted to a selfies.EncoderError with a message that includes the failing SMILES. The function supports aromatic SMILES by internally kekulizing them prior to translation.", "default": ""}, "strict": {"type": "boolean", "description": "If True (default), the function checks that the parsed molecule obeys the current semantic constraints configured for selfies (for example via selfies.set_semantic_constraints). This check is performed by calling the internal _check_bond_constraints(mol, smiles). If the check fails, the function raises selfies.EncoderError. If False, the function will not raise an error for semantic-constraint violations; however, SELFIES strings produced with strict=False are not guaranteed to decode back to a SMILES representing the original molecule. The translation itself is deterministic and does not otherwise depend on the current semantic constraints.", "default": true}, "attribute": {"type": "boolean", "description": "If False (default), the function returns only the SELFIES string. If True, the function constructs and returns an attribution list alongside the SELFIES string; the internal parser is invoked with attributable=attribute so token-level attribution information is collected while constructing the SELFIES. Attribution maps are trimmed of any entries with empty tokens before being returned.", "default": false}}, "required": ["smiles", "attribute", "strict"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the SMILES→SELFIES stage of a generative chemistry pipeline using an aspirin-focused micro-batch that intentionally mixes annotation artifacts, alternate aromaticity conventions, and order-randomized traversals. Given the following raw SMILES records (some contain inline comments/whitespace from a lab notebook export):\n\n1) \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n2) \"OC(=O)C1=CC=CC=C1OC(C)=O\"\n3) \"CC(=O)Oc1ccccc1C(=O)O\"\n4) \"  CC(=O)OC1=CC=CC=C1C(=O)O  # canonical_repeat  \"\n\nPipeline rules:\n- Only feed entries into the encoder if they are clean SMILES strings (no leading/trailing whitespace and no inline notebook-style annotations).\n- Use strict semantic validation for Kekulé-style aromatic encoding (explicit alternating double bonds with uppercase ring atoms), with attribution/annotation outputs suppressed.\n- Use non-strict encoding for aromatic-form SMILES (lowercase aromatic ring atoms) as a robustness control.\n\nReturn the resulting SELFIES for every entry that passes the cleanliness gate, using the strictness rule above.", "answers": "[{\"name\":\"selfies_encoder_encoder\",\"arguments\":{\"smiles\":\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"strict\":true,\"attribute\":false}},{\"name\":\"selfies_encoder_encoder\",\"arguments\":{\"smiles\":\"OC(=O)C1=CC=CC=C1OC(C)=O\",\"strict\":true,\"attribute\":false}},{\"name\":\"selfies_encoder_encoder\",\"arguments\":{\"smiles\":\"CC(=O)Oc1ccccc1C(=O)O\",\"strict\":false,\"attribute\":false}}]"}
{"func_name": "selfies_utils_encoding_utils_batch_selfies_to_flat_hot", "func_desc": "Converts a batch of SELFIES strings into flattened one-hot encodings.\n    \n    This function is a convenience wrapper used in the SELFIES project to prepare batches of SELFIES strings as fixed-length flat feature vectors for machine learning workflows (for example, input to generative models, VAEs, or classifiers that expect 1D vectors). For each SELFIES string in selfies_batch, this function calls selfies.selfies_to_encoding(selfies, vocab_stoi, pad_to_len, enc_type=\"one_hot\") to obtain a sequence of one-hot vectors and then flattens that sequence into a single list of integers (0/1). The mapping from SELFIES symbols to one-hot positions is determined by vocab_stoi. The pad_to_len argument is forwarded to selfies_to_encoding and controls the padding behavior used when converting each SELFIES string to a fixed-length sequence.", "tools": [{"function": {"description": "Converts a batch of SELFIES strings into flattened one-hot encodings.\n\nThis function is a convenience wrapper used in the SELFIES project to prepare batches of SELFIES strings as fixed-length flat feature vectors for machine learning workflows (for example, input to generative models, VAEs, or classifiers that expect 1D vectors). For each SELFIES string in selfies_batch, this function calls selfies.selfies_to_encoding(selfies, vocab_stoi, pad_to_len, enc_type=\"one_hot\") to obtain a sequence of one-hot vectors and then flattens that sequence into a single list of integers (0/1). The mapping from SELFIES symbols to one-hot positions is determined by vocab_stoi. The pad_to_len argument is forwarded to selfies_to_encoding and controls the padding behavior used when converting each SELFIES string to a fixed-length sequence.", "name": "selfies_utils_encoding_utils_batch_selfies_to_flat_hot", "parameters": {"properties": {"selfies_batch": {"type": "array", "items": {"type": "string"}, "description": "A list of SELFIES strings to be encoded. Each element is a SELFIES molecular string (for example, \"[C][O][C]\"). The order of strings is preserved in the returned list so that the i-th output corresponds to the i-th input. This argument is used when constructing datasets or minibatches for model training or evaluation in cheminformatics tasks described in the README.", "default": ""}, "vocab_stoi": {"type": "dict", "additionalProperties": {"type": "integer"}, "description": "A vocabulary mapping from SELFIES symbol (string) to integer index. The index values define the column positions in each one-hot vector produced by selfies_to_encoding; the length of each per-symbol one-hot vector is len(vocab_stoi). vocab_stoi must contain every symbol that appears in the SELFIES strings in selfies_batch or the underlying encoding call may raise an error.", "default": ""}, "pad_to_len": {"type": "integer", "description": "The length to which each SELFIES string is padded when converted to a sequence of one-hot vectors. The value is forwarded unchanged to selfies_to_encoding. Defaults to -1. When using a non-negative value, each encoded (and flattened) output will contain pad_to_len one-hot vectors (one per sequence position) each of length len(vocab_stoi), producing a flattened length of pad_to_len * len(vocab_stoi). When pad_to_len is left at the default or when its semantics are handled by selfies_to_encoding, the actual per-sequence length is determined by selfies_to_encoding; consult that function for exact behavior.", "default": -1}}, "required": ["selfies_batch", "vocab_stoi", "pad_to_len"], "type": "any"}}, "type": "function"}], "query": "We’re standardizing a mixed-quality SELFIES ingestion stream before a multi-task benchmark (VAE pretraining + toxicity classifier + baseline classifier). Each cohort contains candidate SELFIES, but only strings composed exclusively of symbols present in that cohort’s vocabulary should be featurized (treat any out-of-vocabulary token as a failed parse and exclude it from featurization). Use cohort-specific padding rules: for Cohort A (VAE training), pad/truncate to a length equal to 2×(the maximum token count among the retained Cohort A strings); for Cohort B (toxicity minibatch), pad/truncate to a length equal to the maximum token count among the retained Cohort B strings; for Cohort C (baseline minibatch), pad/truncate to a length equal to the median token count among the retained Cohort C strings (if the median is non-integer, use the upper median). Flatten the resulting one-hot sequences.\n\nCohort A candidates (vocab_stoi {\"[C]\":0,\"[H]\":1,\"[O]\":2,\"[nop]\":3}): [\"[C][H][H][H][H]\", \"[C][C][O][H][H][H][H][H][H]\", \"[C][C][C][C][C][C]\", \"[C][N][C]\"].\n\nCohort B candidates (vocab_stoi {\"[C]\":0,\"[O]\":1,\"[N]\":2,\"[=C]\":3,\"[Branch1_1]\":4,\"[Ring1]\":5,\"[nop]\":6}): [\"[C][O][C]\", \"[C][C][O][N]\", \"[O][C]\", \"[C][H][O]\", \"[C][=C][O]\"].\n\nCohort C candidates (vocab_stoi {\"[C]\":0,\"[O]\":1,\"[H]\":2,\"[nop]\":3}): [\"[C][O][C]\", \"[C][C][O][H]\", \"[O][C]\", \"[C][C][N]\", \"[H][O][H]\"].\n\nProduce the flattened one-hot feature vectors for each cohort as separate batch outputs after applying the validity sieve and cohort-specific pad length rules.", "answers": "[{\"name\":\"selfies_utils_encoding_utils_batch_selfies_to_flat_hot\",\"arguments\":{\"selfies_batch\":[\"[C][H][H][H][H]\",\"[C][C][O][H][H][H][H][H][H]\",\"[C][C][C][C][C][C]\"],\"vocab_stoi\":{\"[C]\":0,\"[H]\":1,\"[O]\":2,\"[nop]\":3},\"pad_to_len\":18}},{\"name\":\"selfies_utils_encoding_utils_batch_selfies_to_flat_hot\",\"arguments\":{\"selfies_batch\":[\"[C][O][C]\",\"[C][C][O][N]\",\"[O][C]\",\"[C][=C][O]\"],\"vocab_stoi\":{\"[C]\":0,\"[O]\":1,\"[N]\":2,\"[=C]\":3,\"[Branch1_1]\":4,\"[Ring1]\":5,\"[nop]\":6},\"pad_to_len\":4}},{\"name\":\"selfies_utils_encoding_utils_batch_selfies_to_flat_hot\",\"arguments\":{\"selfies_batch\":[\"[C][O][C]\",\"[C][C][O][H]\",\"[O][C]\",\"[H][O][H]\"],\"vocab_stoi\":{\"[C]\":0,\"[O]\":1,\"[H]\":2,\"[nop]\":3},\"pad_to_len\":3}}]"}
{"func_name": "selfies_utils_encoding_utils_selfies_to_encoding", "func_desc": "selfies.utils.encoding_utils.selfies_to_encoding converts a SELFIES string into an integer label encoding and/or a one-hot encoding suitable for machine learning workflows that operate on sequence representations of molecules. This function is used in the SELFIES library to transform the SELFIES token sequence (a robust molecular string representation described in the README) into numeric encodings for use as model inputs, dataset storage, batching, and downstream tasks such as training generative or predictive models. The function uses the helper functions len_selfies and split_selfies to determine symbol length and to tokenize the SELFIES string.", "tools": [{"function": {"description": "selfies.utils.encoding_utils.selfies_to_encoding converts a SELFIES string into an integer label encoding and/or a one-hot encoding suitable for machine learning workflows that operate on sequence representations of molecules. This function is used in the SELFIES library to transform the SELFIES token sequence (a robust molecular string representation described in the README) into numeric encodings for use as model inputs, dataset storage, batching, and downstream tasks such as training generative or predictive models. The function uses the helper functions len_selfies and split_selfies to determine symbol length and to tokenize the SELFIES string.\n", "name": "selfies_utils_encoding_utils_selfies_to_encoding", "parameters": {"properties": {"selfies": {"type": "string", "description": "The SELFIES string to encode. SELFIES is a sequence of bracketed symbols (for example, \"[C][=O][Ring1]\") representing molecular graphs. The function treats the input as an immutable string; internally it may concatenate the padding symbol \"[nop]\" to a local copy of this string when pad_to_len requires padding, but the caller's variable is not mutated. The symbol length L used for encoding is computed with len_selfies(selfies), and tokenization is performed with split_selfies(selfies).", "default": ""}, "vocab_stoi": {"type": "dict", "additionalProperties": {"type": "integer"}, "description": "A mapping from SELFIES symbols to integer indices. Indices must be integers in the range 0..(len(vocab_stoi)-1), because one-hot vectors are created with length equal to len(vocab_stoi) and the integer indices are used to set a single 1 in those vectors. The mapping is expected (by convention and practical use) to be non-negative and contiguous starting at 0, so that models and indexing behave predictably; if pad_to_len is used, the special padding symbol \"[nop]\" must be present as a key in this dictionary. The function does not modify this dictionary.", "default": ""}, "pad_to_len": {"type": "integer", "description": "If greater than the symbol length L of the input (computed by len_selfies), the function pads the SELFIES string on the right with the padding symbol \"[nop]\" repeated (pad_to_len - L) times before encoding. If pad_to_len is less than or equal to L (including the default -1), no padding is added. Padding is commonly used in batch processing and sequence models to produce fixed-length encodings across examples. Side effect: only a local copy of the SELFIES string is padded; no external state is changed.", "default": -1}, "enc_type": {"type": "string", "description": "Determines the encoding(s) returned. Accepted values are \"label\", \"one_hot\", and \"both\". \"label\" returns the integer label encoding (a list of integers of length L or pad_to_len if padded). \"one_hot\" returns the one-hot encoding (a list of L lists, each inner list of length len(vocab_stoi) with a single 1 at the index given by vocab_stoi). \"both\" returns a tuple (label_encoding, one_hot_encoding). Default is \"both\". If an invalid value is supplied, the function raises ValueError.", "default": "both"}}, "required": ["selfies", "vocab_stoi", "enc_type", "pad_to_len"], "type": "any"}}, "type": "function"}], "query": "We’re assembling a mixed-quality microbatch of SELFIES from two synthesis replicates for a sequence model input pipeline. Convert each replicate’s SELFIES into numeric model inputs using that replicate’s `vocab_stoi`. Only attempt encoding for replicates whose tokenization uses exclusively symbols present in their own `vocab_stoi`. For accepted replicates, generate BOTH integer label encoding and one-hot encoding, and pad/truncate to a fixed length equal to 2×(number of SELFIES symbols in that replicate), using the explicit padding token \"[nop]\" from that replicate’s vocabulary.\n\nReplicate A: SELFIES \"[C][O][C]\" with vocab_stoi {\"[nop]\":0, \"[C]\":1, \"[O]\":2, \"[=O]\":3}.\nReplicate B: SELFIES \"[C][C][O][H]\" with vocab_stoi {\"[C]\":0, \"[O]\":1, \"[H]\":2, \"[nop]\":3}.", "answers": "[{\"name\":\"selfies_utils_encoding_utils_selfies_to_encoding\",\"arguments\":{\"selfies\":\"[C][O][C]\",\"vocab_stoi\":{\"[nop]\":0,\"[C]\":1,\"[O]\":2,\"[=O]\":3},\"pad_to_len\":6,\"enc_type\":\"both\"}},{\"name\":\"selfies_utils_encoding_utils_selfies_to_encoding\",\"arguments\":{\"selfies\":\"[C][C][O][H]\",\"vocab_stoi\":{\"[C]\":0,\"[O]\":1,\"[H]\":2,\"[nop]\":3},\"pad_to_len\":8,\"enc_type\":\"both\"}}]"}
{"func_name": "selfies_utils_matching_utils_find_perfect_matching", "func_desc": "Finds a perfect matching for an undirected graph (without self-loops) and returns a node-to-partner mapping or None if no perfect matching exists.\n    \n    This utility is part of the selfies.utils.matching_utils module and is intended for use inside the SELFIES codebase where algorithms operate on graph representations of molecules (for example, molecular connectivity graphs used when manipulating rings, branches, or during translation between SMILES and SELFIES). The function accepts an adjacency-list representation of an undirected graph and attempts to pair every vertex with exactly one neighbor so that each vertex appears in exactly one matched pair. The implementation first constructs a maximal greedy matching for efficiency and then repeatedly searches for augmenting paths and flips them until either a perfect matching covering all vertices is found or no augmenting path exists (in which case the function reports failure). The input graph must not contain self-loops; edges are expected to be represented by integer indices in the adjacency lists. The returned matching is a compact representation suitable for downstream graph algorithms in SELFIES that require disjoint pairings of nodes.", "tools": [{"function": {"description": "Finds a perfect matching for an undirected graph (without self-loops) and returns a node-to-partner mapping or None if no perfect matching exists.\n\nThis utility is part of the selfies.utils.matching_utils module and is intended for use inside the SELFIES codebase where algorithms operate on graph representations of molecules (for example, molecular connectivity graphs used when manipulating rings, branches, or during translation between SMILES and SELFIES). The function accepts an adjacency-list representation of an undirected graph and attempts to pair every vertex with exactly one neighbor so that each vertex appears in exactly one matched pair. The implementation first constructs a maximal greedy matching for efficiency and then repeatedly searches for augmenting paths and flips them until either a perfect matching covering all vertices is found or no augmenting path exists (in which case the function reports failure). The input graph must not contain self-loops; edges are expected to be represented by integer indices in the adjacency lists. The returned matching is a compact representation suitable for downstream graph algorithms in SELFIES that require disjoint pairings of nodes.", "name": "selfies_utils_matching_utils_find_perfect_matching", "parameters": {"properties": {"graph": {"type": "array", "items": {"type": "array", "items": {"type": "integer"}}, "description": "An adjacency list representing an undirected graph without self-loops. The outer list has length n, where n is the number of nodes; each inner list contains integer indices of neighbors of that node. Node indices are integers in the range [0, n-1]. Because the graph is undirected, each edge should appear in both endpoints' adjacency lists (for example, if j appears in graph[i], then i should appear in graph[j]). The function does not modify the input adjacency lists.", "default": ""}}, "required": ["graph"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing ring-closure pairing on a mixed batch of candidate 6-atom connectivity graphs extracted from a noisy intermediate in a SELFIES round-trip (undirected adjacency lists; reject any candidate that violates basic molecular-graph sanity for this step: no self-loops, symmetric neighbor lists, and every vertex must have degree 2 or 3 as expected for ring atoms with at most one extra chord/substituent contact). From the candidates that pass, attempt perfect matching and return the node-to-partner map (or None if a candidate still cannot be perfectly matched).\n\nCandidate set (encoded exactly):\nA) 0:[1,5,3], 1:[0,2], 2:[1,3], 3:[2,4,0], 4:[3,5], 5:[4,0]\nB) 0:[1,5], 1:[0,2], 2:[1,3], 3:[2,4], 4:[3,5], 5:[4,0]\nC) 0:[0,1,5], 1:[0,2], 2:[1,3], 3:[2,4], 4:[3,5], 5:[4,0]\nD) 0:[1,5], 1:[0,2], 2:[1,3], 3:[2,4], 4:[3,5], 5:[0,4,2]\nE) 0:[1,5,3], 1:[0,2], 2:[1,3,4], 3:[2,4,0], 4:[3,5,2], 5:[4,0]", "answers": "[{\"name\":\"selfies_utils_matching_utils_find_perfect_matching\",\"arguments\":{\"graph\":[[1,5,3],[0,2],[1,3],[2,4,0],[3,5],[4,0]]}},{\"name\":\"selfies_utils_matching_utils_find_perfect_matching\",\"arguments\":{\"graph\":[[1,5],[0,2],[1,3],[2,4],[3,5],[4,0]]}},{\"name\":\"selfies_utils_matching_utils_find_perfect_matching\",\"arguments\":{\"graph\":[[1,5],[0,2],[1,3],[2,4],[3,5],[0,4,2]]}}]"}
{"func_name": "selfies_utils_smiles_utils_smiles_to_atom", "func_desc": "selfies.utils.smiles_utils.smiles_to_atom parses a single SMILES atom token and returns the corresponding selfies.mol_graph.Atom object used by the SELFIES translation pipeline.", "tools": [{"function": {"description": "selfies.utils.smiles_utils.smiles_to_atom parses a single SMILES atom token and returns the corresponding selfies.mol_graph.Atom object used by the SELFIES translation pipeline.\n", "name": "selfies_utils_smiles_utils_smiles_to_atom", "parameters": {"properties": {"atom_symbol": {"type": "string", "description": "A SMILES atom symbol token to parse. This is a single SMILES atom string such as an unbracketed organic atom (\"C\", \"n\", \"O\"), an unbracketed aromatic atom (lowercase like \"c\", \"n\"), or a bracketed atom token (for example \"[13CH@H+]\", \"[O-]\", \"[C@@H]\"). In the SELFIES/SMILES translation context (see the SELFIES README), this function is used by encoder/decoder utilities to convert SMILES atom tokens into the internal Atom representation for constructing or analyzing molecular graphs. The function expects a non-empty Python str containing exactly one SMILES atom token.", "default": ""}}, "required": ["atom_symbol"], "type": "any"}}, "type": "function"}], "query": "We’re triaging a small, messy stream of bracketed SMILES atom tokens emitted by two LC–MS/MS cohorts (technical replicate A and B) before SELFIES SMILES→MolGraph ingestion. For each cohort, parse only those atom tokens that (i) explicitly specify an isotope label, (ii) are chiral (contain '@'), and (iii) carry a net positive charge. Convert every token meeting all three criteria into the internal Atom object via `selfies.utils.smiles_utils.smiles_to_atom`, preserving replicate independence.\n\nCohort A tokens: [\"[13CH@H+]\", \"[CH4]\", \"[2H]\", \"[13CH@H]\", \"[Na+]\", \"[13CH@@H-]\"]\nCohort B tokens: [\"[13CH@H+]\", \"[12CH@H+]\", \"[O-]\", \"[13CH@H+]\", \"C\", \"[13C]\"]", "answers": "[{\"name\":\"selfies_utils_smiles_utils_smiles_to_atom\",\"arguments\":{\"atom_symbol\":\"[13CH@H+]\"}},{\"name\":\"selfies_utils_smiles_utils_smiles_to_atom\",\"arguments\":{\"atom_symbol\":\"[13CH@H+]\"}},{\"name\":\"selfies_utils_smiles_utils_smiles_to_atom\",\"arguments\":{\"atom_symbol\":\"[13CH@H+]\"}}]"}
{"func_name": "selfies_utils_smiles_utils_smiles_to_mol", "func_desc": "Reads a molecular graph representation from a SMILES string and returns a MolecularGraph suitable for downstream SELFIES operations (encoding, decoding, attribution). This function is a deterministic parser that tokenizes the input SMILES, iteratively derives atoms/bonds/structural features by repeatedly calling internal parsing logic, and constructs a selfies.mol_graph.MolecularGraph object that represents the molecular graph implied by the SMILES. In the SELFIES/SMILES workflow described in the project README, this MolecularGraph is the canonical in-memory graph representation used by higher-level functions (for example, the SELFIES encoder/decoder and attribution utilities).", "tools": [{"function": {"description": "Reads a molecular graph representation from a SMILES string and returns a MolecularGraph suitable for downstream SELFIES operations (encoding, decoding, attribution). This function is a deterministic parser that tokenizes the input SMILES, iteratively derives atoms/bonds/structural features by repeatedly calling internal parsing logic, and constructs a selfies.mol_graph.MolecularGraph object that represents the molecular graph implied by the SMILES. In the SELFIES/SMILES workflow described in the project README, this MolecularGraph is the canonical in-memory graph representation used by higher-level functions (for example, the SELFIES encoder/decoder and attribution utilities).\n", "name": "selfies_utils_smiles_utils_smiles_to_mol", "parameters": {"properties": {"smiles": {"type": "string", "description": "The input SMILES string to parse. This is the standard linear textual representation of a molecule (symbols, branches, ring closures, bond orders, etc.). The function does not modify this string. An empty string is considered invalid and will trigger a SMILESParserError with a message indicating \"empty SMILES\" and a position of 0. The SMILES must be syntactically valid for the tokenizer and the parsing routines to succeed.", "default": ""}, "attributable": {"type": "boolean", "description": "Flag indicating whether the returned MolecularGraph should include attribution bookkeeping structures. When True, the MolecularGraph will be constructed with attribution-capable internals (to record mappings from input tokens to graph elements), which enables downstream attribution queries (for example, tracing which input tokens led to particular atoms or SMILES tokens in translation). When False, the MolecularGraph will omit attribution information to reduce memory/overhead. This parameter corresponds directly to the attributable argument passed to the MolecularGraph constructor and has no default in the function signature.", "default": ""}}, "required": ["smiles", "attributable"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the SELFIES token-to-atom attribution setup on a messy mini-cohort of SMILES that includes technical replicates, charged/salt-like entries, and mixtures. Construct MolecularGraph objects only for neutral, single-component SMILES that contain at least one ring closure digit (to ensure nontrivial graph topology). Enable attribution tracking for every accepted structure. Treat technical replicates as independent parses by re-parsing any SMILES string that appears more than once in the raw batch.\n\nRaw batch (as received from ELN export):\n[\n  \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\n  \"CC(=O)OC1=CC=CC=C1C(=O)O\",\n  \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\n  \"C[C@H](N)C(=O)O\",\n  \"[Na+].[O-]C(=O)C1=CC=CC=C1\",\n  \"CCO.CN\"\n]\n\nParse the qualifying structures into the canonical in-memory MolecularGraph representation with attribution enabled, preserving replicate parses where applicable.", "answers": "[{\"name\":\"selfies_utils_smiles_utils_smiles_to_mol\",\"arguments\":{\"smiles\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"attributable\":true}},{\"name\":\"selfies_utils_smiles_utils_smiles_to_mol\",\"arguments\":{\"smiles\":\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"attributable\":true}},{\"name\":\"selfies_utils_smiles_utils_smiles_to_mol\",\"arguments\":{\"smiles\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"attributable\":true}}]"}
{"func_name": "spikeinterface_core_core_tools_check_paths_relative", "func_desc": "spikeinterface.core.core_tools.check_paths_relative checks whether all filesystem paths contained in a dictionary that describes a BaseExtractor can be converted to relative paths with respect to a given folder.\n    \n    This function is used in the SpikeInterface framework (a unified framework for spike sorting) to decide if a dataset description (for example the dict returned by BaseExtractor.to_dict()) can be made portable by rewriting absolute file paths as paths relative to a chosen folder. It examines each path discovered in input_dict (these typically refer to recording files, probe files, or other data artifacts referenced by an extractor) and returns True only if every path can be expressed relative to relative_folder according to the same rules used by the library (URL and remote paths are excluded, Windows drive mismatches prevent relativity, and the Path.relative_to operation must succeed). The function performs checks such as string-based URL detection (\"http\" substring), remote-path detection via is_path_remote, drive equality for Windows paths (including UNC semantics where the host/share is treated as the drive), and a final attempt to compute a relative path using the library's _relative_to helper. The function does not modify files or the input_dict.", "tools": [{"function": {"description": "spikeinterface.core.core_tools.check_paths_relative checks whether all filesystem paths contained in a dictionary that describes a BaseExtractor can be converted to relative paths with respect to a given folder.\n\nThis function is used in the SpikeInterface framework (a unified framework for spike sorting) to decide if a dataset description (for example the dict returned by BaseExtractor.to_dict()) can be made portable by rewriting absolute file paths as paths relative to a chosen folder. It examines each path discovered in input_dict (these typically refer to recording files, probe files, or other data artifacts referenced by an extractor) and returns True only if every path can be expressed relative to relative_folder according to the same rules used by the library (URL and remote paths are excluded, Windows drive mismatches prevent relativity, and the Path.relative_to operation must succeed). The function performs checks such as string-based URL detection (\"http\" substring), remote-path detection via is_path_remote, drive equality for Windows paths (including UNC semantics where the host/share is treated as the drive), and a final attempt to compute a relative path using the library's _relative_to helper. The function does not modify files or the input_dict.", "name": "spikeinterface_core_core_tools_check_paths_relative", "parameters": {"properties": {"input_dict": {"type": "any", "description": "A dictionary describing a BaseExtractor obtained by BaseExtractor.to_dict(). In the SpikeInterface domain this dict encodes metadata and file references for recordings or sortings; the function will extract any file-system-like paths from this dict (via the internal _get_paths_list) and evaluate whether each such path can be made relative to relative_folder for the purpose of making the extractor description portable across environments.", "default": ""}, "relative_folder": {"type": "string", "description": "The candidate base folder to which paths should be made relative. The function will convert this argument to a Path and call resolve().absolute() on it before testing. In practice this is the folder you intend to use as the root of a portable dataset (for example a project or repository folder).", "default": ""}}, "required": ["input_dict", "relative_folder"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a multi-site Neuropixels data release and need a portability audit that mimics what happens when collaborators dump mixed provenance artifacts into a single SpikeInterface extractor descriptor. Below are two raw BaseExtractor-like dicts that may contain local filesystem paths, remote/URL references, and Windows-style paths embedded at different nesting levels (kwargs/properties/annotations). For each dict, run SpikeInterface’s check_paths_relative against a project root chosen as follows: use the closest common ancestor of all *local* filesystem paths that are actually eligible for relativization under SpikeInterface rules (i.e., ignore any path-like strings that are URLs or remote references; if Windows paths are present, the root must be on the same drive/UNC share as the eligible paths). Then verify whether every eligible filesystem path found anywhere in the dict can be rewritten relative to that chosen root.\n\nDataset A (mixed artifacts, Linux paths + remote + URL):\n{\n  \"class\": \"BinaryRecordingExtractor\",\n  \"module\": \"spikeinterface.extractors\",\n  \"version\": \"0.99.0\",\n  \"kwargs\": {\n    \"file_paths\": [\n      \"/home/alex/projects/spike_portable/data/session_01/recording.dat\",\n      \"https://example.org/public/recording_preview.dat\"\n    ],\n    \"dtype\": \"int16\",\n    \"num_channels\": 384,\n    \"sampling_frequency\": 30000.0,\n    \"probe_file\": \"/home/alex/projects/spike_portable/probes/neuropixels.prb\"\n  },\n  \"properties\": {\n    \"aux\": {\n      \"calibration\": \"/home/alex/projects/spike_portable/data/session_01/calibration.json\"\n    }\n  },\n  \"annotations\": {\n    \"session_id\": \"session_01\",\n    \"raw_source\": \"s3://lab-bucket/session_01/recording.dat\"\n  }\n}\n\nDataset B (Windows workstation export + nested path fields):\n{\n  \"class\": \"BinaryRecordingExtractor\",\n  \"module\": \"spikeinterface.extractors\",\n  \"version\": \"0.101.0\",\n  \"kwargs\": {\n    \"file_paths\": [\n      \"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\data\\\\recording.bin\"\n    ],\n    \"sampling_frequency\": 30000.0,\n    \"num_channels\": 384,\n    \"dtype\": \"int16\",\n    \"time_axis\": 0\n  },\n  \"properties\": {\n    \"probe_file\": \"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\probes\\\\neuropixels_384ch.prb\",\n    \"metadata_file\": \"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\data\\\\recording_meta.json\",\n    \"export\": {\n      \"notes\": \"http://intranet.local/notes/NPX_2024_11_03_A\"\n    }\n  },\n  \"annotations\": {\n    \"session_id\": \"NPX_2024_11_03_A\"\n  }\n}\n\nReturn the two boolean results (one per dataset) using check_paths_relative with the computed project root per dataset.", "answers": "[{\"name\":\"spikeinterface_core_core_tools_check_paths_relative\",\"arguments\":{\"input_dict\":{\"class\":\"BinaryRecordingExtractor\",\"module\":\"spikeinterface.extractors\",\"version\":\"0.99.0\",\"kwargs\":{\"file_paths\":[\"/home/alex/projects/spike_portable/data/session_01/recording.dat\",\"https://example.org/public/recording_preview.dat\"],\"dtype\":\"int16\",\"num_channels\":384,\"sampling_frequency\":30000.0,\"probe_file\":\"/home/alex/projects/spike_portable/probes/neuropixels.prb\"},\"properties\":{\"aux\":{\"calibration\":\"/home/alex/projects/spike_portable/data/session_01/calibration.json\"}},\"annotations\":{\"session_id\":\"session_01\",\"raw_source\":\"s3://lab-bucket/session_01/recording.dat\"}},\"relative_folder\":\"/home/alex/projects/spike_portable\"}},{\"name\":\"spikeinterface_core_core_tools_check_paths_relative\",\"arguments\":{\"input_dict\":{\"class\":\"BinaryRecordingExtractor\",\"module\":\"spikeinterface.extractors\",\"version\":\"0.101.0\",\"kwargs\":{\"file_paths\":[\"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\data\\\\recording.bin\"],\"sampling_frequency\":30000.0,\"num_channels\":384,\"dtype\":\"int16\",\"time_axis\":0},\"properties\":{\"probe_file\":\"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\probes\\\\neuropixels_384ch.prb\",\"metadata_file\":\"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\\\\data\\\\recording_meta.json\",\"export\":{\"notes\":\"http://intranet.local/notes/NPX_2024_11_03_A\"}},\"annotations\":{\"session_id\":\"NPX_2024_11_03_A\"}},\"relative_folder\":\"C:\\\\Users\\\\alex\\\\projects\\\\neuropixels_session1\"}}]"}
{"func_name": "spikeinterface_core_core_tools_convert_string_to_bytes", "func_desc": "Convert a memory size string to the corresponding number of bytes.\n    \n    This function is spikeinterface.core.core_tools.convert_string_to_bytes and is used within the SpikeInterface framework to translate human-readable memory specifications (commonly provided when configuring caching sizes, chunking windows, memory limits for sorting backends, exporters and other memory-sensitive components) into an integer byte count that the codebase can use for allocation, comparisons, and configuration.", "tools": [{"function": {"description": "Convert a memory size string to the corresponding number of bytes.\n\nThis function is spikeinterface.core.core_tools.convert_string_to_bytes and is used within the SpikeInterface framework to translate human-readable memory specifications (commonly provided when configuring caching sizes, chunking windows, memory limits for sorting backends, exporters and other memory-sensitive components) into an integer byte count that the codebase can use for allocation, comparisons, and configuration.", "name": "spikeinterface_core_core_tools_convert_string_to_bytes", "parameters": {"properties": {"memory_string": {"type": "string", "description": "Memory size string containing a numeric value immediately followed by a unit suffix. The function checks the last two characters first and, if that two-character suffix is not present in the module's internal _exponents mapping, falls back to the last single character. Examples of acceptable inputs (based on existing usage in the codebase) include \"1G\", \"512Mi\", and \"2T\". The numeric portion may be an integer or a floating-point value (for example, \"1.5G\"). The string must not be empty and must include a recognized suffix; the function does not strip leading or trailing whitespace, so callers should pass a trimmed string. In the SpikeInterface domain this argument is typically supplied by user configuration or internal helpers when specifying memory limits for preprocessing, sorting, or export steps.", "default": ""}}, "required": ["memory_string"], "type": "any"}}, "type": "function"}], "query": "We’re profiling a heterogeneous SpikeInterface pipeline where cache limits are coming from mixed provenance (user CLI flags, lab notebook defaults, and container env vars). Given the following raw memory-cap strings for two cache layers:\n\nwaveform_snippet_cache_candidates = [\"1.5G\", \"1536Mi\", \"1.50Gi\", \"0\", \"-2G\", \"1.5GB\", \"1.5 Gi\", \"1500M\"]\nrecording_cache_candidates = [\"1.5Gi\", \"1600Mi\", \"1.5G\", \"NaN\", \"\", \"2Gi\", \"1024Ki\", \"1.5 GiB\"]\n\nNormalize only the entries that are parseable by SpikeInterface’s memory-string convention (i.e., a positive scalar followed immediately by a valid unit suffix, with no embedded whitespace), and for each cache layer compute the byte count for the smallest valid candidate (tightest cap). Convert those selected memory strings to integer byte values for the low-level config.", "answers": "[{\"name\":\"spikeinterface_core_core_tools_convert_string_to_bytes\",\"arguments\":{\"memory_string\":\"1500M\"}},{\"name\":\"spikeinterface_core_core_tools_convert_string_to_bytes\",\"arguments\":{\"memory_string\":\"1024Ki\"}}]"}
{"func_name": "spikeinterface_core_core_tools_extractor_dict_iterator", "func_desc": "Iterator for recursive traversal of a dictionary produced by extractors in the SpikeInterface framework. This function explores the nested mapping/list structure returned by BaseExtractor.to_dict() and yields a sequence of extractor_dict_element named tuples that describe every leaf value found in the structure. Each yielded element includes the actual leaf value, a name (the last dict key or the propagated list name), and an access_path tuple that records the sequence of dict keys and list indices required to reach that value from the top-level extractor_dict. In the SpikeInterface domain this is used to inspect, serialize, or process all scalar or non-dict/list entries stored by an extractor (for example metadata, numeric parameters, or paths) without copying the underlying data.", "tools": [{"function": {"description": "Iterator for recursive traversal of a dictionary produced by extractors in the SpikeInterface framework. This function explores the nested mapping/list structure returned by BaseExtractor.to_dict() and yields a sequence of extractor_dict_element named tuples that describe every leaf value found in the structure. Each yielded element includes the actual leaf value, a name (the last dict key or the propagated list name), and an access_path tuple that records the sequence of dict keys and list indices required to reach that value from the top-level extractor_dict. In the SpikeInterface domain this is used to inspect, serialize, or process all scalar or non-dict/list entries stored by an extractor (for example metadata, numeric parameters, or paths) without copying the underlying data.\n", "name": "spikeinterface_core_core_tools_extractor_dict_iterator", "parameters": {"properties": {"extractor_dict": {"type": "any", "description": "The input mapping to traverse. In SpikeInterface this is expected to be the dictionary returned by BaseExtractor.to_dict(), i.e., a nested structure of dicts and lists representing an Extractor's serializable state. The function treats dict instances as nested mappings and list instances as ordered sequences whose indices are appended to the access_path. Non-dict, non-list objects are treated as leaves and yielded as values. Keys in dicts are appended to access_path as-is (typically strings used by Extractor.to_dict()); list indices are appended as integers. The function does not modify extractor_dict.", "default": ""}}, "required": ["extractor_dict"], "type": "any"}}, "type": "function"}], "query": "We’re preparing two SpikeInterface `to_dict()` exports for long-term provenance tracking, but we only want to index *archival-stable* leaves (values that won’t change across machines or directory layouts). For each extractor export below, recursively traverse the full nested dict/list structure and emit leaf descriptors (leaf value, leaf name, access_path). Apply a stability sieve while traversing: include only leaves whose value is either (a) a number (int/float) used for acquisition/probe geometry, or (b) a short categorical string (e.g., class/module/version/dtype/manufacturer). Exclude leaves that are filesystem paths or other location-specific identifiers by rule (e.g., anything that looks like an absolute path starting with “/”, or date/session/subject IDs). Run this audit on these two cohort exports: (A) {\"class\":\"RecordingExtractor\",\"module\":\"spikeinterface.extractors\",\"version\":\"0.99.0\",\"kwargs\":{\"file_path\":\"/data/session_01/rec.dat\",\"sampling_frequency\":30000.0,\"num_channels\":64,\"dtype\":\"int16\"},\"annotations\":{\"session_id\":\"S01\",\"subject\":{\"id\":\"mouse_17\",\"age_days\":120},\"probe\":{\"manufacturer\":\"IMEC\",\"contacts\":[{\"x\":0.0,\"y\":0.0},{\"x\":0.0,\"y\":20.0}]}}} and (B) {\"class\":\"NumpyRecording\",\"version\":\"0.101.0\",\"kwargs\":{\"sampling_frequency\":30000.0,\"dtype\":\"int16\",\"file_paths\":[\"/data/rec1.bin\",\"/data/rec2.bin\"],\"channel_ids\":[0,1,2,3]},\"annotations\":{\"subject_id\":\"ratA\",\"session_date\":\"2025-05-14\",\"probe\":{\"manufacturer\":\"neuronexus\",\"contact_spacing_um\":20.0}}}. We need the traversal iterator output for the above sieve (for logging only; don’t copy underlying data).", "answers": "[{\"name\":\"spikeinterface_core_core_tools_extractor_dict_iterator\",\"arguments\":{\"extractor_dict\":{\"class\":\"RecordingExtractor\",\"module\":\"spikeinterface.extractors\",\"version\":\"0.99.0\",\"kwargs\":{\"file_path\":\"/data/session_01/rec.dat\",\"sampling_frequency\":30000.0,\"num_channels\":64,\"dtype\":\"int16\"},\"annotations\":{\"session_id\":\"S01\",\"subject\":{\"id\":\"mouse_17\",\"age_days\":120},\"probe\":{\"manufacturer\":\"IMEC\",\"contacts\":[{\"x\":0.0,\"y\":0.0},{\"x\":0.0,\"y\":20.0}]}}}}},{\"name\":\"spikeinterface_core_core_tools_extractor_dict_iterator\",\"arguments\":{\"extractor_dict\":{\"class\":\"NumpyRecording\",\"version\":\"0.101.0\",\"kwargs\":{\"sampling_frequency\":30000.0,\"dtype\":\"int16\",\"file_paths\":[\"/data/rec1.bin\",\"/data/rec2.bin\"],\"channel_ids\":[0,1,2,3]},\"annotations\":{\"subject_id\":\"ratA\",\"session_date\":\"2025-05-14\",\"probe\":{\"manufacturer\":\"neuronexus\",\"contact_spacing_um\":20.0}}}}}]"}
{"func_name": "spikeinterface_core_generate_generate_single_fake_waveform", "func_desc": "generate a single-channel synthetic spike waveform composed of three exponential phases (depolarization, repolarization, recovery), then apply Gaussian smoothing and align the negative peak to the sample index corresponding to ms_before. This helper is intended for use in SpikeInterface (a unified framework for spike sorting) to create simple, repeatable test waveforms for waveform extraction, sorter testing, tutorials, and unit tests where a realistic but parameterizable extracellular spike shape is required.", "tools": [{"function": {"description": "generate a single-channel synthetic spike waveform composed of three exponential phases (depolarization, repolarization, recovery), then apply Gaussian smoothing and align the negative peak to the sample index corresponding to ms_before. This helper is intended for use in SpikeInterface (a unified framework for spike sorting) to create simple, repeatable test waveforms for waveform extraction, sorter testing, tutorials, and unit tests where a realistic but parameterizable extracellular spike shape is required.\n", "name": "spikeinterface_core_generate_generate_single_fake_waveform", "parameters": {"properties": {"sampling_frequency": {"type": "float", "nullable": true, "description": "Sampling frequency in Hz used to convert time in milliseconds to integer sample counts. The function multiplies this value by ms_before and ms_after to compute the number of samples before and after the spike peak (nbefore and nafter). Although the signature default is None, a numeric float must be provided in practice; passing None or a non-numeric value will cause a TypeError when the function attempts arithmetic with sampling_frequency. This parameter determines waveform temporal resolution and directly affects all derived integer sample counts (nbefore, nafter, ndepo, nrepol, nrefac).", "default": null}, "ms_before": {"type": "float", "description": "Duration in milliseconds of the waveform segment preceding the spike peak (default 1.0 ms). This is used with sampling_frequency to compute nbefore = int(sampling_frequency * ms_before / 1000.0). The function asserts that ms_before > depolarization_ms and that the computed ndepo is less than nafter; if these conditions are not met an AssertionError is raised, indicating the provided ms_before is too short for the requested depolarization phase.", "default": 1.0}, "ms_after": {"type": "float", "description": "Duration in milliseconds of the waveform segment following the spike peak (default 3.0 ms). Used with sampling_frequency to compute nafter = int(sampling_frequency * ms_after / 1000.0). The function asserts that ms_after > depolarization_ms + repolarization_ms and later asserts that the sum of the repolarization and recovery sample counts is less than nafter; violating these constraints raises an AssertionError with messages such as \"ms_after is too short\".", "default": 3.0}, "negative_amplitude": {"type": "float", "description": "Peak negative amplitude (most hyperpolarized point) of the generated waveform, expressed in the same arbitrary amplitude units used by downstream processing (default -1). This value is used as the start or end amplitude for the exponential segments that create the negative deflection and influences the overall waveform scaling prior to smoothing and normalization.", "default": -1}, "positive_amplitude": {"type": "float", "description": "Positive (depolarizing) overshoot amplitude following the negative peak (default 0.15). This sets the intermediate positive plateau amplitude used by the repolarization and recovery exponentials and affects the waveform shape used during sorting algorithm tests or visualization.", "default": 0.15}, "depolarization_ms": {"type": "float", "description": "Duration in milliseconds allocated to the depolarization exponential that forms the leading (negative) phase of the spike (default 0.1 ms). The code converts this into ndepo = int(depolarization_ms * sampling_frequency / 1000.0) samples and uses an exponential growth helper to interpolate between baseline and negative_amplitude. If depolarization_ms is longer than ms_before (after conversion), an AssertionError is raised.", "default": 0.1}, "repolarization_ms": {"type": "float", "description": "Duration in milliseconds allocated to the repolarization exponential that transitions from the negative peak toward positive_amplitude (default 0.6 ms). Converted to nrepol samples and used with an exponential growth helper. The function requires ms_after > depolarization_ms + repolarization_ms at the start and will assert that nrefac + nrepol < nafter later to ensure there is room for the recovery phase.", "default": 0.6}, "recovery_ms": {"type": "float", "description": "Duration in milliseconds allocated to the recovery exponential that returns the waveform from positive_amplitude back to baseline (default 1.1 ms). Converted to nrefac = int(recovery_ms * sampling_frequency / 1000.0) samples and filled using an exponential growth helper. If the sum of nrepol and nrefac is not strictly less than nafter an AssertionError is raised (\"ms_after is too short\").", "default": 1.1}, "smooth_ms": {"type": "float", "description": "Width parameter in milliseconds used to build a Gaussian smoothing kernel (default 0.05 ms). Internally converted to kernel size in samples as smooth_size = smooth_ms / (1 / sampling_frequency * 1000.0), a symmetric kernel over bins = arange(-n, n+1) with n = int(smooth_size * 4). The waveform is convolved with this normalized Gaussian to reduce sharp transitions; after smoothing the waveform is renormalized so its absolute peak amplitude matches the pre-smoothing peak.", "default": 0.05}, "dtype": {"type": "string", "description": "NumPy dtype name used for the returned waveform array (default \"float32\"). The function constructs the working array with numpy.zeros(width, dtype=dtype) and returns an array of this dtype. The string must be a valid NumPy dtype name recognized by numpy.zeros.", "default": "float32"}}, "required": ["recovery_ms", "positive_amplitude", "smooth_ms", "depolarization_ms", "sampling_frequency", "repolarization_ms", "negative_amplitude", "ms_after", "dtype", "ms_before"], "type": "any"}}, "type": "function"}], "query": "We’re building a small but realistic SpikeInterface benchmarking bundle from a messy export of candidate “unit templates” coming from multiple labs. Each candidate has a reported extracellular trough (negative peak), a post-trough overshoot, and nominal phase time constants. Some entries are obvious artifacts (e.g., wrong sign trough, non-finite amplitudes, or physiologically implausible phase ordering).\n\nUse `sampling_frequency=30000` Hz for everything, generate *single-channel* synthetic spikes with the standard three-exponential morphology, Gaussian smoothing, and align the negative peak to the sample index corresponding to a `ms_before` pre-peak window.\n\nRaw candidate table (amplitudes may be in either µV or mV as reported; use the numeric values as given):\n- CAND_01: trough=-120, overshoot=+20, depol=0.12 ms, repol=0.70 ms, recovery=1.40 ms, smooth=0.06 ms, ms_before=1.2, ms_after=4.0, dtype=float32\n- CAND_02: trough=-0.8, overshoot=+0.2, depol=0.12 ms, repol=0.70 ms, recovery=1.30 ms, smooth=0.06 ms, ms_before=1.2, ms_after=3.5, dtype=float64\n- CAND_03: trough=-0.8, overshoot=+0.2, depol=0.15 ms, repol=0.70 ms, recovery=1.00 ms, smooth=0.06 ms, ms_before=1.2, ms_after=3.5, dtype=float64\n- CAND_04: trough=+85, overshoot=-10, depol=0.10 ms, repol=0.60 ms, recovery=1.20 ms, smooth=0.06 ms, ms_before=1.2, ms_after=4.0, dtype=float32\n- CAND_05: trough=-50, overshoot=+15, depol=0.20 ms, repol=0.15 ms, recovery=1.00 ms, smooth=0.06 ms, ms_before=1.2, ms_after=4.0, dtype=float32\n- CAND_06: trough=NaN, overshoot=+10, depol=0.12 ms, repol=0.70 ms, recovery=1.40 ms, smooth=0.06 ms, ms_before=1.2, ms_after=4.0, dtype=float32\n\nPipeline rules:\n1) Only emit waveforms for candidates that look like valid extracellular spikes: trough must be negative, overshoot must be non-negative, and all numeric fields must be finite.\n2) Enforce physiologically plausible phase ordering by requiring `depolarization_ms < repolarization_ms < recovery_ms`.\n3) Keep each candidate’s provided `ms_before`, `ms_after`, `smooth_ms`, and `dtype` unchanged.\n\nGenerate the resulting set of waveforms that pass the above QC rules.", "answers": "[{\"name\":\"spikeinterface_core_generate_generate_single_fake_waveform\",\"arguments\":{\"sampling_frequency\":30000.0,\"ms_before\":1.2,\"ms_after\":4.0,\"negative_amplitude\":-120.0,\"positive_amplitude\":20.0,\"depolarization_ms\":0.12,\"repolarization_ms\":0.7,\"recovery_ms\":1.4,\"smooth_ms\":0.06,\"dtype\":\"float32\"}},{\"name\":\"spikeinterface_core_generate_generate_single_fake_waveform\",\"arguments\":{\"sampling_frequency\":30000.0,\"ms_before\":1.2,\"ms_after\":3.5,\"negative_amplitude\":-0.8,\"positive_amplitude\":0.2,\"depolarization_ms\":0.12,\"repolarization_ms\":0.7,\"recovery_ms\":1.3,\"smooth_ms\":0.06,\"dtype\":\"float64\"}},{\"name\":\"spikeinterface_core_generate_generate_single_fake_waveform\",\"arguments\":{\"sampling_frequency\":30000.0,\"ms_before\":1.2,\"ms_after\":3.5,\"negative_amplitude\":-0.8,\"positive_amplitude\":0.2,\"depolarization_ms\":0.15,\"repolarization_ms\":0.7,\"recovery_ms\":1.0,\"smooth_ms\":0.06,\"dtype\":\"float64\"}}]"}
{"func_name": "spikeinterface_core_generate_generate_unit_locations", "func_desc": "spikeinterface.core.generate.generate_unit_locations: Generate random 3D unit locations for simulated neurons or test datasets, constrained by recording channel geometry and inter-unit distance rules.\n    \n    This function is part of the SpikeInterface framework used to create spatial layouts of neuronal units relative to extracellular recording channels for simulations, benchmarking, waveform extraction, quality-metric evaluation, and other spike-sorting workflows. The function samples num_units 3D coordinates (x, y, z) so that x and y lie within the bounding box of the provided channel_locations expanded by margin_um, z lies between minimum_z and maximum_z, and pairwise Euclidean distances between generated units meet a minimum_distance constraint when requested. Reproducible sampling is supported via an explicit random seed (seed). The function uses NumPy's Generator (numpy.random.default_rng) to draw uniform random values and an internal helper (_generate_multimodal) when distribution=\"multimodal\" to mimic layered (multimodal) y-axis distributions.", "tools": [{"function": {"description": "spikeinterface.core.generate.generate_unit_locations: Generate random 3D unit locations for simulated neurons or test datasets, constrained by recording channel geometry and inter-unit distance rules.\n\nThis function is part of the SpikeInterface framework used to create spatial layouts of neuronal units relative to extracellular recording channels for simulations, benchmarking, waveform extraction, quality-metric evaluation, and other spike-sorting workflows. The function samples num_units 3D coordinates (x, y, z) so that x and y lie within the bounding box of the provided channel_locations expanded by margin_um, z lies between minimum_z and maximum_z, and pairwise Euclidean distances between generated units meet a minimum_distance constraint when requested. Reproducible sampling is supported via an explicit random seed (seed). The function uses NumPy's Generator (numpy.random.default_rng) to draw uniform random values and an internal helper (_generate_multimodal) when distribution=\"multimodal\" to mimic layered (multimodal) y-axis distributions.", "name": "spikeinterface_core_generate_generate_unit_locations", "parameters": {"properties": {"num_units": {"type": "integer", "description": "Number of unit locations to generate. In the spike-sorting domain this corresponds to the number of simulated neurons whose spatial coordinates will be produced.", "default": ""}, "channel_locations": {"type": "array", "items": {"type": "float"}, "description": "2D array of shape (num_channels, 2) giving the (x, y) coordinates of each recording channel. These coordinates define the recording probe geometry and are used to compute the allowable x and y sampling ranges by taking the channel extents and expanding them by margin_um.", "default": ""}, "margin_um": {"type": "float", "description": "Margin in micrometers added to the minimum and maximum x and y channel coordinates when defining the sampling region for unit x and y coordinates. Increasing margin_um allows units to be placed beyond the outermost channels; default is 20.0.", "default": 20.0}, "minimum_z": {"type": "float", "description": "Minimum z coordinate (depth) for generated unit locations. This defines the lower bound of the third spatial dimension for units; default is 5.0.", "default": 5.0}, "maximum_z": {"type": "float", "description": "Maximum z coordinate (depth) for generated unit locations. This defines the upper bound of the third spatial dimension for units; default is 40.0.", "default": 40.0}, "minimum_distance": {"type": "float", "description": "Minimum allowable Euclidean distance between any two generated units in micrometers. If set to None, no inter-unit distance constraint is enforced. When set to a numeric value, the function iteratively resamples units that violate this constraint until a valid configuration is found or max_iteration is reached; default is 20.0.", "default": 20.0}, "max_iteration": {"type": "integer", "description": "Maximum number of iterative resampling attempts to satisfy the minimum_distance constraint. The algorithm recomputes pairwise Euclidean distances after each resampling pass and stops early if a solution is found; default is 100.", "default": 100}, "distance_strict": {"type": "boolean", "description": "If True and no configuration satisfying minimum_distance is found within max_iteration attempts, the function raises a ValueError; if False (default) it emits a warnings.warn message and returns the best configuration produced. Use True to enforce strict failure behavior in automated pipelines.", "default": false}, "distribution": {"type": "string", "description": "Sampling distribution for the y coordinate. Allowed values are \"uniform\" or \"multimodal\". \"uniform\" samples y uniformly across the allowed y-range. \"multimodal\" uses an internal helper to generate num_modes modes along the y axis to mimic layered unit distributions (e.g., cortical layers). When used with minimum_distance not None there is no guarantee of a perfectly multimodal outcome because units violating distance constraints are resampled and may fall between modes; default is \"uniform\".", "default": "uniform"}, "num_modes": {"type": "integer", "description": "When distribution=\"multimodal\", the number of modes (layers) to generate along the y axis. This controls how many peaks the internal multimodal generator will produce; default is 2.", "default": 2}, "seed": {"type": "integer", "nullable": true, "description": "Integer seed for numpy.random.default_rng to make sampling reproducible. If None (default), the RNG is not explicitly seeded and results are non-deterministic across runs.", "default": null}}, "required": ["num_units", "channel_locations", "seed", "minimum_z", "minimum_distance", "margin_um", "distribution", "maximum_z", "max_iteration", "distance_strict", "num_modes"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a spike-sorting benchmark under messy probe metadata conditions. For each probe cohort below, first sanitize the provided channel geometry by keeping only channel coordinates that are finite and unique (drop any duplicates and any entries with NaN/inf). Then run unit-location generation using a layered (multimodal) cortical placement along the y-axis with 3 modes and a hard minimum-distance constraint with up to 200 resampling iterations.\n\nParameterization must be derived from the cleaned geometry per cohort:\n- Set margin_um to 15 µm if the cleaned x-span (max_x - min_x) is ≤ 25 µm; otherwise set margin_um to 30 µm.\n- Use minimum_z = 10 µm for all cohorts.\n- Set maximum_z to 60 µm unless the cleaned y-extent (max_y - min_y) is ≥ 150 µm, in which case set maximum_z to 80 µm.\n- Keep distance_strict enabled.\n\nCohort A (single-shank, Neuropixels-like, but with a duplicated contact and one corrupted entry): channel_locations (x,y) list is:\n[(0,0),(0,20),(0,40),(0,60),(0,80),(0,100),(0,120),(0,140),(0,160),(0,180),(0,200),(0,220),(20,0),(20,20),(20,40),(20,60),(20,80),(20,100),(20,120),(20,140),(20,160),(20,180),(20,200),(20,220),(20,220),(nan,100)]. Generate 12 units with minimum_distance 35 µm and seed 1234.\n\nCohort B (4×4 grid test array, but one channel is duplicated and one has an invalid coordinate): channel_locations (x,y) list is:\n[(0,0),(40,0),(80,0),(120,0),(0,40),(40,40),(80,40),(120,40),(0,80),(40,80),(80,80),(120,80),(0,120),(40,120),(80,120),(120,120),(120,120),(inf,40)]. Generate 50 units with minimum_distance 25 µm and seed 12345.\n\nCohort C (16-channel linear probe, but with repeated entries and a corrupted y): channel_locations (x,y) list is:\n[(0,0),(0,10),(0,20),(0,30),(0,40),(0,50),(0,60),(0,70),(0,80),(0,90),(0,100),(0,110),(0,120),(0,130),(0,140),(0,150),(0,150),(0,nan)]. Generate 25 units with minimum_distance 25 µm and seed 12345.", "answers": "[{\"name\":\"spikeinterface_core_generate_generate_unit_locations\",\"arguments\":{\"num_units\":12,\"channel_locations\":[[0.0,0.0],[0.0,20.0],[0.0,40.0],[0.0,60.0],[0.0,80.0],[0.0,100.0],[0.0,120.0],[0.0,140.0],[0.0,160.0],[0.0,180.0],[0.0,200.0],[0.0,220.0],[20.0,0.0],[20.0,20.0],[20.0,40.0],[20.0,60.0],[20.0,80.0],[20.0,100.0],[20.0,120.0],[20.0,140.0],[20.0,160.0],[20.0,180.0],[20.0,200.0],[20.0,220.0]],\"margin_um\":15.0,\"minimum_z\":10.0,\"maximum_z\":80.0,\"minimum_distance\":35.0,\"max_iteration\":200,\"distance_strict\":true,\"distribution\":\"multimodal\",\"num_modes\":3,\"seed\":1234}},{\"name\":\"spikeinterface_core_generate_generate_unit_locations\",\"arguments\":{\"num_units\":50,\"channel_locations\":[[0.0,0.0],[40.0,0.0],[80.0,0.0],[120.0,0.0],[0.0,40.0],[40.0,40.0],[80.0,40.0],[120.0,40.0],[0.0,80.0],[40.0,80.0],[80.0,80.0],[120.0,80.0],[0.0,120.0],[40.0,120.0],[80.0,120.0],[120.0,120.0]],\"margin_um\":30.0,\"minimum_z\":10.0,\"maximum_z\":60.0,\"minimum_distance\":25.0,\"max_iteration\":200,\"distance_strict\":true,\"distribution\":\"multimodal\",\"num_modes\":3,\"seed\":12345}},{\"name\":\"spikeinterface_core_generate_generate_unit_locations\",\"arguments\":{\"num_units\":25,\"channel_locations\":[[0.0,0.0],[0.0,10.0],[0.0,20.0],[0.0,30.0],[0.0,40.0],[0.0,50.0],[0.0,60.0],[0.0,70.0],[0.0,80.0],[0.0,90.0],[0.0,100.0],[0.0,110.0],[0.0,120.0],[0.0,130.0],[0.0,140.0],[0.0,150.0]],\"margin_um\":15.0,\"minimum_z\":10.0,\"maximum_z\":80.0,\"minimum_distance\":25.0,\"max_iteration\":200,\"distance_strict\":true,\"distribution\":\"multimodal\",\"num_modes\":3,\"seed\":12345}}]"}
{"func_name": "spikeinterface_core_generate_synthesize_random_firings", "func_desc": "synthesize_random_firings generates a single-segment synthetic dataset of spike times and unit labels for use in spike-sorting development, testing, and benchmarking within the SpikeInterface framework. The function simulates num_units independent units firing with Poisson-like average rates over a recording of given duration and sampling frequency, enforces a refractory period in samples, optionally perturbs half of each unit's spikes to produce less-flat autocorrelograms, and returns concatenated, time-sorted spike times and their corresponding unit labels.", "tools": [{"function": {"description": "synthesize_random_firings generates a single-segment synthetic dataset of spike times and unit labels for use in spike-sorting development, testing, and benchmarking within the SpikeInterface framework. The function simulates num_units independent units firing with Poisson-like average rates over a recording of given duration and sampling frequency, enforces a refractory period in samples, optionally perturbs half of each unit's spikes to produce less-flat autocorrelograms, and returns concatenated, time-sorted spike times and their corresponding unit labels.\n", "name": "spikeinterface_core_generate_synthesize_random_firings", "parameters": {"properties": {"num_units": {"type": "integer", "description": "Number of units (neurons) to simulate. This controls the number of distinct integer labels produced (0 .. num_units-1). Default: 20. In practical spike-sorting workflows this represents the number of ground-truth units present in the synthetic recording segment.", "default": 20}, "sampling_frequency": {"type": "float", "description": "Sampling rate in Hz used to convert between time (seconds) and discrete sample indices. Default: 30000.0. Internally, duration is converted to a segment size in samples as int(sampling_frequency * duration) and the returned times are integer sample indices relative to the start of the segment.", "default": 30000.0}, "duration": {"type": "float", "description": "Duration of the simulated segment in seconds. Default: 60. This determines the total number of samples (segment length) and, together with firing_rates, the expected number of spikes per unit.", "default": 60}, "refractory_period_ms": {"type": "float", "description": "Minimal allowed inter-spike interval per unit expressed in milliseconds. This is converted internally to an integer refractory_sample = int(refractory_period_ms / 1000.0 * sampling_frequency) and used to remove spikes that violate the refractory constraint. Default: 4.0. Practically, increasing this value reduces occurrences of very close spike times for the same unit.", "default": 4.0}, "firing_rates": {"type": "array", "items": {"type": "float"}, "description": "Target firing rate(s) in Hz. If a single float is provided, all units will use that firing rate; if a list (or list-like) of floats is provided it should specify a rate per unit. The function expands or validates this input to produce a per-unit firing rate vector of length num_units using an internal helper. Default: 3.0. These rates are used to compute an expected integer number of spikes per unit as int(rate * duration), which is then sampled and possibly pruned by the refractory constraint.", "default": 3.0}, "add_shift_shuffle": {"type": "boolean", "description": "If True, for each unit approximately half of the generated spikes are shifted forward by a small positive integer sample offset to make the autocorrelogram less flat. The shift values are computed as shift = a + (b - a) * x**2 with a = refractory_sample and b = refractory_sample*20 and x drawn uniformly in [0,1). After shifting, spikes falling outside the segment are discarded. Default: False. This option is intended to introduce realistic clustering of some spike times while preserving the refractory-based pruning.", "default": false}, "seed": {"type": "integer", "nullable": true, "description": "Seed for the random number generator (passed to numpy.random.default_rng) to make the generation deterministic and reproducible. Default: None. When provided, the same seed will yield the same spike times and labels across runs; when None behavior is nondeterministic.", "default": null}}, "required": ["firing_rates", "seed", "duration", "sampling_frequency", "refractory_period_ms", "num_units", "add_shift_shuffle"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing spike-sorting robustness under heteroscedastic firing statistics using two single-segment synthetic cohorts at identical channel timing (20 kHz) and unit count (12), but with cohort-specific censoring and protocol branching.\n\nCommon setup: generate 12 independent units with the shift/shuffle perturbation enabled to avoid perfectly flat autocorrelograms.\n\nCohort A (unit-test, short): duration 10.0 s, seed 12345. Start from the raw candidate rate vector rA = [2.0, 3.5, 5.0, 1.0, 8.0, 4.0, 6.5, 2.5, 7.0, 3.0, 9.0, 1.5] Hz and apply a sieve before synthesis: keep only biologically plausible candidates within [1.5, 8.0] Hz. Use the resulting count as num_units and the filtered vector as firing_rates. Set refractory_period_ms using a branching rule based on the retained cohort’s median rate: if the median is at least 3.5 Hz use 2.5 ms, otherwise 3.0 ms.\n\nCohort B (stability, long): duration 120.0 s, seed 202401. Start from rB = [2.0, 3.5, 1.5, 6.0, 4.0, 2.5, 8.0, 5.0, 3.0, 7.0, 2.2, 4.8] Hz and apply a different sieve to model stationarity constraints: keep only rates within [2.0, 7.0] Hz. Use the resulting count as num_units and the filtered vector as firing_rates. Set refractory_period_ms using a branching rule based on the retained cohort’s median rate: if the median is at least 3.5 Hz use 3.5 ms, otherwise 4.0 ms.", "answers": "[{\"name\":\"spikeinterface_core_generate_synthesize_random_firings\",\"arguments\":{\"num_units\":9,\"sampling_frequency\":20000.0,\"duration\":10.0,\"refractory_period_ms\":2.5,\"firing_rates\":[2.0,3.5,5.0,8.0,4.0,6.5,2.5,7.0,3.0],\"add_shift_shuffle\":true,\"seed\":12345}},{\"name\":\"spikeinterface_core_generate_synthesize_random_firings\",\"arguments\":{\"num_units\":10,\"sampling_frequency\":20000.0,\"duration\":120.0,\"refractory_period_ms\":3.5,\"firing_rates\":[2.0,3.5,6.0,4.0,2.5,5.0,3.0,7.0,2.2,4.8],\"add_shift_shuffle\":true,\"seed\":202401}}]"}
{"func_name": "spikeinterface_core_globals_set_global_dataset_folder", "func_desc": "Set the global dataset folder used by SpikeInterface.\n    \n    This function records a filesystem location (provided as a string) as the global dataset folder for the running Python process. The supplied folder string is converted internally to a pathlib.Path and stored in the module-level global variable dataset_folder. The module-level boolean dataset_folder_set is also set to True. Many SpikeInterface utilities that manage example datasets, dataset downloaders, and I/O helpers consult this global dataset_folder as the default location to read from or write to; calling this function therefore changes the default storage/lookup location for those components across the entire SpikeInterface runtime.", "tools": [{"function": {"description": "Set the global dataset folder used by SpikeInterface.\n\nThis function records a filesystem location (provided as a string) as the global dataset folder for the running Python process. The supplied folder string is converted internally to a pathlib.Path and stored in the module-level global variable dataset_folder. The module-level boolean dataset_folder_set is also set to True. Many SpikeInterface utilities that manage example datasets, dataset downloaders, and I/O helpers consult this global dataset_folder as the default location to read from or write to; calling this function therefore changes the default storage/lookup location for those components across the entire SpikeInterface runtime.", "name": "spikeinterface_core_globals_set_global_dataset_folder", "parameters": {"properties": {"folder": {"type": "string", "description": "Filesystem path to use as the global dataset folder, expressed as a string. The path may be absolute or relative; it is converted internally to a pathlib.Path via Path(folder) and assigned to the global variable dataset_folder. This parameter is the canonical way, within SpikeInterface, to specify where recordings, sorting outputs, and example datasets should be stored or looked up by dataset-related helpers.", "default": ""}}, "required": ["folder"], "type": "any"}}, "type": "function"}], "query": "We’re running a SpikeInterface benchmarking session that alternates storage roots depending on the execution context detected from a small run manifest. Here is the manifest (order is chronological):\n\n1) {\"cohort\": \"repro\", \"storage_root\": \"/mnt/shared_ssd/spikeinterface_datasets\", \"mount_state\": \"rw\", \"free_gb\": 812, \"tag\": \"team-baseline\"}\n2) {\"cohort\": \"scratch\", \"storage_root\": \"/mnt/scratch/alex/spikeinterface_datasets\", \"mount_state\": \"rw\", \"free_gb\": 36, \"tag\": \"alex-iter\"}\n3) {\"cohort\": \"repro\", \"storage_root\": \"/mnt/shared_ssd/spikeinterface_datasets\", \"mount_state\": \"ro\", \"free_gb\": 790, \"tag\": \"post-backup\"}\n4) {\"cohort\": \"scratch\", \"storage_root\": \"/mnt/scratch/alex/spikeinterface_datasets\", \"mount_state\": \"rw\", \"free_gb\": 9, \"tag\": \"low-space\"}\n\nFor each entry that is safe for writing benchmark artifacts, set SpikeInterface’s process-wide global dataset folder to that entry’s storage_root at the moment that cohort begins, so that all subsequent example recordings, dataset downloads, and sorter outputs resolve under the correct root for that cohort segment.", "answers": "[{\"name\":\"spikeinterface_core_globals_set_global_dataset_folder\",\"arguments\":{\"folder\":\"/mnt/shared_ssd/spikeinterface_datasets\"}},{\"name\":\"spikeinterface_core_globals_set_global_dataset_folder\",\"arguments\":{\"folder\":\"/mnt/scratch/alex/spikeinterface_datasets\"}}]"}
{"func_name": "spikeinterface_core_globals_set_global_tmp_folder", "func_desc": "Set the global temporary folder path used by SpikeInterface.\n    \n    This function defines the module-level temporary folder that many SpikeInterface components use to store intermediate files, caches, logs, or files passed to external sorters and exporters (for example during preprocessing, running sorters, waveform extraction, and export). Calling this function updates internal global state so that subsequent operations in the same Python process will read and write temporary artifacts under the provided path.", "tools": [{"function": {"description": "Set the global temporary folder path used by SpikeInterface.\n\nThis function defines the module-level temporary folder that many SpikeInterface components use to store intermediate files, caches, logs, or files passed to external sorters and exporters (for example during preprocessing, running sorters, waveform extraction, and export). Calling this function updates internal global state so that subsequent operations in the same Python process will read and write temporary artifacts under the provided path.", "name": "spikeinterface_core_globals_set_global_tmp_folder", "parameters": {"properties": {"folder": {"type": "string", "description": "Filesystem path, expressed as a string, that will be used as the global temporary folder for SpikeInterface. The function converts this string to a pathlib.Path and assigns it to the module-level variable temp_folder. The exact practical significance is that components which create intermediate files (e.g., preprocessing pipelines, sorter wrappers, waveform extractors, exporters to Phy) will use this location instead of any previously configured location.", "default": ""}}, "required": ["folder"], "type": "any"}}, "type": "function"}], "query": "We’re staging a heterogeneous SpikeInterface spike-sorting run where each recording session carries a lightweight provenance tag and a declared scratch target. Before each cohort/session starts, set SpikeInterface’s global temporary folder to the session’s scratch target only when the path is on a high-throughput local filesystem intended for heavy sorter scratch use (i.e., paths rooted at `/scratch` or `/mnt/ssd`). Apply this rule to the following run manifest in the listed order:\n\nrun_manifest = [\n  {\"cohort\": \"Neuropixels_cluster\", \"env\": \"shared_linux_cluster\", \"scratch_target\": \"/scratch/neuropixels/tmp_spikeinterface\", \"notes\": \"high-throughput scratch\"},\n  {\"cohort\": \"Alex_session\", \"env\": \"shared_hpc_node\", \"scratch_target\": \"/scratch/users/alexm/spikeinterface_tmp\", \"notes\": \"Kilosort wrapper; local NVMe scratch\"},\n  {\"cohort\": \"ExternalSSD_batch\", \"env\": \"workstation\", \"scratch_target\": \"/mnt/ssd/spikeinterface_tmp\", \"notes\": \"external SSD; large batch\"}\n]\n\nConfigure the global temp folder accordingly immediately prior to launching each cohort/session so that all intermediate artifacts, caches, logs, and external sorter/exporter scratch outputs land under the correct scratch root.", "answers": "[{\"name\":\"spikeinterface_core_globals_set_global_tmp_folder\",\"arguments\":{\"folder\":\"/scratch/neuropixels/tmp_spikeinterface\"}},{\"name\":\"spikeinterface_core_globals_set_global_tmp_folder\",\"arguments\":{\"folder\":\"/scratch/users/alexm/spikeinterface_tmp\"}},{\"name\":\"spikeinterface_core_globals_set_global_tmp_folder\",\"arguments\":{\"folder\":\"/mnt/ssd/spikeinterface_tmp\"}}]"}
{"func_name": "spikeinterface_core_sorting_tools_v__to_list_of_spiketrain_numpy", "func_desc": "Slower implementation of vector_to_dict for a single recording segment using a NumPy boolean mask. In the SpikeInterface framework this function is used during post-processing of spike sorting outputs to convert a vectorized representation of spikes (sample time indices and corresponding unit labels for one segment) into a Python list of per-unit spike trains (numpy arrays). This is useful when preparing per-unit spike trains for downstream tasks such as waveform extraction, quality metrics computation, visualization, or export.", "tools": [{"function": {"description": "Slower implementation of vector_to_dict for a single recording segment using a NumPy boolean mask. In the SpikeInterface framework this function is used during post-processing of spike sorting outputs to convert a vectorized representation of spikes (sample time indices and corresponding unit labels for one segment) into a Python list of per-unit spike trains (numpy arrays). This is useful when preparing per-unit spike trains for downstream tasks such as waveform extraction, quality metrics computation, visualization, or export.\n", "name": "spikeinterface_core_sorting_tools_v__to_list_of_spiketrain_numpy", "parameters": {"properties": {"sample_indices": {"type": "array", "items": {"type": "float"}, "description": "1D numpy array of sample indices (timepoints) for all detected spikes in one recording segment. Each entry denotes the sample index of one spike measured on the recording timeline. The dtype and integer nature are preserved in the returned per-unit arrays. Length must equal the length of unit_indices.", "default": ""}, "unit_indices": {"type": "array", "items": {"type": "float"}, "description": "1D numpy array of integer unit labels for each spike in the same order as sample_indices. Each entry assigns the corresponding spike in sample_indices to a unit index. Values are expected to be integers in the range [0, num_units - 1]. Length must equal the length of sample_indices.", "default": ""}, "num_units": {"type": "integer", "description": "Total number of units (clusters) expected for the segment. This sets the length of the returned list and determines which unit indices are collected. The function iterates u from 0 to num_units - 1 and selects spikes where unit_indices == u.", "default": ""}}, "required": ["sample_indices", "unit_indices", "num_units"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing two single-segment spike-sorting outputs for downstream waveform extraction/QC, but the exported vectors are messy. For each segment, first apply a physiological plausibility sieve on the vectorized spikes: keep only events whose unit label is a valid integer unit index for that segment and whose sample index lies within the segment’s recording bounds (inclusive of 0, exclusive of the end). Then, because waveform extraction expects monotonic spike times per unit, treat sample_indices as potentially unsorted and regroup the retained events into per-unit spike trains using the unit index as the grouping key, emitting one NumPy array per unit from 0..num_units-1 (including empty arrays).\n\nSegment A (cohort A): 4 detected units (0–3), segment length = 2500 samples. sample_indices = [120, 305, -7, 450, 451, 900, 1200, 1210, 1500, 1800, 2200, 2600, 305] and unit_indices = [0, 1, 2, 1, 2, 0, 3, 3, 2, 1, 0, 0, 5].\n\nSegment B (cohort B): hippocampal segment length = 300000 samples (10 s at 30 kHz), 3 sorted units (0–2). sample_indices = [105, 210, 215, 400, 402, 800, 805, 1200, 1600, 1610, 1615, 2000, 299999, 300000] and unit_indices = [0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 2, 1, 2, -1].", "answers": "[{\"name\":\"spikeinterface_core_sorting_tools_v__to_list_of_spiketrain_numpy\",\"arguments\":{\"sample_indices\":[120,305,450,451,900,1200,1210,1500,1800,2200],\"unit_indices\":[0,1,1,2,0,3,3,2,1,0],\"num_units\":4}},{\"name\":\"spikeinterface_core_sorting_tools_v__to_list_of_spiketrain_numpy\",\"arguments\":{\"sample_indices\":[105,210,215,400,402,800,805,1200,1600,1610,1615,2000,299999],\"unit_indices\":[0,1,1,0,2,1,1,2,0,0,2,1,2],\"num_units\":3}}]"}
{"func_name": "spikeinterface_core_sortinganalyzer__t_analyzer_extension_params", "func_desc": "spikeinterface.core.sortinganalyzer.get_default_analyzer_extension_params: Retrieve the default parameter values declared by a SortingAnalyzer extension's _set_params method.", "tools": [{"function": {"description": "spikeinterface.core.sortinganalyzer.get_default_analyzer_extension_params: Retrieve the default parameter values declared by a SortingAnalyzer extension's _set_params method.\n", "name": "spikeinterface_core_sortinganalyzer__t_analyzer_extension_params", "parameters": {"properties": {"extension_name": {"type": "string", "description": "The registered name of an analyzer extension within SpikeInterface's SortingAnalyzer extension registry. In the SpikeInterface domain, extensions implement additional analysis functionality on sorting outputs; extension_name is used to locate the extension class via get_extension_class(extension_name). This function expects the extension to define a class method or instance method named _set_params whose signature contains parameters with optional default values.", "default": ""}}, "required": ["extension_name"], "type": "any"}}, "type": "function"}], "query": "We’re auditing a mixed spike-sorting QC benchmark where each SortingAnalyzer replicate may target a different downstream QC module depending on metadata. We have three replicates with the following run annotations (same recording/sorting cohort, independent analyzers):\n\nreplicate_01: extension_hint=\"quality_metrics\" (planned QC summary)\nreplicate_02: extension_hint=\"quality_metrics\" (planned QC summary)\nreplicate_03: extension_hint=\"quality_metrics_v2\" (legacy label from an older config export)\n\nBefore any overrides, retrieve the declared default parameter values for whichever built-in SortingAnalyzer extension each replicate resolves to under this rule: treat names case-insensitively, normalize whitespace, and accept only built-in extensions whose normalized name contains the token \"quality_metrics\". Use the resolved extension name to query the defaults so we can compare baseline arguments across replicates.", "answers": "[{\"name\":\"spikeinterface_core_sortinganalyzer__t_analyzer_extension_params\",\"arguments\":{\"extension_name\":\"quality_metrics\"}},{\"name\":\"spikeinterface_core_sortinganalyzer__t_analyzer_extension_params\",\"arguments\":{\"extension_name\":\"quality_metrics\"}},{\"name\":\"spikeinterface_core_sortinganalyzer__t_analyzer_extension_params\",\"arguments\":{\"extension_name\":\"quality_metrics_v2\"}}]"}
{"func_name": "spikeinterface_core_sortinganalyzer_get_extension_class", "func_desc": "spikeinterface.core.sortinganalyzer.get_extension_class: Retrieve an extension class by name and verify it is registered with the internal extension registry used by the SortingAnalyzer API in SpikeInterface.\n    \n    This function looks up a registered extension class by its declared extension_name in the global registry (_possible_extensions). It is used throughout the SortingAnalyzer and extension machinery of SpikeInterface to locate the concrete class that implements an extension (for example, post-processing, metrics, or other SortingAnalyzer extensions) so that the analyzer can instantiate or query that extension. If the requested extension is not yet registered but is a known builtin extension (listed in _builtin_extensions), the function can optionally import the module that provides and registers the extension, updating the global registry as a side effect. The function therefore both performs a registry lookup and may perform dynamic import to ensure builtin extensions become available to the SortingAnalyzer workflow.", "tools": [{"function": {"description": "spikeinterface.core.sortinganalyzer.get_extension_class: Retrieve an extension class by name and verify it is registered with the internal extension registry used by the SortingAnalyzer API in SpikeInterface.\n\nThis function looks up a registered extension class by its declared extension_name in the global registry (_possible_extensions). It is used throughout the SortingAnalyzer and extension machinery of SpikeInterface to locate the concrete class that implements an extension (for example, post-processing, metrics, or other SortingAnalyzer extensions) so that the analyzer can instantiate or query that extension. If the requested extension is not yet registered but is a known builtin extension (listed in _builtin_extensions), the function can optionally import the module that provides and registers the extension, updating the global registry as a side effect. The function therefore both performs a registry lookup and may perform dynamic import to ensure builtin extensions become available to the SortingAnalyzer workflow.", "name": "spikeinterface_core_sortinganalyzer_get_extension_class", "parameters": {"properties": {"extension_name": {"type": "string", "description": "The name identifier of the extension to retrieve. This must match the extension.extension_name attribute of a class that has been registered in the global _possible_extensions registry. In practical SpikeInterface use, this name corresponds to the string used to refer to an extension when interacting with SortingAnalyzer (for example, requesting a particular post-processing or metric extension).", "default": ""}, "auto_import": {"type": "boolean", "description": "If True (default), and the extension_name is not currently registered but exists in the _builtin_extensions mapping, the function will import the corresponding module via importlib.import_module(module). This import has the side effect of allowing that module to run its registration code and update the global _possible_extensions registry so the extension becomes available. If False, the function will not import modules automatically and will raise a ValueError instructing the caller to import the related module manually before use. Note that importing may be relatively expensive and can raise ImportError or other import-time exceptions if the module or its dependencies are not available.", "default": true}}, "required": ["extension_name", "auto_import"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating SortingAnalyzer results from multiple labs and need to preflight the extension registry before running any downstream computations. The manifest of requested post-processing extensions was scraped from mixed config files and includes: [\"template_metrics\", \"Template_Metrics\", \"quality_metrics \", \"quality-metrics\", \"unit_locations\", \"\", null, \"quality_metrics\", \"template_metrics\", \"unknown_extension\"]. Normalize each candidate by trimming surrounding whitespace and lowercasing; treat hyphens as underscores. Then, for the cleaned names, keep only entries that match a known builtin SortingAnalyzer extension name and de-duplicate while preserving first-seen order. For each retained extension name, retrieve its extension class with dynamic auto-import enabled so builtin providers are registered if missing.", "answers": "[{\"name\":\"spikeinterface_core_sortinganalyzer_get_extension_class\",\"arguments\":{\"extension_name\":\"template_metrics\",\"auto_import\":true}},{\"name\":\"spikeinterface_core_sortinganalyzer_get_extension_class\",\"arguments\":{\"extension_name\":\"quality_metrics\",\"auto_import\":true}},{\"name\":\"spikeinterface_core_sortinganalyzer_get_extension_class\",\"arguments\":{\"extension_name\":\"unit_locations\",\"auto_import\":true}}]"}
{"func_name": "spikeinterface_curation_auto_merge_estimate_contamination", "func_desc": "Estimate the contamination of a spike train by counting refractory period violations and converting that count into a contamination fraction used in spike sorting curation.", "tools": [{"function": {"description": "Estimate the contamination of a spike train by counting refractory period violations and converting that count into a contamination fraction used in spike sorting curation.\n", "name": "spikeinterface_curation_auto_merge_estimate_contamination", "parameters": {"properties": {"spike_train": {"type": "array", "items": {"type": "float"}, "description": "The unit's spike train as a 1-D numpy array of spike times expressed in sample indices (integer sample timestamps). In the context of SpikeInterface (a unified framework for spike sorting), this array represents the detected spike events for one unit. The function will cast this array to numpy.int64 internally before computing violations; the original array is not modified.", "default": ""}, "sf": {"type": "float", "description": "The sampling frequency (in Hz) of the spike train. This value is used to convert refractory period limits given in milliseconds to sample counts (t_c and t_r are computed as refractory_period[*] * 1e-3 * sf). sf must be provided in the same temporal units expected for conversion to samples.", "default": ""}, "T": {"type": "integer", "description": "The duration of the spike train in samples (integer). This is the total number of samples in the recording epoch used to normalize the contamination estimate; providing an incorrect T (for example smaller than the largest spike index) will yield an incorrect estimate or may trigger runtime errors.", "default": ""}, "refractory_period": {"type": "tuple", "prefixItems": [{"type": "float"}, {"type": "float"}], "description": "The censored and refractory period (t_c, t_r) used, expressed in milliseconds. The first element is the censored time t_c (ms) and the second is the refractory time t_r (ms). These two values are converted to sample counts via sf and are used to define the time windows for counting violations and computing the contamination metric.", "default": ""}}, "required": ["spike_train", "sf", "T", "refractory_period"], "type": "any"}}, "type": "function"}], "query": "We’re running an automated spike-sorting curation QC pass on two putative single units from the same probe, but the raw spike trains contain acquisition/clock artifacts and need cohort-specific censoring rules.\n\nUse sf=30 kHz for both.\n\nUnit A (10 min recording): T=18,000,000 samples. Spike times (sample indices) are:\n[1200, 45000, 90010, 135050, 180000, 225020, 225150, 270000, 315000, 360000, 405000, 450000, 495000, 540000, 585000, 630000, 675000, 720000, 765000, 810000, 855000, 900000, 945000, 990000, 1035000, 1080000, 1125000, 1170000, 1215000, 1260000, 1305000, 1350000, 1395000, 1440000, 1485000, 1530000, 1575000, 1620000, 1665000, 1710000].\nApply a de-duplication rule for likely double-detected events: any inter-spike interval < 0.7 ms should be treated as the same biological spike (keep the first timestamp only). After that, estimate contamination using refractory windows [0.5 ms, 1.5 ms].\n\nUnit B (10 s excerpt): T=300,000 samples. Spike times are:\n[150, 310, 470, 635, 805, 980, 1155, 1330, 1505, 1680, 1855, 2030, 2205, 2380, 2555, 2730].\nApply an edge-censoring rule to avoid partial-window effects: only use spikes whose timestamps are at least 2.0 ms after the start and at least 2.0 ms before the end of the recording. Then estimate contamination using refractory windows [0.5 ms, 2.0 ms].\n\nFor each unit, run the contamination estimator on the cleaned spike train with the specified windows.", "answers": "[{\"name\":\"spikeinterface_curation_auto_merge_estimate_contamination\",\"arguments\":{\"spike_train\":[1200,45000,90010,135050,180000,225020,270000,315000,360000,405000,450000,495000,540000,585000,630000,675000,720000,765000,810000,855000,900000,945000,990000,1035000,1080000,1125000,1170000,1215000,1260000,1305000,1350000,1395000,1440000,1485000,1530000,1575000,1620000,1665000,1710000],\"sf\":30000,\"T\":18000000,\"refractory_period\":[0.5,1.5]}},{\"name\":\"spikeinterface_curation_auto_merge_estimate_contamination\",\"arguments\":{\"spike_train\":[150,310,470,635,805,980,1155,1330,1505,1680,1855,2030,2205,2380,2555,2730],\"sf\":30000,\"T\":300000,\"refractory_period\":[0.5,2.0]}}]"}
{"func_name": "spikeinterface_curation_auto_merge_resolve_pairs", "func_desc": "Resolve nested unit-merge mappings produced when merging units recursively during curation.\n    \n    This function is used primarily by auto_merge_units within the SpikeInterface curation workflow to produce a condensed, non-nested representation of all unit merges that have been applied so far. The returned mapping is suitable for downstream visualization (for example, the plot_potential_merge widget) and for producing a final list of merged unit identifiers for post-processing and quality-metric calculations. In the SpikeInterface domain, \"unit ids\" refer to the identifiers used by a SortingExtractor (integers or strings identifying sorted units), and the mappings record which unit ids were merged into which surviving unit id.", "tools": [{"function": {"description": "Resolve nested unit-merge mappings produced when merging units recursively during curation.\n\nThis function is used primarily by auto_merge_units within the SpikeInterface curation workflow to produce a condensed, non-nested representation of all unit merges that have been applied so far. The returned mapping is suitable for downstream visualization (for example, the plot_potential_merge widget) and for producing a final list of merged unit identifiers for post-processing and quality-metric calculations. In the SpikeInterface domain, \"unit ids\" refer to the identifiers used by a SortingExtractor (integers or strings identifying sorted units), and the mappings record which unit ids were merged into which surviving unit id.", "name": "spikeinterface_curation_auto_merge_resolve_pairs", "parameters": {"properties": {"existing_merges": {"type": "any", "description": "A dictionary representing merges that have already been applied. The keys are unit ids (the surviving unit id after a previous merge) and the values are lists of unit ids that were merged into that key. For example, an entry {A: [B, C]} means units B and C were merged into unit A. This function expects a dict object; passing None has a documented behavior described below. The function performs a shallow copy of this dict at start, so the top-level mapping passed in will not be mutated, but the lists inside the mapping are not deep-copied by this function.", "default": ""}, "new_merges": {"type": "any", "description": "A dictionary representing new merges to incorporate. The keys are the new surviving unit ids produced by the latest merge step and the values are lists of unit ids to merge into that key. For example, {D: [A, E]} means units A and E should be merged into unit D. The function will inspect these lists to detect references to keys already present in existing_merges and will expand those references by replacing referenced keys with their constituent unit lists from existing_merges. The function expects new_merges to be a dict and note that the lists provided as values in new_merges may be mutated in-place by this function (they are extended and elements removed when nested merges are resolved).", "default": ""}}, "required": ["existing_merges", "new_merges"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating SpikeInterface curation merge provenance across three replicates, but the raw auto-merge logs are messy: some proposals reference already-merged composite unit ids, and some proposals include placeholders from earlier QC passes. For each replicate, resolve merge provenance into a visualization-ready, non-nested mapping where each *surviving* unit id directly enumerates all contributing original unit ids by recursively expanding any referenced composite ids found in the current merge history.\n\nApply a curation rule consistent with how we build final cohorts: only treat a proposed survivor as valid for condensation when its proposal list contains at least one unit id that is itself a composite in the existing merge history (i.e., the proposal is actually chaining prior merges). For each replicate below, run the condensation only on the survivors that satisfy that criterion.\n\nReplicate A (template review consolidation): existing merge history is {5: [7, 9], 12: [13, 15], 99: [100, 101]}; new auto-merge proposals are {20: [5, 12, 22], 21: [23, 24]}.\n\nReplicate B (multiple candidate survivors, mixed provenance): existing merge history is {10: [12, 15], 20: [21, 22], 50: [51, 52]}; new auto-merge proposals are {30: [10, 33], 40: [20, 44], 60: [61, 62]}.\n\nReplicate C (multi-step merge after prior merges, plus a clean merge): existing merge history is {5: [2, 3], 8: [6, 7], 70: [71, 72]}; new auto-merge proposals are {10: [5, 8], 12: [9, 11], 73: [70, 74]}.\n\nReturn the resolved, condensed mappings for each replicate (only for survivors that meet the chaining criterion).", "answers": "[{\"name\":\"spikeinterface_curation_auto_merge_resolve_pairs\",\"arguments\":{\"existing_merges\":{\"5\":[7,9],\"12\":[13,15],\"99\":[100,101]},\"new_merges\":{\"20\":[5,12,22]}}},{\"name\":\"spikeinterface_curation_auto_merge_resolve_pairs\",\"arguments\":{\"existing_merges\":{\"10\":[12,15],\"20\":[21,22],\"50\":[51,52]},\"new_merges\":{\"30\":[10,33],\"40\":[20,44]}}},{\"name\":\"spikeinterface_curation_auto_merge_resolve_pairs\",\"arguments\":{\"existing_merges\":{\"5\":[2,3],\"8\":[6,7],\"70\":[71,72]},\"new_merges\":{\"10\":[5,8],\"73\":[70,74]}}}]"}
{"func_name": "spikeinterface_curation_auto_merge_smooth_correlogram", "func_desc": "spikeinterface.curation.auto_merge.smooth_correlogram smooths cross-correlograms by convolving them with a Gaussian kernel. This function is used in the automatic curation/auto-merge workflow of SpikeInterface to reduce high-frequency noise in computed cross-correlograms (time-lag histograms between spike trains) so that downstream heuristics or metrics that decide whether two units should be merged are more robust. The smoothing is implemented by constructing a normalized Gaussian kernel from the provided bins and sigma (in milliseconds) and applying a fast Fourier-based convolution (scipy.signal.fftconvolve) along the correlogram time-lag axis.", "tools": [{"function": {"description": "spikeinterface.curation.auto_merge.smooth_correlogram smooths cross-correlograms by convolving them with a Gaussian kernel. This function is used in the automatic curation/auto-merge workflow of SpikeInterface to reduce high-frequency noise in computed cross-correlograms (time-lag histograms between spike trains) so that downstream heuristics or metrics that decide whether two units should be merged are more robust. The smoothing is implemented by constructing a normalized Gaussian kernel from the provided bins and sigma (in milliseconds) and applying a fast Fourier-based convolution (scipy.signal.fftconvolve) along the correlogram time-lag axis.\n", "name": "spikeinterface_curation_auto_merge_smooth_correlogram", "parameters": {"properties": {"correlograms": {"type": "array", "items": {"type": "float"}, "description": "Array containing one or more correlograms to be smoothed. The function expects a multi-dimensional array where the time-lag bins are on axis 2 (i.e., correlograms.shape[2] must equal the length of bins). In the SpikeInterface auto-merge context this is typically a 3D array (for example, number of correlograms x number of channels/units x number of time-lag bins). Each element represents counts or rates for a given time lag and will be convolved along axis 2. The input array is not modified in-place; a new array is returned.", "default": ""}, "bins": {"type": "array", "items": {"type": "float"}, "description": "1D array of time-lag bin centers/positions corresponding to the third axis of correlograms. Units must match sigma_smooth_ms (milliseconds). The Gaussian smoothing kernel is computed using these bin coordinates as kernel support (kernel = exp(-bins**2 / (2 * sigma_smooth_ms**2))). The length of bins must equal correlograms.shape[2]; otherwise scipy.signal.fftconvolve will raise an error.", "default": ""}, "sigma_smooth_ms": {"type": "float", "description": "Standard deviation of the Gaussian smoothing kernel, expressed in milliseconds. Default is 0.6. This parameter controls the amount of temporal smoothing: larger values produce broader, stronger smoothing across neighboring time-lag bins. Must be a positive, non-zero float; extremely small values may produce negligible smoothing and very large values may oversmooth features important for merging decisions.", "default": 0.6}}, "required": ["correlograms", "bins", "sigma_smooth_ms"], "type": "any"}}, "type": "function"}], "query": "We’re running the cross-correlogram denoising step in a SpikeInterface auto-merge curation pass, but the inputs are a mixed-quality dump from two probe layouts. Treat each correlogram slice independently (each unit-pair × shank/channel trace) and only denoise traces that look like a plausible short-latency interaction: the lag-0 bin must be the global maximum of that trace and the trace must have nonzero support on both sides of zero-lag (at least one nonzero bin for negative lags and at least one nonzero bin for positive lags). For traces that pass, smooth them with spikeinterface.curation.auto_merge.smooth_correlogram using a Gaussian kernel with sigma = 0.8 ms and the cohort-appropriate bins (Cohort A uses 1 ms bin centers from −5 to +5; Cohort B uses 1 ms bin centers from −3 to +3). The raw data are:\n\nCohort A (multi-shank replicates), correlograms shape (3 unit pairs × 2 shanks × 11 bins):\ncorrelograms = [\n  [[0,1,3,5,9,12,9,5,3,1,0],[0,2,4,6,10,13,10,6,4,2,0]],\n  [[1,2,4,7,11,15,11,7,4,2,1],[0,1,2,4,8,14,8,4,2,1,0]],\n  [[0,0,1,3,6,10,6,3,1,0,0],[0,1,3,5,9,11,9,5,3,1,0]]\n]\nbins_A = [-5,-4,-3,-2,-1,0,1,2,3,4,5]\n\nCohort B (compact channel set), correlograms shape (2 unit pairs × 2 channels × 7 bins):\ncorrelograms = [\n  [[0,3,5,10,4,2,0],[1,4,9,12,6,3,1]],\n  [[0,2,6,9,5,1,0],[2,5,8,11,7,4,2]]\n]\nbins_B = [-3,-2,-1,0,1,2,3]\n\nReturn the smoothed correlograms for only the traces that satisfy the plausibility rule, preserving their original cohort/unit-pair/shank(channel) identity in how you batch them for smoothing.", "answers": "[{\"name\":\"spikeinterface_curation_auto_merge_smooth_correlogram\",\"arguments\":{\"correlograms\":[[0,1,3,5,9,12,9,5,3,1,0],[0,2,4,6,10,13,10,6,4,2,0],[1,2,4,7,11,15,11,7,4,2,1],[0,1,2,4,8,14,8,4,2,1,0],[0,0,1,3,6,10,6,3,1,0,0],[0,1,3,5,9,11,9,5,3,1,0]],\"bins\":[-5,-4,-3,-2,-1,0,1,2,3,4,5],\"sigma_smooth_ms\":0.8}},{\"name\":\"spikeinterface_curation_auto_merge_smooth_correlogram\",\"arguments\":{\"correlograms\":[[0,3,5,10,4,2,0],[1,4,9,12,6,3,1],[0,2,6,9,5,1,0],[2,5,8,11,7,4,2]],\"bins\":[-3,-2,-1,0,1,2,3],\"sigma_smooth_ms\":0.8}}]"}
{"func_name": "spikeinterface_generation_drift_tools_make_linear_displacement", "func_desc": "make_linear_displacement generates a sequence of 2D positions that interpolate linearly between a start and stop position. In the SpikeInterface generation/drift_tools context, this function is used to create a temporal sequence of probe or source displacements (x,y coordinates) for simulating linear drift of an extracellular recording setup; the returned sequence can be fed into synthetic recording generation or drift simulation routines to model gradual movement of the probe relative to neural tissue.", "tools": [{"function": {"description": "make_linear_displacement generates a sequence of 2D positions that interpolate linearly between a start and stop position. In the SpikeInterface generation/drift_tools context, this function is used to create a temporal sequence of probe or source displacements (x,y coordinates) for simulating linear drift of an extracellular recording setup; the returned sequence can be fed into synthetic recording generation or drift simulation routines to model gradual movement of the probe relative to neural tissue.\n", "name": "spikeinterface_generation_drift_tools_make_linear_displacement", "parameters": {"properties": {"start": {"type": "array", "items": {"type": "float"}, "description": "A 1D array of 2 elements giving the start position [x, y]. This represents the initial probe (or source) displacement in the same units used by downstream simulation code (e.g., micrometers). The function treats this as the position at the first timestep.", "default": ""}, "stop": {"type": "array", "items": {"type": "float"}, "description": "A 1D array of 2 elements giving the stop position [x, y]. This represents the final probe (or source) displacement and is treated as the position at the last timestep. Both start and stop must have exactly two components corresponding to the two spatial dimensions.", "default": ""}, "num_step": {"type": "integer", "description": "The number of discrete timesteps (positions) to generate between start and stop, inclusive. Default: 10. When num_step > 1, the function returns num_step positions that are linearly spaced from start to stop (start is the first row, stop is the last row). When num_step == 1, the function returns a single position equal to the midpoint (start + stop) / 2.", "default": 10}}, "required": ["start", "stop", "num_step"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a synthetic drift benchmark where the intended probe motion is *nominally* a linear translation over a 20‑minute epoch, but our run metadata is messy: some sessions report coordinates in µm, others in mm, and a few are malformed. Each session provides a start/stop 2D displacement for the probe (in the order x,y). Build replicate displacement profiles only for sessions that plausibly represent a physical probe translation in microns: interpret any coordinate pair whose absolute values are all < 1 as being reported in mm and convert to µm; keep sessions whose converted displacement path length is between 10 and 60 µm. For each retained session, generate two replicates: Replicate A uses an endpoint-inclusive sampling grid with one sample per minute plus the initial position; Replicate B uses exactly one fewer step than Replicate A. Return the (x,y) sequences for all retained sessions so they can be passed directly into the drift simulator.\n\nRaw session metadata (start, stop):\n- sess_01: start=[0,0], stop=[15,-30]\n- sess_02: start=[0.0,0.0], stop=[0.015,-0.030]\n- sess_03: start=[0,0], stop=[150,-300]\n- sess_04: start=[0,0], stop=[5,-5]\n- sess_05: start=[0,0], stop=[-20,0]\n- sess_06: start=[0,0], stop=[15,-30,2]", "answers": "[{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0,0],\"stop\":[15,-30],\"num_step\":21}},{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0,0],\"stop\":[15,-30],\"num_step\":20}},{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0.0,0.0],\"stop\":[15.0,-30.0],\"num_step\":21}},{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0.0,0.0],\"stop\":[15.0,-30.0],\"num_step\":20}},{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0,0],\"stop\":[-20,0],\"num_step\":21}},{\"name\":\"spikeinterface_generation_drift_tools_make_linear_displacement\",\"arguments\":{\"start\":[0,0],\"stop\":[-20,0],\"num_step\":20}}]"}
{"func_name": "spikeinterface_postprocessing_corr___correlogram_for_one_segment", "func_desc": "Compute cross-correlograms for one recording segment using an optimized algorithm adapted from the Phy package (originally by Cyrille Rossant). This function is used in SpikeInterface post-processing to quantify temporal relationships between spike trains produced by spike sorting: it bins time differences between spikes from all pairs of units within a specified window (measured in samples) and returns integer counts per lag-time bin for every unit pair. The implementation is optimized for large spike collections by iteratively shifting the spike train, masking out spikes outside the window, binning delays by floor division, and using ravel_multi_index plus bincount to increment counts efficiently.", "tools": [{"function": {"description": "Compute cross-correlograms for one recording segment using an optimized algorithm adapted from the Phy package (originally by Cyrille Rossant). This function is used in SpikeInterface post-processing to quantify temporal relationships between spike trains produced by spike sorting: it bins time differences between spikes from all pairs of units within a specified window (measured in samples) and returns integer counts per lag-time bin for every unit pair. The implementation is optimized for large spike collections by iteratively shifting the spike train, masking out spikes outside the window, binning delays by floor division, and using ravel_multi_index plus bincount to increment counts efficiently.\n", "name": "spikeinterface_postprocessing_corr___correlogram_for_one_segment", "parameters": {"properties": {"spike_times": {"type": "array", "items": {"type": "float"}, "description": "A 1-D array of spike times in samples (not seconds). This array must contain the timestamps for spikes from all units in the segment and must have the same length as spike_unit_indices. The algorithm assumes spike_times are sorted in non-decreasing order (temporal order); incorrect ordering will produce incorrect correlograms. Each element is an integer sample index representing when a spike occurred in the recording.", "default": ""}, "spike_unit_indices": {"type": "array", "items": {"type": "float"}, "description": "A 1-D array of integer labels indicating the unit associated with each spike in spike_times. This array must have the same shape and ordering as spike_times (i.e., spike_unit_indices[i] is the unit label for spike_times[i]). Unit labels are used as array indices when building the correlogram; in typical usage they should map to 0..(num_units-1) indices (contiguous integer labels), otherwise indexing errors or out-of-range exceptions may occur.", "default": ""}, "window_size": {"type": "integer", "description": "The half-window size (in samples) over which to search for matching spikes for cross-correlation. The function computes the number of lag bins from this window size and bin_size (via the internal _compute_num_bins call). window_size must be a positive integer expressed in the same sample units as spike_times and bin_size. Larger window_size increases the number of lag bins and thus memory and computation.", "default": ""}, "bin_size": {"type": "integer", "description": "The size (in samples) of each lag-time bin. bin_size must be a positive integer expressed in samples. Delays between spikes are converted to bin indices using floor division by bin_size; the binning determines the temporal resolution of the correlograms.", "default": ""}}, "required": ["spike_times", "spike_unit_indices", "window_size", "bin_size"], "type": "any"}}, "type": "function"}], "query": "We’re validating spike-train synchrony QC on one recording segment where timestamps may be partially corrupted and timing regimes are mixed. Use the same 3 units (0, 1, 2), but apply a data-sieve + branching protocol before computing pairwise cross-correlograms.\n\nDataset A (dense/local spiking; already in samples):\nspike_times_A = [100, 110, 120, 130, 145, 160, 175, 190, 205, 220]\nspike_unit_indices_A = [0, 1, 0, 2, 1, 0, 2, 1, 0, 2]\nCompute correlograms only if the spike times are strictly increasing and all unit indices are within {0,1,2}. For this regime, use a ±50-sample window and 5-sample lag bins.\n\nDataset B (intended 10 s at 30 kHz; timestamps provided in samples but contain acquisition artifacts):\nraw_spike_times_B = [150, 320, 320, -20, 900, 1200, 3000, 3050, 4500, 9000, 12000, 18000]\nraw_spike_unit_indices_B = [0, 1, 1, 2, 0, 2, 1, 1, 2, 0, 2, 1]\nBefore correlograms, keep only events with non-negative timestamps and enforce strictly increasing timestamps by dropping repeated time points (treat duplicates as retransmission artifacts). Proceed only if remaining unit indices are within {0,1,2}. Then compute correlograms using time-derived parameters converted to samples at 30 kHz: half-window 5 ms and bin width 0.5 ms.\n\nRun the correlogram computation separately for Dataset A and the cleaned Dataset B using the optimized SpikeInterface post-processing function.", "answers": "[{\"name\":\"spikeinterface_postprocessing_corr___correlogram_for_one_segment\",\"arguments\":{\"spike_times\":[100,110,120,130,145,160,175,190,205,220],\"spike_unit_indices\":[0,1,0,2,1,0,2,1,0,2],\"window_size\":50,\"bin_size\":5}},{\"name\":\"spikeinterface_postprocessing_corr___correlogram_for_one_segment\",\"arguments\":{\"spike_times\":[150,320,900,1200,3000,3050,4500,9000,12000,18000],\"spike_unit_indices\":[0,1,0,2,1,1,2,0,2,1],\"window_size\":150,\"bin_size\":15}}]"}
{"func_name": "spikeinterface_postprocessing_loca__ls_make_radial_order_parents", "func_desc": "Pre-computes a radial parent lookup structure used by enforce_decrease_shells in the localization post-processing tools of SpikeInterface. This function analyzes the spatial probe geometry and a per-channel neighbor mask to determine, for each channel, an ordered list of \"child\" neighbor channels and for each child the set of already-seen \"parent\" neighbor indices that lie closer to the source channel. The resulting structure is intended to be used by enforce_decrease_shells to enforce a radial decrease constraint (for example, decreasing spike amplitude or score with radial distance) when post-processing localization or quality metrics.", "tools": [{"function": {"description": "Pre-computes a radial parent lookup structure used by enforce_decrease_shells in the localization post-processing tools of SpikeInterface. This function analyzes the spatial probe geometry and a per-channel neighbor mask to determine, for each channel, an ordered list of \"child\" neighbor channels and for each child the set of already-seen \"parent\" neighbor indices that lie closer to the source channel. The resulting structure is intended to be used by enforce_decrease_shells to enforce a radial decrease constraint (for example, decreasing spike amplitude or score with radial distance) when post-processing localization or quality metrics.\n", "name": "spikeinterface_postprocessing_loca__ls_make_radial_order_parents", "parameters": {"properties": {"geom": {"type": "array", "items": {"type": "float"}, "description": "Array containing spatial coordinates for each recording channel. The function uses len(geom) as the number of channels and passes geom to the internal make_shell and make_shells helpers to compute spatial shells. Practically, geom represents the probe geometry (one row per channel, coordinate columns depend on probe dimensionality) and determines how \"closeness\" and shells are computed for radial growth.", "default": ""}, "neighbours_mask": {"type": "array", "items": {"type": "float"}, "description": "Per-channel neighbor mask that identifies which channels should be considered neighbors of each channel. Each element iterated from neighbours_mask is interpreted as a boolean-like mask or array convertible to indices via numpy.flatnonzero; the function converts each row to a list of neighbor channel indices and only considers neighbors with indices < n_channels. In practice this mask restricts the set of candidate channels for which radial parent relationships are computed (e.g., channels within a recording adjacency or a pruning radius).", "default": ""}, "n_jumps_per_growth": {"type": "integer", "description": "Number of shell \"jumps\" to use when growing the search for progressively more distant channels during parent discovery. Defaults to 1. This parameter controls the granularity of each growth step: larger values make each growth step reach farther channels, which reduces the number of iterations but coarsens the radial ordering used to find parents. It is passed to make_shell when expanding the local neighborhood for a channel.", "default": 1}, "n_jumps_parent": {"type": "integer", "description": "Number of shell jumps used when precomputing per-channel parent shells via make_shells. Defaults to 3. This parameter controls the radius used to determine candidate parent channels for any channel and is used to compute the static shells array (shells[new_chan]) that is intersected with already-seen channels to identify parents.", "default": 3}}, "required": ["geom", "neighbours_mask", "n_jumps_parent", "n_jumps_per_growth"], "type": "any"}}, "type": "function"}], "query": "We’re validating enforce_decrease_shells under realistic probe QC conditions where the neighbor graph must be pruned based on geometry-derived plausibility.\n\nYou have two 6-channel probe cohorts with provided geometries and initial per-source neighbour masks. For each cohort, precompute the radial parent lookup structures (children ordered by increasing radial distance from each source channel; parent indices restricted to already-seen closer neighbors) using n_jumps_per_growth=1 and n_jumps_parent=2, but first apply this QC gating rule to the neighbour mask:\n\nFor each source channel i, keep only neighbor channels j that are (a) enabled in the provided neighbour mask and (b) within a cohort-specific distance cutoff defined as: cutoff = (median of all nonzero inter-channel distances in that cohort’s geometry) + 1e-6. This is intended to drop long-range edges while keeping the natural local lattice.\n\nCohort A geometry (µm): [[0.0,0.0],[20.0,0.0],[40.0,0.0],[0.0,20.0],[20.0,20.0],[40.0,20.0]] with initial neighbour mask (rows=source): [[true,true,true,true,true,false],[true,true,true,true,true,true],[true,true,true,false,true,true],[true,true,false,true,true,false],[true,true,true,true,true,true],[false,true,true,false,true,true]].\n\nCohort B geometry (µm): ch0=(0,0), ch1=(0,20), ch2=(0,40), ch3=(20,0), ch4=(20,20), ch5=(20,40) with initial fully-connected within-block neighbour mask (all true 6×6).\n\nReturn the radial parent lookup for each cohort as separate outputs after QC-gating the masks by the cohort-specific cutoff rule.", "answers": "[{\"name\":\"spikeinterface_postprocessing_loca__ls_make_radial_order_parents\",\"arguments\":{\"geom\":[[0.0,0.0],[20.0,0.0],[40.0,0.0],[0.0,20.0],[20.0,20.0],[40.0,20.0]],\"neighbours_mask\":[[true,true,false,true,false,false],[true,true,true,false,true,false],[false,true,true,false,false,true],[true,false,false,true,true,false],[false,true,false,true,true,true],[false,false,true,false,true,true]],\"n_jumps_per_growth\":1,\"n_jumps_parent\":2}},{\"name\":\"spikeinterface_postprocessing_loca__ls_make_radial_order_parents\",\"arguments\":{\"geom\":[[0,0],[0,20],[0,40],[20,0],[20,20],[20,40]],\"neighbours_mask\":[[true,true,false,true,false,false],[true,true,true,false,true,false],[false,true,true,false,false,true],[true,false,false,true,true,false],[false,true,false,true,true,true],[false,false,true,false,true,true]],\"n_jumps_per_growth\":1,\"n_jumps_parent\":2}}]"}
{"func_name": "spikeinterface_preprocessing_detect_bad_channels_detrend", "func_desc": "spikeinterface.preprocessing.detect_bad_channels.detrend subtracts a median-filtered trend from a 1-D signal (vector) using endpoint tapering. This function is used in spike preprocessing (for example, in detect_bad_channels) to remove slowly varying baseline or trend from an extracellular recording channel prior to computing metrics for bad-channel detection or other downstream analyses.", "tools": [{"function": {"description": "spikeinterface.preprocessing.detect_bad_channels.detrend subtracts a median-filtered trend from a 1-D signal (vector) using endpoint tapering. This function is used in spike preprocessing (for example, in detect_bad_channels) to remove slowly varying baseline or trend from an extracellular recording channel prior to computing metrics for bad-channel detection or other downstream analyses.\n", "name": "spikeinterface_preprocessing_detect_bad_channels_detrend", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input 1-D array (vector) containing the sampled signal for a single channel or trace. In the SpikeInterface preprocessing context, this typically represents a time series of extracellular voltage values for one channel. The function treats x as read-only and returns a new array; it does not modify x in place. x must be non-empty because the implementation accesses x[0] and x[-1].", "default": ""}, "nmed": {"type": "integer", "description": "Number of points used for the median filter kernel (median filter length). This integer controls the time scale of the trend that is removed: larger values remove slower-varying trends. There is no default; the caller must supply a positive integer. The implementation computes ntap = ceil(nmed / 2) and pads the signal by repeating the first and last samples ntap times before applying scipy.signal.medfilt with kernel size nmed.", "default": ""}}, "required": ["x", "nmed"], "type": "any"}}, "type": "function"}], "query": "We’re QC’ing two single-channel extracellular voltage replicates from the same probe prior to bad-channel scoring, but the snippets are heterogeneous in duration. Apply spikeinterface.preprocessing.detect_bad_channels.detrend to each replicate using a median filter length chosen *relative to the snippet length*: use an odd window equal to the largest odd integer \u00160% of the number of samples in the snippet (standard endpoint tapering). Process these two raw traces:\n- Replicate A: [12.0, 12.5, 13.1, 13.8, 14.2, 14.0, 13.6, 13.0, 12.4, 12.1, 11.9, 12.2, 12.8, 13.3, 13.7, 13.9, 13.5, 12.9, 12.3, 12.0]\n- Replicate B: [12.0, 12.5, 13.0, 13.8, 14.2, 14.0, 13.6, 13.1, 12.7, 12.2]\nReturn the detrended traces for downstream metric computation.", "answers": "[{\"name\":\"spikeinterface_preprocessing_detect_bad_channels_detrend\",\"arguments\":{\"x\":[12.0,12.5,13.1,13.8,14.2,14.0,13.6,13.0,12.4,12.1,11.9,12.2,12.8,13.3,13.7,13.9,13.5,12.9,12.3,12.0],\"nmed\":11}},{\"name\":\"spikeinterface_preprocessing_detect_bad_channels_detrend\",\"arguments\":{\"x\":[12.0,12.5,13.0,13.8,14.2,14.0,13.6,13.1,12.7,12.2],\"nmed\":5}}]"}
{"func_name": "spikeinterface_preprocessing_phase_shift_apply_frequency_shift", "func_desc": "Apply a sub-sample-accurate frequency (phase) shift to a multi-channel signal buffer.\n    \n    This function is intended for preprocessing extracellular recordings in SpikeInterface prior to spike sorting or waveform extraction. It shifts each channel by a (possibly fractional) number of samples using the Fourier shift theorem: the signal is transformed to the frequency domain with a real FFT (rFFT), a complex phase rotation that corresponds to the requested time shift is applied per-frequency-bin and per-channel, and the result is transformed back to the time domain with an inverse real FFT (irFFT). This produces time shifts that are accurate below the sampling-period resolution and so are useful for aligning channels, correcting propagation delays between channels, or fine temporal registration of multichannel recordings before downstream spike-sorting, quality-metric computation, or waveform extraction.", "tools": [{"function": {"description": "Apply a sub-sample-accurate frequency (phase) shift to a multi-channel signal buffer.\n\nThis function is intended for preprocessing extracellular recordings in SpikeInterface prior to spike sorting or waveform extraction. It shifts each channel by a (possibly fractional) number of samples using the Fourier shift theorem: the signal is transformed to the frequency domain with a real FFT (rFFT), a complex phase rotation that corresponds to the requested time shift is applied per-frequency-bin and per-channel, and the result is transformed back to the time domain with an inverse real FFT (irFFT). This produces time shifts that are accurate below the sampling-period resolution and so are useful for aligning channels, correcting propagation delays between channels, or fine temporal registration of multichannel recordings before downstream spike-sorting, quality-metric computation, or waveform extraction.", "name": "spikeinterface_preprocessing_phase_shift_apply_frequency_shift", "parameters": {"properties": {"signal": {"type": "array", "items": {"type": "float"}, "description": "Input real-valued signal array to be shifted. This array contains the time samples along the axis specified by the axis parameter and the channel layout on the remaining axes. For typical SpikeInterface preprocessing, signal is a 2D array with shape (num_time_samples, num_channels) and dtype compatible with scipy.fft.rfft/irfft. The function will perform an rFFT along the specified axis to compute the frequency-domain representation.", "default": ""}, "shift_samples": {"type": "array", "items": {"type": "float"}, "description": "1-D array of sample shifts, one entry per channel. Each value is the desired shift for the corresponding channel expressed in units of samples; fractional values are allowed to request sub-sample shifts. Conceptually, the time shift in seconds equals shift_samples / sampling_rate (sampling_rate is not an argument to this function and must be handled by the caller). The array length (shift_samples.size) must match the number of channels in signal along the non-shift axis (for axis=0 this means the second dimension: signal.shape[1] when signal is 2D). If sizes do not match, NumPy broadcasting or elementwise multiplication will fail and a ValueError (or related broadcasting exception) can be raised.", "default": ""}, "axis": {"type": "integer", "description": "Axis along which to perform the shift. Default is 0. Currently, only axis=0 is supported by this implementation and passing any other value will cause a NotImplementedError. The axis specifies the time/sample axis over which the rFFT/irFFT are computed and therefore which axis is translated in time by shift_samples.", "default": 0}}, "required": ["signal", "shift_samples", "axis"], "type": "any"}}, "type": "function"}], "query": "We’re doing pre-spike-sorting delay compensation on a 3‑channel extracellular probe, but the snippet cache is messy (mixed polarity, occasional saturated channels, and variable snippet length). Each snippet is a time×channels buffer and must be phase-shifted along the time axis (axis=0) with the Fourier shift theorem. Use the following protocol:\n\n1) Treat a snippet as *eligible for fine-registration* only if its per-channel peak-to-peak dynamic range is non-trivial on at least two channels (peak-to-peak > 1.0 in at least two channels). \n2) For eligible snippets, estimate the per-channel delay rule from intrinsic polarity:\n   - If a channel’s mean value over the snippet is negative, apply an “earlier” correction of −0.25 samples to that channel.\n   - Otherwise, apply a “later” correction of +0.25 samples to that channel.\n3) Additionally, compensate the putative propagation direction across the shank by applying an extra incremental delay proportional to channel index: add (channel_index × 0.75) samples to each channel (with channel_index = 0,1,2). \n\nRun this on the raw snippet cache below:\n(A) 8-sample buffer [[12.0,10.0,8.0],[11.5,10.2,8.1],[11.0,10.4,8.3],[10.7,10.6,8.5],[10.3,10.7,8.7],[10.0,10.8,8.8],[9.8,10.9,9.0],[9.6,11.0,9.1]]\n(B) 12-sample buffer [[12.3,8.1,-3.2],[11.7,7.9,-3.0],[10.9,7.4,-2.8],[9.8,6.8,-2.5],[8.6,6.1,-2.2],[7.5,5.4,-1.8],[6.7,4.8,-1.4],[6.1,4.3,-1.1],[5.8,4.0,-0.9],[5.6,3.8,-0.7],[5.5,3.7,-0.6],[5.4,3.6,-0.5]]\n(C) 10-sample buffer [[12.0,9.0,7.0],[11.5,9.2,7.4],[11.0,9.5,7.8],[10.6,9.7,8.1],[10.2,9.9,8.3],[9.8,10.1,8.5],[9.5,10.0,8.6],[9.3,9.8,8.7],[9.1,9.6,8.8],[9.0,9.4,8.9]]", "answers": "[{\"name\":\"spikeinterface_preprocessing_phase_shift_apply_frequency_shift\",\"arguments\":{\"signal\":[[12.0,10.0,8.0],[11.5,10.2,8.1],[11.0,10.4,8.3],[10.7,10.6,8.5],[10.3,10.7,8.7],[10.0,10.8,8.8],[9.8,10.9,9.0],[9.6,11.0,9.1]],\"shift_samples\":[0.25,1.0,1.75],\"axis\":0}},{\"name\":\"spikeinterface_preprocessing_phase_shift_apply_frequency_shift\",\"arguments\":{\"signal\":[[12.3,8.1,-3.2],[11.7,7.9,-3.0],[10.9,7.4,-2.8],[9.8,6.8,-2.5],[8.6,6.1,-2.2],[7.5,5.4,-1.8],[6.7,4.8,-1.4],[6.1,4.3,-1.1],[5.8,4.0,-0.9],[5.6,3.8,-0.7],[5.5,3.7,-0.6],[5.4,3.6,-0.5]],\"shift_samples\":[0.25,1.0,1.25],\"axis\":0}},{\"name\":\"spikeinterface_preprocessing_phase_shift_apply_frequency_shift\",\"arguments\":{\"signal\":[[12.0,9.0,7.0],[11.5,9.2,7.4],[11.0,9.5,7.8],[10.6,9.7,8.1],[10.2,9.9,8.3],[9.8,10.1,8.5],[9.5,10.0,8.6],[9.3,9.8,8.7],[9.1,9.6,8.8],[9.0,9.4,8.9]],\"shift_samples\":[0.25,1.0,1.75],\"axis\":0}}]"}
{"func_name": "spikeinterface_preprocessing_prepr___get_kriging_kernel_distance", "func_desc": "Get the kriging kernel between two sets of spatial channel locations for use in spike-sorting preprocessing.", "tools": [{"function": {"description": "Get the kriging kernel between two sets of spatial channel locations for use in spike-sorting preprocessing.\n", "name": "spikeinterface_preprocessing_prepr___get_kriging_kernel_distance", "parameters": {"properties": {"locations_1": {"type": "array", "items": {"type": "float"}, "description": "2D array of shape (N1, D) giving spatial coordinates of the first set of channels/contacts. N1 is the number of channels in the first set and D is the spatial dimensionality (e.g., 2 for [x, y]). In the SpikeInterface domain this represents channel/contact positions (typically in micrometers) used to compute how much one channel influences another for kriging-style spatial interpolation or channel-based weighting in preprocessing.", "default": ""}, "locations_2": {"type": "array", "items": {"type": "float"}, "description": "2D array of shape (N2, D) giving spatial coordinates of the second set of channels/contacts. N2 is the number of channels in the second set and D must match the dimensionality of locations_1. The returned kernel is evaluated pairwise between each location in locations_1 and each location in locations_2.", "default": ""}, "sigma_um": {"type": "array", "items": {"type": "float"}, "description": "Scale parameter(s) for the Gaussian kernel, typically given in micrometers and representing the spatial decay length of channel influence. If a scalar float is provided, a single isotropic scale is used and the function computes pairwise distances with scipy.spatial.distance.cdist using distance_metric and then applies kernal = exp(-(dist / sigma_um) ** p). If a list is provided, it must contain two elements (sigma_x, sigma_y) to mimic Kilosort2.5 behavior where separate scales are applied per spatial dimension; in this branch the code explicitly computes absolute differences along the first two coordinates and applies kernal = exp(- (|dx|/sigma_x)**p - (|dy|/sigma_y)**p). When sigma_um is a list the function forces a cityblock-like separable treatment of dimensions and ignores the distance_metric argument.", "default": ""}, "p": {"type": "float", "description": "Exponent applied to the normalized distance in the kernel: kernel = exp(- (distance / sigma) ** p) (or the separable per-dimension equivalent when sigma_um is a list). In practice p controls how sharply the kernel decays with distance (common choices are p=2 for Gaussian decay). For typical, physically meaningful kernels p should be positive; nonpositive values may produce non-decaying or numerically unstable results.", "default": ""}, "distance_metric": {"type": "string", "description": "Metric name passed to scipy.spatial.distance.cdist when sigma_um is a scalar. Default is \"euclidean\". This parameter is ignored when sigma_um is a list (the list case uses per-dimension absolute differences and does not call cdist).", "default": "euclidean"}}, "required": ["locations_1", "locations_2", "sigma_um", "p", "distance_metric"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing kriging-kernel calibration for spike-sorting on a mixed set of Neuropixels/shank geometry replicates where some channel coordinate tables contain acquisition artifacts. For each replicate, first treat locations as valid channel/site coordinates only if each entry is a 2D point with finite numeric values and the point is unique within its own list (deduplicate exact repeats). Then compute the kriging kernel distance matrix only for replicates where, after this cleaning, both the source set (locations_1) has at least 3 channels and the target set (locations_2) has at least 2 sites.\n\nUse replicate-specific decay rules:\n- If the cleaned geometry spans both x and y in the source set (non-zero range in both axes), use isotropic decay with sigma equal to twice the median nearest-neighbor spacing among sources, Gaussian exponent p=2, and euclidean distance.\n- If the cleaned source geometry is effectively linear (range in exactly one axis is zero), use anisotropic Kilosort-like decay with sigma_x equal to the median nearest-neighbor spacing along x (use 20 if undefined), sigma_y equal to twice the median nearest-neighbor spacing along y (use 40 if undefined), exponent p=2.\n\nRaw replicate inputs:\n1) sources locations_1=[[0.0,0.0],[0.0,20.0],[20.0,0.0],[20.0,20.0],[20.0,20.0]]; targets locations_2=[[10.0,10.0],[40.0,0.0]]\n2) sources locations_1=[[0,0],[0,20],[0,40],[0,60],[0,60],[null,10]]; targets locations_2=[[10,10],[10,30],[10,50]]\n3) sources locations_1=[[0,0],[20,0],[40,0],[60,0]]; targets locations_2=[[10,15],[35,15],[55,15],[55,15]]\n4) sources locations_1=[[0,0],[0,0],[0,0]]; targets locations_2=[[10,10],[10,30]]", "answers": "[{\"name\":\"spikeinterface_preprocessing_prepr___get_kriging_kernel_distance\",\"arguments\":{\"locations_1\":[[0.0,0.0],[0.0,20.0],[20.0,0.0],[20.0,20.0]],\"locations_2\":[[10.0,10.0],[40.0,0.0]],\"sigma_um\":40.0,\"p\":2.0,\"distance_metric\":\"euclidean\"}},{\"name\":\"spikeinterface_preprocessing_prepr___get_kriging_kernel_distance\",\"arguments\":{\"locations_1\":[[0,0],[0,20],[0,40],[0,60]],\"locations_2\":[[10,10],[10,30],[10,50]],\"sigma_um\":[20,40],\"p\":2}},{\"name\":\"spikeinterface_preprocessing_prepr___get_kriging_kernel_distance\",\"arguments\":{\"locations_1\":[[0,0],[20,0],[40,0],[60,0]],\"locations_2\":[[10,15],[35,15],[55,15]],\"sigma_um\":[20,40],\"p\":2}}]"}
{"func_name": "spikeinterface_qualitymetrics_misc_metrics_amplitude_cutoff", "func_desc": "spikeinterface.qualitymetrics.misc_metrics.amplitude_cutoff: Compute an approximate fraction of spikes that are missing for a single unit by analyzing the empirical distribution of spike amplitudes. This function is used in spike sorting quality assessment to estimate how many low-amplitude spikes may have been lost because they fall below the detection threshold; it is intended to be used on the amplitudes (in microvolts) of all detected spikes for one unit and is referenced by higher-level helpers such as compute_amplitude_cutoffs.", "tools": [{"function": {"description": "spikeinterface.qualitymetrics.misc_metrics.amplitude_cutoff: Compute an approximate fraction of spikes that are missing for a single unit by analyzing the empirical distribution of spike amplitudes. This function is used in spike sorting quality assessment to estimate how many low-amplitude spikes may have been lost because they fall below the detection threshold; it is intended to be used on the amplitudes (in microvolts) of all detected spikes for one unit and is referenced by higher-level helpers such as compute_amplitude_cutoffs.\n", "name": "spikeinterface_qualitymetrics_misc_metrics_amplitude_cutoff", "parameters": {"properties": {"amplitudes": {"type": "array", "items": {"type": "float"}, "description": "1D array of spike amplitudes for a single unit, expressed in microvolts (uV). These are the measured peak or trough amplitudes used to characterize the amplitude distribution of the unit. The function treats this array as the empirical sample from which to build a probability density function (PDF) via a histogram; amplitudes must therefore contain at least one value and should represent the same quantity (same sign convention and units) used across units in the recording/analysis pipeline.", "default": ""}, "num_histogram_bins": {"type": "integer", "description": "The number of bins to use when constructing the amplitude histogram (passed to numpy.histogram). A larger number gives finer resolution of the empirical PDF but requires more samples; default is 500. This parameter controls the discretization of the amplitude axis used to estimate the PDF and therefore influences sensitivity to small-scale features in the distribution.", "default": 500}, "histogram_smoothing_value": {"type": "integer", "description": "Controls the amount of smoothing applied to the raw histogram to estimate a continuous PDF. The value is passed as the sigma parameter to scipy.ndimage.gaussian_filter1d and defaults to 3. Higher values produce a smoother PDF that reduces sensitivity to high-frequency noise in the histogram, while lower values retain more fine detail; choose based on expected sample size and noise.", "default": 3}, "amplitudes_bins_min_ratio": {"type": "integer", "description": "Minimum required ratio between the number of amplitude samples (len(amplitudes)) and num_histogram_bins. If len(amplitudes) / num_histogram_bins is less than this integer threshold, the function will not attempt a robust histogram-based estimation and will return numpy.nan. Default is 5. This guard prevents unreliable estimates when the per-bin sample count would be too small.", "default": 5}}, "required": ["amplitudes", "histogram_smoothing_value", "amplitudes_bins_min_ratio", "num_histogram_bins"], "type": "any"}}, "type": "function"}], "query": "I’m doing a quick post-sort QC sweep on two putative single units, but the exported amplitude vectors are messy and were logged with mixed conventions. For each unit, first sanitize the amplitude series to retain only finite, strictly positive values that look like physiological spike amplitudes (treat any negative values, zeros, NaNs, or non-physiologically large entries as acquisition artifacts). Then compute `spikeinterface.qualitymetrics.misc_metrics.amplitude_cutoff` with an adaptive histogram rule per unit: choose `num_histogram_bins` as the integer-rounded square-root of the number of retained spikes, set `histogram_smoothing_value` to 4 if the retained amplitude range (max-min) is at least 50 µV, otherwise 2, and set `amplitudes_bins_min_ratio` to 3 when the retained spike count is below 30, otherwise 2.\n\nRaw cohorts:\n- Cohort A (motor cortex, peak amplitudes in µV, but export includes a few corrupted markers): [98.2, 102.5, 110.3, 95.7, 120.1, 105.4, 99.9, 101.2, 115.7, 108.3, 112.6, 97.4, 93.5, 118.9, 104.1, 100.5, 96.8, 111.7, 109.2, 103.6, 0.0, -12.0, 9999.0, NaN]\n- Cohort B (unit #7, peak-to-peak amplitudes in µV; includes a couple of glitch values): [78.2, 81.5, 76.9, 83.1, 79.8, 85.4, 74.6, 88.0, 90.3, 72.5, 95.1, 69.7, 66.2, 92.4, 84.7, 80.9, 77.6, 86.8, 89.5, 73.3, 71.1, 82.6, 87.9, 91.2, 68.4, 64.9, 62.7, 96.3, 98.8, 100.5, 58.1, 55.6, 53.9, 51.2, 49.8, 47.5, 45.9, 44.1, 42.6, 41.3, -1.0, 0.0, NaN, 1e6]\n\nReturn the estimated missing-spike fraction for each cohort after applying the above rules.", "answers": "[{\"name\":\"spikeinterface_qualitymetrics_misc_metrics_amplitude_cutoff\",\"arguments\":{\"amplitudes\":[98.2,102.5,110.3,95.7,120.1,105.4,99.9,101.2,115.7,108.3,112.6,97.4,93.5,118.9,104.1,100.5,96.8,111.7,109.2,103.6],\"num_histogram_bins\":4,\"histogram_smoothing_value\":2,\"amplitudes_bins_min_ratio\":3}},{\"name\":\"spikeinterface_qualitymetrics_misc_metrics_amplitude_cutoff\",\"arguments\":{\"amplitudes\":[78.2,81.5,76.9,83.1,79.8,85.4,74.6,88.0,90.3,72.5,95.1,69.7,66.2,92.4,84.7,80.9,77.6,86.8,89.5,73.3,71.1,82.6,87.9,91.2,68.4,64.9,62.7,96.3,98.8,100.5,58.1,55.6,53.9,51.2,49.8,47.5,45.9,44.1,42.6,41.3],\"num_histogram_bins\":6,\"histogram_smoothing_value\":4,\"amplitudes_bins_min_ratio\":2}}]"}
{"func_name": "spikeinterface_qualitymetrics_misc_metrics_presence_ratio", "func_desc": "Calculate the presence ratio for a single unit across a recording by dividing the number of temporal bins in which the unit is \"active\" by the total number of temporal bins. This metric is used in spike sorting quality assessment (see SpikeInterface quality metrics) to quantify how consistently a sorted unit is present throughout the recording: values close to 1 indicate the unit fires across most of the recording, values close to 0 indicate the unit is only present in a small fraction of the recording.", "tools": [{"function": {"description": "Calculate the presence ratio for a single unit across a recording by dividing the number of temporal bins in which the unit is \"active\" by the total number of temporal bins. This metric is used in spike sorting quality assessment (see SpikeInterface quality metrics) to quantify how consistently a sorted unit is present throughout the recording: values close to 1 indicate the unit fires across most of the recording, values close to 0 indicate the unit is only present in a small fraction of the recording.\n", "name": "spikeinterface_qualitymetrics_misc_metrics_presence_ratio", "parameters": {"properties": {"spike_train": {"type": "array", "items": {"type": "float"}, "description": "1-D array of spike times for this unit, expressed in samples. These are the event times used to assign spikes to temporal bins; they do not need to be sorted. In the spike-sorting domain this represents the sample indices of detected spikes for a single unit produced by a sorter or post-processing step.", "default": ""}, "total_length": {"type": "integer", "description": "Total length of the recording in samples. This parameter documents the expected recording duration (in samples) for the spike_train and is part of the function API in SpikeInterface quality metrics; note that the current implementation does not use this value internally but callers should supply the recording length in samples for API consistency and future compatibility.", "default": ""}, "bin_edges": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. An explicit array of bin edge positions (in samples) to use to partition the recording into temporal bins. Mutually exclusive with num_bin_edges. If provided, these edges are passed directly to numpy.histogram and the effective number of histogram bins is len(bin_edges) - 1. Values in spike_train that fall outside the provided edges are not counted in the returned histogram bins (consistent with numpy.histogram behavior). Providing bin_edges allows callers to define uneven or externally computed bin boundaries (for example, to align bins to behavioral epochs).", "default": null}, "num_bin_edges": {"type": "integer", "nullable": true, "description": "Optional. The number of bin edges to use to compute the presence ratio (mutually exclusive with bin_edges). Interpreted as the number of edges, so the effective number of histogram bins used is num_bin_edges - 1. If bin_edges is not provided, num_bin_edges is passed to numpy.histogram as the bins argument (an integer), which causes numpy to compute an even partitioning across the range of spike times. This parameter controls the temporal resolution of the presence ratio: larger num_bin_edges (hence more bins) yields a finer-grained assessment of presence across time.", "default": null}, "bin_n_spikes_thres": {"type": "integer", "description": "Minimum number of spikes required within a bin for that bin to be considered \"active\" (default: 0). In practice, with the default 0 a bin is considered active if it contains at least one spike; setting this to a larger integer makes the presence ratio stricter by requiring more spikes within a bin to count it as presence. Must be >= 0.", "default": 0}}, "required": ["spike_train", "total_length", "bin_n_spikes_thres", "num_bin_edges"], "type": "any"}}, "type": "function"}], "query": "We’re re-running unit-stability QC across two recordings, but the upstream sorter produced messy spike trains (edge hits, occasional clock resets). Use the presence ratio definition with an activity threshold of 2 spikes/bin, but apply recording-specific binning rules derived from the data quality:\n\n1) First, sanitize each spike train by keeping only spike times that fall within the recording bounds [0, total_length). Treat the remaining times as the final spike train.\n2) Choose the binning resolution based on unit firing sparsity after sanitization:\n   - If the sanitized spike train contains fewer than 15 spikes, use 20 equal-width bins (21 bin edges).\n   - Otherwise, use 60 equal-width bins (61 bin edges).\n\nRecordings:\n(A) Short acute session: fs=30 kHz, total_length=300000 samples. Candidate spike times for one unit: [-10, 1200, 5400, 12100, 30500, 61200, 89000, 90500, 150200, 151100, 210500, 270900, 299100, 300000, 450000].\n(B) Long chronic session: fs=30 kHz, total_length=18000000 samples. Candidate spike times for unit #7: [-200, 1500, 29990, 60010, 120000, 450000, 900000, 1200500, 2400000, 3600000, 5400000, 7200000, 9000000, 10800000, 12600000, 14400000, 16200000, 17900000, 18000000].\n\nCompute the presence ratio for each recording/unit under these rules.", "answers": "[{\"name\":\"spikeinterface_qualitymetrics_misc_metrics_presence_ratio\",\"arguments\":{\"spike_train\":[1200,5400,12100,30500,61200,89000,90500,150200,151100,210500,270900,299100],\"total_length\":300000,\"num_bin_edges\":21,\"bin_n_spikes_thres\":2}},{\"name\":\"spikeinterface_qualitymetrics_misc_metrics_presence_ratio\",\"arguments\":{\"spike_train\":[1500,29990,60010,120000,450000,900000,1200500,2400000,3600000,5400000,7200000,9000000,10800000,12600000,14400000,16200000,17900000],\"total_length\":18000000,\"num_bin_edges\":61,\"bin_n_spikes_thres\":2}}]"}
{"func_name": "spikeinterface_sorters_container_tools_find_recording_folders", "func_desc": "spikeinterface.sorters.container_tools.find_recording_folders finds the minimal set of filesystem folders that contain recording file paths described in a SpikeInterface-style dictionary and prepares them for use as container mount points.\n    \n    This function is used in the container tooling of SpikeInterface (a framework to run spike sorters in Docker/Singularity containers) to determine which host folders need to be mounted into a container so the sorter can access the recording files. It extracts file paths from the provided dictionary using the internal helper _get_paths_list, resolves each path, reduces them to their parent directories, and attempts to compute a single common parent folder when possible to minimize the number of mounts. The function performs only path computations and does not perform any I/O, create mounts, or modify the input dictionary.", "tools": [{"function": {"description": "spikeinterface.sorters.container_tools.find_recording_folders finds the minimal set of filesystem folders that contain recording file paths described in a SpikeInterface-style dictionary and prepares them for use as container mount points.\n\nThis function is used in the container tooling of SpikeInterface (a framework to run spike sorters in Docker/Singularity containers) to determine which host folders need to be mounted into a container so the sorter can access the recording files. It extracts file paths from the provided dictionary using the internal helper _get_paths_list, resolves each path, reduces them to their parent directories, and attempts to compute a single common parent folder when possible to minimize the number of mounts. The function performs only path computations and does not perform any I/O, create mounts, or modify the input dictionary.", "name": "spikeinterface_sorters_container_tools_find_recording_folders", "parameters": {"properties": {"d": {"type": "any", "description": "A dictionary containing recording metadata and file paths in the format expected by SpikeInterface container utilities. The helper _get_paths_list(d=d) is called to extract a list of raw file-system paths (strings) from this dictionary. The caller is responsible for providing a dictionary where recording file paths can be found by that helper; if _get_paths_list raises an exception for an unexpected structure or missing keys, this function will not catch it.", "default": ""}}, "required": ["d"], "type": "any"}}, "type": "function"}], "query": "We’re planning container mount points for a mixed-quality spike-sorting ingestion batch where only *actionable* recording assets should drive mounts. For each cohort dictionary below (treat as independent replicates), compute the minimal set of host filesystem folders to mount **using these rules**:\n\n1) Only consider file-path-like entries that resolve to an absolute filesystem location and correspond to typical recording dependencies: raw binary signals, extractor-declared file paths, probe/geometry definitions, and recording parameter/config files.\n2) Treat anything that looks like derived/cached/intermediate products (e.g., feature caches, analysis outputs) or operator notes/logs as non-critical for container access and therefore not mount-driving.\n3) Paths may include redundant segments like '.'/'..' or multiple slashes; normalize them via path resolution before computing parent folders.\n4) After collecting eligible paths, reduce them to parent directories and minimize mounts by collapsing to a single common parent directory when possible; otherwise return the minimal covering set of parent folders.\n\nDo not perform I/O and do not modify the dictionaries.\n\nCohort A (multi-session extractor + scratch artifacts):\n{\n  \"recording\": {\n    \"extractor\": \"BinaryRecordingExtractor\",\n    \"kwargs\": {\n      \"file_paths\": [\n        \"/data/neuropixels/session_2025-05-14/recording.bin\",\n        \"/data/neuropixels/session_2025-05-14/channel_positions.npy\",\n        \"/data/neuropixels/session_2025-05-15/recording.bin\",\n        \"/data/neuropixels/session_2025-05-15/../session_2025-05-15/_scratch/waveforms_cache.npy\"\n      ],\n      \"sampling_frequency\": 30000.0,\n      \"num_channels\": 384\n    }\n  }\n}\n\nCohort B (raw + params + probe + cached features + relative path noise):\n{\n  \"recording\": {\n    \"files\": {\n      \"raw_path\": \"/data/ephys/session_01/raw//recording.bin\",\n      \"params_path\": \"/data/ephys/session_01/raw/params.json\"\n    },\n    \"probe\": {\n      \"geometry_path\": \"/data/ephys/common/probes/probeA.json\"\n    },\n    \"cache\": {\n      \"features_path\": \"/data/ephys/session_01/cache/features/feat.npy\"\n    },\n    \"qc\": {\n      \"report_path\": \"../reports/session_01_qc.pdf\"\n    }\n  }\n}\n\nCohort C (main + aux + shared probe + logs + derived exports):\n{\n  \"main_recording\": {\n    \"folder_path\": \"/data/experiments/session_01/recordings\",\n    \"binary_file\": \"/data/experiments/session_01/recordings/raw_signal.bin\",\n    \"params_file\": \"/data/experiments/session_01/config/recording_params.json\",\n    \"exported_phy\": \"/data/experiments/session_01/exports/phy\"\n  },\n  \"aux_recording\": {\n    \"folder_path\": \"/data/experiments/session_02/recordings\",\n    \"binary_file\": \"/data/experiments/session_02/recordings/raw_signal.bin\",\n    \"probe_file\": \"/data/experiments/shared/probes/neuropixels_v1.prb\"\n  },\n  \"metadata\": {\n    \"experiment_log\": \"/data/experiments/shared/logs/exp_log_2024_07_15.txt\"\n  }\n}\n\nReturn the computed minimal mount-point folder list for each cohort (A, B, C) under these rules.", "answers": "[{\"name\":\"spikeinterface_sorters_container_tools_find_recording_folders\",\"arguments\":{\"d\":{\"recording\":{\"extractor\":\"BinaryRecordingExtractor\",\"kwargs\":{\"file_paths\":[\"/data/neuropixels/session_2025-05-14/recording.bin\",\"/data/neuropixels/session_2025-05-14/channel_positions.npy\",\"/data/neuropixels/session_2025-05-15/recording.bin\",\"/data/neuropixels/session_2025-05-15/../session_2025-05-15/_scratch/waveforms_cache.npy\"],\"sampling_frequency\":30000.0,\"num_channels\":384}}}}},{\"name\":\"spikeinterface_sorters_container_tools_find_recording_folders\",\"arguments\":{\"d\":{\"recording\":{\"files\":{\"raw_path\":\"/data/ephys/session_01/raw//recording.bin\",\"params_path\":\"/data/ephys/session_01/raw/params.json\"},\"probe\":{\"geometry_path\":\"/data/ephys/common/probes/probeA.json\"},\"cache\":{\"features_path\":\"/data/ephys/session_01/cache/features/feat.npy\"},\"qc\":{\"report_path\":\"../reports/session_01_qc.pdf\"}}}}},{\"name\":\"spikeinterface_sorters_container_tools_find_recording_folders\",\"arguments\":{\"d\":{\"main_recording\":{\"folder_path\":\"/data/experiments/session_01/recordings\",\"binary_file\":\"/data/experiments/session_01/recordings/raw_signal.bin\",\"params_file\":\"/data/experiments/session_01/config/recording_params.json\",\"exported_phy\":\"/data/experiments/session_01/exports/phy\"},\"aux_recording\":{\"folder_path\":\"/data/experiments/session_02/recordings\",\"binary_file\":\"/data/experiments/session_02/recordings/raw_signal.bin\",\"probe_file\":\"/data/experiments/shared/probes/neuropixels_v1.prb\"},\"metadata\":{\"experiment_log\":\"/data/experiments/shared/logs/exp_log_2024_07_15.txt\"}}}}]"}
{"func_name": "spikeinterface_sortingcomponents_c__ing_isosplit_isocut_isosplit", "func_desc": "spikeinterface.sortingcomponents.clustering.isosplit_isocut.isosplit\n    Performs clustering of feature vectors using a Python/numba implementation of the Isosplit algorithm (Jeremy Magland's isosplit6) tailored for spike sorting components. This function is typically used in spike sorting pipelines to cluster spike waveform features or low-dimensional embeddings so that putative neural units (clusters) are automatically discovered from extracellular recording features. The implementation first optionally initializes cluster assignments with k-means (many centroids) and then iteratively agglomerates clusters using a dip-test based \"isocut\" criterion to merge similar clusters and enforce a minimum cluster size.", "tools": [{"function": {"description": "spikeinterface.sortingcomponents.clustering.isosplit_isocut.isosplit\nPerforms clustering of feature vectors using a Python/numba implementation of the Isosplit algorithm (Jeremy Magland's isosplit6) tailored for spike sorting components. This function is typically used in spike sorting pipelines to cluster spike waveform features or low-dimensional embeddings so that putative neural units (clusters) are automatically discovered from extracellular recording features. The implementation first optionally initializes cluster assignments with k-means (many centroids) and then iteratively agglomerates clusters using a dip-test based \"isocut\" criterion to merge similar clusters and enforce a minimum cluster size.", "name": "spikeinterface_sortingcomponents_c__ing_isosplit_isocut_isosplit", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "Input data matrix containing samples to cluster. Expected shape is (num_samples, num_dim), where num_samples is the number of spike-derived feature vectors (e.g., PCA components or waveform features) and num_dim is the dimensionality of each feature vector. The values and dtype in X are preserved during centroid and covariance computations; invalid shapes (not 2-D) or incompatible dtypes will raise standard NumPy errors from downstream numerical routines.", "default": ""}, "initial_labels": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional initial integer label array of length num_samples to use as starting cluster assignments instead of performing the internal k-means initialization. When provided, these labels are normalized internally to contiguous 0-based integer labels. Use this parameter to resume clustering from a previous assignment or to provide a domain-specific initialization (for example, pre-clustered spike events). If None (default), the routine generates initial labels by running scipy.cluster.vq.kmeans2 with n_init centers. The provided array must have one label per row of X; mismatched lengths will result in standard array-shape errors.", "default": null}, "n_init": {"type": "integer", "description": "Initial number of k-means centroids to use when initial_labels is None. Default is 200. In the spike-sorting context this parameter controls the granularity of the initial partitioning of spike feature space: larger n_init can help separate fine-grained structure but increases runtime. The implementation guards against excessively large n_init relative to the number of samples and min_cluster_size: if n_init >= num_samples or n_init > num_samples // min_cluster_size, a warning is emitted and n_init is reduced to max(1, num_samples // (min_cluster_size * 2)).", "default": 200}, "max_iterations_per_pass": {"type": "integer", "description": "Maximum number of pairwise comparison iterations allowed in a single agglomerative pass. Default is 500. Each pass attempts to merge cluster pairs and redistribute labels; if this limit is exceeded the function emits a warning (\"isosplit : max iterations per pass exceeded\") and proceeds to the next control step. This protects against pathological label oscillations or very slow convergence.", "default": 500}, "min_cluster_size": {"type": "integer", "description": "Minimum allowed cluster size in number of samples. Default is 10. Clusters with fewer samples than min_cluster_size are considered too small and are merged with neighboring clusters during the agglomerative passes. In spike sorting, this helps avoid creating spurious units from a few noisy waveforms. Changing this value affects the algorithm's propensity to merge small clusters and therefore affects unit count stability.", "default": 10}, "isocut_threshold": {"type": "float", "description": "Threshold used by the isocut dip-test merge criterion. Default is 2.0. During pairwise cluster comparisons, a \"dipscore\" (measure from the isocut procedure) is computed; when dipscore < isocut_threshold the algorithm treats the pair as not sufficiently separated and merges them. Lowering this value makes merging stricter (fewer merges), raising it makes merging more permissive.", "default": 2.0}, "seed": {"type": "integer", "nullable": true, "description": "Optional random seed forwarded to scipy.cluster.vq.kmeans2 when initial_labels is None to control the k-means initialization. Default is None, which yields non-deterministic initialization subject to the execution environment's RNG. Providing an integer produces reproducible k-means initializations across runs (useful for deterministic pipeline behavior and reproducible spike-sorting experiments).", "default": null}}, "required": ["X", "seed", "n_init", "isocut_threshold", "min_cluster_size", "max_iterations_per_pass"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing an isosplit/isocut clustering stage inside a spike-sorting pipeline where some detected events are likely non-spike artifacts after preprocessing. You’re given three independent PCA feature cohorts (12 events each; rows = events). For each cohort, first screen the events using a cohort-specific plausibility rule to remove obvious artifacts, then run isosplit on the retained events only. Keep the isocut merge threshold fixed at 1.8 and use a reproducible seed of 42 for all runs. Cohort A (3D): retain only events whose third PCA component is positive; then cluster with k-means multi-centroid initialization using n_init=8, max_iterations_per_pass=300, min_cluster_size=4. Use X_A = [[0.12,-1.05,0.33],[0.18,-0.98,0.29],[0.15,-1.1,0.35],[2.05,1.92,-0.4],[1.98,2.1,-0.35],[2.12,1.85,-0.45],[-1.5,0.2,1.1],[-1.42,0.15,1.05],[-1.55,0.28,1.18],[-0.1,0.05,-0.2],[-0.08,0.02,-0.18],[-0.12,0.08,-0.22]]. Cohort B (2D): retain only events whose Euclidean norm is <= 2.05 (treat extreme-radius points as drift/outlier detections); then cluster with n_init=10, max_iterations_per_pass=200, min_cluster_size=3. Use X_B = [[-2.1,0.3],[-1.9,0.1],[-2.2,0.4],[-1.8,0.2],[0.0,1.1],[0.2,0.9],[-0.1,1.0],[0.1,1.2],[2.0,-0.5],[1.8,-0.6],[2.2,-0.4],[1.9,-0.7]]. Cohort C (2D): retain only events with y >= 0 (reject events with negative second component as likely baseline/edge artifacts); then cluster with n_init=6, max_iterations_per_pass=300, min_cluster_size=2. Use X_C = [[0.12,-0.05],[0.15,-0.02],[0.11,-0.08],[2.95,3.1],[3.05,2.85],[2.9,3.0],[-2.1,1.95],[-2.25,2.05],[-1.95,1.85],[0.5,2.2],[0.55,2.1],[0.45,2.3]]. Return cluster assignments for each cohort on the retained events (reporting only what was clustered).", "answers": "[{\"name\":\"spikeinterface_sortingcomponents_c__ing_isosplit_isocut_isosplit\",\"arguments\":{\"X\":[[0.12,-1.05,0.33],[0.18,-0.98,0.29],[0.15,-1.1,0.35],[-1.5,0.2,1.1],[-1.42,0.15,1.05],[-1.55,0.28,1.18]],\"n_init\":8,\"max_iterations_per_pass\":300,\"min_cluster_size\":4,\"isocut_threshold\":1.8,\"seed\":42}},{\"name\":\"spikeinterface_sortingcomponents_c__ing_isosplit_isocut_isosplit\",\"arguments\":{\"X\":[[-1.9,0.1],[-1.8,0.2],[0.0,1.1],[0.2,0.9],[-0.1,1.0],[0.1,1.2],[2.0,-0.5],[1.8,-0.6],[1.9,-0.7]],\"n_init\":10,\"max_iterations_per_pass\":200,\"min_cluster_size\":3,\"isocut_threshold\":1.8,\"seed\":42}},{\"name\":\"spikeinterface_sortingcomponents_c__ing_isosplit_isocut_isosplit\",\"arguments\":{\"X\":[[2.95,3.1],[3.05,2.85],[2.9,3.0],[-2.1,1.95],[-2.25,2.05],[-1.95,1.85],[0.5,2.2],[0.55,2.1],[0.45,2.3]],\"n_init\":6,\"max_iterations_per_pass\":300,\"min_cluster_size\":2,\"isocut_threshold\":1.8,\"seed\":42}}]"}
{"func_name": "spikeinterface_sortingcomponents_cl__ut_ensure_continuous_labels", "func_desc": "Ensure that a set of integer labels is remapped to a contiguous zero-based range [0, N-1].", "tools": [{"function": {"description": "Ensure that a set of integer labels is remapped to a contiguous zero-based range [0, N-1].\n", "name": "spikeinterface_sortingcomponents_cl__ut_ensure_continuous_labels", "parameters": {"properties": {"labels": {"type": "array", "items": {"type": "float"}, "description": "Array of labels produced by a clustering or sorting step in SpikeInterface. In the SpikeInterface clustering/isotonic components (for example, isosplit/isocut), labels are often used as indices into arrays or as initial integer identifiers for clusters or spikes; this function remaps the existing label values to a compact, continuous set of integers starting at 0 so they can be used reliably as array indices or initial indices. The array is inspected with numpy.unique to determine the distinct label values; the original dtype is preserved when allocating the output buffer.", "default": ""}}, "required": ["labels"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating spike-sorting outputs from three recording replicates where the sorter emits a mix of true unit IDs and a dedicated noise code (-1). Before downstream feature tensorization, standardize each replicate’s labels by remapping only the non-noise unit IDs into a contiguous zero-based range [0..N-1] suitable for direct NumPy indexing, while preserving any occurrences of the noise code (-1) as noise (not remapped into the unit-ID range). Apply this separately per replicate to the following label streams: (A) [10, 10, -1, 42, 10, 42, 7, -1, 7, 7], (B) [0, 0, 5, 5, 10, 10, 10, -1], and (C) [10, 10, 42, 42, 42, 7, 7, 100, 10].", "answers": "[{\"name\":\"spikeinterface_sortingcomponents_cl__ut_ensure_continuous_labels\",\"arguments\":{\"labels\":[10,10,-1,42,10,42,7,-1,7,7]}},{\"name\":\"spikeinterface_sortingcomponents_cl__ut_ensure_continuous_labels\",\"arguments\":{\"labels\":[0,0,5,5,10,10,10,-1]}},{\"name\":\"spikeinterface_sortingcomponents_cl__ut_ensure_continuous_labels\",\"arguments\":{\"labels\":[10,10,42,42,42,7,7,100,10]}}]"}
{"func_name": "spikeinterface_sortingcomponents_mo__n_dredge_newton_solve_rigid", "func_desc": "Solve the rigid Newton step used in the motion-correction dredge component of SpikeInterface.\n    \n    This function builds and solves the linear Newton system for a rigid motion update p used in SpikeInterface's sortingcomponents.motion.dredge workflow. It combines a prior inverse covariance term Sigma0inv with a data-derived negative Hessian of the likelihood (negHU) computed from subsampling/soft-weights, and a right-hand side assembled from the displacement matrix D and optional boundary/adjacent-block contributions. The result p is the Newton update that would be applied to rigid motion parameters in downstream motion-correction steps of the SpikeInterface pipeline.", "tools": [{"function": {"description": "Solve the rigid Newton step used in the motion-correction dredge component of SpikeInterface.\n\nThis function builds and solves the linear Newton system for a rigid motion update p used in SpikeInterface's sortingcomponents.motion.dredge workflow. It combines a prior inverse covariance term Sigma0inv with a data-derived negative Hessian of the likelihood (negHU) computed from subsampling/soft-weights, and a right-hand side assembled from the displacement matrix D and optional boundary/adjacent-block contributions. The result p is the Newton update that would be applied to rigid motion parameters in downstream motion-correction steps of the SpikeInterface pipeline.", "name": "spikeinterface_sortingcomponents_mo__n_dredge_newton_solve_rigid", "parameters": {"properties": {"D": {"type": "array", "items": {"type": "float"}, "description": "T x T displacement matrix. In the SpikeInterface motion-dredge context, D encodes observed displacements (differences) between time points or blocks; it is used to form the right-hand side of the Newton linear system. The function expects a square matrix with the same time dimension T used by U and Sigma0inv.", "default": ""}, "U": {"type": "array", "items": {"type": "float"}, "description": "T x T subsampling or soft-weights matrix. In practice within SpikeInterface this matrix weights contributions from different time points or samples (e.g., subsampling patterns or soft assignment weights) when computing both the negative Hessian of the likelihood and the Newton right-hand side. U must have the same square shape T x T as D.", "default": ""}, "Sigma0inv": {"type": "array", "items": {"type": "float"}, "description": "Prior inverse covariance matrix (numpy.ndarray). This matrix encodes the Gaussian prior precision on the rigid parameters and is added to the data-derived negative Hessian to form the coefficient matrix in the Newton linear system. It must be conformable with negHU (typically T x T).", "default": ""}, "Pb_prev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional boundary/previous-block prior term. When provided, this array supplies prior or previous-block parameter information that newton_rhs uses to modify the right-hand side targ for continuity or regularization across adjacent blocks in blockwise motion correction. Pass None when no previous-block prior information is available.", "default": null}, "Db_prevcur": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional displacement term between previous and current blocks. When supplied, this array provides observed displacements specifically across the boundary from a previous block to the current block; newton_rhs uses it to augment the Newton right-hand side to account for inter-block motion. Pass None if no previous→current boundary displacement is used.", "default": null}, "Ub_prevcur": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional subsampling/weight matrix for previous→current boundary. This optional T x T (or block-conformable) weights matrix is passed to neg_hessian_likelihood_term and newton_rhs so boundary-weighted likelihood/Hessian contributions are included. Pass None when there are no boundary weighting contributions from previous to current block.", "default": null}, "Db_curprev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional displacement term between current and previous blocks (current→previous). When provided, this array supplies observed displacements across the boundary in the reverse direction; newton_rhs uses it to form the right-hand side to enforce consistency across blocks. Use None if not applicable.", "default": null}, "Ub_curprev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional subsampling/weight matrix for current→previous boundary. This optional weights matrix is used by neg_hessian_likelihood_term and newton_rhs to incorporate boundary-weighted likelihood/Hessian contributions from the current-to-previous direction. Pass None if no such contribution is present.", "default": null}}, "required": ["D", "U", "Sigma0inv", "Pb_prev", "Db_curprev", "Ub_prevcur", "Ub_curprev", "Db_prevcur"], "type": "any"}}, "type": "function"}], "query": "We’re validating SpikeInterface’s rigid Newton step inside a motion-correction dredge pass where some time-blocks are well-conditioned and others are boundary-dominated. You’re given two candidate 4×4 cohorts (each cohort provides D, U, and Sigma0inv plus optional boundary couplings). Treat each cohort as a separate replicate, but apply workflow-like gating:\n\n1) Only run the rigid Newton solve for replicates whose U looks like a usable soft-assignment similarity matrix (symmetric with unit diagonal).\n2) For usable replicates, decide whether to include boundary coupling based on whether the replicate’s prior precision is strictly diagonal (interpret that as a strong independent prior where boundary terms are allowed to regularize the edges). If Sigma0inv is not strictly diagonal, run the solve as an interior-only update with all boundary inputs treated as absent.\n\nReplicate R1 (candidate boundary-dominated):\n- D = [[0.0,0.2,-0.1,0.0],[-0.2,0.0,0.15,0.05],[0.1,-0.15,0.0,-0.25],[0.0,-0.05,0.25,0.0]]\n- U = [[1.0,0.6,0.2,0.1],[0.6,1.0,0.5,0.2],[0.2,0.5,1.0,0.7],[0.1,0.2,0.7,1.0]]\n- Sigma0inv = diag(10,10,10,10)\n- Previous-block coupling: Pb_prev=[0.05,-0.02,0.01,0.0], Db_prevcur=[[0.0,0.1,0.0,0.0],[-0.1,0.0,0.05,0.0],[0.0,-0.05,0.0,0.02],[0.0,0.0,-0.02,0.0]], Ub_prevcur=[[0.5,0.2,0.0,0.0],[0.2,0.5,0.2,0.0],[0.0,0.2,0.5,0.2],[0.0,0.0,0.2,0.5]]\n- Reverse coupling: Db_curprev=[[0.0,-0.08,0.0,0.0],[0.08,0.0,-0.04,0.0],[0.0,0.04,0.0,-0.01],[0.0,0.0,0.01,0.0]], Ub_curprev=[[0.4,0.1,0.0,0.0],[0.1,0.4,0.1,0.0],[0.0,0.1,0.4,0.1],[0.0,0.0,0.1,0.4]]\n\nReplicate R2 (candidate interior-only):\n- D = [[0.0,1.2,2.1,3.0],[-1.2,0.0,0.9,1.8],[-2.1,-0.9,0.0,0.7],[-3.0,-1.8,-0.7,0.0]]\n- U = [[1.0,0.8,0.5,0.3],[0.8,1.0,0.6,0.4],[0.5,0.6,1.0,0.7],[0.3,0.4,0.7,1.0]]\n- Sigma0inv = [[2.0,-0.5,0.0,0.0],[-0.5,2.0,-0.5,0.0],[0.0,-0.5,2.0,-0.5],[0.0,0.0,-0.5,2.0]]\n- Boundary couplings are available in the lab notebook but should only be used if the rule above selects boundary mode.\n\nCompute the rigid motion Newton update vector p for every replicate that passes the U check, using boundary mode vs interior-only mode per the Sigma0inv rule.", "answers": "[{\"name\":\"spikeinterface_sortingcomponents_mo__n_dredge_newton_solve_rigid\",\"arguments\":{\"D\":[[0.0,0.2,-0.1,0.0],[-0.2,0.0,0.15,0.05],[0.1,-0.15,0.0,-0.25],[0.0,-0.05,0.25,0.0]],\"U\":[[1.0,0.6,0.2,0.1],[0.6,1.0,0.5,0.2],[0.2,0.5,1.0,0.7],[0.1,0.2,0.7,1.0]],\"Sigma0inv\":[[10.0,0.0,0.0,0.0],[0.0,10.0,0.0,0.0],[0.0,0.0,10.0,0.0],[0.0,0.0,0.0,10.0]],\"Pb_prev\":[0.05,-0.02,0.01,0.0],\"Db_prevcur\":[[0.0,0.1,0.0,0.0],[-0.1,0.0,0.05,0.0],[0.0,-0.05,0.0,0.02],[0.0,0.0,-0.02,0.0]],\"Ub_prevcur\":[[0.5,0.2,0.0,0.0],[0.2,0.5,0.2,0.0],[0.0,0.2,0.5,0.2],[0.0,0.0,0.2,0.5]],\"Db_curprev\":[[0.0,-0.08,0.0,0.0],[0.08,0.0,-0.04,0.0],[0.0,0.04,0.0,-0.01],[0.0,0.0,0.01,0.0]],\"Ub_curprev\":[[0.4,0.1,0.0,0.0],[0.1,0.4,0.1,0.0],[0.0,0.1,0.4,0.1],[0.0,0.0,0.1,0.4]]}},{\"name\":\"spikeinterface_sortingcomponents_mo__n_dredge_newton_solve_rigid\",\"arguments\":{\"D\":[[0.0,1.2,2.1,3.0],[-1.2,0.0,0.9,1.8],[-2.1,-0.9,0.0,0.7],[-3.0,-1.8,-0.7,0.0]],\"U\":[[1.0,0.8,0.5,0.3],[0.8,1.0,0.6,0.4],[0.5,0.6,1.0,0.7],[0.3,0.4,0.7,1.0]],\"Sigma0inv\":[[2.0,-0.5,0.0,0.0],[-0.5,2.0,-0.5,0.0],[0.0,-0.5,2.0,-0.5],[0.0,0.0,-0.5,2.0]],\"Pb_prev\":null,\"Db_prevcur\":null,\"Ub_prevcur\":null,\"Db_curprev\":null,\"Ub_curprev\":null}}]"}
{"func_name": "spikeinterface_sortingcomponents_motion_dredge_laplacian", "func_desc": "spikeinterface.sortingcomponents.motion.dredge.laplacian constructs a dense discrete 1D Laplacian matrix (with an added small ridge/identity term) used as a regularization / smoothing operator in SpikeInterface sorting components (for example, within motion or drift regularization steps of custom sorters built with the sortingcomponents). The returned matrix is a square numpy.ndarray of shape (n, n) whose main diagonal and immediate off-diagonals encode a second-difference operator scaled by lambd, with an additional eps-scaled identity term to improve numerical stability; an optional ridge_mask allows spatially varying small diagonal augmentation.", "tools": [{"function": {"description": "spikeinterface.sortingcomponents.motion.dredge.laplacian constructs a dense discrete 1D Laplacian matrix (with an added small ridge/identity term) used as a regularization / smoothing operator in SpikeInterface sorting components (for example, within motion or drift regularization steps of custom sorters built with the sortingcomponents). The returned matrix is a square numpy.ndarray of shape (n, n) whose main diagonal and immediate off-diagonals encode a second-difference operator scaled by lambd, with an additional eps-scaled identity term to improve numerical stability; an optional ridge_mask allows spatially varying small diagonal augmentation.\n", "name": "spikeinterface_sortingcomponents_motion_dredge_laplacian", "parameters": {"properties": {"n": {"type": "integer", "description": "The size of the square matrix to construct; the number of rows and columns of the returned Laplacian operator. This integer specifies the number of discrete points over which the second-difference (discrete Laplacian) is defined. The function allocates and returns a dense (n, n) numpy.ndarray; very large n will increase memory and time quadratically and may raise MemoryError from numpy.", "default": ""}, "wink": {"type": "boolean", "description": "If True (default True), adjust the first and last diagonal entries by subtracting 0.5 * lambd so that endpoint diagonal values are halved relative to interior diagonals. Practically, this modifies the boundary behavior of the discrete Laplacian (commonly used to approximate natural/Neumann-like boundary conditions in 1D regularization). If False, endpoints are treated the same as interior points.", "default": true}, "eps": {"type": "float", "description": "Small nonnegative scalar added (times ridge_mask if provided) to the diagonal for numerical stability (default 0.001). This acts as a ridge (Tikhonov) term that prevents the matrix from being singular or ill-conditioned when used as a penalty/precision matrix in optimization steps related to motion/drift estimation.", "default": 0.001}, "lambd": {"type": "float", "description": "Scaling for the discrete Laplacian operator (default 1.0). lambd sets the strength of the second-difference penalty: interior diagonal entries are initialized as lambd (plus the eps term), and each immediate off-diagonal entry between adjacent indices is -0.5 * lambd, producing the standard tridiagonal Laplacian stencil for a 1D chain of n points.", "default": 1.0}, "ridge_mask": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional 1D numpy array used to modulate the eps term elementwise across the diagonal. When provided, the code computes per-index diagonal values as lambd + eps * ridge_mask[i] and fills the main diagonal with this array. ridge_mask must therefore be broadcastable to length n (typically a 1D array of shape (n,)); if its length does not match n, numpy.fill_diagonal will raise a ValueError. If None (default), the eps term is a scalar added uniformly so the diagonal is lambd + eps.", "default": null}}, "required": ["n", "ridge_mask", "lambd", "wink", "eps"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing drift/motion regularization for a heterogeneous set of probe segments where the depth discretization and edge behavior vary by segment quality. For each segment below, construct a dense 1D Laplacian regularizer using Neumann-like boundary handling (wink=true) and smoothness weight lambd=2.5. Apply an eps rule based on bin-count parity: for segments with an even number of bins use eps=5e-4; for segments with an odd number of bins use eps=0.005. Apply an edge-stabilizing ridge_mask only when the segment has at least 50 bins: if n is at least 60, double the diagonal ridge only at the first and last bin; if n is between 50 and 59 inclusive, double the ridge for the first 5 bins and last 5 bins; otherwise use no ridge_mask (uniform ridge only). Raw segment bin counts: [64, 50, 47]. Return the resulting Laplacian matrices in the same order as the input segments for downstream cross-validation of edge vs interior stability.", "answers": "[{\"name\":\"spikeinterface_sortingcomponents_motion_dredge_laplacian\",\"arguments\":{\"n\":64,\"wink\":true,\"eps\":0.0005,\"lambd\":2.5,\"ridge_mask\":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]}},{\"name\":\"spikeinterface_sortingcomponents_motion_dredge_laplacian\",\"arguments\":{\"n\":50,\"wink\":true,\"eps\":0.0005,\"lambd\":2.5,\"ridge_mask\":[2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,2.0]}},{\"name\":\"spikeinterface_sortingcomponents_motion_dredge_laplacian\",\"arguments\":{\"n\":47,\"wink\":true,\"eps\":0.005,\"lambd\":2.5,\"ridge_mask\":null}}]"}
{"func_name": "spikeinterface_sortingcomponents_motion_dredge_newton_rhs", "func_desc": "spikeinterface.sortingcomponents.motion.dredge.newton_rhs computes the right-hand side vector for a Newton update in the \"dredge\" alignment routine used by SpikeInterface sorting components. This function returns the gradient of the cost function evaluated at P=0 for the batch case, or the gradient augmented with alignment terms for the online (sequential) case where previous/current batch coupling is provided. It is typically used inside a Newton solver that updates a permutation-like matrix P to align clusters or templates across recording batches during spike sorting and motion correction.", "tools": [{"function": {"description": "spikeinterface.sortingcomponents.motion.dredge.newton_rhs computes the right-hand side vector for a Newton update in the \"dredge\" alignment routine used by SpikeInterface sorting components. This function returns the gradient of the cost function evaluated at P=0 for the batch case, or the gradient augmented with alignment terms for the online (sequential) case where previous/current batch coupling is provided. It is typically used inside a Newton solver that updates a permutation-like matrix P to align clusters or templates across recording batches during spike sorting and motion correction.\n", "name": "spikeinterface_sortingcomponents_motion_dredge_newton_rhs", "parameters": {"properties": {"Db": {"type": "array", "items": {"type": "float"}, "description": "A 2-D numeric array representing multiplicative factors for the pairwise term of the cost function for the current batch. In practice within the dredge alignment context, Db contains values that weight the contribution between pairs of units (for example, pairwise costs or affinities). Must be a numpy.ndarray with shape compatible with Ub for elementwise multiplication (commonly a square matrix of size n_units x n_units). This argument is required for both batch and online modes.", "default": ""}, "Ub": {"type": "array", "items": {"type": "float"}, "description": "A 2-D numeric array of the same shape as Db representing the pairwise interaction coefficients (for example, pairwise potentials or statistical weights) used to compute the gradient at P=0. Ub is elementwise-multiplied with Db inside the function; therefore Ub must be provided as a numpy.ndarray and have a shape broadcast-compatible with Db (typically identical shape).", "default": ""}, "Pb_prev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. When None (default), the function runs in batch mode and returns only the gradient at P=0. When provided (online mode), Pb_prev is a numpy.ndarray representing the previous estimate of the permutation or alignment matrix that maps units from the previous batch to the current batch; it is used to compute the alignment term that modifies the gradient. If Pb_prev is not None, the function expects the *_prevcur and _curprev arrays to also be provided and shape-compatible; otherwise a runtime exception will occur.", "default": null}, "Db_prevcur": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. Used only in online mode (Pb_prev is not None). A numpy.ndarray representing the Db-like multiplicative factors coupling the previous batch (rows) to the current batch (columns). It participates in a correction term subtracted from the right-hand side; it must have shapes compatible with Ub_prevcur for elementwise multiplication and with Ub_prevcur.sum operations used in the online formula.", "default": null}, "Ub_prevcur": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. Used only in online mode. A numpy.ndarray representing the pairwise interaction coefficients from previous batch units to current batch units. In the online formula, Ub_prevcur contributes both via its transpose in an alignment matrix multiplication and via elementwise products with Db_prevcur. Ub_prevcur must be provided and shape-compatible with Pb_prev and Db_prevcur when Pb_prev is not None.", "default": null}, "Db_curprev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. Used only in online mode. A numpy.ndarray representing the Db-like multiplicative factors coupling the current batch (rows) to the previous batch (columns). It is used together with Ub_curprev to form a correction term added to the right-hand side. Must be provided and shape-compatible with Ub_curprev when Pb_prev is not None.", "default": null}, "Ub_curprev": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "Optional. Used only in online mode. A numpy.ndarray representing the pairwise interaction coefficients from current batch units to previous batch units. Ub_curprev is used in computing an alignment matrix ((Ub_prevcur.T + Ub_curprev) @ Pb_prev) and in elementwise products with Db_curprev; it must be provided and shape-compatible with Pb_prev, Db_curprev, and Ub_prevcur when Pb_prev is not None.", "default": null}}, "required": ["Db", "Ub", "Pb_prev", "Db_curprev", "Ub_prevcur", "Ub_curprev", "Db_prevcur"], "type": "any"}}, "type": "function"}], "query": "We are running an online (sequential) dredge-alignment Newton-step diagnostic for a 3-unit cohort across two consecutive spike-sorting batches, but the batch-to-batch coupling terms are coming from a heterogeneous acquisition where some interaction matrices are imperfect.\n\nYou have two candidate parameterizations (replicate A and replicate B). For each replicate, build the inputs to `spikeinterface.sortingcomponents.motion.dredge.newton_rhs` under the following QC-driven protocol and then compute the Newton right-hand-side vector evaluated at P=0 in the *online* mode (i.e., include the intra-batch (Db, Ub) term and the cross-batch augmentation via Pb_prev and the two directed interaction pairs).\n\nProtocol rules (apply independently per replicate):\n1) Treat each D-matrix (Db, Db_prevcur, Db_curprev) as a similarity matrix that must be symmetric with ones on the diagonal; if it is not symmetric, replace it with (D + Dᵀ)/2 before passing it to the function.\n2) Treat each U-matrix (Ub, Ub_prevcur, Ub_curprev) as an interaction/flow matrix that should be skew-symmetric; if it is not skew-symmetric, replace it with (U − Uᵀ)/2 before passing it to the function.\n3) Keep Pb_prev as provided.\n\nInputs provided:\nReplicate A inputs:\n- Db = [[1.0, 0.8, 0.5], [0.8, 1.0, 0.6], [0.5, 0.6, 1.0]]\n- Ub = [[0.0, 0.2, -0.1], [0.2, 0.0, 0.15], [-0.1, 0.15, 0.0]]\n- Pb_prev = [[0.9, 0.1, 0.0], [0.1, 0.8, 0.1], [0.0, 0.1, 0.9]]\n- Db_prevcur = [[1.0, 0.7, 0.4], [0.7, 1.0, 0.5], [0.4, 0.5, 1.0]]\n- Ub_prevcur = [[0.05, -0.02, 0.01], [-0.01, 0.04, -0.015], [0.02, -0.01, 0.03]]\n- Db_curprev = [[1.0, 0.6, 0.3], [0.6, 1.0, 0.4], [0.3, 0.4, 1.0]]\n- Ub_curprev = [[-0.03, 0.02, -0.01], [0.01, -0.025, 0.02], [-0.02, 0.015, -0.03]]\n\nReplicate B inputs:\n- Db = [[1.0, 0.8, 0.5], [0.8, 1.0, 0.6], [0.5, 0.6, 1.0]]\n- Ub = [[0.0, 0.3, 0.1], [0.3, 0.0, 0.2], [0.1, 0.2, 0.0]]\n- Pb_prev = [[0.9, 0.1, 0.0], [0.1, 0.8, 0.1], [0.0, 0.1, 0.9]]\n- Db_prevcur = [[1.0, 0.7, 0.4], [0.6, 1.0, 0.5], [0.3, 0.5, 1.0]]\n- Ub_prevcur = [[0.0, 0.25, 0.15], [0.2, 0.0, 0.18], [0.12, 0.16, 0.0]]\n- Db_curprev = [[1.0, 0.6, 0.3], [0.7, 1.0, 0.4], [0.4, 0.5, 1.0]]\n- Ub_curprev = [[0.0, 0.28, 0.12], [0.22, 0.0, 0.14], [0.1, 0.18, 0.0]]", "answers": "[{\"name\":\"spikeinterface_sortingcomponents_motion_dredge_newton_rhs\",\"arguments\":{\"Db\":[[1.0,0.8,0.5],[0.8,1.0,0.6],[0.5,0.6,1.0]],\"Ub\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"Pb_prev\":[[0.9,0.1,0.0],[0.1,0.8,0.1],[0.0,0.1,0.9]],\"Db_prevcur\":[[1.0,0.7,0.4],[0.7,1.0,0.5],[0.4,0.5,1.0]],\"Ub_prevcur\":[[0.0,-0.005,-0.005],[0.005,0.0,-0.0025],[0.005,0.0025,0.0]],\"Db_curprev\":[[1.0,0.6,0.3],[0.6,1.0,0.4],[0.3,0.4,1.0]],\"Ub_curprev\":[[0.0,0.005,0.005],[-0.005,0.0,0.0025],[-0.005,-0.0025,0.0]]}},{\"name\":\"spikeinterface_sortingcomponents_motion_dredge_newton_rhs\",\"arguments\":{\"Db\":[[1.0,0.8,0.5],[0.8,1.0,0.6],[0.5,0.6,1.0]],\"Ub\":[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\"Pb_prev\":[[0.9,0.1,0.0],[0.1,0.8,0.1],[0.0,0.1,0.9]],\"Db_prevcur\":[[1.0,0.65,0.35],[0.65,1.0,0.5],[0.35,0.5,1.0]],\"Ub_prevcur\":[[0.0,0.025,0.015],[-0.025,0.0,0.01],[-0.015,-0.01,0.0]],\"Db_curprev\":[[1.0,0.65,0.35],[0.65,1.0,0.45],[0.35,0.45,1.0]],\"Ub_curprev\":[[0.0,0.03,0.01],[-0.03,0.0,-0.02],[-0.01,0.02,0.0]]}}]"}
{"func_name": "spikeinterface_widgets_unit_waveforms_get_waveforms_scales", "func_desc": "Return x and amplitude scales used to plot waveform templates arranged by channel\n    location for spike-sorting visualization.\n    \n    This function computes a horizontal coordinate matrix (xvectors) that places each\n    sample of template waveforms along an x-axis offset by channel x-positions,\n    a scalar y_scale that maps waveform amplitude units to spatial vertical units,\n    a per-channel vertical offset (y_offset) derived from channel y-positions, and\n    an estimated horizontal inter-channel interval (delta_x). These outputs are\n    intended for use when drawing template waveforms (e.g., template stacks or\n    unit waveform overlays) on a 2D layout determined by electrode/contact\n    locations, a common step in SpikeInterface visualization and waveform analysis\n    workflows.", "tools": [{"function": {"description": "Return x and amplitude scales used to plot waveform templates arranged by channel\nlocation for spike-sorting visualization.\n\nThis function computes a horizontal coordinate matrix (xvectors) that places each\nsample of template waveforms along an x-axis offset by channel x-positions,\na scalar y_scale that maps waveform amplitude units to spatial vertical units,\na per-channel vertical offset (y_offset) derived from channel y-positions, and\nan estimated horizontal inter-channel interval (delta_x). These outputs are\nintended for use when drawing template waveforms (e.g., template stacks or\nunit waveform overlays) on a 2D layout determined by electrode/contact\nlocations, a common step in SpikeInterface visualization and waveform analysis\nworkflows.", "name": "spikeinterface_widgets_unit_waveforms_get_waveforms_scales", "parameters": {"properties": {"templates": {"type": "array", "items": {"type": "float"}, "description": "Array of template waveforms used to determine\namplitude range and number of samples. The code expects templates to\nhave samples along axis 1 (for example shape (n_templates, nsamples,\nn_channels) as used in SpikeInterface waveform extractors). The global\nmaximum and minimum of this array are used to scale amplitudes so that\nplotted waveforms fit the spatial layout. The length of the first axis\n(len(templates)) is used only when x_offset_units is True to scale x\nchannel locations.", "default": ""}, "channel_locations": {"type": "array", "items": {"type": "float"}, "description": "2D array of channel coordinates with\nshape (n_channels, 2). Axis 1 must contain [x, y] positions for each\nchannel in the same spatial units (e.g., micrometers or electrode\ngrid units). These coordinates define the spatial layout: x values are\nused as horizontal offsets and y values as vertical offsets (y_offset)\nfor plotting. Inconsistent shapes or improper ordering will raise\nnumpy broadcasting or indexing errors.", "default": ""}, "nbefore": {"type": "integer", "description": "Number of samples before the event/time-lock point in the\ntemplates. Used to center the x vector so that sample index nbefore maps\nto x = 0. This determines the horizontal placement of the time-lock\npoint within each plotted waveform.", "default": ""}, "x_offset_units": {"type": "boolean", "description": "If False (default), channel x positions from\nchannel_locations are used directly as horizontal offsets (spatial\nunits). If True, channel x positions are multiplied by len(templates)\n(the number of templates along axis 0) before being used as offsets.\nThis provides an alternate unit convention where x offsets are scaled\nby the number of templates; it is retained for backward compatibility\nwith previous plotting conventions in SpikeInterface.", "default": false}, "widen_narrow_scale": {"type": "float", "description": "Multiplicative factor applied to the computed\nhorizontal sample spacing (delta_x). Values > 1 widen the horizontal\nspacing between sample points (useful to emphasize waveform shape),\nvalues < 1 compress it. Default is 1.0. This parameter only affects\nthe computed x vector scale and does not mutate the input arrays.", "default": 1.0}}, "required": ["templates", "channel_locations", "nbefore", "x_offset_units", "widen_narrow_scale"], "type": "any"}}, "type": "function"}], "query": "We’re preparing template-stack overlays for two recording cohorts, but the raw export includes a mix of biologically plausible spike templates and obvious acquisition artifacts. For each cohort, first screen templates by their peak-to-peak amplitude (computed across all samples and channels): keep only templates with 5 <= ptp <= 120 (same units as the templates). For the surviving templates, compute waveform plotting scales using channel geometry with nbefore=2 and x_offset_units=false. Apply a widening rule driven by the probe geometry: estimate the inter-contact spacing along x from the channel_locations; if delta_x <= 20 µm, use widen_narrow_scale=1.3, otherwise use widen_narrow_scale=1.25. Cohort A: tetrode-style 2×2 grid with channel micrometer coordinates ch0 (0,0), ch1 (20,0), ch2 (0,20), ch3 (20,20) and 3 candidate unit templates (3×5×4): U0=[[-20,-15,-10,-5],[-40,-30,-20,-10],[-60,-45,-30,-15],[-40,-30,-20,-10],[-20,-15,-10,-5]]; U1=[[10,8,6,4],[20,16,12,8],[30,24,18,12],[20,16,12,8],[10,8,6,4]]; U2=[[500,500,500,500],[500,500,500,500],[500,500,500,500],[500,500,500,500],[500,500,500,500]]. Cohort B: 4-channel probe with channel micrometer coordinates ch0 (0,0), ch1 (20,0), ch2 (0,25), ch3 (20,25) and 2 candidate unit templates (2×7×4): V0=[[0,-5,2,1],[1,-10,3,0],[2,-20,5,-1],[1,-12,4,0],[0,-6,2,1],[-1,-3,1,2],[0,-1,0,1]]; V1=[[0,-2,1,0],[2,-6,2,-1],[4,-15,4,-2],[3,-9,3,-1],[1,-4,2,0],[0,-2,1,1],[0,-1,0,1]]. Return xvectors, y_scale, y_offset, and delta_x for each cohort after screening so plots are comparable across replicates.", "answers": "[{\"name\":\"spikeinterface_widgets_unit_waveforms_get_waveforms_scales\",\"arguments\":{\"templates\":[[[-20.0,-15.0,-10.0,-5.0],[-40.0,-30.0,-20.0,-10.0],[-60.0,-45.0,-30.0,-15.0],[-40.0,-30.0,-20.0,-10.0],[-20.0,-15.0,-10.0,-5.0]],[[10.0,8.0,6.0,4.0],[20.0,16.0,12.0,8.0],[30.0,24.0,18.0,12.0],[20.0,16.0,12.0,8.0],[10.0,8.0,6.0,4.0]]],\"channel_locations\":[[0.0,0.0],[20.0,0.0],[0.0,20.0],[20.0,20.0]],\"nbefore\":2,\"x_offset_units\":false,\"widen_narrow_scale\":1.3}},{\"name\":\"spikeinterface_widgets_unit_waveforms_get_waveforms_scales\",\"arguments\":{\"templates\":[[[0.0,-5.0,2.0,1.0],[1.0,-10.0,3.0,0.0],[2.0,-20.0,5.0,-1.0],[1.0,-12.0,4.0,0.0],[0.0,-6.0,2.0,1.0],[-1.0,-3.0,1.0,2.0],[0.0,-1.0,0.0,1.0]],[[0.0,-2.0,1.0,0.0],[2.0,-6.0,2.0,-1.0],[4.0,-15.0,4.0,-2.0],[3.0,-9.0,3.0,-1.0],[1.0,-4.0,2.0,0.0],[0.0,-2.0,1.0,1.0],[0.0,-1.0,0.0,1.0]]],\"channel_locations\":[[0.0,0.0],[20.0,0.0],[0.0,25.0],[20.0,25.0]],\"nbefore\":2,\"x_offset_units\":false,\"widen_narrow_scale\":1.3}}]"}
{"func_name": "stk_utilities_utilities_normalize_vector", "func_desc": "stk.utilities.utilities.normalize_vector\n    Normalizes a numeric vector using its Euclidean (L2) norm and returns a new array containing the unit vector. This utility is used throughout stk for geometric calculations in molecular construction and manipulation (for example, normalizing coordinate difference vectors, orientation axes for placing building blocks, and direction vectors used in assembly algorithms).", "tools": [{"function": {"description": "stk.utilities.utilities.normalize_vector\nNormalizes a numeric vector using its Euclidean (L2) norm and returns a new array containing the unit vector. This utility is used throughout stk for geometric calculations in molecular construction and manipulation (for example, normalizing coordinate difference vectors, orientation axes for placing building blocks, and direction vectors used in assembly algorithms).", "name": "stk_utilities_utilities_normalize_vector", "parameters": {"properties": {"vector": {"type": "array", "items": {"type": "float"}, "description": "The input vector to normalize. This must be a one-dimensional NumPy array (dtype and shape are preserved where possible). The function computes the Euclidean norm with numpy.linalg.norm and divides the input by that scalar. The original array is not modified; a new NumPy array is returned.", "default": ""}}, "required": ["vector"], "type": "any"}}, "type": "function"}], "query": "We’re triaging raw orientation axes coming out of an automated conformer/placement stage for a molecular assembly run. Each reported axis is a 3D direction vector that may include artifacts from degeneracies (near-zero magnitude) or logging errors (non-finite components). From the following candidate axes: A=[2.0, -3.0, 6.0], B=[3.0, -4.0, 12.0], C=[0.0, 0.0, 0.0], D=[1.0, 2.0, NaN], apply the orientation-axis preprocessing step by converting only physically meaningful axes into unit orientation axes via L2 normalization. Return the resulting unit vectors for downstream building-block placement.", "answers": "[{\"name\":\"stk_utilities_utilities_normalize_vector\",\"arguments\":{\"vector\":[2.0,-3.0,6.0]}},{\"name\":\"stk_utilities_utilities_normalize_vector\",\"arguments\":{\"vector\":[3.0,-4.0,12.0]}}]"}
{"func_name": "stk_utilities_utilities_quaternion", "func_desc": "stk.utilities.utilities.quaternion: Compute a translation + rotation quaternion from three scalar parameters.\n    \n    Returns a quaternion suitable for encoding a 3D rotation (and used in stk for applying orientations to molecular building blocks during construction and assembly). The implementation follows the Shoemake algorithm for generating uniform random rotations (see K. Shoemake, Uniform random rotations, Graphics Gems III, 1992) by mapping three scalar parameters into a four-component quaternion. The function is used in the stk library to sample or apply rotations when orienting molecules, constructing supramolecular assemblies, and performing random rotational perturbations during automated molecular design workflows.", "tools": [{"function": {"description": "stk.utilities.utilities.quaternion: Compute a translation + rotation quaternion from three scalar parameters.\n\nReturns a quaternion suitable for encoding a 3D rotation (and used in stk for applying orientations to molecular building blocks during construction and assembly). The implementation follows the Shoemake algorithm for generating uniform random rotations (see K. Shoemake, Uniform random rotations, Graphics Gems III, 1992) by mapping three scalar parameters into a four-component quaternion. The function is used in the stk library to sample or apply rotations when orienting molecules, constructing supramolecular assemblies, and performing random rotational perturbations during automated molecular design workflows.", "name": "stk_utilities_utilities_quaternion", "parameters": {"properties": {"u": {"type": "array", "items": {"type": "float"}, "description": "A list of three floating-point parameters [a, b, c] used to construct the quaternion. Each element corresponds to one of the random parameters in Shoemake's method: a controls the distribution between components (used inside square roots), and b and c enter as angular phases through 2 * pi * b and 2 * pi * c. In typical use within stk these three values are sampled uniformly from the interval [0, 1] to produce a uniformly distributed rotation in SO(3). The function requires that u is iterable with exactly three elements; incorrect length will raise a ValueError from the attempt to unpack. Providing non-numeric entries may raise a TypeError or result in propagation of invalid numeric values. Supplying values outside the range [0, 1] is not supported by the Shoemake sampling assumption and will result in mathematically undefined or NaN components when square roots of negative numbers occur.", "default": ""}}, "required": ["u"], "type": "any"}}, "type": "function"}], "query": "In an stk supramolecular assembly robustness study, we’re sampling orientation quaternions for multiple stochastic docking attempts of the same building block, but only keeping perturbations that are meaningfully distinct in the (b,c) plane to avoid near-duplicate rotational proposals. Use the Shoemake SO(3) mapping from three scalars (a,b,c) to a rotation quaternion for each attempt. Our raw parameter proposals (a fixed by upstream seeding) are:\n- Attempt A: a=0.37, b=0.81, c=0.14\n- Attempt B: a=0.37, b=0.84, c=0.15\n- Attempt C: a=0.37, b=0.835, c=0.145\n- Attempt D: a=0.37, b=0.90, c=0.10\n\nApply quaternion generation only for attempts where the Euclidean separation from every other attempt in (b,c) space is at least 0.01, then return the corresponding quaternions for the retained attempts for downstream orientation benchmarking.", "answers": "[{\"name\":\"stk_utilities_utilities_quaternion\",\"arguments\":{\"u\":[0.37,0.81,0.14]}},{\"name\":\"stk_utilities_utilities_quaternion\",\"arguments\":{\"u\":[0.37,0.84,0.15]}},{\"name\":\"stk_utilities_utilities_quaternion\",\"arguments\":{\"u\":[0.37,0.90,0.10]}}]"}
{"func_name": "stk_utilities_utilities_vector_angle", "func_desc": "stk.utilities.utilities.vector_angle\n    Returns the angle between two vectors in radians. This utility function is part of the stk library's geometry helpers used when constructing and manipulating molecular and supramolecular structures (for example, measuring the angle between bond displacement vectors, normals of molecular fragments, or orientation vectors of building blocks during assembly and alignment operations).", "tools": [{"function": {"description": "stk.utilities.utilities.vector_angle\nReturns the angle between two vectors in radians. This utility function is part of the stk library's geometry helpers used when constructing and manipulating molecular and supramolecular structures (for example, measuring the angle between bond displacement vectors, normals of molecular fragments, or orientation vectors of building blocks during assembly and alignment operations).", "name": "stk_utilities_utilities_vector_angle", "parameters": {"properties": {"vector1": {"type": "array", "items": {"type": "float"}, "description": "The first vector. This is interpreted as a numeric 1-D array of Cartesian components (e.g., a displacement or orientation vector in 3D space) used in molecular-geometry calculations inside stk. The function does not modify this array. If this array is exactly equal element-wise to vector2 (checked with numpy.equal), the function returns 0.0 immediately.", "default": ""}, "vector2": {"type": "array", "items": {"type": "float"}, "description": "The second vector. This is interpreted as a numeric 1-D array of Cartesian components analogous to vector1 and used to compute the geometric angle between the two. The function does not modify this array.", "default": ""}}, "required": ["vector1", "vector2"], "type": "any"}}, "type": "function"}], "query": "We’re triaging three replicate pose proposals from a supramolecular docking/alignment run. For each replicate, decide whether an inter-vector angle is a meaningful QA metric based on basic geometric plausibility, and then compute only the angles that pass. Use this rule: treat any vector with a vanishing Euclidean norm as an undefined direction and skip its angle check; additionally, if the two vectors in a replicate are already orthogonal by construction (dot product exactly 0 from the provided components), treat the angle as a trivial control and skip it. Replicate payloads: (i) docking: bond displacement A→B = [1.2, -0.8, 0.5] vs ligand orientation = [-0.4, 0.9, 0.1]; (ii) alignment-1: displacement = [1.0, 2.0, 0.0] vs fragment normal = [0.0, 0.0, 1.0]; (iii) alignment-2: fragment normal = [0.0, 0.0, 1.0] vs target orientation = [1.0, 0.0, 1.0].", "answers": "[{\"name\":\"stk_utilities_utilities_vector_angle\",\"arguments\":{\"vector1\":[1.2,-0.8,0.5],\"vector2\":[-0.4,0.9,0.1]}},{\"name\":\"stk_utilities_utilities_vector_angle\",\"arguments\":{\"vector1\":[0.0,0.0,1.0],\"vector2\":[1.0,0.0,1.0]}}]"}
{"func_name": "tape_models_modeling_utils_gelu", "func_desc": "Implementation of the Gaussian Error Linear Unit (GELU) activation used in TAPE model implementations.\n    \n    This function computes the GELU nonlinearity as used in many deep learning models provided in the TAPE repository (for example Transformer, LSTM, ResNet, UniRep and trRosetta implementations described in the README). GELU is a smooth, non-linear activation that weights each element of the input tensor by the Gaussian cumulative distribution function; it is commonly used in modern transformer-style architectures and language models to provide improved optimization and representational properties compared to hard rectifiers. The implementation follows the exact mathematical form:\n        x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n    which is equivalent to x * Phi(x) where Phi is the standard normal CDF expressed via the error function. See Hendrycks & Gimpel (2016) for the original GELU description: https://arxiv.org/abs/1606.08415\n    \n    Note: an alternative approximation used in some implementations (for example OpenAI GPT) is\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n    which is numerically different and can produce slightly different model outputs; the TAPE codebase uses the erf-based exact form above.", "tools": [{"function": {"description": "Implementation of the Gaussian Error Linear Unit (GELU) activation used in TAPE model implementations.\n\nThis function computes the GELU nonlinearity as used in many deep learning models provided in the TAPE repository (for example Transformer, LSTM, ResNet, UniRep and trRosetta implementations described in the README). GELU is a smooth, non-linear activation that weights each element of the input tensor by the Gaussian cumulative distribution function; it is commonly used in modern transformer-style architectures and language models to provide improved optimization and representational properties compared to hard rectifiers. The implementation follows the exact mathematical form:\n    x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\nwhich is equivalent to x * Phi(x) where Phi is the standard normal CDF expressed via the error function. See Hendrycks & Gimpel (2016) for the original GELU description: https://arxiv.org/abs/1606.08415\n\nNote: an alternative approximation used in some implementations (for example OpenAI GPT) is\n    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\nwhich is numerically different and can produce slightly different model outputs; the TAPE codebase uses the erf-based exact form above.", "name": "tape_models_modeling_utils_gelu", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "Input tensor of activations. This is the tensor on which the GELU nonlinearity is applied elementwise. In the TAPE codebase this will typically be the output of a linear or convolutional layer (float-valued activations) within models used for protein sequence embedding and downstream prediction tasks. The function expects a PyTorch tensor; non-floating dtypes will typically raise an error when calling torch.erf or will be implicitly promoted by PyTorch operations. The function does not modify x in-place.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "We’re running a post-linear activation QA pass on heterogeneous pre-activation tensors from a TAPE-style protein Transformer. The capture stream is noisy: some tensors are real-valued hidden states, while others contain sentinel-filled padding or corrupted NaNs/Inf from upstream mixed-precision overflow.\n\nGiven these candidate tensors:\n1) encoder hidden-state batch (shape 2×3×4):\n[[[0.0, -0.5, 1.2, 3.0], [2.5, -1.0, 0.3, -2.2], [0.7, 0.0, -0.8, 1.5]], [[-1.5, 0.4, 0.0, 2.0], [1.1, -0.2, 0.9, -0.4], [0.0, 0.0, 0.5, -1.3]]]\n2) calibration panel (shape 2×5):\n[[-2.0, -1.0, -0.5, 0.0, 0.5], [1.0, 2.0, 3.0, -3.0, 0.25]]\n3) overflow probe (shape 1×6):\n[[0.1, -0.1, 999.0, -999.0, 1e20, -1e20]]\n4) corruption probe (shape 1×4):\n[[0.0, NaN, 1.0, Inf]]\n\nApply the exact erf-form GELU elementwise only to tensors that are numerically well-formed for scientific inspection: all entries must be finite (no NaN/Inf) and the dynamic range must be plausible for post-linear hidden activations (absolute value not exceeding 20). Return GELU-transformed tensors for every tensor that passes these criteria.", "answers": "[{\"name\":\"tape_models_modeling_utils_gelu\",\"arguments\":{\"x\":[[[0.0,-0.5,1.2,3.0],[2.5,-1.0,0.3,-2.2],[0.7,0.0,-0.8,1.5]],[[-1.5,0.4,0.0,2.0],[1.1,-0.2,0.9,-0.4],[0.0,0.0,0.5,-1.3]]]}},{\"name\":\"tape_models_modeling_utils_gelu\",\"arguments\":{\"x\":[[-2.0,-1.0,-0.5,0.0,0.5],[1.0,2.0,3.0,-3.0,0.25]]}}]"}
{"func_name": "tdc_chem_utils_evaluator_c__ulate_internal_pairwise_similarities", "func_desc": "Calculates pairwise internal chemical similarities for a list of SMILES strings using molecular fingerprints.\n    \n    This function is used in TDC's cheminformatics utilities to quantify chemical similarity within a single set of molecules (for example, to assess dataset redundancy, chemical diversity, or to select nearest-neighbor analogs when preparing splits or analyses). It converts the input SMILES strings to molecule objects via get_mols, computes fingerprints via get_fingerprints, and then computes pairwise Tanimoto similarities using RDKit's DataStructs.BulkTanimotoSimilarity in a memory-efficient incremental manner. The returned matrix is symmetric and the diagonal is explicitly set to zero to avoid counting self-similarity in downstream thresholding or aggregation steps.", "tools": [{"function": {"description": "Calculates pairwise internal chemical similarities for a list of SMILES strings using molecular fingerprints.\n\nThis function is used in TDC's cheminformatics utilities to quantify chemical similarity within a single set of molecules (for example, to assess dataset redundancy, chemical diversity, or to select nearest-neighbor analogs when preparing splits or analyses). It converts the input SMILES strings to molecule objects via get_mols, computes fingerprints via get_fingerprints, and then computes pairwise Tanimoto similarities using RDKit's DataStructs.BulkTanimotoSimilarity in a memory-efficient incremental manner. The returned matrix is symmetric and the diagonal is explicitly set to zero to avoid counting self-similarity in downstream thresholding or aggregation steps.", "name": "tdc_chem_utils_evaluator_c__ulate_internal_pairwise_similarities", "parameters": {"properties": {"smiles_list": {"type": "array", "items": {"type": "float"}, "description": "Ordered list of SMILES strings representing the chemical structures to compare. Each element should be a SMILES string (e.g., \"CCO\"). The order of this list determines the row/column ordering in the returned similarity matrix. The function does not modify this list. Note: conversion from SMILES to molecule objects and fingerprint computation are performed by the helper functions get_mols and get_fingerprints; errors or failures in parsing or fingerprinting (for example, invalid SMILES) will propagate from those helpers.", "default": ""}}, "required": ["smiles_list"], "type": "any"}}, "type": "function"}], "query": "We’re doing a cheminformatics QC pass prior to clustering, but the raw SMILES dump contains salts/mixtures, isotopic labels, and vendor “placeholders”. Build internal pairwise Tanimoto similarity matrices (diagonal forced to zero) for two derived cohorts using these rules:\n\nCohort A (lead-like analogs): start from the following records and keep only entries that represent a single, neutral, organic small molecule (i.e., exactly one SMILES fragment with no '.' separators, no bracketed atoms like '[Na+]' or isotope labels, and containing at least one carbon). Use the retained SMILES to compute the full symmetric internal similarity matrix.\nRaw cohort A records:\n- \"CC(=O)OC1=CC=CC=C1C(=O)O\" (aspirin)\n- \"CC(=O)NC1=CC=C(O)C=C1O\" (acetaminophen)\n- \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\" (ibuprofen)\n- \"C1=CC=C(C=C1)O\" (phenol)\n- \"CC1=CC=CC=C1\" (toluene)\n- \"[Na+].O=C([O-])C\" (sodium acetate)\n- \"C1=CC=CC=C1.[Cl-]\" (benzene + chloride artifact)\n\nCohort B (screening candidates): from the following records, compute an internal similarity matrix only for entries that are chemically “minimal organics” (single-fragment SMILES, no bracketed atoms/isotopes/charges, and heavy-atom count <= 4). Use the retained SMILES to compute the full symmetric internal similarity matrix.\nRaw cohort B records:\n- \"CCO\" (ethanol)\n- \"CCN\" (ethylamine)\n- \"CC(=O)C\" (acetone)\n- \"CC(=O)O\" (acetic acid)\n- \"c1ccccc1\" (benzene)\n- \"[2H]CCO\" (deuterated ethanol)\n- \"O\" (water blank)\n\nReturn the two cohort-specific symmetric similarity matrices so we can inspect nearest-neighbor redundancy before splitting.", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_c__ulate_internal_pairwise_similarities\",\"arguments\":{\"smiles_list\":[\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"CC(=O)NC1=CC=C(O)C=C1O\",\"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\",\"C1=CC=C(C=C1)O\",\"CC1=CC=CC=C1\"]}},{\"name\":\"tdc_chem_utils_evaluator_c__ulate_internal_pairwise_similarities\",\"arguments\":{\"smiles_list\":[\"CCO\",\"CCN\",\"CC(=O)C\",\"CC(=O)O\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_calculate_pc_descriptors", "func_desc": "Calculate physical–chemical (PC) descriptor vectors for a list of molecules represented as SMILES strings.\n    \n    This function is used in TDC's cheminformatics utilities to produce numerical feature vectors that summarize molecular properties (for example, molecular weight, logP, polar surface area) required by downstream tasks such as ADME prediction, model evaluation, and benchmark construction. For each SMILES string in the input list, the function calls the internal helper _calculate_pc_descriptors(smiles, pc_descriptors) to compute the requested descriptor values, collects the non-None results, and returns them as a NumPy array suitable as input features for machine learning models or statistical analyses.", "tools": [{"function": {"description": "Calculate physical–chemical (PC) descriptor vectors for a list of molecules represented as SMILES strings.\n\nThis function is used in TDC's cheminformatics utilities to produce numerical feature vectors that summarize molecular properties (for example, molecular weight, logP, polar surface area) required by downstream tasks such as ADME prediction, model evaluation, and benchmark construction. For each SMILES string in the input list, the function calls the internal helper _calculate_pc_descriptors(smiles, pc_descriptors) to compute the requested descriptor values, collects the non-None results, and returns them as a NumPy array suitable as input features for machine learning models or statistical analyses.", "name": "tdc_chem_utils_evaluator_calculate_pc_descriptors", "parameters": {"properties": {"smiles": {"type": "array", "items": {"type": "float"}, "description": "A Python list of SMILES strings. Each element is a textual representation of a single small-molecule structure using the SMILES notation. The function iterates over this list and attempts to compute the requested descriptors for each SMILES. Practical significance: this is the set of molecules for which PC descriptors are being computed for use as model features in TDC tasks (for example, single-instance prediction tasks such as ADME).", "default": ""}, "pc_descriptors": {"type": "array", "items": {"type": "float"}, "description": "A Python list of descriptor names (strings). Each string identifies a physical–chemical descriptor to compute for every molecule (for example, \"MolWt\", \"LogP\", \"TPSA\"). The helper _calculate_pc_descriptors is expected to recognize these names and return a numeric vector with one value per requested descriptor. Practical significance: this list defines which numerical features will be produced and hence which molecular properties will be available to downstream ML models and evaluations.", "default": ""}}, "required": ["smiles", "pc_descriptors"], "type": "any"}}, "type": "function"}], "query": "We’re curating a small, messy calibration set for an ADME featurization smoke test where the upstream registration system sometimes produces duplicate encodings and non-druglike entries. Start from this raw SMILES panel (order is arbitrary and may include repeats): [\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\", \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\", \"CC(=O)NC1=CC=C(O)C=C1O\", \"CC(=O)OC1=CC=CC=C1C(=O)O\", \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\", \"C1=CC=CC=C1\", \"C(C)(C)(C)(C)\"].\n\nBuild two feature matrices:\n1) A baseline druglike panel: compute the full PC descriptor set [\"MolWt\", \"LogP\", \"TPSA\", \"HBD\", \"HBA\"] only for structures that (i) contain at least one hetero atom (any of N/O/S) and (ii) have an explicit carbonyl group (a \"C(=O)\" motif) to match our assay inclusion criteria.\n2) A lipophilicity/polarity ablation panel: from the same raw panel, compute [\"MolWt\", \"LogP\", \"TPSA\"] only for entries that satisfy (i) and (ii) above and additionally contain at least one aromatic ring (lowercase aromatic atoms present).\n\nKeep the within-cohort ordering identical to the order in which qualifying SMILES first appear in the raw list, and treat alternate but equivalent encodings as independent entries (don’t canonicalize/deduplicate). Return the two NumPy-ready matrices as separate outputs for downstream benchmarking.", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_calculate_pc_descriptors\",\"arguments\":{\"smiles\":[\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"Cn1cnc2c1c(=O)n(C)c(=O)n2C\",\"CC(=O)NC1=CC=C(O)C=C1O\",\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"],\"pc_descriptors\":[\"MolWt\",\"LogP\",\"TPSA\",\"HBD\",\"HBA\"]}},{\"name\":\"tdc_chem_utils_evaluator_calculate_pc_descriptors\",\"arguments\":{\"smiles\":[\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"Cn1cnc2c1c(=O)n(C)c(=O)n2C\",\"CC(=O)NC1=CC=C(O)C=C1O\",\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"],\"pc_descriptors\":[\"MolWt\",\"LogP\",\"TPSA\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_canonicalize", "func_desc": "Convert a SMILES string into a deterministic canonical SMILES representation suitable for use in TDC data pipelines, evaluators, and molecule-generation workflows.", "tools": [{"function": {"description": "Convert a SMILES string into a deterministic canonical SMILES representation suitable for use in TDC data pipelines, evaluators, and molecule-generation workflows.\n", "name": "tdc_chem_utils_evaluator_canonicalize", "parameters": {"properties": {"smiles": {"type": "string", "description": "Input SMILES string representing a small-molecule structure. This parameter is the molecular input provided by users or upstream TDC data functions (for example, dataset loaders, processors, or molecule-generation oracles). The function expects a textual SMILES encoding (type str). The practical significance in the TDC domain is that canonicalizing SMILES produces a consistent, reproducible string form that is useful for deduplication, deterministic dataset splits, feature extraction, model input normalization, and fair evaluation on benchmarks.", "default": ""}}, "required": ["smiles"], "type": "any"}}, "type": "function"}], "query": "We’re preparing a TDC molecule registry where only entries that are already RDKit-parseable SMILES (i.e., they contain at least one explicit atomic symbol and no whitespace) are allowed to enter the canonicalization stage. From the following raw ligand manifest, perform a canonicalization pass only on the records that satisfy those syntactic constraints, preserving replicate records as separate entries for downstream dedup QA: (i) \"CC(=O)Oc1ccccc1C(=O)O\"; (ii) \"C1=CC=CC=C1O\"; (iii) \"CC(=O)Oc1ccccc1C(=O)O\".", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_canonicalize\",\"arguments\":{\"smiles\":\"CC(=O)Oc1ccccc1C(=O)O\"}},{\"name\":\"tdc_chem_utils_evaluator_canonicalize\",\"arguments\":{\"smiles\":\"C1=CC=CC=C1O\"}},{\"name\":\"tdc_chem_utils_evaluator_canonicalize\",\"arguments\":{\"smiles\":\"CC(=O)Oc1ccccc1C(=O)O\"}}]"}
{"func_name": "tdc_chem_utils_evaluator_continuous_kldiv", "func_desc": "tdc.chem_utils.evaluator.continuous_kldiv computes the continuous Kullback–Leibler divergence between two empirical continuous distributions represented as numpy arrays. In the Therapeutics Data Commons (TDC) context, this function is used to quantify how a distribution of sampled molecular property values (for example, properties produced by a generative oracle or model) diverges from a baseline distribution (for example, property values in a reference dataset), enabling evaluation of distributional fidelity in generation and benchmarking workflows.\n    \n    This function fits Gaussian kernel density estimates (KDEs) to the two input arrays, evaluates the two estimated densities on a common grid spanning the combined support of the inputs, and then computes the KL divergence D_KL(P || Q) using scipy.stats.entropy (natural-log base, result in nats). It applies small additive constants to avoid zeros in densities and to stabilize KDE evaluation. Note that the implementation mutates the input numpy arrays in-place by adding a small constant; pass copies if you need to preserve the original arrays.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.continuous_kldiv computes the continuous Kullback–Leibler divergence between two empirical continuous distributions represented as numpy arrays. In the Therapeutics Data Commons (TDC) context, this function is used to quantify how a distribution of sampled molecular property values (for example, properties produced by a generative oracle or model) diverges from a baseline distribution (for example, property values in a reference dataset), enabling evaluation of distributional fidelity in generation and benchmarking workflows.\n\nThis function fits Gaussian kernel density estimates (KDEs) to the two input arrays, evaluates the two estimated densities on a common grid spanning the combined support of the inputs, and then computes the KL divergence D_KL(P || Q) using scipy.stats.entropy (natural-log base, result in nats). It applies small additive constants to avoid zeros in densities and to stabilize KDE evaluation. Note that the implementation mutates the input numpy arrays in-place by adding a small constant; pass copies if you need to preserve the original arrays.", "name": "tdc_chem_utils_evaluator_continuous_kldiv", "parameters": {"properties": {"X_baseline": {"type": "array", "items": {"type": "float"}, "description": "1-D numpy array of baseline continuous samples. In TDC usage this typically contains reference measurements or property values (for example, experimentally observed molecular properties) that define the target distribution P. The function treats these values as empirical samples used to fit a Gaussian KDE for P. The array must be numeric and finite; NaNs or infinities will cause KDE or entropy computations to fail. The function will add 1e-5 to this array in-place to avoid zeros before KDE fitting.", "default": ""}, "X_sampled": {"type": "array", "items": {"type": "float"}, "description": "1-D numpy array of sampled continuous values to compare against the baseline (defines distribution Q). In TDC workflows this commonly comes from a generative model, oracle outputs, or resampled data whose distributional divergence from the baseline is being evaluated. This array must be numeric and finite; the function will add 1e-5 to this array in-place to avoid zeros before KDE fitting.", "default": ""}}, "required": ["X_baseline", "X_sampled"], "type": "any"}}, "type": "function"}], "query": "We’re triaging logP distributional-fidelity results from three independent model/reference runs, but the property extractor occasionally emits non-physical placeholders or saturation artifacts. For each run, build the baseline distribution P and sampled distribution Q by retaining only finite logP values that fall within the physically plausible window [-2.0, 7.0]. Additionally, if a run’s baseline P contains any sub-zero logP values (indicative of a different chemotype regime), compute the divergence on the baseline-centric window bounded by the 10th and 90th percentiles of P (apply the same windowing to Q for that run) before calling the KDE-based continuous KL routine. Otherwise, use the plausibility-filtered arrays as-is. Then compute D_KL(P||Q) (nats) with the continuous KDE method.\n\nRaw runs (baseline vs sampled), in the same order:\n(1) baseline [1.2, 2.5, 2.7, 3.0, 3.1, 3.3, 3.8, 4.0, 4.2, 4.5] vs sampled [0.9, 1.5, 2.0, 2.2, 2.9, 3.4, 3.6, 3.9, 4.1, 4.8]\n(2) baseline [1.2, 1.5, 1.7, 2.0, 2.1, 2.3, 2.5, 2.7, 3.0, 3.2] vs sampled [1.0, 1.3, 1.6, 1.9, 2.0, 2.4, 2.8, 3.1, 3.3, 3.5]\n(3) baseline [1.12, 1.45, 1.33, 0.98, 1.76, 1.21, 1.05, 1.62, 1.39, 1.28] vs sampled [1.90, 2.05, 1.84, 2.22, 1.73, 2.10, 1.95, 2.30, 1.88, 2.15]", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_continuous_kldiv\",\"arguments\":{\"X_baseline\":[1.2,2.5,2.7,3.0,3.1,3.3,3.8,4.0,4.2,4.5],\"X_sampled\":[0.9,1.5,2.0,2.2,2.9,3.4,3.6,3.9,4.1,4.8]}},{\"name\":\"tdc_chem_utils_evaluator_continuous_kldiv\",\"arguments\":{\"X_baseline\":[1.2,1.5,1.7,2.0,2.1,2.3,2.5,2.7,3.0,3.2],\"X_sampled\":[1.0,1.3,1.6,1.9,2.0,2.4,2.8,3.1,3.3,3.5]}},{\"name\":\"tdc_chem_utils_evaluator_continuous_kldiv\",\"arguments\":{\"X_baseline\":[1.12,1.45,1.33,0.98,1.76,1.21,1.05,1.62,1.39,1.28],\"X_sampled\":[1.9,2.05,1.84,2.22,1.73,2.1,1.95,2.3,1.88,2.15]}}]"}
{"func_name": "tdc_chem_utils_evaluator_novelty", "func_desc": "tdc.chem_utils.evaluator.novelty evaluates the novelty of a set of generated SMILES strings relative to a reference training set. In the Therapeutics Data Commons (TDC) context, this function quantifies how many generated small-molecule candidates are new compared to molecules seen during training, which is a common metric when assessing molecule generation oracles and generative models for drug discovery.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.novelty evaluates the novelty of a set of generated SMILES strings relative to a reference training set. In the Therapeutics Data Commons (TDC) context, this function quantifies how many generated small-molecule candidates are new compared to molecules seen during training, which is a common metric when assessing molecule generation oracles and generative models for drug discovery.\n", "name": "tdc_chem_utils_evaluator_novelty", "parameters": {"properties": {"generated_smiles_lst": {"type": "array", "items": {"type": "float"}, "description": "List of SMILES strings produced by a generative model or oracle. This argument is the set under evaluation; the function first passes it through unique_lst_of_smiles to remove duplicate entries (unique_lst_of_smiles is expected to return a list of SMILES strings). The practical role of this parameter is to represent the candidate molecules whose novelty relative to the training data is being measured.", "default": ""}, "training_smiles_lst": {"type": "array", "items": {"type": "float"}, "description": "List of SMILES strings that were used to train the generative model or that represent the known reference corpus. This list is also passed through unique_lst_of_smiles to remove duplicates before comparison. In the TDC workflow, this represents the known chemical space against which generated molecules are compared to assess novelty.", "default": ""}}, "required": ["generated_smiles_lst", "training_smiles_lst"], "type": "any"}}, "type": "function"}], "query": "We’re re-running novelty evaluation under a more realistic curation protocol for two generative campaigns (antibacterial and GPCR) where the raw dumps include counterions/mixtures and stereochemical variants. For each cohort, compute novelty only on the **chemically comparable subset** of generated structures: keep only those generated SMILES that are a **single connected component** (i.e., no '.'), and that **do not encode stereochemistry** (i.e., contain none of '@', '/', '\\\\'). Use each cohort’s provided training list as-is as the reference set. Report the novelty fraction for each cohort after applying this QC gate to the generated list.\n\nCohort A (antibacterial):\nGenerated SMILES = [\"CC(=O)Oc1ccccc1C(=O)O\", \"CCN(CC)CCOC(=O)c1ccccc1\", \"COC1=CC=CC=C1OC\", \"CC(C)OC(=O)N1CCCC1\", \"C1=CC=C(C=C1)C(=O)O\", \"CCN(CC)CCOC(=O)c1ccccc1Cl\", \"CCOC(=O)NCCN\", \"CCCN(CC)CCOC(=O)c1ccccc1\"]\nTraining SMILES = [\"C1=CC=C(C=C1)C(=O)O\", \"CC(=O)Oc1ccccc1C(=O)O\", \"CCN(CC)CCOC(=O)c1ccccc1\", \"CCOC(=O)NCCN\", \"CCN(CC)CCOC(=O)c1ccccc1Cl\"]\n\nCohort B (GPCR):\nGenerated SMILES = [\"CC1=CC(=O)C=CC1O\", \"CN1CCCC1C2=CC=CC=C2\", \"CCN(CC)CCOC(=O)C3=CC=CC=C3Cl\", \"CC(C)OC1=CC=CC=C1C(=O)O\", \"CCOC(=O)NCCC4=CC=CC=C4\"]\nTraining SMILES = [\"CC1=CC(=O)C=CC1O\", \"CCOC(=O)NCCC4=CC=CC=C4\", \"CC(C)OC1=CC=CC=C1C(=O)O\", \"CCCN(CC)CCOC(=O)C5=CC=CC=C5Cl\", \"CN1CCCC1C6=CC=CC=C6\"]", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_novelty\",\"arguments\":{\"generated_smiles_lst\":[\"CC(=O)Oc1ccccc1C(=O)O\",\"CCN(CC)CCOC(=O)c1ccccc1\",\"COC1=CC=CC=C1OC\",\"CC(C)OC(=O)N1CCCC1\",\"C1=CC=C(C=C1)C(=O)O\",\"CCN(CC)CCOC(=O)c1ccccc1Cl\",\"CCOC(=O)NCCN\",\"CCCN(CC)CCOC(=O)c1ccccc1\"],\"training_smiles_lst\":[\"C1=CC=C(C=C1)C(=O)O\",\"CC(=O)Oc1ccccc1C(=O)O\",\"CCN(CC)CCOC(=O)c1ccccc1\",\"CCOC(=O)NCCN\",\"CCN(CC)CCOC(=O)c1ccccc1Cl\"]}},{\"name\":\"tdc_chem_utils_evaluator_novelty\",\"arguments\":{\"generated_smiles_lst\":[\"CC1=CC(=O)C=CC1O\",\"CN1CCCC1C2=CC=CC=C2\",\"CCN(CC)CCOC(=O)C3=CC=CC=C3Cl\",\"CC(C)OC1=CC=CC=C1C(=O)O\",\"CCOC(=O)NCCC4=CC=CC=C4\"],\"training_smiles_lst\":[\"CC1=CC(=O)C=CC1O\",\"CCOC(=O)NCCC4=CC=CC=C4\",\"CC(C)OC1=CC=CC=C1C(=O)O\",\"CCCN(CC)CCOC(=O)C5=CC=CC=C5Cl\",\"CN1CCCC1C6=CC=CC=C6\"]}}]"}
{"func_name": "tdc_chem_utils_evaluator_uniqueness", "func_desc": "tdc.chem_utils.evaluator.uniqueness evaluates the uniqueness of a collection of SMILES strings by canonicalizing and deduplicating them and returning the fraction of distinct molecules present. This function is typically used within the Therapeutics Data Commons (TDC) workflow to quantify molecular diversity of outputs from molecule generation oracles, distribution-learning models, and dataset curation steps; a higher value indicates greater diversity among the provided SMILES.", "tools": [{"function": {"description": "tdc.chem_utils.evaluator.uniqueness evaluates the uniqueness of a collection of SMILES strings by canonicalizing and deduplicating them and returning the fraction of distinct molecules present. This function is typically used within the Therapeutics Data Commons (TDC) workflow to quantify molecular diversity of outputs from molecule generation oracles, distribution-learning models, and dataset curation steps; a higher value indicates greater diversity among the provided SMILES.\n", "name": "tdc_chem_utils_evaluator_uniqueness", "parameters": {"properties": {"list_of_smiles": {"type": "array", "items": {"type": "float"}, "description": "A Python list of SMILES strings (each element is expected to be a SMILES string representing a molecule). The function passes this list to unique_lst_of_smiles to canonicalize representations and remove duplicates; therefore different SMILES that represent the same chemical structure are treated as identical after canonicalization. The caller must provide an iterable list object; non-list inputs or list elements that are not valid SMILES strings or not handled by unique_lst_of_smiles may cause downstream errors. The function does not modify the input list object in place.", "default": ""}}, "required": ["list_of_smiles"], "type": "any"}}, "type": "function"}], "query": "We’re doing a TDC-style post-generation diversity audit, but the raw SMILES dump is messy and includes salts/solvates, obvious parse artifacts, and duplicate encodings. Start from this single mixed pool (in the given order): [\"CCO\",\"OCC\",\"c1ccccc1\",\"C1=CC=CC=C1\",\"CC(=O)O\",\"O=C(O)C\",\"CCN\",\"NCC\",\"C[C@H](N)O\",\"C[C@@H](N)O\",\"[Na+].[O-]C(=O)C\",\"CC(=O)[O-].[Na+].\",\"C1CC\",\"\",\"not_a_smiles\",\"ClCCl\"].\n\nApply a data-sieve before computing uniqueness: keep only entries that look like single-component organic molecules (i.e., no disconnected fragments indicated by '.', no bracketed ions/atoms, and no empty/non-parsable strings). Then compute the uniqueness fraction separately for two cohorts defined by intrinsic properties of the surviving SMILES:\n\nCohort A (\"simple neutrals\"): survivors with no stereochemistry markers ('@') and no explicit charge markers ('+' or '-').\nCohort B (\"stereochem flagged\"): survivors that contain stereochemistry markers ('@').\n\nUse canonicalization/deduplication semantics as usual (different SMILES encodings of the same molecule should collapse if they canonicalize to the same structure). Return the uniqueness for each cohort.", "answers": "[{\"name\":\"tdc_chem_utils_evaluator_uniqueness\",\"arguments\":{\"list_of_smiles\":[\"CCO\",\"OCC\",\"c1ccccc1\",\"C1=CC=CC=C1\",\"CC(=O)O\",\"O=C(O)C\",\"CCN\",\"NCC\",\"ClCCl\"]}},{\"name\":\"tdc_chem_utils_evaluator_uniqueness\",\"arguments\":{\"list_of_smiles\":[\"C[C@H](N)O\",\"C[C@@H](N)O\"]}}]"}
{"func_name": "tdc_chem_utils_featurize__xyz2mol_int_atom", "func_desc": "tdc.chem_utils.featurize._xyz2mol.int_atom maps a chemical element symbol string to a 1-based integer index used by the XYZ-to-molecule featurization utilities in TDC. This function is used in the pipeline that converts atomic coordinates and element labels (from XYZ files or similar sources) into integer-coded atom features required by downstream molecule construction and machine-learning featurizers.", "tools": [{"function": {"description": "tdc.chem_utils.featurize._xyz2mol.int_atom maps a chemical element symbol string to a 1-based integer index used by the XYZ-to-molecule featurization utilities in TDC. This function is used in the pipeline that converts atomic coordinates and element labels (from XYZ files or similar sources) into integer-coded atom features required by downstream molecule construction and machine-learning featurizers.\n", "name": "tdc_chem_utils_featurize__xyz2mol_int_atom", "parameters": {"properties": {"atom": {"type": "string", "description": "The atomic symbol or name for a single atom (for example 'C', 'H', 'O', or full names if present). The function lowercases this input before lookup, so the caller may provide uppercase, lowercase, or mixed-case element symbols; the value must be a Python str. The mapping is performed against the global list __ATOM_LIST__ (expected to contain lowercase element identifiers). There are no defaults; the caller must supply a valid element string.", "default": ""}}, "required": ["atom"], "type": "any"}}, "type": "function"}], "query": "We’re running an XYZ-to-molecule featurization QC pass on a merged cohort of atom labels extracted from multiple XYZ parsers. The raw label list is intentionally messy due to upstream tokenization and includes case/whitespace variants and non-element placeholders: [\" O \", \"cl\", \"Fe\", \"Xx\", \"\", \"CL\", \"o\"]. For consistency with the XYZ2Mol atom-typing step, canonicalize each label by stripping surrounding whitespace and normalizing element-symbol casing (first letter uppercase, second letter lowercase when present). Then, convert to the 1-based integer-coded atom indices using tdc.chem_utils.featurize._xyz2mol.int_atom, but only for labels that represent valid element symbols after canonicalization (i.e., survive this normalization as a plausible element token for featurization). Return the integer mapping calls in the same order as the surviving labels appear in the raw list.", "answers": "[{\"name\":\"tdc_chem_utils_featurize__xyz2mol_int_atom\",\"arguments\":{\"atom\":\"O\"}},{\"name\":\"tdc_chem_utils_featurize__xyz2mol_int_atom\",\"arguments\":{\"atom\":\"Cl\"}},{\"name\":\"tdc_chem_utils_featurize__xyz2mol_int_atom\",\"arguments\":{\"atom\":\"Fe\"}},{\"name\":\"tdc_chem_utils_featurize__xyz2mol_int_atom\",\"arguments\":{\"atom\":\"Cl\"}},{\"name\":\"tdc_chem_utils_featurize__xyz2mol_int_atom\",\"arguments\":{\"atom\":\"O\"}}]"}
{"func_name": "tdc_chem_utils_featurize__xyz2mol_xyz2AC", "func_desc": "Convert a list of atom types and their 3D coordinates into an atom connectivity (AC) matrix and an RDKit molecule object used by TDC featurization pipelines.\n    \n    This function is used in therapeutic-molecule featurization within TDC to transform raw molecular geometry (atom identifiers and Cartesian coordinates) into two representations required by downstream workflows: (1) an atom connectivity matrix that encodes which atoms are bonded (used by graph-based featurizers and split/evaluation procedures) and (2) an RDKit molecule object (rdkit.Chem.rdchem.Mol) that can be used for chemistry-aware operations, canonicalization, and integration with other RDKit-based tools. The function dispatches to one of two connectivity-inference implementations: a Huckel-based method when use_huckel is True, and a van-der-Waals distance heuristic when use_huckel is False (the default). It does not perform file I/O; it constructs in-memory objects for immediate use by dataset processing, model input preparation, or oracle evaluation in TDC.", "tools": [{"function": {"description": "Convert a list of atom types and their 3D coordinates into an atom connectivity (AC) matrix and an RDKit molecule object used by TDC featurization pipelines.\n\nThis function is used in therapeutic-molecule featurization within TDC to transform raw molecular geometry (atom identifiers and Cartesian coordinates) into two representations required by downstream workflows: (1) an atom connectivity matrix that encodes which atoms are bonded (used by graph-based featurizers and split/evaluation procedures) and (2) an RDKit molecule object (rdkit.Chem.rdchem.Mol) that can be used for chemistry-aware operations, canonicalization, and integration with other RDKit-based tools. The function dispatches to one of two connectivity-inference implementations: a Huckel-based method when use_huckel is True, and a van-der-Waals distance heuristic when use_huckel is False (the default). It does not perform file I/O; it constructs in-memory objects for immediate use by dataset processing, model input preparation, or oracle evaluation in TDC.", "name": "tdc_chem_utils_featurize__xyz2mol_xyz2AC", "parameters": {"properties": {"atoms": {"type": "array", "items": {"type": "float"}, "description": "Integer atom types for each atom in the molecule, in the same order as the coordinates provided in xyz. Each list element is an integer representing the atomic number or atom type expected by the downstream connectivity routines. This list defines which element each coordinate corresponds to and is required for correct bond inference and RDKit molecule construction.", "default": ""}, "xyz": {"type": "array", "items": {"type": "float"}, "description": "Cartesian coordinates corresponding to the atoms list. This numpy array contains the 3D positions used to infer inter-atomic distances and thereby propose bonds. The order of rows (or entries) in this array must match the order of atoms in the atoms parameter. The function relies on these coordinates to compute distance-based or Huckel-based connectivity.", "default": ""}, "charge": {"type": "integer", "description": "Formal total molecular charge used when constructing the RDKit molecule and when selecting bond orders in Huckel-based inference. This integer disambiguates valence and electron counts during connectivity and RDKit molecule creation and should reflect the true net charge of the molecule being processed.", "default": ""}, "use_huckel": {"type": "boolean", "description": "Whether to use the Huckel-based connectivity inference method (True) or the default van der Waals distance heuristic (False). Default is False. When True, the function delegates to xyz2AC_huckel which attempts to infer bond orders and connectivity consistent with Huckel-like rules and the provided charge; when False, it delegates to xyz2AC_vdW which uses geometric (distance-based) criteria derived from atomic van der Waals radii to propose bonds. Choose True when bond order inference informed by formal electronic considerations is required; choose False for faster, purely geometric adjacency suitable for many graph-based featurizers.", "default": false}}, "required": ["atoms", "xyz", "charge", "use_huckel"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing the geometry-to-graph step in a TDC therapeutic-molecule featurization pipeline using messy in-memory XYZ-style replicates (no file I/O). Each replicate is a (atoms, xyz) pair in Å. First, perform a lightweight geometry QC: only attempt geometry-to-graph conversion for structures where (i) atom count matches the coordinate rows, (ii) all coordinates are finite numbers, and (iii) no pair of atoms is closer than 0.4 Å (to screen out collapsed/duplicate atoms). For each QC-passing replicate, infer the atom connectivity (AC) matrix and build the RDKit Mol with total charge = 0. Use the default van-der-Waals distance heuristic unless the replicate contains any non-hydrogen atom (atomic number > 1) with |x| or |y| or |z| > 1.0 Å, in which case switch to the Huckel-based method to stabilize bonding inference for more spatially extended geometries.\n\nReplicates:\nA) ammonia-like geometry: atoms [7, 1, 1, 1], xyz [[0.0, 0.0, 0.0], [0.9377, 0.0, -0.3816], [-0.4689, 0.8121, -0.3816], [-0.4689, -0.8121, -0.3816]]\nB) water geometry: atoms [8, 1, 1], xyz [[0.0, 0.0, 0.0], [0.9572, 0.0, 0.0], [-0.2390, 0.9270, 0.0]]\nC) water with a coordinate artifact (contains a non-finite entry): atoms [8, 1, 1], xyz [[0.0, 0.0, 0.0], [0.9572, 0.0, 0.0], [NaN, 0.9270, 0.0]]\nD) collapsed water replicate (two atoms unrealistically close): atoms [8, 1, 1], xyz [[0.0, 0.0, 0.0], [0.05, 0.0, 0.0], [-0.2390, 0.9270, 0.0]]", "answers": "[{\"name\":\"tdc_chem_utils_featurize__xyz2mol_xyz2AC\",\"arguments\":{\"atoms\":[7,1,1,1],\"xyz\":[[0.0,0.0,0.0],[0.9377,0.0,-0.3816],[-0.4689,0.8121,-0.3816],[-0.4689,-0.8121,-0.3816]],\"charge\":0,\"use_huckel\":false}},{\"name\":\"tdc_chem_utils_featurize__xyz2mol_xyz2AC\",\"arguments\":{\"atoms\":[8,1,1],\"xyz\":[[0.0,0.0,0.0],[0.9572,0.0,0.0],[-0.239,0.927,0.0]],\"charge\":0,\"use_huckel\":false}}]"}
{"func_name": "tdc_chem_utils_featurize_molconvert_smiles2graph2D", "func_desc": "convert SMILES string into a two-dimensional molecular graph feature suitable for graph-based molecular\n    featurization in TDC's small-molecule workflows (e.g., single-instance prediction, ADME benchmarks,\n    and molecule generation/oracle evaluations). This function canonicalizes the input SMILES string,\n    parses it into an RDKit molecule, and returns a mapping from atom indices to element symbols and\n    an adjacency matrix that encodes bond types as integer codes. The outputs are intended to be used\n    for constructing graph inputs for graph neural networks, featurizers, or any downstream model that\n    expects atom-wise indices and a bond-typed adjacency matrix.", "tools": [{"function": {"description": "convert SMILES string into a two-dimensional molecular graph feature suitable for graph-based molecular\nfeaturization in TDC's small-molecule workflows (e.g., single-instance prediction, ADME benchmarks,\nand molecule generation/oracle evaluations). This function canonicalizes the input SMILES string,\nparses it into an RDKit molecule, and returns a mapping from atom indices to element symbols and\nan adjacency matrix that encodes bond types as integer codes. The outputs are intended to be used\nfor constructing graph inputs for graph neural networks, featurizers, or any downstream model that\nexpects atom-wise indices and a bond-typed adjacency matrix.", "name": "tdc_chem_utils_featurize_molconvert_smiles2graph2D", "parameters": {"properties": {"smiles": {"type": "string", "description": "A SMILES string representing a small molecule. This is the raw string input used\nby TDC featurizers and data loaders. The function first canonicalizes this SMILES using the\nrepository's canonicalize(smiles) helper (to produce a consistent representation across\ndatasets and runs) and then converts the canonical SMILES to an RDKit molecule via\nsmiles2mol(smiles). The caller must provide a valid, non-empty SMILES; invalid or\nunparsable SMILES will cause the underlying canonicalize/smiles2mol or RDKit routines to\nraise an exception.", "default": ""}}, "required": ["smiles"], "type": "any"}}, "type": "function"}], "query": "We’re validating a TDC small-molecule featurization step against a messy intake from an ADME calibration plate. The plate manifest contains mixed-quality SMILES, duplicates, salts/mixtures, and placeholders: [\"Cn1cnc2c1c(=O)n(C)c(=O)n2C\", \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\", \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\", \"C1(CC\", \"\", \"CC(=O)O.[Na+]\", \"CCO.CCO\", \"N[N+](=O)[O-]\", \"[H]\", \"O=C(O)C(O)(O)O\"]. For downstream GNN inputs, generate 2D molecular graphs only for entries that represent a single, chemically valid organic molecule after canonicalization (i.e., RDKit-parseable and not a multi-fragment mixture/salt). For technical-replicate checking, if an eligible SMILES occurs more than once in the manifest, featurize each occurrence independently (one output per occurrence). Return the atom-index→element-symbol mapping and the bond-typed adjacency matrix for every retained occurrence, in manifest order among retained entries.", "answers": "[{\"name\":\"tdc_chem_utils_featurize_molconvert_smiles2graph2D\",\"arguments\":{\"smiles\":\"Cn1cnc2c1c(=O)n(C)c(=O)n2C\"}},{\"name\":\"tdc_chem_utils_featurize_molconvert_smiles2graph2D\",\"arguments\":{\"smiles\":\"Cn1cnc2c1c(=O)n(C)c(=O)n2C\"}},{\"name\":\"tdc_chem_utils_featurize_molconvert_smiles2graph2D\",\"arguments\":{\"smiles\":\"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"}},{\"name\":\"tdc_chem_utils_featurize_molconvert_smiles2graph2D\",\"arguments\":{\"smiles\":\"N[N+](=O)[O-]\"}},{\"name\":\"tdc_chem_utils_featurize_molconvert_smiles2graph2D\",\"arguments\":{\"smiles\":\"O=C(O)C(O)(O)O\"}}]"}
{"func_name": "tdc_evaluator_centroid", "func_desc": "tdc.evaluator.centroid computes the centroid (mean position) of all points in a vectorset X and returns that centroid value. In the Therapeutics Data Commons (TDC) context, this function is useful for summarizing the central tendency of numeric feature vectors or embeddings (for example, molecular descriptors, learned representations, or coordinate-based features) when evaluating datasets or preprocessing data for model evaluation.", "tools": [{"function": {"description": "tdc.evaluator.centroid computes the centroid (mean position) of all points in a vectorset X and returns that centroid value. In the Therapeutics Data Commons (TDC) context, this function is useful for summarizing the central tendency of numeric feature vectors or embeddings (for example, molecular descriptors, learned representations, or coordinate-based features) when evaluating datasets or preprocessing data for model evaluation.\n", "name": "tdc_evaluator_centroid", "parameters": {"properties": {"X": {"type": "array", "items": {"type": "float"}, "description": "(N,D) matrix where N is the number of points and D is the number of coordinate dimensions. Each row corresponds to one point (for example, a sample embedding or feature vector in a TDC dataset). The function treats X as numeric and computes the mean across rows (axis=0), effectively computing the mean position in each coordinate direction. Supplying a non-numeric array or an array with incompatible shape will result in numpy raising an exception when attempting to compute the mean.", "default": ""}}, "required": ["X"], "type": "any"}}, "type": "function"}], "query": "We’re preparing reference embeddings for two experimental cohorts, but the replicate exports are noisy: some vectors contain obvious instrumentation artifacts (non-finite values) and some replicates are clearly mis-serialized (dimensionality mismatch within the cohort). For each cohort, compute the centroid only over the replicates that (i) have the cohort’s intended dimensionality (Cohort A is 3D, Cohort B is 4D) and (ii) contain only finite real numbers. \n\nCohort A (baseline chemistry set) raw replicates (mixed quality): [[0.12, -1.5, 2.3], [0.08, -1.2, 2.1], [0.15, -1.7, 2.4], [0.10, -1.4, 2.2], [0.11, -1.3, 2.0], [0.09, -1.1, NaN], [0.13, -1.6], [Infinity, -1.4, 2.2]].\n\nCohort B (toxicity subset) raw replicates (mixed quality): [[0.12, 1.34, -0.56, 2.0], [0.08, 1.10, -0.60, 1.8], [0.20, 1.50, -0.40, 2.2], [0.15, 1.40, -0.50, 2.1], [0.10, 1.30, -0.55], [0.11, 1.20, -0.58, NaN]].\n\nReturn the centroid for each cohort as the reference embedding for downstream clustering/evaluation.", "answers": "[{\"name\":\"tdc_evaluator_centroid\",\"arguments\":{\"X\":[[0.12,-1.5,2.3],[0.08,-1.2,2.1],[0.15,-1.7,2.4],[0.1,-1.4,2.2],[0.11,-1.3,2.0]]}},{\"name\":\"tdc_evaluator_centroid\",\"arguments\":{\"X\":[[0.12,1.34,-0.56,2.0],[0.08,1.1,-0.6,1.8],[0.2,1.5,-0.4,2.2],[0.15,1.4,-0.5,2.1]]}}]"}
{"func_name": "tdc_evaluator_kabsch", "func_desc": "tdc.evaluator.kabsch computes the optimal rotation matrix that rigidly aligns two sets of paired points using the Kabsch algorithm. In the Therapeutics Data Commons (TDC) context, this function is used to align paired coordinate sets such as atomic coordinates of molecular conformations or structural fragments so that downstream comparisons (for example RMSD calculation, structural matching, docking post-processing, or evaluation of conformation generators) reflect only rotational differences and not translations. The function implements the covariance-based SVD approach described by the Kabsch algorithm and returns a D x D orthonormal rotation matrix U that, when applied to P, minimizes the root-mean-square deviation between P and Q.", "tools": [{"function": {"description": "tdc.evaluator.kabsch computes the optimal rotation matrix that rigidly aligns two sets of paired points using the Kabsch algorithm. In the Therapeutics Data Commons (TDC) context, this function is used to align paired coordinate sets such as atomic coordinates of molecular conformations or structural fragments so that downstream comparisons (for example RMSD calculation, structural matching, docking post-processing, or evaluation of conformation generators) reflect only rotational differences and not translations. The function implements the covariance-based SVD approach described by the Kabsch algorithm and returns a D x D orthonormal rotation matrix U that, when applied to P, minimizes the root-mean-square deviation between P and Q.\n", "name": "tdc_evaluator_kabsch", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "float"}, "description": "An (N, D) array of N points in D-dimensional space representing the first set of paired vectors (for example, 3D atomic coordinates of a molecule). P is expected to have been translated so its centroid is at the origin prior to calling this function; if P is not centroid-centered, the computed rotation will not correctly represent a pure rotation aligning the original point clouds.", "default": ""}, "Q": {"type": "array", "items": {"type": "float"}, "description": "An (N, D) array of N points in D-dimensional space representing the second set of paired vectors that P is to be aligned to (for example, target atomic coordinates). Q is also expected to be centroid-centered (centroid at the origin). P and Q must have the same shape and point ordering so that P[i] corresponds to Q[i] for all i.", "default": ""}}, "required": ["P", "Q"], "type": "any"}}, "type": "function"}], "query": "We’re running a ligand-fragment structural QA step where the upstream featurizer sometimes swaps chirality by emitting a left-handed transform (improper rotation). We have 3 candidate paired 5-atom coordinate replicates, each already centroid-centered. For each replicate, compute the Kabsch rotation U (3×3) aligning P onto Q. Then apply a chirality gate: only keep alignments whose resulting rotation is a proper rotation (det(U) > 0), since improper rotations indicate a mirrored embedding that must be rejected before RMSD scoring. Replicates:\n- Replicate A (centroid-centered): P=[[-1.2,0.5,0.0],[-0.3,-0.8,0.4],[0.0,0.0,-0.9],[0.7,0.6,0.3],[0.8,-0.3,0.2]]; Q=[[-1.0,0.7,-0.1],[-0.5,-0.7,0.5],[-0.1,-0.1,-0.8],[0.6,0.5,0.4],[1.0,-0.4,0.0]].\n- Replicate B (centroid-centered): P=[[0.12,-0.34,0.05],[1.05,0.22,-0.18],[-0.88,0.75,0.40],[0.30,-1.10,0.62],[-0.59,0.47,-0.89]]; Q=[[0.10,-0.30,0.02],[0.98,0.35,-0.12],[-0.92,0.70,0.45],[0.25,-1.05,0.66],[-0.55,0.30,-0.95]].\n- Replicate C is a suspected mirror: use the same P as Replicate A, but Q is mirrored across the yz-plane: Q_mirror=[[1.0,0.7,-0.1],[0.5,-0.7,0.5],[0.1,-0.1,-0.8],[-0.6,0.5,0.4],[-1.0,-0.4,0.0]].\nReturn the Kabsch U matrices only for replicates that pass the det(U) > 0 gate.", "answers": "[{\"name\":\"tdc_evaluator_kabsch\",\"arguments\":{\"P\":[[-1.2,0.5,0.0],[-0.3,-0.8,0.4],[0.0,0.0,-0.9],[0.7,0.6,0.3],[0.8,-0.3,0.2]],\"Q\":[[-1.0,0.7,-0.1],[-0.5,-0.7,0.5],[-0.1,-0.1,-0.8],[0.6,0.5,0.4],[1.0,-0.4,0.0]]}},{\"name\":\"tdc_evaluator_kabsch\",\"arguments\":{\"P\":[[0.12,-0.34,0.05],[1.05,0.22,-0.18],[-0.88,0.75,0.4],[0.3,-1.1,0.62],[-0.59,0.47,-0.89]],\"Q\":[[0.1,-0.3,0.02],[0.98,0.35,-0.12],[-0.92,0.7,0.45],[0.25,-1.05,0.66],[-0.55,0.3,-0.95]]}},{\"name\":\"tdc_evaluator_kabsch\",\"arguments\":{\"P\":[[-1.2,0.5,0.0],[-0.3,-0.8,0.4],[0.0,0.0,-0.9],[0.7,0.6,0.3],[0.8,-0.3,0.2]],\"Q\":[[1.0,0.7,-0.1],[0.5,-0.7,0.5],[0.1,-0.1,-0.8],[-0.6,0.5,0.4],[-1.0,-0.4,0.0]]}}]"}
{"func_name": "tdc_evaluator_kabsch_rotate", "func_desc": "tdc.evaluator.kabsch_rotate rotates the input point set P to best align with the reference point set Q using the Kabsch algorithm. In the context of TDC (Therapeutics Data Commons), this function is typically used to align molecular point clouds or atomic coordinates (predicted vs. reference conformations) prior to computing alignment-sensitive evaluation metrics such as RMSD; it computes a rigid-body orthogonal rotation (no scaling) that minimizes mean squared deviation between corresponding points.", "tools": [{"function": {"description": "tdc.evaluator.kabsch_rotate rotates the input point set P to best align with the reference point set Q using the Kabsch algorithm. In the context of TDC (Therapeutics Data Commons), this function is typically used to align molecular point clouds or atomic coordinates (predicted vs. reference conformations) prior to computing alignment-sensitive evaluation metrics such as RMSD; it computes a rigid-body orthogonal rotation (no scaling) that minimizes mean squared deviation between corresponding points.\n", "name": "tdc_evaluator_kabsch_rotate", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "float"}, "description": "(N, D) array of N points in D dimensions representing the source coordinates to be rotated. Each row is a point (for example, an atom coordinate in a molecular conformation). This argument is the data that will be transformed to best match Q; it must have the same shape and point correspondence ordering as Q. The function does not modify the caller's original array in place; it returns a new array containing the rotated coordinates.", "default": ""}, "Q": {"type": "array", "items": {"type": "float"}, "description": "(N, D) array of N points in D dimensions representing the target/reference coordinates (for example, a reference molecular conformation). Points in Q are treated as the fixed reference to which P is aligned. Q must have the same shape as P and the same point-to-point correspondence (i.e., row i in P corresponds to row i in Q).", "default": ""}}, "required": ["P", "Q"], "type": "any"}}, "type": "function"}], "query": "We’re validating a ligand pose-tracking benchmark where some replicates are contaminated by inconsistent atom-mapping (symmetry/permutation artifacts). Each replicate contains a predicted coordinate set P and a crystallographic reference Q with purported 1:1 correspondence. Before RMSD scoring, run a Kabsch rigid alignment only on replicates that are internally self-consistent under centroid sanity checks: compute the centroid of P and Q for each replicate, and proceed with rotation only if the Euclidean distance between the two centroids is <= 0.20 Å (treat larger centroid offsets as mapping/assignment failures to be excluded from the alignment stage). Dataset:\n- Replicate 1: P = [[1.2, -0.5, 0.3], [0.8, 0.1, -1.0], [-0.4, 0.9, 0.7], [1.0, 1.5, -0.2]]; Q = [[1.0, -0.6, 0.4], [0.9, 0.0, -1.1], [-0.5, 1.0, 0.8], [0.9, 1.4, -0.3]].\n- Replicate 2: P = [[1.2, -0.5, 0.3], [0.8, 1.1, -0.2], [-0.4, 0.6, 1.5], [1.0, 0.0, -1.0]]; Q = [[1.0, -0.6, 0.4], [0.9, 1.0, -0.1], [-0.5, 0.7, 1.4], [0.9, -0.1, -0.9]].\nReturn the rotated P only for the replicates that pass the centroid-distance criterion, for downstream RMSD computation.", "answers": "[{\"name\":\"tdc_evaluator_kabsch_rotate\",\"arguments\":{\"P\":[[1.2,-0.5,0.3],[0.8,0.1,-1.0],[-0.4,0.9,0.7],[1.0,1.5,-0.2]],\"Q\":[[1.0,-0.6,0.4],[0.9,0.0,-1.1],[-0.5,1.0,0.8],[0.9,1.4,-0.3]]}}]"}
{"func_name": "tdc_evaluator_kabsch_weighted_rmsd", "func_desc": "Compute the weighted root-mean-square deviation (RMSD) between two sets of points P and Q after optimal rigid-body alignment using the weighted Kabsch algorithm. This function is used in structural comparison tasks common in therapeutics and molecular modeling within TDC (for example, comparing predicted ligand or protein atom coordinates to reference structures to evaluate pose prediction or conformational similarity). The function delegates the core computation to kabsch_weighted and returns the scalar weighted RMSD value computed from the aligned coordinates.", "tools": [{"function": {"description": "Compute the weighted root-mean-square deviation (RMSD) between two sets of points P and Q after optimal rigid-body alignment using the weighted Kabsch algorithm. This function is used in structural comparison tasks common in therapeutics and molecular modeling within TDC (for example, comparing predicted ligand or protein atom coordinates to reference structures to evaluate pose prediction or conformational similarity). The function delegates the core computation to kabsch_weighted and returns the scalar weighted RMSD value computed from the aligned coordinates.\n", "name": "tdc_evaluator_kabsch_weighted_rmsd", "parameters": {"properties": {"P": {"type": "array", "items": {"type": "float"}, "description": "An (N, D) array of N points in D dimensions representing the first coordinate set (e.g., predicted atom coordinates). Each row is a point; D is typically 2 or 3 for planar or 3D molecular coordinates. The values should be numeric and are interpreted in the same units as Q (for molecular data commonly angstroms).", "default": ""}, "Q": {"type": "array", "items": {"type": "float"}, "description": "An (N, D) array of N points in D dimensions representing the second coordinate set (e.g., reference atom coordinates). P and Q must have identical shapes: same number of points N and same dimensionality D. The correspondence between rows of P and Q is assumed (point i in P corresponds to point i in Q).", "default": ""}, "W": {"type": "array", "items": {"type": "float"}, "nullable": true, "description": "A length-N 1D array of nonnegative weights for each point correspondence. If provided, weights scale each point's contribution to the optimal alignment and to the RMSD calculation so that more important atoms or coordinates can influence the result more strongly (for example, weighting heavy atoms more than hydrogens). If None (default), all points are treated with equal weight.", "default": null}}, "required": ["P", "Q", "W"], "type": "any"}}, "type": "function"}], "query": "We’re running a ligand pose QC pass where only chemically credible atom correspondences should contribute to the weighted rigid-body superposition. For each replicate, first screen the per-atom correspondence by checking that each predicted–reference pair has a separation <= 0.35 Å; treat any pair beyond this tolerance as an unmatched/ambiguous mapping and exclude it from the RMSD computation by giving it zero weight. For the remaining matched atoms, keep the provided chemical-importance weighting scheme (heavy atoms dominate, hydrogens are down-weighted) and compute the scalar weighted RMSD (Å) after optimal weighted Kabsch alignment.\n\nReplicate A (6 atoms): predicted P=[[0.15,1.02,-0.10],[1.05,0.98,0.05],[1.95,1.10,0.02],[0.10,2.00,0.00],[2.10,2.05,0.10],[1.00,2.10,-0.05]], reference Q=[[0.00,1.00,0.00],[1.00,1.00,0.00],[2.00,1.00,0.00],[0.00,2.00,0.00],[2.00,2.00,0.00],[1.00,2.00,0.00]], base weights W=[2.0,2.0,2.0,2.0,0.5,0.5].\n\nReplicate B (5 atoms): predicted P=[[0.12,1.05,-0.33],[1.48,0.52,0.10],[2.10,-0.40,0.88],[0.95,-1.20,1.15],[-0.30,0.10,0.75]], reference Q=[[0.00,1.10,-0.30],[1.55,0.60,0.05],[2.05,-0.35,0.95],[1.00,-1.10,1.20],[-0.25,0.05,0.80]], base weights W=[12.0,12.0,14.0,16.0,1.0].", "answers": "[{\"name\":\"tdc_evaluator_kabsch_weighted_rmsd\",\"arguments\":{\"P\":[[0.15,1.02,-0.1],[1.05,0.98,0.05],[1.95,1.1,0.02],[0.1,2.0,0.0],[2.1,2.05,0.1],[1.0,2.1,-0.05]],\"Q\":[[0.0,1.0,0.0],[1.0,1.0,0.0],[2.0,1.0,0.0],[0.0,2.0,0.0],[2.0,2.0,0.0],[1.0,2.0,0.0]],\"W\":[2.0,2.0,2.0,2.0,0.5,0.5]}},{\"name\":\"tdc_evaluator_kabsch_weighted_rmsd\",\"arguments\":{\"P\":[[0.12,1.05,-0.33],[1.48,0.52,0.1],[2.1,-0.4,0.88],[0.95,-1.2,1.15],[-0.3,0.1,0.75]],\"Q\":[[0.0,1.1,-0.3],[1.55,0.6,0.05],[2.05,-0.35,0.95],[1.0,-1.1,1.2],[-0.25,0.05,0.8]],\"W\":[12.0,12.0,14.0,16.0,1.0]}}]"}
{"func_name": "tdc_evaluator_rmsd", "func_desc": "tdc.evaluator.rmsd calculates the root-mean-square deviation (RMSD) between two sets of vectors. In the Therapeutics Data Commons (TDC) context, this function is used as a numeric evaluator to quantify average Euclidean deviation between predicted and reference vector representations (for example, predicted molecular coordinates, embeddings, or other D-dimensional descriptors) across N points. The implementation computes RMSD as sqrt(sum((V - W)**2) / N), where N is the number of points (rows) in V. The function converts its inputs to numpy arrays internally and returns a single scalar RMSD value that can be used to compare model predictions to ground truth in benchmarking and evaluation workflows.", "tools": [{"function": {"description": "tdc.evaluator.rmsd calculates the root-mean-square deviation (RMSD) between two sets of vectors. In the Therapeutics Data Commons (TDC) context, this function is used as a numeric evaluator to quantify average Euclidean deviation between predicted and reference vector representations (for example, predicted molecular coordinates, embeddings, or other D-dimensional descriptors) across N points. The implementation computes RMSD as sqrt(sum((V - W)**2) / N), where N is the number of points (rows) in V. The function converts its inputs to numpy arrays internally and returns a single scalar RMSD value that can be used to compare model predictions to ground truth in benchmarking and evaluation workflows.\n", "name": "tdc_evaluator_rmsd", "parameters": {"properties": {"V": {"type": "array", "items": {"type": "float"}, "description": "(N, D) array representing N points with D dimensions each. In TDC usage, V typically holds reference (ground-truth) vectors such as experimentally-determined coordinates or target embeddings. The function will internally call numpy.array(V) to ensure array semantics. N is determined as len(V). If V is empty (len(V) == 0), a division-by-zero will occur.", "default": ""}, "W": {"type": "array", "items": {"type": "float"}, "description": "(N, D) array representing N points with D dimensions each corresponding to the predicted vectors to be compared against V. In TDC usage, W typically holds model predictions. The function will internally call numpy.array(W). V and W are expected to have the same shape (N, D); mismatched shapes may trigger numpy broadcasting or runtime errors and can lead to incorrect results.", "default": ""}}, "required": ["V", "W"], "type": "any"}}, "type": "function"}], "query": "We’re triaging coordinate-prediction replicates from a 3D modeling run where some outputs may be underdetermined (e.g., effectively planar) or have inconsistent dimensionality. For each replicate below, compute RMSD between reference coordinates V and predicted coordinates W only if both tensors have identical shape and the reference geometry is truly 3D (i.e., the reference has non-zero variance along x, y, and z). Replicates:\nA) V = [[0.0, 0.0, 0.0], [1.5, 0.0, 0.0], [1.5, 1.5, 0.0], [0.0, 1.5, 0.0], [0.75, 0.75, 1.2]]; W = [[0.1, -0.1, 0.0], [1.6, 0.0, 0.1], [1.4, 1.6, -0.1], [-0.1, 1.4, 0.0], [0.8, 0.7, 1.3]]\nB) V = [[0.0, 0.0, 0.0], [1.5, 0.0, 0.0], [1.5, 1.0, 0.0], [0.0, 1.0, 0.0]]; W = [[0.1, -0.1, 0.0], [1.6, 0.1, -0.1], [1.4, 1.1, 0.0], [-0.1, 0.9, 0.1]]\nC) V = [[0.0, 0.0, 0.0], [1.2, 0.0, 0.0], [0.0, 1.2, 0.0], [0.0, 0.0, 1.2]]; W = [[0.1, -0.1, 0.0], [1.1, 0.1, -0.1], [-0.1, 1.3, 0.0], [0.0, 0.1, 1.1]]\nReport the scalar RMSD for each replicate that passes QC for downstream benchmarking.", "answers": "[{\"name\":\"tdc_evaluator_rmsd\",\"arguments\":{\"V\":[[0.0,0.0,0.0],[1.5,0.0,0.0],[1.5,1.5,0.0],[0.0,1.5,0.0],[0.75,0.75,1.2]],\"W\":[[0.1,-0.1,0.0],[1.6,0.0,0.1],[1.4,1.6,-0.1],[-0.1,1.4,0.0],[0.8,0.7,1.3]]}},{\"name\":\"tdc_evaluator_rmsd\",\"arguments\":{\"V\":[[0.0,0.0,0.0],[1.2,0.0,0.0],[0.0,1.2,0.0],[0.0,0.0,1.2]],\"W\":[[0.1,-0.1,0.0],[1.1,0.1,-0.1],[-0.1,1.3,0.0],[0.0,0.1,1.1]]}}]"}
{"func_name": "tdc_utils_label_binarize", "func_desc": "Binarization of a label list given a pre-specified numeric threshold for use in TDC data processing and label transformation pipelines.\n    \n    This function converts a list of continuous or ordinal labels into binary labels (0/1) according to a threshold. It is used in TDC's data processing utilities when transforming regression-style or scored outputs into binary classes for downstream single-instance prediction tasks, leaderboard evaluation, or classifier training. The output is a new numpy array of integer 0/1 values and the input list is not modified in place.", "tools": [{"function": {"description": "Binarization of a label list given a pre-specified numeric threshold for use in TDC data processing and label transformation pipelines.\n\nThis function converts a list of continuous or ordinal labels into binary labels (0/1) according to a threshold. It is used in TDC's data processing utilities when transforming regression-style or scored outputs into binary classes for downstream single-instance prediction tasks, leaderboard evaluation, or classifier training. The output is a new numpy array of integer 0/1 values and the input list is not modified in place.", "name": "tdc_utils_label_binarize", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "A list of labels to be binarized. In the TDC context this typically contains numeric assay measurements, predicted scores, or other scalar labels produced by datasets or model outputs. Each element of y is compared to threshold with a numeric comparison; elements must be comparable to a float (for example, ints or floats). The function will produce a 1-D output array with the same length as len(y).", "default": ""}, "threshold": {"type": "float", "description": "The numeric threshold used to decide class membership. For order=\"ascending\", elements strictly greater than threshold become 1 and all others become 0. For order=\"descending\", elements strictly less than threshold become 1 and all others become 0. Note that values exactly equal to threshold are treated as \"not greater\" and \"not less\" and therefore become 0 in both ordering modes.", "default": ""}, "order": {"type": "string", "description": "Determines the direction of the binarization rule and defaults to \"ascending\". If order is \"ascending\", a label is mapped to 1 when label > threshold and to 0 otherwise. If order is \"descending\", a label is mapped to 1 when label < threshold and to 0 otherwise. Any other string value for order is invalid and triggers an AttributeError. This parameter is useful when converting a continuous score that is either positively or negatively correlated with the desired positive class (for example, higher potency scores -> positive versus lower toxicity scores -> positive).", "default": "ascending"}}, "required": ["y", "threshold", "order"], "type": "any"}}, "type": "function"}], "query": "We’re curating two assay-result cohorts for a TDC docking-to-classifier pipeline where raw scoring exports can contain non-finite artifacts. Use an ascending binarization rule where a compound is active (1) only if its score is strictly greater than the threshold, otherwise inactive (0). First, take the docking score cohort [5.3, 7.8, 6.0, 4.9, 8.1, 5.0] and set the activity threshold to the cohort median. Second, take the potency-like cohort [0.12, 0.75, 0.76, 1.03, 0.5, 0.91] and set the activity threshold to the cohort’s 75th percentile (upper quartile). Apply the same binarization rule in both cohorts using their cohort-specific thresholds.", "answers": "[{\"name\":\"tdc_utils_label_binarize\",\"arguments\":{\"y\":[5.3,7.8,6.0,4.9,8.1,5.0],\"threshold\":5.65,\"order\":\"ascending\"}},{\"name\":\"tdc_utils_label_binarize\",\"arguments\":{\"y\":[0.12,0.75,0.76,1.03,0.5,0.91],\"threshold\":0.8725,\"order\":\"ascending\"}}]"}
{"func_name": "tdc_utils_label_convert_to_log", "func_desc": "Convert labels from nanomolar concentration units to negative-log \"p\" scale used in TDC label processing.\n    \n    This helper is part of TDC's data processing utilities for label transformation and is used when models and evaluation metrics expect potency or concentration labels on a logarithmic \"p\" scale (for example, converting IC50/EC50 values reported in nanomolar to pIC50-like values). The function converts the input list to a NumPy array and delegates the unit and log conversion to convert_y_unit by calling convert_y_unit(np.array(y), \"nM\", \"p\"). The operation is pure (no external side effects) and returns a NumPy array of float log-transformed labels suitable for downstream machine learning pipelines in TDC.", "tools": [{"function": {"description": "Convert labels from nanomolar concentration units to negative-log \"p\" scale used in TDC label processing.\n\nThis helper is part of TDC's data processing utilities for label transformation and is used when models and evaluation metrics expect potency or concentration labels on a logarithmic \"p\" scale (for example, converting IC50/EC50 values reported in nanomolar to pIC50-like values). The function converts the input list to a NumPy array and delegates the unit and log conversion to convert_y_unit by calling convert_y_unit(np.array(y), \"nM\", \"p\"). The operation is pure (no external side effects) and returns a NumPy array of float log-transformed labels suitable for downstream machine learning pipelines in TDC.", "name": "tdc_utils_label_convert_to_log", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "A list of labels to convert. Each element is expected to represent a concentration-like measurement in nanomolar (nM) as a numeric value (for example, IC50 values reported in nM). The function accepts the Python list type as given by the signature and will convert it to a NumPy array internally. Non-numeric entries (for example, strings that cannot be cast to float) can raise a ValueError or TypeError during conversion or may propagate through convert_y_unit; empty lists are accepted and will produce an empty NumPy array.", "default": ""}}, "required": ["y"], "type": "any"}}, "type": "function"}], "query": "We’re aggregating IC50 readouts (in nM) from three kinase assay cohorts prior to TDC model training, but the raw exports include censoring artifacts and unit-driven edge cases. Apply a potency-label QC sieve, then convert only the scientifically admissible values to TDC’s negative-log “p” scale (nM → p). Use these rules: (i) keep only strictly positive, finite concentrations; (ii) drop any entries that indicate right-censoring/saturation or instrument overflow (values at or above 10,000 nM); (iii) for ultra-potent measurements below the assay’s quantitation floor of 1.0 nM, cap them to 1.0 nM before conversion. Cohort A raw = [12.5, 85.0, 310.2, 1450.0, 10000.0, -3.0] nM; Cohort B raw = [15.2, 87.0, 350.5, 1200.0, 0.0, 99999.0] nM; Cohort C raw (5-compound panel + reruns) = [0.8, 5.0, 25.0, 120.0, 1500.0, 0.4, 25000.0] nM. Return one converted array per cohort after applying the above QC/censoring logic.", "answers": "[{\"name\":\"tdc_utils_label_convert_to_log\",\"arguments\":{\"y\":[12.5,85.0,310.2,1450.0]}},{\"name\":\"tdc_utils_label_convert_to_log\",\"arguments\":{\"y\":[15.2,87.0,350.5,1200.0]}},{\"name\":\"tdc_utils_label_convert_to_log\",\"arguments\":{\"y\":[1.0,5.0,25.0,120.0,1500.0,1.0]}}]"}
{"func_name": "tdc_utils_label_label_dist", "func_desc": "tdc.utils.label.label_dist plots the distribution of a list of labels (y) using seaborn and matplotlib so users of the Therapeutics Data Commons (TDC) can visually inspect label properties such as skewness, central tendency, spread, and outliers for datasets used in therapeutic machine-learning tasks.", "tools": [{"function": {"description": "tdc.utils.label.label_dist plots the distribution of a list of labels (y) using seaborn and matplotlib so users of the Therapeutics Data Commons (TDC) can visually inspect label properties such as skewness, central tendency, spread, and outliers for datasets used in therapeutic machine-learning tasks.\n", "name": "tdc_utils_label_label_dist", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "A list of label values for a dataset in TDC. In the therapeutics ML domain these labels typically represent target properties or measurements (for example, activity values, ADME endpoints, or other continuous numeric outcomes). The function computes the numeric median and mean of this list (using numpy) and visualizes the distribution; therefore y must contain numeric values appropriate for median/mean computation. Practical significance: inspecting y with this function helps detect class imbalance, heavy tails, outliers, or multimodality that can inform preprocessing, model selection, and evaluation strategies for TDC tasks.", "default": ""}, "name": {"type": "string", "nullable": true, "description": "Dataset name to show in the plot title. When provided, the plot title will read \"Label Distribution of <name> Dataset\"; when omitted (None), the title will be \"Label Distribution\". In TDC workflows this is used to annotate plots so that distribution diagnostics are traceable back to a specific benchmark or dataset.", "default": null}}, "required": ["y", "name"], "type": "any"}}, "type": "function"}], "query": "We’re doing a label-forensics pass before training two TDC regression tasks, but the exported assay tables are messy and include unit mix-ups and sentinel artifacts. For each cohort, generate a label distribution plot titled with the cohort name, but only after applying cohort-specific QC rules:\n\n1) BBB permeability (logP) cohort: raw labels are [1.2, 0.8, 1.5, 2.3, 1.9, 0.4, 0.6, 1.1, 2.0, 2.5, 3.1, 0.2, 0.9, 1.7, 2.2]. Plot only values that fall within a chemically plausible range for logP (inclusive) of 0 to 3.\n\n2) Kinase potency cohort: raw labels are IC50 values but may be a mixture of nM and µM due to inconsistent lab exports: [0.8, 1.1, 1.3, 2.0, 2.5, 3.2, 4.8, 5.0, 6.4, 7.1, 8.9, 10.5, 12.0, 15.2, 18.7, 22.4, 30.0, 45.5, 60.2, 120.0]. Treat any value < 1 as reported in µM and convert it to nM (multiply by 1000); treat all other values as already in nM. Then plot the converted nM distribution. Use titles 'BBB_LogP_Regression' and 'Kinase_IC50_nM' respectively.", "answers": "[{\"name\":\"tdc_utils_label_label_dist\",\"arguments\":{\"y\":[1.2,0.8,1.5,2.3,1.9,0.4,0.6,1.1,2.0,2.5,0.2,0.9,1.7,2.2],\"name\":\"BBB_LogP_Regression\"}},{\"name\":\"tdc_utils_label_label_dist\",\"arguments\":{\"y\":[800.0,1.1,1.3,2.0,2.5,3.2,4.8,5.0,6.4,7.1,8.9,10.5,12.0,15.2,18.7,22.4,30.0,45.5,60.2,120.0],\"name\":\"Kinase_IC50_nM\"}}]"}
{"func_name": "tdc_utils_label_label_transform", "func_desc": "tdc.utils.label.label_transform transforms a list of raw labels used in TDC benchmarks into a numpy array suitable for downstream evaluation or model training by optionally performing binarization or log-scale conversion.\n    \n    This helper function is used in TDC data processing workflows to prepare target values from datasets (for example continuous biochemical measurements such as Kd or IC50 reported in nM) into either binary class labels for classification tasks or into a log-transformed scale commonly used in pharmacology (e.g., p-scale). It is intended to be called by dataset loaders or preprocessing pipelines when a task requires label normalization or thresholding.", "tools": [{"function": {"description": "tdc.utils.label.label_transform transforms a list of raw labels used in TDC benchmarks into a numpy array suitable for downstream evaluation or model training by optionally performing binarization or log-scale conversion.\n\nThis helper function is used in TDC data processing workflows to prepare target values from datasets (for example continuous biochemical measurements such as Kd or IC50 reported in nM) into either binary class labels for classification tasks or into a log-transformed scale commonly used in pharmacology (e.g., p-scale). It is intended to be called by dataset loaders or preprocessing pipelines when a task requires label normalization or thresholding.", "name": "tdc_utils_label_label_transform", "parameters": {"properties": {"y": {"type": "array", "items": {"type": "float"}, "description": "A list of labels in the original dataset order. Elements are expected to be numeric when thresholding or log-conversion is requested. This list may contain continuous values (for example binding affinities in nM) or already-binary values; the function preserves the input order and returns an array of the same length.", "default": ""}, "binary": {"type": "boolean", "description": "If True and the input list y contains more than two unique values, perform binarization using the supplied threshold and order. Binarization converts a continuous label into a binary class label (1 or 0) to support binary classification benchmarks in TDC (for example converting potency measurements into active/inactive labels). If binary is False, no binarization is attempted.", "default": ""}, "threshold": {"type": "float", "description": "Numeric threshold used for binarization when binary is True and y contains more than two unique values. The threshold is compared elementwise against y to assign class 1 or 0 according to the order parameter. The threshold is not applied when the function does not perform binarization.", "default": ""}, "convert_to_log": {"type": "boolean", "description": "If True and the input list y contains more than two unique values and binarization does not occur (binary is False or not applicable), convert continuous values to a log-scale using the internal convert_y_unit function with unit conversion from \"nM\" to \"p\". This option is useful when preparing continuous pharmacological measurements for regression tasks where a negative-log transform (p-scale) is conventional.", "default": ""}, "verbose": {"type": "boolean", "description": "Whether to print intermediate processing statements to standard error. When True, the function prints a message before binarization and before log conversion. Default is True. The messages are written to sys.stderr and flushed immediately.", "default": true}, "order": {"type": "string", "description": "Determines the direction of binarization when binary is True. If \"descending\", a label is assigned 1 when the original value is strictly less than threshold (y < threshold) and 0 otherwise. If \"ascending\", a label is assigned 1 when the original value is strictly greater than threshold (y > threshold) and 0 otherwise. Default is \"descending\". This parameter controls whether smaller values are considered the positive class (\"descending\") or larger values are considered the positive class (\"ascending\").", "default": "descending"}}, "required": ["y", "binary", "threshold", "convert_to_log", "verbose", "order"], "type": "any"}}, "type": "function"}], "query": "We’re curating two IC50 (nM) assay cohorts before benchmarking, but the raw exports include a mix of usable measurements and QC artifacts. Apply a label-prep protocol that branches by cohort and by within-cohort QC rules.\n\nCohort A (biochemical IC50, nM): [5.0, 25.0, 80.0, 150.0, 600.0, 0.0, -3.0, 1000000.0]. Treat values as usable only if they are strictly positive and fall within the instrument’s validated dynamic range of 1–100000 nM. Using only those usable values, generate two targets:\n1) Classification replicate: binarize with a potency cutoff equal to the median of the usable Cohort A values; more potent compounds (smaller IC50 than the cutoff) are labeled active. Enable verbose logging and keep the descending-order convention.\n2) Regression replicate: keep labels continuous but convert usable Cohort A values to a log (p-style) scale; enable verbose logging and keep the descending-order convention.\n\nCohort B (cell-based kinase IC50, nM): [5.2, 15.7, 48.0, 120.5, 300.0, 9999.0, 0.0]. Treat values as usable only if they are strictly positive and <= 500 nM (values above this are outside the assay’s reportable window). Using only those usable values, produce binary active/inactive labels using a potency cutoff equal to the 75th percentile of the usable Cohort B values; more potent compounds (smaller IC50 than the cutoff) are labeled active. Suppress verbose output and keep the descending-order convention.", "answers": "[{\"name\":\"tdc_utils_label_label_transform\",\"arguments\":{\"y\":[5.0,25.0,80.0,150.0,600.0],\"binary\":true,\"threshold\":80.0,\"convert_to_log\":false,\"verbose\":true,\"order\":\"descending\"}},{\"name\":\"tdc_utils_label_label_transform\",\"arguments\":{\"y\":[5.0,25.0,80.0,150.0,600.0],\"binary\":false,\"threshold\":80.0,\"convert_to_log\":true,\"verbose\":true,\"order\":\"descending\"}},{\"name\":\"tdc_utils_label_label_transform\",\"arguments\":{\"y\":[5.2,15.7,48.0,120.5,300.0],\"binary\":true,\"threshold\":120.5,\"convert_to_log\":false,\"verbose\":false,\"order\":\"descending\"}}]"}
{"func_name": "tdc_utils_load_atom_to_one_hot", "func_desc": "tdc.utils.load.atom_to_one_hot converts a chemical atom label into a one-hot encoded numpy vector suitable for use in TDC molecular featurization pipelines and downstream machine learning models. This helper is used when building per-atom feature vectors (for example in graph-based molecular representations) where each allowed atom type is represented by a unique index and the function produces a vector with a single 1 at that index and 0s elsewhere.", "tools": [{"function": {"description": "tdc.utils.load.atom_to_one_hot converts a chemical atom label into a one-hot encoded numpy vector suitable for use in TDC molecular featurization pipelines and downstream machine learning models. This helper is used when building per-atom feature vectors (for example in graph-based molecular representations) where each allowed atom type is represented by a unique index and the function produces a vector with a single 1 at that index and 0s elsewhere.\n", "name": "tdc_utils_load_atom_to_one_hot", "parameters": {"properties": {"atom": {"type": "string", "description": "The atom label to convert to a one-hot vector. In the TDC context this is typically an element symbol or atom type string (e.g., 'C', 'N', 'O') drawn from molecular data. The function uses this exact string to look up the index in allowed_atom_list; it must match one of the entries exactly.", "default": ""}, "allowed_atom_list": {"type": "array", "items": {"type": "float"}, "description": "The ordered list of permitted atom labels that defines the one-hot encoding scheme. Each element in this list is a string corresponding to an atom type used across the dataset or featurization. The length and ordering of this list determine the size and index assignment of the output vector; the i-th entry in this list corresponds to the i-th position in the returned one-hot vector.", "default": ""}}, "required": ["atom", "allowed_atom_list"], "type": "any"}}, "type": "function"}], "query": "We’re validating atom-type one-hot encoding under messy real-world vocabulary drift across multiple featurization configs. You’re given three candidate atom vocabularies (order matters):\n\nV1 = [\"C\",\"N\",\"O\",\"F\",\"Cl\"]\nV2 = [\"C\",\"N\",\"O\",\"F\",\"Cl\",\"Br\",\"I\"]\nV3 = [\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n\nAnd a small set of atom labels observed in parsed mol blocks: [\"O\",\"Cl\",\"Zn\",\"Br\"]. For each vocabulary, one-hot encode only those observed labels that are actually representable under that vocabulary (i.e., appear in the vocabulary exactly as written). Preserve the vocabulary ordering when encoding.", "answers": "[{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"O\",\"allowed_atom_list\":[\"C\",\"N\",\"O\",\"F\",\"Cl\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"Cl\",\"allowed_atom_list\":[\"C\",\"N\",\"O\",\"F\",\"Cl\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"O\",\"allowed_atom_list\":[\"C\",\"N\",\"O\",\"F\",\"Cl\",\"Br\",\"I\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"Cl\",\"allowed_atom_list\":[\"C\",\"N\",\"O\",\"F\",\"Cl\",\"Br\",\"I\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"Br\",\"allowed_atom_list\":[\"C\",\"N\",\"O\",\"F\",\"Cl\",\"Br\",\"I\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"O\",\"allowed_atom_list\":[\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"Cl\",\"allowed_atom_list\":[\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]}},{\"name\":\"tdc_utils_load_atom_to_one_hot\",\"arguments\":{\"atom\":\"Br\",\"allowed_atom_list\":[\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]}}]"}
{"func_name": "tdc_utils_misc_fuzzy_search", "func_desc": "Fuzzy matching between a user-provided dataset name and the canonical dataset name used by the TDC (Therapeutics Data Commons) library. This function is used inside TDC data-loading and utility workflows to resolve minor differences in user input (case differences, an optional \"tdc.\" prefix, or small typographical variations) to the exact dataset identifier that TDC expects when retrieving a dataset, evaluating a benchmark, or recording leaderboard submissions.", "tools": [{"function": {"description": "Fuzzy matching between a user-provided dataset name and the canonical dataset name used by the TDC (Therapeutics Data Commons) library. This function is used inside TDC data-loading and utility workflows to resolve minor differences in user input (case differences, an optional \"tdc.\" prefix, or small typographical variations) to the exact dataset identifier that TDC expects when retrieving a dataset, evaluating a benchmark, or recording leaderboard submissions.\n", "name": "tdc_utils_misc_fuzzy_search", "parameters": {"properties": {"name": {"type": "string", "description": "Input dataset name provided by a user or calling code. In the TDC domain this is typically a short identifier for a benchmark or dataset (for example, names returned by TDC retrieval utilities or passed to data loaders). The function treats this string case-insensitively, strips a leading \"tdc.\" prefix if present, and then attempts to match it against the canonical dataset identifier(s) for that task.", "default": ""}, "dataset_names": {"type": "string", "description": "The exact dataset name(s) used by TDC against which the input name is compared. In the TDC workflow this value represents the canonical dataset identifier(s) registered for a given task or dataset collection. The function tests membership of the normalized input name in this value and, if no exact case-insensitive match is found, computes a closest fuzzy match using the internal get_closet_match helper to propose the canonical name to return.", "default": ""}}, "required": ["name", "dataset_names"], "type": "any"}}, "type": "function"}], "query": "We’re validating two user-submitted TDC dataset identifiers prior to ingestion, but the candidate registries are noisy and include mixed casing, underscore/hyphen variants, and occasional version-tagged aliases. Apply a registry-selection rule based on the user token itself: if the user-provided dataset identifier contains an explicit namespace prefix (case-insensitive) like \"tdc.\", resolve it only against the cohort’s \"core safety-panel\" registry; otherwise resolve it against the cohort’s \"expanded multitask\" registry. In addition, treat registries as raw comma-separated dumps (may include duplicates and inconsistent casing), and return the exact canonical dataset name that best matches the user-provided identifier for downstream TDC loading.\n\nUser tokens observed in this batch:\n- Cohort A token: \"tdc.hERG\" with core safety-panel registry dump: \"herg, hERG_Karim, hergcentral, hERG\"\n- Cohort B token: \"tdc.HERG\" with expanded multitask registry dump: \"hERG, herg_karim, herg_moltox, hiv, tox21\"", "answers": "[{\"name\":\"tdc_utils_misc_fuzzy_search\",\"arguments\":{\"name\":\"tdc.hERG\",\"dataset_names\":\"herg, hERG_Karim, hergcentral, hERG\"}},{\"name\":\"tdc_utils_misc_fuzzy_search\",\"arguments\":{\"name\":\"tdc.HERG\",\"dataset_names\":\"hERG, herg_karim, herg_moltox, hiv, tox21\"}}]"}
{"func_name": "tdc_utils_misc_to_submission_format", "func_desc": "Convert evaluation results into a submission-ready summary for TDC leaderboards.", "tools": [{"function": {"description": "Convert evaluation results into a submission-ready summary for TDC leaderboards.\n", "name": "tdc_utils_misc_to_submission_format", "parameters": {"properties": {"results": {"type": "any", "description": "A dictionary of evaluation metrics collected across multiple runs (typical use: five runs) for a TDC benchmark. The dictionary is expected to map metric identifiers (e.g., 'ROC-AUC', 'RMSE', or dataset-specific metric names produced by TDC Evaluator) to an iterable (e.g., list) of per-run records. Each per-run record must be a single-key mapping (a dict with one key) whose value is the numeric score for that run. The function constructs a pandas.DataFrame from this input and computes aggregated statistics per metric. This argument is the primary input used to convert per-run metric outputs (from model evaluation on TDC datasets and splits) into the compact format required for leaderboard submission.", "default": ""}}, "required": ["results"], "type": "any"}}, "type": "function"}], "query": "We’re consolidating heterogeneous TDC benchmark outputs from two experiments into submission-ready summaries, but the raw logs include audit-only artifacts and task-specific reporting constraints.\n\nCohort A (molecular property study) produced per-seed results for two tasks with run IDs formatted as `run_#`. Toxicity is a ROC-AUC metric and solubility is an RMSE metric. The raw capture includes a few non-numeric or sentinel entries from interrupted runs; only retain per-run values that are finite real numbers and fall within the physically meaningful range for the metric (ROC-AUC in [0,1]; RMSE > 0). Preserve the original metric names and the original run identifiers exactly as written for retained runs.\n- Toxicity (ROC-AUC): run_1=0.812, run_2=0.798, run_3=0.821, run_4=0.805, run_5=0.810\n- Solubility (RMSE): run_1=0.540, run_2=0.558, run_3=0.549, run_4=0.552, run_5=0.545\n\nCohort B (separate 5-run model) reports three metrics with run IDs formatted as `run1..run5`. Due to a pre-registration rule, keep only discrimination metrics for leaderboard submission when their central tendency across seeds meets a minimum stability threshold: include ROC-AUC and PR-AUC only if their mean across the five runs is at least 0.40; always include RMSE as long as all retained values are positive and finite. Preserve metric names and run identifiers exactly as provided.\n- ROC-AUC: run1=0.842, run2=0.855, run3=0.849, run4=0.861, run5=0.853\n- PR-AUC: run1=0.391, run2=0.405, run3=0.398, run4=0.412, run5=0.407\n- RMSE: run1=0.732, run2=0.719, run3=0.725, run4=0.713, run5=0.721\n\nConvert each cohort’s retained per-run metrics into the compact TDC submission/leaderboard summary format.", "answers": "[{\"name\":\"tdc_utils_misc_to_submission_format\",\"arguments\":{\"results\":{\"ROC-AUC\":[{\"run_1\":0.812},{\"run_2\":0.798},{\"run_3\":0.821},{\"run_4\":0.805},{\"run_5\":0.81}],\"RMSE\":[{\"run_1\":0.54},{\"run_2\":0.558},{\"run_3\":0.549},{\"run_4\":0.552},{\"run_5\":0.545}]}}},{\"name\":\"tdc_utils_misc_to_submission_format\",\"arguments\":{\"results\":{\"ROC-AUC\":[{\"run1\":0.842},{\"run2\":0.855},{\"run3\":0.849},{\"run4\":0.861},{\"run5\":0.853}],\"PR-AUC\":[{\"run1\":0.391},{\"run2\":0.405},{\"run3\":0.398},{\"run4\":0.412},{\"run5\":0.407}],\"RMSE\":[{\"run1\":0.732},{\"run2\":0.719},{\"run3\":0.725},{\"run4\":0.713},{\"run5\":0.721}]}}}]"}
{"func_name": "tdc_utils_retrieve_retrieve_dataset_names", "func_desc": "Return all available dataset names for a given TDC learning task.\n    \n    This function looks up the module-level mapping dataset_names and returns the list of dataset identifiers that belong to the specified learning task in the Therapeutics Data Commons (TDC) hierarchy. In the TDC domain, tasks correspond to high-level learning problems (for example, the single-instance prediction task 'ADME') and each task exposes multiple dataset variants (for example, 'HIA_Hou'). The returned dataset names are intended to be used when instantiating dataset loaders (for example, passing a name to a task-specific constructor such as ADME(name='HIA_Hou')) or when enumerating available benchmarks for model development, evaluation, and leaderboard submission.", "tools": [{"function": {"description": "Return all available dataset names for a given TDC learning task.\n\nThis function looks up the module-level mapping dataset_names and returns the list of dataset identifiers that belong to the specified learning task in the Therapeutics Data Commons (TDC) hierarchy. In the TDC domain, tasks correspond to high-level learning problems (for example, the single-instance prediction task 'ADME') and each task exposes multiple dataset variants (for example, 'HIA_Hou'). The returned dataset names are intended to be used when instantiating dataset loaders (for example, passing a name to a task-specific constructor such as ADME(name='HIA_Hou')) or when enumerating available benchmarks for model development, evaluation, and leaderboard submission.", "name": "tdc_utils_retrieve_retrieve_dataset_names", "parameters": {"properties": {"name": {"type": "string", "description": "The canonical name of the TDC learning task to query. This string must match one of the task keys used in the TDC codebase (for example, 'ADME', 'BenchmarkGroup' task names, or other task identifiers exposed by TDC). The function uses this exact value to index the internal dataset_names mapping and does not perform fuzzy matching; providing an incorrect or misspelled task name will result in a lookup failure.", "default": ""}}, "required": ["name"], "type": "any"}}, "type": "function"}], "query": "We’re validating our TDC task taxonomy ingestion before running an ADME QSAR benchmark sweep. The upstream metadata feed is messy and may include placeholders, wrong-case task labels, and whitespace-padded entries. Given this raw task label stream: ['ADME', ' adme ', 'ADME\\n', 'ADME-legacy', 'PK', '', null], perform dataset registry enumeration only for task labels that normalize to the canonical, current TDC learning task identifier for absorption/distribution/metabolism/excretion (i.e., trim surrounding whitespace and standardize case, and require the label to match exactly after normalization). Run the lookup once per qualifying label so we can detect duplicates in the feed by comparing returned registries across calls.", "answers": "[{\"name\":\"tdc_utils_retrieve_retrieve_dataset_names\",\"arguments\":{\"name\":\"ADME\"}},{\"name\":\"tdc_utils_retrieve_retrieve_dataset_names\",\"arguments\":{\"name\":\"ADME\"}},{\"name\":\"tdc_utils_retrieve_retrieve_dataset_names\",\"arguments\":{\"name\":\"ADME\"}}]"}
{"func_name": "torchdrug_utils_torch_sparse_coo_tensor", "func_desc": "Construct a sparse COO tensor without performing index validation. This function is a thin, high-performance wrapper used in the torchdrug library to build sparse tensors (COO format) for graph-structured data such as adjacency matrices or sparse node/edge features. It delegates construction to a low-level backend (torch_ext.sparse_coo_tensor_unsafe) and therefore avoids the index checks performed by torch.sparse_coo_tensor, providing faster construction for workloads in TorchDrug where indices are known to be valid in advance (for example, creating adjacency representations of molecular graphs or minibatch graph unions during training).", "tools": [{"function": {"description": "Construct a sparse COO tensor without performing index validation. This function is a thin, high-performance wrapper used in the torchdrug library to build sparse tensors (COO format) for graph-structured data such as adjacency matrices or sparse node/edge features. It delegates construction to a low-level backend (torch_ext.sparse_coo_tensor_unsafe) and therefore avoids the index checks performed by torch.sparse_coo_tensor, providing faster construction for workloads in TorchDrug where indices are known to be valid in advance (for example, creating adjacency representations of molecular graphs or minibatch graph unions during training).\n", "name": "torchdrug_utils_torch_sparse_coo_tensor", "parameters": {"properties": {"indices": {"type": "array", "items": {"type": "float"}, "description": "2D indices describing the nonzero locations in COO format. Expected shape is (2, n), where the first row contains row indices and the second row contains column indices for n nonzero entries. In TorchDrug this is typically used to represent edge endpoints or coordinate pairs for sparse features; the function uses these indices verbatim without checking bounds or uniqueness, so the caller is responsible for ensuring they are valid for the provided size.", "default": ""}, "values": {"type": "array", "items": {"type": "float"}, "description": "1D tensor of length n containing the values corresponding to each column in indices. Each entry in values is placed at the coordinate given by the corresponding column in indices. In graph and molecular workflows within TorchDrug, values often represent edge weights, adjacency indicators, or sparse feature values. The function does not validate alignment beyond relying on the backend, so mismatched lengths (values length != n) will result in a backend error.", "default": ""}, "size": {"type": "array", "items": {"type": "float"}, "description": "List specifying the desired size (shape) of the resulting sparse tensor. This list defines the overall dimensions of the sparse COO tensor (for example, [num_rows, num_cols] for a 2D sparse matrix). The provided indices are interpreted against this size; if indices contain out-of-range entries relative to size, undefined behavior or errors from the underlying implementation may occur because no bounds checking is performed.", "default": ""}}, "required": ["indices", "values", "size"], "type": "any"}}, "type": "function"}], "query": "In a TorchDrug directed-graph micro-benchmark (indices are pre-validated; use the unsafe COO constructor), I’m simulating three replicate adjacency builds from raw edge logs that may contain sentinel-coded missing weights. Each replicate is a 4-node graph (size [4, 4]) with the following directed edge lists (source→target) and per-edge weights:\n\nReplicate R1 edges: 0→1 (w=1.0), 1→2 (w=1.0), 2→0 (w=1.0), 3→1 (w=1.0).\nReplicate R2 edges: 0→1 (w=1.0), 1→2 (w=1.0), 2→0 (w=1.0), 3→3 (w=-1.0 sentinel).\nReplicate R3 edges: 0→1 (w=1.0), 1→2 (w=0.5), 2→3 (w=2.0), 3→3 (w=1.0).\n\nConstruct sparse COO adjacency tensors for each replicate, but treat sentinel-coded missing weights as unit weight (w=1.0) at construction time. Output each adjacency in COO form with explicit indices, values, and size [4, 4].", "answers": "[{\"name\":\"torchdrug_utils_torch_sparse_coo_tensor\",\"arguments\":{\"indices\":[[0,1,2,3],[1,2,0,1]],\"values\":[1.0,1.0,1.0,1.0],\"size\":[4,4]}},{\"name\":\"torchdrug_utils_torch_sparse_coo_tensor\",\"arguments\":{\"indices\":[[0,1,2,3],[1,2,0,3]],\"values\":[1.0,1.0,1.0,1.0],\"size\":[4,4]}},{\"name\":\"torchdrug_utils_torch_sparse_coo_tensor\",\"arguments\":{\"indices\":[[0,1,2,3],[1,2,3,3]],\"values\":[1.0,0.5,2.0,1.0],\"size\":[4,4]}}]"}
{"func_name": "torchio_visualization_get_num_bins", "func_desc": "Get the optimal number of bins for a histogram using the Freedman–Diaconis rule.\n    \n    This function implements the Freedman–Diaconis heuristic to choose a bin width that balances histogram resolution and variability of the data. In the context of TorchIO's visualization tools for 3D medical images (for example, plotting intensity histograms of MRI scans to inspect intensity distributions and augmentation effects), this routine computes an integer number of bins intended to minimize the integral of the squared difference between the histogram (relative frequency density) and the underlying theoretical probability density (see the Freedman–Diaconis rule). Concretely, the function computes the 25th and 75th percentiles q25 and q75 of the input values (percentiles are computed over all elements of x, i.e., the input is effectively flattened), forms the interquartile range IQR = q75 - q25, computes the bin width as 2 * IQR * n**(-1/3) where n is the number of samples len(x), and returns round((x.max() - x.min()) / bin_width) as the number of bins.", "tools": [{"function": {"description": "Get the optimal number of bins for a histogram using the Freedman–Diaconis rule.\n\nThis function implements the Freedman–Diaconis heuristic to choose a bin width that balances histogram resolution and variability of the data. In the context of TorchIO's visualization tools for 3D medical images (for example, plotting intensity histograms of MRI scans to inspect intensity distributions and augmentation effects), this routine computes an integer number of bins intended to minimize the integral of the squared difference between the histogram (relative frequency density) and the underlying theoretical probability density (see the Freedman–Diaconis rule). Concretely, the function computes the 25th and 75th percentiles q25 and q75 of the input values (percentiles are computed over all elements of x, i.e., the input is effectively flattened), forms the interquartile range IQR = q75 - q25, computes the bin width as 2 * IQR * n**(-1/3) where n is the number of samples len(x), and returns round((x.max() - x.min()) / bin_width) as the number of bins.", "name": "torchio_visualization_get_num_bins", "parameters": {"properties": {"x": {"type": "array", "items": {"type": "float"}, "description": "A NumPy array of numeric values (typically image intensity samples from TorchIO image objects) used to compute the histogram. The array is treated as a flat collection of values when computing percentiles and min/max. The array must be non-empty and should contain finite numeric values; NaNs or infinities in x will affect percentile and min/max computations and may lead to invalid results or exceptions.", "default": ""}}, "required": ["x"], "type": "any"}}, "type": "function"}], "query": "We’re harmonizing histogram QC settings for TorchIO-style MRI intensity visualizations across three acquisition-derived cohorts, but the exports are messy. Each cohort comes as a list of candidate voxel intensities that may include non-finite sentinels. For each cohort, first retain only finite numeric measurements (i.e., discard NaN and ±Inf), then compute the histogram bin count using the Freedman–Diaconis rule exactly (flatten, compute q25/q75, IQR, bin_width = 2*IQR*n^(−1/3), then round((max−min)/bin_width)). Apply this independently to: (A) brain-masked raw voxels: [12, 15, 14, 13, 18, 21, 19, 20, 17, 16, 22, 24, 23, 25, 27, 26, 28, 30, 29, 31, NaN]; (B) small T1 patch: [0.12, 0.15, 0.18, 0.20, 0.22, 0.25, 0.28, 0.30, 0.33, 0.35, 0.40, 0.55, Inf]; (C) normalization-check patch (can legitimately include negatives): [-0.12, -0.08, -0.05, -0.02, 0.00, 0.03, 0.05, 0.07, 0.10, 0.14, 0.18, 0.22, 0.28, 0.35, 0.42, 0.50, 0.61, 0.75, 0.92, 1.10, -Inf].", "answers": "[{\"name\":\"torchio_visualization_get_num_bins\",\"arguments\":{\"x\":[12,15,14,13,18,21,19,20,17,16,22,24,23,25,27,26,28,30,29,31]}},{\"name\":\"torchio_visualization_get_num_bins\",\"arguments\":{\"x\":[0.12,0.15,0.18,0.2,0.22,0.25,0.28,0.3,0.33,0.35,0.4,0.55]}},{\"name\":\"torchio_visualization_get_num_bins\",\"arguments\":{\"x\":[-0.12,-0.08,-0.05,-0.02,0.0,0.03,0.05,0.07,0.1,0.14,0.18,0.22,0.28,0.35,0.42,0.5,0.61,0.75,0.92,1.1]}}]"}
{"func_name": "useful_rdkit_utils_useful_rdkit_utils_smi2morgan_fp", "func_desc": "Convert a SMILES string to an RDKit Morgan (circular) fingerprint bit vector.\n    \n    This function is used in cheminformatics workflows (similarity searching, clustering,\n    and machine learning) to convert a molecule represented as a SMILES string into a fixed-length\n    binary fingerprint that encodes presence/absence of circular substructures. Internally the function\n    builds an RDKit Mol from the provided SMILES using Chem.MolFromSmiles and then computes a\n    Morgan fingerprint as an RDKit bit vector via AllChem.GetMorganFingerprintAsBitVect.", "tools": [{"function": {"description": "Convert a SMILES string to an RDKit Morgan (circular) fingerprint bit vector.\n\nThis function is used in cheminformatics workflows (similarity searching, clustering,\nand machine learning) to convert a molecule represented as a SMILES string into a fixed-length\nbinary fingerprint that encodes presence/absence of circular substructures. Internally the function\nbuilds an RDKit Mol from the provided SMILES using Chem.MolFromSmiles and then computes a\nMorgan fingerprint as an RDKit bit vector via AllChem.GetMorganFingerprintAsBitVect.", "name": "useful_rdkit_utils_useful_rdkit_utils_smi2morgan_fp", "parameters": {"properties": {"smi": {"type": "string", "description": "A SMILES string representing the molecule to be fingerprinted.\nRole and practical significance: input chemical structure encoded as a text string.\nThe function passes this string to RDKit's Chem.MolFromSmiles to construct an RDKit Mol.\nIf the SMILES is invalid or cannot be parsed by RDKit, Chem.MolFromSmiles returns None\nand this function will return None (see failure modes below). Provide canonical or\nnon-canonical SMILES as available; no additional preprocessing is performed by this function.", "default": ""}, "radius": {"type": "integer", "description": "Radius of the Morgan fingerprint (default: 2).\nRole and practical significance: controls the size of the circular atom neighborhoods\nused to generate hashed substructure identifiers. A larger radius encodes larger local\nenvironments (more chemical context) at the cost of potentially more collisions and\ngreater sensitivity to small changes in structure. This value is passed directly to\nRDKit's GetMorganFingerprintAsBitVect as the radius parameter.", "default": 2}, "nBits": {"type": "integer", "description": "Number of bits in the returned fingerprint bit vector (default: 2048).\nRole and practical significance: sets the fixed dimensionality of the binary fingerprint.\nThe fingerprint is a binary (bit) vector of length nBits produced by hashing the circular\nsubstructure identifiers into this fixed-size space. Choose nBits to balance memory/compute\ncosts and collision rate for downstream tasks.", "default": 2048}}, "required": ["smi", "radius", "nBits"], "type": "any"}}, "type": "function"}], "query": "We’re stress-testing a ligand-similarity stage with a deliberately messy in-memory registry of candidate structures (some are salts/mixtures or malformed strings). Use RDKit Morgan **bit vectors** for only those entries that represent a single, parseable small molecule given as exactly one SMILES token (no dot-disconnected fragments) and containing only organic atoms (H, B, C, N, O, F, P, S, Cl, Br, I). For the remaining valid molecules, adapt fingerprint resolution to molecular size: use **radius = 2** when the SMILES string length is < 30 characters; otherwise use **radius = 3**. Keep **nBits = 1024** for all computed fingerprints. Raw registry:\n- warfarin: \"CC(=O)CC(c1ccccc1O)c2ccc(O)cc2C(=O)O\"\n- caffeine: \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\"\n- sodium acetate (as mixture): \"CC(=O)[O-].[Na+]\"\n- malformed record: \"C1(CC\"\nReturn the batch of computed fingerprints for the entries that pass QC under these rules (with their per-entry radius values).", "answers": "[{\"name\":\"useful_rdkit_utils_useful_rdkit_utils_smi2morgan_fp\",\"arguments\":{\"smi\":\"CC(=O)CC(c1ccccc1O)c2ccc(O)cc2C(=O)O\",\"radius\":3,\"nBits\":1024}},{\"name\":\"useful_rdkit_utils_useful_rdkit_utils_smi2morgan_fp\",\"arguments\":{\"smi\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"radius\":2,\"nBits\":1024}}]"}
{"func_name": "useful_rdkit_utils_useful_rdkit_utils_smi2numpy_fp", "func_desc": "Convert a SMILES string to an RDKit Morgan fingerprint and (internally) populate a numpy array of fingerprint bits.", "tools": [{"function": {"description": "Convert a SMILES string to an RDKit Morgan fingerprint and (internally) populate a numpy array of fingerprint bits.\n", "name": "useful_rdkit_utils_useful_rdkit_utils_smi2numpy_fp", "parameters": {"properties": {"smi": {"type": "string", "description": "SMILES string for a single molecule. In the cheminformatics domain (see README), SMILES is a compact text representation of a molecule; this function parses that string with RDKit's Chem.MolFromSmiles to produce an RDKit Mol used to compute a fingerprint. If the SMILES cannot be parsed, the function will not compute a fingerprint and will return None.", "default": ""}, "radius": {"type": "integer", "description": "Fingerprint radius for the Morgan fingerprint (default 2). In practice this controls how many bond hops around each atom are considered when constructing circular substructure keys; larger radii capture larger local environments and affect similarity and descriptor behavior used in tasks such as similarity searching, clustering, or machine learning feature generation.", "default": 2}, "nBits": {"type": "integer", "description": "Number of fingerprint bits (default 2048). This is the length of the bit vector produced by the Morgan fingerprint procedure; it controls the dimensionality of the fingerprint representation used in downstream analyses (e.g., computing Tanimoto similarity or using as features for models).", "default": 2048}}, "required": ["smi", "radius", "nBits"], "type": "any"}}, "type": "function"}], "query": "We’re validating a mini-batch of SMILES from a medicinal-chemistry plate where salt/solvate annotations and replicate labels are mixed into the raw strings. Use RDKit Morgan fingerprints but adapt the featurization protocol as follows: (a) only featurize entries that appear to be a single, neutral organic molecule (i.e., a single connected component with no explicit ionic charge tokens in the SMILES); (b) for molecules with at least one ring closure digit in the SMILES, use radius = 3; otherwise use radius = 2; (c) for higher-complexity strings (length > 30 characters), use a 2048-bit fingerprint; otherwise use 1024 bits. Raw batch (keep their record IDs as-is, but the featurizer should receive only the molecule SMILES portion):\n- plateA_rep1 caffeine: \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\"\n- plateA_rep2 caffeine: \"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\"\n- plateB aspirin (free acid): \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n- plateB aspirin sodium salt: \"[Na+].CC(=O)OC1=CC=CC=C1C(=O)[O-]\"", "answers": "[{\"name\":\"useful_rdkit_utils_useful_rdkit_utils_smi2numpy_fp\",\"arguments\":{\"smi\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"radius\":3,\"nBits\":2048}},{\"name\":\"useful_rdkit_utils_useful_rdkit_utils_smi2numpy_fp\",\"arguments\":{\"smi\":\"Cn1cnc2n(C)c(=O)n(C)c(=O)c12\",\"radius\":3,\"nBits\":2048}},{\"name\":\"useful_rdkit_utils_useful_rdkit_utils_smi2numpy_fp\",\"arguments\":{\"smi\":\"CC(=O)OC1=CC=CC=C1C(=O)O\",\"radius\":3,\"nBits\":1024}}]"}
